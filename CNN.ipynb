{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e14fb057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import tqdm \n",
    "import glob, os, pickle\n",
    "import time\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import copy\n",
    "import tifffile\n",
    "import torch.nn as nn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import fire, math\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets as Datasets\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.nn as nn\n",
    "#from pathpretrain.models import generate_model#, ModelTrainer\n",
    "#from pathpretrain.ModelTrainer import calc_best_confusion\n",
    "from PIL import Image\n",
    "import kornia.augmentation as K, kornia.geometry.transform as G\n",
    "from pathpretrain.datasets import NPYDataset, PickleDataset, NPYRotatingStack\n",
    "from pathpretrain.schedulers import Scheduler\n",
    "#from pathpretrain.models import fit\n",
    "from kornia.losses import DiceLoss\n",
    "# import pysnooper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ec7d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathpretrain.models import MLP\n",
    "class AuxNet(nn.Module):\n",
    "    def __init__(self,net,n_aux_features):\n",
    "        super().__init__()\n",
    "        self.net=net\n",
    "        self.features=self.net.features\n",
    "        self.output=self.net.output\n",
    "        self.n_features=self.net.output.in_features\n",
    "        self.n_aux_features=n_aux_features\n",
    "        self.transform_nn=nn.Sequential(nn.Linear(self.n_aux_features,self.n_features),nn.LeakyReLU())\n",
    "        self.gate_nn=MLP(self.n_features,[32],dropout_p=0.2,binary=False)#nn.Linear(self.n_features,1)\n",
    "        \n",
    "        for param in self.features[:4].parameters():\n",
    "            param.requires_grad=False\n",
    "\n",
    "    def forward(self,x,z=None):\n",
    "        x=self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        if z is not None:\n",
    "            z=self.transform_nn(z)\n",
    "            #print(x.shape,z.shape,self.gate_nn(x).shape,self.gate_nn(z).shape)\n",
    "            gate_h=F.softmax(torch.cat([self.gate_nn(xz) for xz in [x,z]],1),1)\n",
    "            x = gate_h[:,0].unsqueeze(1) * x + gate_h[:,1].unsqueeze(1) * z\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dda5fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(model_name,\n",
    "                  use_pretrained,\n",
    "                  pretrained_model_file_path,\n",
    "                  use_cuda=False,\n",
    "                  use_data_parallel=True,\n",
    "                  net_extra_kwargs=None,\n",
    "                  load_ignore_extra=False,\n",
    "                  num_classes=None,\n",
    "                  in_channels=3,\n",
    "                  remap_to_cpu=True,\n",
    "                  remove_module=False,\n",
    "                  semantic_segmentation=False,\n",
    "                  n_aux_features=None):\n",
    "    from pytorchcv.model_provider import get_model\n",
    "    import segmentation_models_pytorch as smp\n",
    "\n",
    "    kwargs = {\"pretrained\": use_pretrained}\n",
    "    if num_classes is not None:\n",
    "        kwargs[\"num_classes\"] = num_classes\n",
    "    if in_channels is not None:\n",
    "        kwargs[\"in_channels\"] = in_channels\n",
    "    if net_extra_kwargs is not None:\n",
    "        kwargs.update(net_extra_kwargs)\n",
    "\n",
    "    if not semantic_segmentation:\n",
    "        if kwargs['pretrained']:\n",
    "            kwargs['pretrained']=False\n",
    "            net = get_model(model_name, **kwargs)\n",
    "            net_shape_dict = {k:v.shape for k,v in net.state_dict().items()}\n",
    "            kwargs['num_classes']=1000\n",
    "            kwargs['pretrained']=True\n",
    "            net_pretrained=get_model(model_name, **kwargs).state_dict()\n",
    "            net.load_state_dict({k:v for k,v in net_pretrained.items() if v.shape==net_shape_dict[k]},strict=False)\n",
    "        else:\n",
    "            net = get_model(model_name, **(kwargs))\n",
    "\n",
    "        if n_aux_features is not None:\n",
    "            net=AuxNet(net,n_aux_features)\n",
    "\n",
    "    else:\n",
    "        net = smp.Unet(model_name, classes=num_classes, in_channels=in_channels)\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "537f92c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(architecture, num_classes, semantic_segmentation, pretrained=False, n_aux_features=None):\n",
    "    #    from pytorchcv.pytorch.utils import prepare_model\n",
    "    if os.path.exists(architecture):\n",
    "        model = torch.load(architecture,map_location='cpu')\n",
    "    else:\n",
    "        model = prepare_model(architecture,\n",
    "                          use_pretrained=pretrained,\n",
    "                          pretrained_model_file_path='',\n",
    "                          use_cuda=False,\n",
    "                          num_classes=num_classes,\n",
    "                          semantic_segmentation=semantic_segmentation,\n",
    "                          n_aux_features=n_aux_features)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bcd582",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainerr:\n",
    "    \n",
    "    def __init__(self, model, n_epoch=300, validation_dataloader=None, optimizer_opts=dict(name='adam', lr=1e-3, weight_decay=1e-4), scheduler_opts=dict(scheduler='warm_restarts', lr_scheduler_decay=0.5, T_max=10, eta_min=5e-8, T_mult=2), loss_fn='ce', reduction='mean', num_train_batches=None, opt_level='O1', checkpoints_dir='checkpoints',tensor_dataset=False,transforms=None,semantic_segmentation=False,save_metric='loss',save_after_n_batch=0):\n",
    "        self.model = model\n",
    "        # self.amp_handle = amp.init(enabled=True)\n",
    "        optimizers = {'adam': torch.optim.Adam, 'sgd': torch.optim.SGD}\n",
    "        loss_functions = {'bce': nn.BCEWithLogitsLoss(reduction=reduction), 'ce': nn.CrossEntropyLoss(\n",
    "            reduction=reduction), 'mse': nn.MSELoss(reduction=reduction), 'nll': nn.NLLLoss(reduction=reduction),'dice':DiceLoss()}\n",
    "        if 'name' not in list(optimizer_opts.keys()):\n",
    "            optimizer_opts['name'] = 'adam'\n",
    "        self.optimizer = optimizers[optimizer_opts.pop('name')](\n",
    "            self.model.parameters(), **optimizer_opts)\n",
    "        if False and torch.cuda.is_available():\n",
    "            self.cuda = True\n",
    "            self.model, self.optimizer = amp.initialize(\n",
    "                self.model, self.optimizer, opt_level=opt_level)\n",
    "        else:\n",
    "            self.cuda = False\n",
    "        self.scheduler = Scheduler(\n",
    "            optimizer=self.optimizer, opts=scheduler_opts)\n",
    "        self.n_epoch = n_epoch\n",
    "        self.validation_dataloader = validation_dataloader\n",
    "        self.loss_fn = loss_functions[loss_fn]\n",
    "        self.loss_fn_name = loss_fn\n",
    "        self.bce = (self.loss_fn_name == 'bce')\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.original_loss_fn = copy.deepcopy(loss_functions[loss_fn])\n",
    "        self.num_train_batches = num_train_batches\n",
    "        self.val_loss_fn = copy.deepcopy(loss_functions[loss_fn])\n",
    "        self.verbosity=0\n",
    "        self.checkpoints_dir=checkpoints_dir\n",
    "        self.tensor_dataset=tensor_dataset\n",
    "        self.transforms=transforms\n",
    "        self.semantic_segmentation=semantic_segmentation\n",
    "        self.save_metric=save_metric\n",
    "        self.save_after_n_batch=save_after_n_batch\n",
    "        self.train_batch_count=0\n",
    "        self.initial_seed=0\n",
    "        self.seed=0\n",
    "\n",
    "    def save_checkpoint(self,model,epoch,batch=0):\n",
    "        os.makedirs(self.checkpoints_dir,exist_ok=True)\n",
    "        out_name = f\"{batch}.batch\" if batch else f\"{epoch}.epoch\"\n",
    "        torch.save(model,os.path.join(self.checkpoints_dir,f\"{out_name}.checkpoint.pth\"))\n",
    "\n",
    "    def calc_loss(self, y_pred, y_true):\n",
    "        \"\"\"Calculates loss supplied in init statement and modified by reweighting.\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred:tensor\n",
    "                Predictions.\n",
    "        y_true:tensor\n",
    "                True values.\n",
    "        Returns\n",
    "        -------\n",
    "        loss\n",
    "        \"\"\"\n",
    "\n",
    "        return self.loss_fn(y_pred, y_true)\n",
    "\n",
    "    def calc_val_loss(self, y_pred, y_true):\n",
    "        \"\"\"Calculates loss supplied in init statement on validation set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred:tensor\n",
    "                Predictions.\n",
    "        y_true:tensor\n",
    "                True values.\n",
    "        Returns\n",
    "        -------\n",
    "        val_loss\n",
    "        \"\"\"\n",
    "\n",
    "        return self.val_loss_fn(y_pred, y_true)\n",
    "\n",
    "    def reset_loss_fn(self):\n",
    "        \"\"\"Resets loss to original specified loss.\"\"\"\n",
    "        self.loss_fn = self.original_loss_fn\n",
    "\n",
    "    def add_class_balance_loss(self, y, custom_weights=''):\n",
    "        \"\"\"Updates loss function to handle class imbalance by weighting inverse to class appearance.\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset:DynamicImageDataset\n",
    "                Dataset to balance by.\n",
    "        \"\"\"\n",
    "        self.class_weights = compute_class_weight('balanced',np.unique(y),y)#dataset.get_class_weights() if not custom_weights else np.array(\n",
    "            #list(map(float, custom_weights.split(','))))\n",
    "        if custom_weights:\n",
    "            self.class_weights = self.class_weights / sum(self.class_weights)\n",
    "        print('Weights:', self.class_weights)\n",
    "        self.original_loss_fn = copy.deepcopy(self.loss_fn)\n",
    "        weight = torch.tensor(self.class_weights, dtype=torch.float)\n",
    "        if torch.cuda.is_available():\n",
    "            weight = weight.cuda()\n",
    "        if self.loss_fn_name == 'ce':\n",
    "            self.loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
    "        elif self.loss_fn_name == 'nll':\n",
    "            self.loss_fn = nn.NLLLoss(weight=weight)\n",
    "        else:  # modify below for multi-target\n",
    "            self.loss_fn = lambda y_pred, y_true: sum([self.class_weights[i] * self.original_loss_fn(\n",
    "                y_pred[y_true == i], y_true[y_true == i]) if sum(y_true == i) else 0. for i in range(3)])\n",
    "    \n",
    "\n",
    "    def loss_backward(self, loss):\n",
    "        \"\"\"Backprop using mixed precision for added speed boost.\n",
    "        Parameters\n",
    "        ----------\n",
    "        loss:loss\n",
    "                Torch loss calculated.\n",
    "        \"\"\"\n",
    "        # with self.amp_handle.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "        # \tscaled_loss.backward()\n",
    "        # loss.backward()\n",
    "        if self.cuda:\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "    # @pysnooper.snoop()\n",
    "    def train_loop(self, epoch, train_dataloader):\n",
    "        \"\"\"One training epoch, calculate predictions, loss, backpropagate.\n",
    "        Parameters\n",
    "        ----------\n",
    "        epoch:int\n",
    "                Current epoch.\n",
    "        train_dataloader:DataLoader\n",
    "                Training data.\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "                Training loss for epoch\n",
    "        \"\"\"\n",
    "        self.model.train(True)\n",
    "        running_loss = 0.\n",
    "        n_batch = len(\n",
    "            train_dataloader.dataset) // train_dataloader.batch_size if self.num_train_batches == None else self.num_train_batches\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            starttime = time.time()\n",
    "            X, y_true = batch[:2]\n",
    "            if len(batch)==3: Z=batch[2]\n",
    "            else: Z=None\n",
    "\n",
    "            if i == n_batch:\n",
    "                break\n",
    "\n",
    "            # X = Variable(batch[0], requires_grad=True)\n",
    "            # y_true = Variable(batch[1])\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                X = X.cuda()\n",
    "                y_true = y_true.cuda()\n",
    "                if Z is not None: Z=Z.cuda()\n",
    "\n",
    "            if self.tensor_dataset:\n",
    "                if self.semantic_segmentation: X,y_true=self.transforms['train'](X,y_true)\n",
    "                else: X=self.transforms['train'](X)\n",
    "\n",
    "            y_pred = self.model(X) if Z is None else self.model(X,Z)\n",
    "            # y_true=y_true.argmax(dim=1)\n",
    "\n",
    "            loss = self.calc_loss(y_pred, y_true if self.semantic_segmentation else y_true.flatten())\n",
    "          #  loss.requires_grad=True# .view(-1,1)\n",
    "            train_loss = loss.item()\n",
    "            running_loss += train_loss\n",
    "            self.optimizer.zero_grad()\n",
    "            self.loss_backward(loss)  # loss.backward()\n",
    "            self.optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "            endtime = time.time()\n",
    "            if self.verbosity >=1:\n",
    "                print(\"Epoch {}[{}/{}] Time:{}, Train Loss:{}\".format(epoch,\n",
    "                                                                  i, n_batch, round(endtime - starttime, 3), train_loss))\n",
    "            self.train_batch_count+=1\n",
    "            if self.save_after_n_batch and self.train_batch_count%self.save_after_n_batch==0:\n",
    "                val_loss,val_f1=self.val_loop(epoch, self.val_dataloader)\n",
    "                self.batch_val_losses.append(val_loss)\n",
    "                self.batch_val_f1.append(val_f1)\n",
    "                self.save_best_val_model(val_loss, val_f1, self.batch_val_losses, self.batch_val_f1, epoch, True, self.train_batch_count)\n",
    "                self.model.train(True)\n",
    "        \n",
    "        self.scheduler.step()\n",
    "        running_loss /= n_batch\n",
    "        return running_loss\n",
    "\n",
    "    def val_loop(self, epoch, val_dataloader, print_val_confusion=True, save_predictions=True):\n",
    "        \"\"\"Calculate loss over validation set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        epoch:int\n",
    "                Current epoch.\n",
    "        val_dataloader:DataLoader\n",
    "                Validation iterator.\n",
    "        print_val_confusion:bool\n",
    "                Calculate confusion matrix and plot.\n",
    "        save_predictions:int\n",
    "                Print validation results.\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "                Validation loss for epoch.\n",
    "        \"\"\"\n",
    "        self.model.train(False)\n",
    "        n_batch = len(val_dataloader.dataset) // val_dataloader.batch_size\n",
    "        running_loss = 0.\n",
    "        Y = {'pred': [], 'true': []}\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(val_dataloader):\n",
    "                # X = Variable(batch[0], requires_grad=True)\n",
    "                # y_true = Variable(batch[1])\n",
    "                X, y_true = batch[:2]\n",
    "                if len(batch)==3: Z=batch[2]\n",
    "                else: Z=None\n",
    "                if torch.cuda.is_available():\n",
    "                    X = X.cuda()\n",
    "                    y_true = y_true.cuda()\n",
    "                    if Z is not None: Z=Z.cuda()\n",
    "\n",
    "                if self.tensor_dataset:\n",
    "                    if self.semantic_segmentation: X,y_true=self.transforms['val'](X,y_true)\n",
    "                    else: X=self.transforms['val'](X)\n",
    "\n",
    "                y_pred = self.model(X) if Z is None else self.model(X,Z)\n",
    "                # y_true=y_true.argmax(dim=1)\n",
    "                # if save_predictions:\n",
    "                Y['true'].append(\n",
    "                    y_true.detach().cpu().numpy().astype(int).flatten())\n",
    "                y_pred_numpy = ((y_pred if not self.bce else self.sigmoid(\n",
    "                    y_pred)).detach().cpu().numpy()).astype(float)\n",
    "                if self.loss_fn_name in ['ce','dice']:\n",
    "                    y_pred_numpy = y_pred_numpy.argmax(axis=1)\n",
    "                Y['pred'].append(y_pred_numpy.flatten())\n",
    "\n",
    "                loss = self.calc_val_loss(y_pred, y_true if self.semantic_segmentation else y_true.flatten())  # .view(-1,1)\n",
    "                val_loss = loss.item()\n",
    "                running_loss += val_loss\n",
    "                if self.verbosity >=1:\n",
    "                    print(\"Epoch {}[{}/{}] Val Loss:{}\".format(epoch, i, n_batch, val_loss))\n",
    "        # if print_val_confusion and save_predictions:\n",
    "        y_pred, y_true = np.hstack(Y['pred']).flatten(), np.hstack(Y['true']).flatten()\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        running_loss /= n_batch\n",
    "        return running_loss, f1_score(y_true, y_pred,average='macro')\n",
    "\n",
    "    # @pysnooper.snoop(\"test_loop.log\")\n",
    "    def test_loop(self, test_dataloader):\n",
    "        \"\"\"Calculate final predictions on loss.\n",
    "        Parameters\n",
    "        ----------\n",
    "        test_dataloader:DataLoader\n",
    "                Test dataset.\n",
    "        Returns\n",
    "        -------\n",
    "        array\n",
    "                Predictions or embeddings.\n",
    "        \"\"\"\n",
    "        # self.model.train(False) KEEP DROPOUT? and BATCH NORM??\n",
    "        self.model.eval()\n",
    "        y_pred = []\n",
    "        Y_true = []\n",
    "        running_loss = 0.\n",
    "        n_batch = len(\n",
    "            test_dataloader.dataset) // test_dataloader.batch_size\n",
    "        with torch.no_grad():\n",
    "            for i, batch in tqdm.tqdm(enumerate(test_dataloader),total=n_batch):\n",
    "                #X = Variable(batch[0],requires_grad=False)\n",
    "                X, y_true = batch[:2]\n",
    "                if len(batch)==3: Z=batch[2]\n",
    "                else: Z=None\n",
    "                if torch.cuda.is_available():\n",
    "                    X = X.cuda()\n",
    "                    y_true = y_true.cuda()\n",
    "                    if Z is not None: Z=Z.cuda()\n",
    "\n",
    "                prediction = self.model(X) if Z is None else self.model(X,Z)\n",
    "                y_pred.append(prediction.detach().cpu().numpy())\n",
    "                Y_true.append(y_true.detach().cpu().numpy())\n",
    "        y_pred = np.concatenate(y_pred, axis=0)  # torch.cat(y_pred,0)\n",
    "        y_true = np.concatenate(Y_true, axis=0).flatten()\n",
    "        return y_pred,y_true\n",
    "    \n",
    "    def fitt(self, train_dataloader, verbose=False, print_every=10, save_model=True, plot_training_curves=False, plot_save_file=None, print_val_confusion=True,, save_val_predictions=True):\n",
    "        # choose model with best f1\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_f1 = []\n",
    "        self.batch_val_losses = []\n",
    "        self.batch_val_f1 = []\n",
    "        if verbose:\n",
    "            self.verbosity+=1\n",
    "        for epoch in range(self.n_epoch):\n",
    "            self.seed=self.initial_seed+epoch\n",
    "            np.random.seed(self.seed)\n",
    "            start_time = time.time()\n",
    "            train_loss = self.train_loop(epoch, train_dataloader)\n",
    "            current_time = time.time()\n",
    "            train_time = current_time - start_time\n",
    "            self.train_losses.append(train_loss)\n",
    "            val_loss, val_f1 = self.val_loop(epoch, self.validation_dataloader,\n",
    "                                     print_val_confusion=print_val_confusion, save_predictions=save_val_predictions)\n",
    "            val_time = time.time() - current_time\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.val_f1.append(val_f1)\n",
    "            self.batch_val_losses.append(val_loss)\n",
    "            self.batch_val_f1.append(val_f1)\n",
    "            # if True:#verbose and not (epoch % print_every):\n",
    "            if plot_training_curves:\n",
    "                self.plot_train_val_curves(plot_save_file)\n",
    "            print(\"Epoch {}: Train Loss {}, Val Loss {}, Train Time {}, Val Time {}\".format(\n",
    "                epoch, train_loss, val_loss, train_time, val_time))\n",
    "            print('Training complete in {:.0f}m {:.0f}s'.format(train_time // 60, train_time % 60))\n",
    "            self.save_best_val_model(val_loss, val_f1, self.val_losses, self.val_f1, epoch, save_model)\n",
    "            if \"save_every\" in dir(train_dataloader.dataset) and train_dataloader.dataset.save_every and epoch%train_dataloader.dataset.save_every==0:\n",
    "                train_dataloader.dataset.load_image_annot()\n",
    "        if save_model:\n",
    "            print(\"Saving best model at epoch {}\".format(self.best_epoch))\n",
    "            self.model.load_state_dict(self.best_model_state_dict)\n",
    "        return self, self.min_val_loss_f1, self.best_epoch\n",
    "\n",
    "    def save_best_val_model(self, val_loss, val_f1, val_loss_list, val_f1_list, epoch, save_model=True, batch=0):\n",
    "        if (val_loss <= min(val_loss_list) if self.save_metric=='loss' else val_f1 >= max(val_f1_list)) and save_model:\n",
    "            print(\"New best model at epoch {}\".format(epoch))\n",
    "            self.min_val_loss_f1 = val_loss if self.save_metric=='loss' else val_f1\n",
    "            self.best_epoch = epoch\n",
    "            if batch: self.best_batch = batch\n",
    "            self.best_model_state_dict = copy.deepcopy(self.model.state_dict())\n",
    "            self.save_checkpoint(self.best_model_state_dict,epoch,batch)\n",
    "\n",
    "    def plot_train_val_curves(self, save_file==None):\n",
    "            \"\"\"Plots training and validation curves.\n",
    "            Parameters\n",
    "            ----------\n",
    "            save_file:str\n",
    "                    File to save to.\n",
    "            \"\"\"\n",
    "            plt.figure()\n",
    "            sns.lineplot('epoch', 'loss', hue='variable',\n",
    "                         data=pd.DataFrame(np.vstack((np.arange(len(self.train_losses)), self.train_losses, self.val_losses)).T,\n",
    "                                           columns=['epoch', 'train', 'val']).melt(id_vars=['epoch'], value_vars=['train', 'val']))\n",
    "            if save_file is not None:\n",
    "                plt.savefig(save_file, dpi=300)\n",
    "            \n",
    "\n",
    "    def predict(self, test_dataloader):\n",
    "            \"\"\"Make classification segmentation predictions on testing data.\n",
    "            Parameters\n",
    "            ----------\n",
    "            test_dataloader:DataLoader\n",
    "                    Test data.\n",
    "            Returns\n",
    "            -------\n",
    "            array\n",
    "                    Predictions.\n",
    "            \"\"\"\n",
    "            y_pred,y_true = self.test_loop(test_dataloader)\n",
    "            return y_pred,y_true\n",
    "\n",
    "    def fit_predict(self, train_dataloader, test_dataloader):\n",
    "            \"\"\"Fit model to training data and make classification segmentation predictions on testing data.\n",
    "            Parameters\n",
    "            ----------\n",
    "            train_dataloader:DataLoader\n",
    "                    Train data.\n",
    "            test_dataloader:DataLoader\n",
    "                    Test data.\n",
    "            Returns\n",
    "            -------\n",
    "            array\n",
    "                    Predictions.\n",
    "            \"\"\"\n",
    "            return self.fit(train_dataloader)[0].predict(test_dataloader)\n",
    "\n",
    "    def return_model(self):\n",
    "            \"\"\"Returns pytorch model.\n",
    "            \"\"\"\n",
    "            return self.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fe0fec6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 281>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    279\u001b[0m     fire\u001b[38;5;241m.\u001b[39mFire(train_model)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m--> 279\u001b[0m     \u001b[43mfire\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hiss/lib/python3.8/site-packages/fire/core.py:141\u001b[0m, in \u001b[0;36mFire\u001b[0;34m(component, command, name)\u001b[0m\n\u001b[1;32m    138\u001b[0m   context\u001b[38;5;241m.\u001b[39mupdate(caller_globals)\n\u001b[1;32m    139\u001b[0m   context\u001b[38;5;241m.\u001b[39mupdate(caller_locals)\n\u001b[0;32m--> 141\u001b[0m component_trace \u001b[38;5;241m=\u001b[39m \u001b[43m_Fire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomponent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparsed_flag_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m component_trace\u001b[38;5;241m.\u001b[39mHasError():\n\u001b[1;32m    144\u001b[0m   _DisplayError(component_trace)\n",
      "File \u001b[0;32m~/anaconda3/envs/hiss/lib/python3.8/site-packages/fire/core.py:466\u001b[0m, in \u001b[0;36m_Fire\u001b[0;34m(component, args, parsed_flag_args, context, name)\u001b[0m\n\u001b[1;32m    463\u001b[0m is_class \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39misclass(component)\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m   component, remaining_args \u001b[38;5;241m=\u001b[39m \u001b[43m_CallAndUpdateTrace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcomponent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m      \u001b[49m\u001b[43mremaining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcomponent_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtreatment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mroutine\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomponent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m   handled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m FireError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m~/anaconda3/envs/hiss/lib/python3.8/site-packages/fire/core.py:681\u001b[0m, in \u001b[0;36m_CallAndUpdateTrace\u001b[0;34m(component, args, component_trace, treatment, target)\u001b[0m\n\u001b[1;32m    679\u001b[0m   component \u001b[38;5;241m=\u001b[39m loop\u001b[38;5;241m.\u001b[39mrun_until_complete(fn(\u001b[38;5;241m*\u001b[39mvarargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 681\u001b[0m   component \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvarargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m treatment \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    684\u001b[0m   action \u001b[38;5;241m=\u001b[39m trace\u001b[38;5;241m.\u001b[39mINSTANTIATED_CLASS\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(inputs_dir, learning_rate, n_epochs, crop_size, resize, mean, std, num_classes, architecture, batch_size, predict, model_save_loc, pretrained_save_loc, predictions_save_path, predict_set, verbose, class_balance, extract_embeddings, extract_embeddings_df, embedding_out_dir, gpu_id, checkpoints_dir, tensor_dataset, pickle_dataset, label_map, semantic_segmentation, save_metric, custom_dataset, save_predictions, pretrained, save_after_n_batch, include_test_set, use_npy_rotate, sample_frac, sample_every, num_workers, npy_rotate_sets_pkl)\u001b[0m\n\u001b[1;32m    203\u001b[0m optimizer_opts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    204\u001b[0m                       lr\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[1;32m    205\u001b[0m                       weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m    207\u001b[0m scheduler_opts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarm_restarts\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    208\u001b[0m                       lr_scheduler_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m    209\u001b[0m                       T_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m    210\u001b[0m                       eta_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-8\u001b[39m,\n\u001b[1;32m    211\u001b[0m                       T_mult\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    213\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ModelTrainerr(model,\n\u001b[1;32m    214\u001b[0m                        n_epochs,\n\u001b[0;32m--> 215\u001b[0m                        \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m predict \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m    216\u001b[0m                        optimizer_opts,\n\u001b[1;32m    217\u001b[0m                        scheduler_opts,\n\u001b[1;32m    218\u001b[0m                        loss_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdice\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (semantic_segmentation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m class_balance) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mce\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    219\u001b[0m                        checkpoints_dir\u001b[38;5;241m=\u001b[39mcheckpoints_dir,\n\u001b[1;32m    220\u001b[0m                        tensor_dataset\u001b[38;5;241m=\u001b[39mtensor_dataset,\n\u001b[1;32m    221\u001b[0m                        transforms\u001b[38;5;241m=\u001b[39mtransformers,\n\u001b[1;32m    222\u001b[0m                        semantic_segmentation\u001b[38;5;241m=\u001b[39msemantic_segmentation,\n\u001b[1;32m    223\u001b[0m                        save_metric\u001b[38;5;241m=\u001b[39msave_metric,\n\u001b[1;32m    224\u001b[0m                        save_after_n_batch\u001b[38;5;241m=\u001b[39msave_after_n_batch)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pretrained_save_loc):\n\u001b[1;32m    227\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(pretrained_save_loc,map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgpu_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gpu_id\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val'"
     ]
    }
   ],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,x):\n",
    "        return x.view(x.shape[0],-1)\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "\n",
    "def generate_transformers(image_size=224, resize=256, mean=[], std=[], include_jitter=False):\n",
    "    train_transform = [transforms.Resize((resize,resize))]\n",
    "    if include_jitter:\n",
    "        train_transform.append(transforms.ColorJitter(brightness=0.4,\n",
    "                                            contrast=0.4, saturation=0.4, hue=0.1))\n",
    "    train_transform.extend([transforms.RandomHorizontalFlip(p=0.5),\n",
    "           transforms.RandomVerticalFlip(p=0.5),\n",
    "           transforms.RandomRotation(90),\n",
    "           transforms.RandomResizedCrop((image_size,image_size)),\n",
    "           transforms.ToTensor(),\n",
    "           transforms.Normalize(mean if mean else [0.5, 0.5, 0.5],\n",
    "                                std if std else [0.1, 0.1, 0.1])\n",
    "           ])\n",
    "    train_transform=transforms.Compose(train_transform)\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((resize,resize)),\n",
    "        transforms.CenterCrop((image_size,image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean if mean else [0.5, 0.5, 0.5],\n",
    "                             std if std else [0.1, 0.1, 0.1])\n",
    "    ])\n",
    "    normalization_transform = transforms.Compose([transforms.Resize((resize,resize)),\n",
    "                                                  transforms.CenterCrop(\n",
    "                                                      (image_size,image_size)),\n",
    "                                                  transforms.ToTensor()])\n",
    "    return {'train': train_transform, 'val': val_transform, 'test': val_transform, 'norm': normalization_transform}\n",
    "\n",
    "def generate_kornia_transforms(image_size=224, resize=256, mean=[], std=[], include_jitter=False):\n",
    "    mean=torch.tensor(mean) if mean else torch.tensor([0.5, 0.5, 0.5])\n",
    "    std=torch.tensor(std) if std else torch.tensor([0.1, 0.1, 0.1])\n",
    "    if torch.cuda.is_available():\n",
    "        mean=mean.cuda()\n",
    "        std=std.cuda()\n",
    "    train_transforms=[G.Resize((resize,resize))]\n",
    "    if include_jitter:\n",
    "        train_transforms.append(K.ColorJitter(brightness=0.4, contrast=0.4,\n",
    "                                   saturation=0.4, hue=0.1))\n",
    "    train_transforms.extend([K.RandomHorizontalFlip(p=0.5),\n",
    "           K.RandomVerticalFlip(p=0.5),\n",
    "           K.RandomRotation(90),\n",
    "           K.RandomResizedCrop((image_size,image_size)),\n",
    "           K.Normalize(mean,std)\n",
    "           ])\n",
    "    val_transforms=[G.Resize((resize,resize)),\n",
    "           K.CenterCrop((image_size,image_size)),\n",
    "           K.Normalize(mean,std)\n",
    "           ]\n",
    "    transforms=dict(train=nn.Sequential(*train_transforms),\n",
    "                val=nn.Sequential(*val_transforms))\n",
    "    if torch.cuda.is_available():\n",
    "        for k in transforms:\n",
    "            transforms[k]=transforms[k].cuda()\n",
    "    return transforms\n",
    "\n",
    "class SegmentationTransform(nn.Module):\n",
    "    def __init__(self,resize,image_size,mean,std,include_jitter=False,Set=\"train\"):\n",
    "        super().__init__()\n",
    "        self.resize=G.Resize((resize,resize),align_corners=False)\n",
    "        self.mask_resize=lambda x: torch.nn.functional.interpolate(x, size=(resize,resize), mode='nearest', align_corners=None)#G.Resize((resize,resize),interpolation='nearest',align_corners=False)#\n",
    "        self.jit=K.ColorJitter(brightness=0.4, contrast=0.4,\n",
    "                                   saturation=0.4, hue=0.1) if include_jitter else (lambda x: x)\n",
    "        # self.rotations=nn.ModuleList([\n",
    "        #        K.augmentation.RandomAffine([-90., 90.], [0., 0.15], [0.5, 1.5], [0., 0.15])\n",
    "        #        # K.RandomHorizontalFlip(p=0.5),\n",
    "        #        # K.RandomVerticalFlip(p=0.5),\n",
    "        #        # K.RandomRotation(90),#K.RandomResizedCrop((image_size,image_size),interpolation=\"nearest\")\n",
    "        #        ])\n",
    "        # self.rotations_mask=nn.ModuleList([\n",
    "        #        K.augmentation.RandomAffine([-90., 90.], [0., 0.15], [0.5, 1.5], [0., 0.15],resample=\"NEAREST\")\n",
    "        #        ])\n",
    "        self.affine=K.augmentation.RandomAffine([-90., 90.], [0., 0.15], None, [0., 0.15])\n",
    "        self.affine_mask=K.augmentation.RandomAffine([-90., 90.], [0., 0.15], None, [0., 0.15],resample=\"NEAREST\",align_corners=False)\n",
    "        self.normalize=K.Normalize(mean,std)\n",
    "        self.crop,self.mask_crop=K.CenterCrop((image_size,image_size)),K.CenterCrop((image_size,image_size),resample=\"NEAREST\")\n",
    "        self.Set=Set\n",
    "\n",
    "    def forward(self,input,mask):\n",
    "        mask=mask.unsqueeze(1).float()#torch.cat([mask.unsqueeze(1)]*3,1)\n",
    "        if self.Set=='train':\n",
    "            img=self.jit(self.resize(input))\n",
    "            mask_out=self.mask_resize(mask)\n",
    "            img=self.affine(img)\n",
    "            mask_out=self.affine_mask(mask_out,self.affine._params)\n",
    "            # for rotation in self.rotations: img=rotation(img)\n",
    "            img=self.normalize(img)\n",
    "            # for i in range(len(self.rotations_mask)): mask_out=self.rotations_mask[i](mask_out,self.rotations[i]._params)\n",
    "        else:\n",
    "            img=self.normalize(self.crop(self.resize(input)))\n",
    "            mask_out=self.mask_crop(self.mask_resize(mask))\n",
    "        return img,mask_out.squeeze(1).long()#[:,0,...]\n",
    "\n",
    "def generate_kornia_segmentation_transforms(image_size=224, resize=256, mean=[], std=[], include_jitter=False):  # add this then IoU metric\n",
    "    mean=torch.tensor(mean) if mean else torch.tensor([0.5, 0.5, 0.5])\n",
    "    std=torch.tensor(std) if std else torch.tensor([0.1, 0.1, 0.1])\n",
    "    transforms={k:SegmentationTransform(resize,image_size,mean,std,include_jitter=False,Set=k) for k in ['train','val']}\n",
    "    if torch.cuda.is_available():\n",
    "        for k in transforms:\n",
    "            transforms[k]=transforms[k].cuda()\n",
    "    return transforms\n",
    "\n",
    "# @pysnooper.snoop()\n",
    "def train_model(inputs_dir='inputs_training',\n",
    "                learning_rate=1e-4,\n",
    "                n_epochs=300,\n",
    "                crop_size=224,\n",
    "                resize=256,\n",
    "                mean=[0.5, 0.5, 0.5],\n",
    "                std=[0.1, 0.1, 0.1],\n",
    "                num_classes=3,\n",
    "                architecture='resnet50',\n",
    "                batch_size=32,\n",
    "                predict=False,\n",
    "                model_save_loc='saved_model.pkl',\n",
    "                pretrained_save_loc='pretrained_model.pkl',\n",
    "                predictions_save_path='predictions.pkl',\n",
    "                predict_set='test',\n",
    "                verbose=False,\n",
    "                class_balance=True,\n",
    "                extract_embeddings=\"\",\n",
    "                extract_embeddings_df=\"\",\n",
    "                embedding_out_dir=\"./\",\n",
    "                gpu_id=-1,\n",
    "                checkpoints_dir=\"checkpoints\",\n",
    "                tensor_dataset=False,\n",
    "                pickle_dataset=True,\n",
    "                label_map=dict(),\n",
    "                semantic_segmentation=False,\n",
    "                save_metric=\"loss\",\n",
    "                custom_dataset=None,\n",
    "                save_predictions=True,\n",
    "                pretrained=False,\n",
    "                save_after_n_batch=0,\n",
    "                include_test_set=False,\n",
    "                use_npy_rotate=False,\n",
    "                sample_frac=1.,\n",
    "                sample_every=0,\n",
    "                num_workers=0,\n",
    "                npy_rotate_sets_pkl=\"\"\n",
    "                ):\n",
    "    assert save_metric in ['loss','f1']\n",
    "    if use_npy_rotate: tensor_dataset,pickle_dataset=False,False\n",
    "    else: sample_every=0\n",
    "    if predict: include_test_set=True\n",
    "    if predict: assert not use_npy_rotate\n",
    "    if extract_embeddings: assert predict, \"Must be in prediction mode to extract embeddings\"\n",
    "    if tensor_dataset: assert not pickle_dataset, \"Cannot have pickle and tensor classes activated\"\n",
    "    if semantic_segmentation and custom_dataset is None: assert tensor_dataset==True, \"For now, can only perform semantic segmentation with TensorDataset\"\n",
    "    if gpu_id>=0: torch.cuda.set_device(gpu_id)\n",
    "    transformers=generate_transformers if not tensor_dataset else generate_kornia_transforms\n",
    "    if semantic_segmentation: transformers=generate_kornia_segmentation_transforms\n",
    "    transformers = transformers(\n",
    "        image_size=crop_size, resize=resize, mean=mean, std=std)\n",
    "    if custom_dataset is not None:\n",
    "        assert predict\n",
    "        datasets={}\n",
    "        datasets['custom']=custom_dataset\n",
    "        predict_set='custom'\n",
    "    else:\n",
    "        if tensor_dataset:\n",
    "            datasets = {x: torch.load(os.path.join(inputs_dir,f\"{x}_data.pth\")) for x in (['train','val']+(['test'] if include_test_set else [])) if os.path.exists(os.path.join(inputs_dir,f\"{x}_data.pth\"))}\n",
    "            for k in datasets:\n",
    "                if len(datasets[k].tensors[1].shape)>1 and not semantic_segmentation: datasets[k]=TensorDataset(datasets[k].tensors[0],datasets[k].tensors[1].flatten())\n",
    "        elif pickle_dataset:\n",
    "            datasets = {x: PickleDataset(os.path.join(inputs_dir,f\"{x}_data.pkl\"),transformers[x],label_map) for x in (['train','val']+(['test'] if include_test_set else [])) if os.path.exists(os.path.join(inputs_dir,f\"{x}_data.pkl\"))}\n",
    "        elif use_npy_rotate:\n",
    "            datasets = {x: NPYRotatingStack(os.path.join(inputs_dir,x),transformers[x],(sample_frac if x=='train' else 1.),sample_every,label_map,npy_rotate_sets_pkl,x) for x in (['train','val']+(['test'] if include_test_set else []))}\n",
    "        else:\n",
    "            datasets = {x: Datasets.ImageFolder(os.path.join(\n",
    "                inputs_dir, x), transformers[x]) for x in (['train','val']+(['test'] if include_test_set else []))}\n",
    "\n",
    "    if verbose: print(datasets)\n",
    "\n",
    "    dataloaders = {x: DataLoader(\n",
    "        datasets[x], batch_size=batch_size, num_workers=num_workers, shuffle=(x == 'train' and not predict), worker_init_fn=worker_init_fn) for x in datasets}\n",
    "\n",
    "    model = generate_model(architecture,\n",
    "                           num_classes,\n",
    "                           semantic_segmentation=semantic_segmentation,\n",
    "                           pretrained=pretrained,\n",
    "                           n_aux_features=None if semantic_segmentation or \"n_aux_features\" not in dir(datasets.get('train',datasets.get('custom',None))) else datasets.get('train',datasets.get('custom',None)).n_aux_features)\n",
    "    '''ct=0\n",
    "    for child in model.children():\n",
    "        if ct <=1:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "        ct += 1\n",
    "'''\n",
    "    if verbose: print(model)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    optimizer_opts = dict(name='adam',\n",
    "                          lr=learning_rate,\n",
    "                          weight_decay=1e-4)\n",
    "\n",
    "    scheduler_opts = dict(scheduler='warm_restarts',\n",
    "                          lr_scheduler_decay=0.5,\n",
    "                          T_max=10,\n",
    "                          eta_min=5e-8,\n",
    "                          T_mult=2)\n",
    "\n",
    "    trainer = ModelTrainerr(model,\n",
    "                           n_epochs,\n",
    "                           None if predict else dataloaders['val'],\n",
    "                           optimizer_opts,\n",
    "                           scheduler_opts,\n",
    "                           loss_fn='dice' if (semantic_segmentation and not class_balance) else 'ce',\n",
    "                           checkpoints_dir=checkpoints_dir,\n",
    "                           tensor_dataset=tensor_dataset,\n",
    "                           transforms=transformers,\n",
    "                           semantic_segmentation=semantic_segmentation,\n",
    "                           save_metric=save_metric,\n",
    "                           save_after_n_batch=save_after_n_batch)\n",
    "\n",
    "    if os.path.exists(pretrained_save_loc):\n",
    "        trainer.model.load_state_dict(torch.load(pretrained_save_loc,map_location=f\"cuda:{gpu_id}\" if gpu_id>=0 else \"cpu\"))\n",
    "\n",
    "    if not predict:\n",
    "\n",
    "        if class_balance:\n",
    "            trainer.add_class_balance_loss(datasets['train'].targets if not tensor_dataset else datasets['train'].tensors[1].numpy().flatten())\n",
    "\n",
    "        trainer, min_val_loss_f1, best_epoch=trainer.fit(dataloaders['train'],verbose=verbose)\n",
    "\n",
    "        torch.save(trainer.model.state_dict(), model_save_loc)\n",
    "\n",
    "        return trainer.model\n",
    "\n",
    "    else:\n",
    "        # assert not tensor_dataset, \"Only ImageFolder and NPYDatasets allowed\"\n",
    "\n",
    "        if os.path.exists(model_save_loc):\n",
    "            trainer.model.load_state_dict(torch.load(model_save_loc,map_location=f\"cuda:{gpu_id}\" if gpu_id>=0 else \"cpu\"))\n",
    "\n",
    "        if extract_embeddings:\n",
    "            assert not semantic_segmentation, \"Semantic Segmentation not implemented for whole slide segmentation\"\n",
    "            trainer.model=nn.Sequential(trainer.model.fitfeatures,Reshape())#,trainer.model.output\n",
    "            if predict_set=='custom':\n",
    "                dataset=datasets['custom']\n",
    "                assert 'embed' in dir(dataset), \"Embedding method required for dataset with model input, batch size and embedding output directory as arguments.\"\n",
    "            else:\n",
    "                assert len(extract_embeddings_df)>0 and os.path.exists(extract_embeddings_df), \"Must load data from SQL database or pickle if not using custom dataset\"\n",
    "                if extract_embeddings_df.endswith(\".db\"):\n",
    "                    from pathflowai.utils import load_sql_df\n",
    "                    patch_info=load_sql_df(extract_embeddings_df,resize)\n",
    "                elif extract_embeddings_df.endswith(\".pkl\"):\n",
    "                    patch_info=pd.read_pickle(extract_embeddings_df)\n",
    "                    assert patch_info['patch_size'].iloc[0]==resize, \"Patch size pickle does not match.\"\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "                dataset=NPYDataset(patch_info,extract_embeddings,transformers[\"test\"],tensor_dataset)\n",
    "            return dataset.embed(trainer.model,batch_size,embedding_out_dir)\n",
    "            # return \"Output Embeddings\"\n",
    "        else:\n",
    "            Y = dict()\n",
    "\n",
    "            Y['pred'],Y['true'] = trainer.predict(dataloaders[predict_set])\n",
    "\n",
    "            # Y['model'] = trainer.model\n",
    "\n",
    "            # Y['true'] = datasets[predict_set].targets\n",
    "\n",
    "            if save_predictions: torch.save(Y, predictions_save_path)\n",
    "\n",
    "            return Y\n",
    "\n",
    "def main():\n",
    "    fire.Fire(train_model)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed17ac52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': <pathpretrain.datasets.NPYRotatingStack object at 0x2b313b582370>, 'val': <pathpretrain.datasets.NPYRotatingStack object at 0x2b3150f23370>}\n",
      "ResNet(\n",
      "  (features): Sequential(\n",
      "    (init_block): ResInitBlock(\n",
      "      (conv): ConvBlock(\n",
      "        (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (stage1): Sequential(\n",
      "      (unit1): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (identity_conv): ConvBlock(\n",
      "          (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit2): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit3): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (stage2): Sequential(\n",
      "      (unit1): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (identity_conv): ConvBlock(\n",
      "          (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit2): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit3): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit4): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (stage3): Sequential(\n",
      "      (unit1): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (identity_conv): ConvBlock(\n",
      "          (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit2): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit3): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit4): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit5): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit6): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (stage4): Sequential(\n",
      "      (unit1): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (identity_conv): ConvBlock(\n",
      "          (conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit2): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit3): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (final_pool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  )\n",
      "  (output): Linear(in_features=2048, out_features=3, bias=True)\n",
      ")\n",
      "Weights: [ 0.40405693 17.45029129  2.13768574]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dartfs-hpc/rc/home/3/f006n33/anaconda3/envs/hiss/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2], y=[0 0 0 ... 0 0 0] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0[0/17270] Time:0.537, Train Loss:1.1551508903503418\n",
      "Epoch 0[1/17270] Time:0.24, Train Loss:2.3365426063537598\n",
      "Epoch 0[2/17270] Time:0.238, Train Loss:1.4757189750671387\n",
      "Epoch 0[3/17270] Time:0.231, Train Loss:2.113722085952759\n",
      "Epoch 0[4/17270] Time:0.233, Train Loss:2.450373649597168\n",
      "Epoch 0[5/17270] Time:0.233, Train Loss:1.8642264604568481\n",
      "Epoch 0[6/17270] Time:0.235, Train Loss:1.3310364484786987\n",
      "Epoch 0[7/17270] Time:0.23, Train Loss:1.077194094657898\n",
      "Epoch 0[8/17270] Time:0.242, Train Loss:1.1080318689346313\n",
      "Epoch 0[9/17270] Time:0.231, Train Loss:1.485329270362854\n",
      "Epoch 0[10/17270] Time:0.284, Train Loss:1.2868874073028564\n",
      "Epoch 0[11/17270] Time:0.232, Train Loss:2.5559022426605225\n",
      "Epoch 0[12/17270] Time:0.235, Train Loss:1.172218918800354\n",
      "Epoch 0[13/17270] Time:0.232, Train Loss:1.0493922233581543\n",
      "Epoch 0[14/17270] Time:0.231, Train Loss:1.0232887268066406\n",
      "Epoch 0[15/17270] Time:0.23, Train Loss:0.9538516402244568\n",
      "Epoch 0[16/17270] Time:0.232, Train Loss:1.2352594137191772\n",
      "Epoch 0[17/17270] Time:0.24, Train Loss:0.9363041520118713\n",
      "Epoch 0[18/17270] Time:0.253, Train Loss:1.1108092069625854\n",
      "Epoch 0[19/17270] Time:0.232, Train Loss:1.0592007637023926\n",
      "Epoch 0[20/17270] Time:0.239, Train Loss:1.0299322605133057\n",
      "Epoch 0[21/17270] Time:0.233, Train Loss:1.2087388038635254\n",
      "Epoch 0[22/17270] Time:0.236, Train Loss:0.7231692671775818\n",
      "Epoch 0[23/17270] Time:0.235, Train Loss:0.9991126656532288\n",
      "Epoch 0[24/17270] Time:0.236, Train Loss:0.996292233467102\n",
      "Epoch 0[25/17270] Time:0.233, Train Loss:0.8164421319961548\n",
      "Epoch 0[26/17270] Time:0.235, Train Loss:1.1489075422286987\n",
      "Epoch 0[27/17270] Time:0.228, Train Loss:0.7812321782112122\n",
      "Epoch 0[28/17270] Time:0.232, Train Loss:1.053369164466858\n",
      "Epoch 0[29/17270] Time:0.243, Train Loss:0.7689021825790405\n",
      "Epoch 0[30/17270] Time:0.224, Train Loss:0.829975962638855\n",
      "Epoch 0[31/17270] Time:0.246, Train Loss:0.7912043929100037\n",
      "Epoch 0[32/17270] Time:0.229, Train Loss:0.872555673122406\n",
      "Epoch 0[33/17270] Time:0.233, Train Loss:0.9000622034072876\n",
      "Epoch 0[34/17270] Time:0.24, Train Loss:0.7850921154022217\n",
      "Epoch 0[35/17270] Time:0.231, Train Loss:0.8746135830879211\n",
      "Epoch 0[36/17270] Time:0.237, Train Loss:1.1272422075271606\n",
      "Epoch 0[37/17270] Time:0.245, Train Loss:1.489027738571167\n",
      "Epoch 0[38/17270] Time:0.229, Train Loss:0.892419159412384\n",
      "Epoch 0[39/17270] Time:0.233, Train Loss:1.155059576034546\n",
      "Epoch 0[40/17270] Time:0.23, Train Loss:1.013562560081482\n",
      "Epoch 0[41/17270] Time:0.233, Train Loss:0.6657506227493286\n",
      "Epoch 0[42/17270] Time:0.247, Train Loss:1.185372233390808\n",
      "Epoch 0[43/17270] Time:0.227, Train Loss:0.7931548953056335\n",
      "Epoch 0[44/17270] Time:0.24, Train Loss:0.8612170815467834\n",
      "Epoch 0[45/17270] Time:0.233, Train Loss:1.0377120971679688\n",
      "Epoch 0[46/17270] Time:0.236, Train Loss:1.4694610834121704\n",
      "Epoch 0[47/17270] Time:0.226, Train Loss:1.499326467514038\n",
      "Epoch 0[48/17270] Time:0.233, Train Loss:0.6944563388824463\n",
      "Epoch 0[49/17270] Time:0.224, Train Loss:0.7604783773422241\n",
      "Epoch 0[50/17270] Time:0.238, Train Loss:1.2082725763320923\n",
      "Epoch 0[51/17270] Time:0.241, Train Loss:1.505497932434082\n",
      "Epoch 0[52/17270] Time:0.257, Train Loss:1.1682003736495972\n",
      "Epoch 0[53/17270] Time:0.231, Train Loss:0.7632262706756592\n",
      "Epoch 0[54/17270] Time:0.24, Train Loss:0.7918012738227844\n",
      "Epoch 0[55/17270] Time:0.235, Train Loss:0.9935055375099182\n",
      "Epoch 0[56/17270] Time:0.238, Train Loss:0.9302351474761963\n",
      "Epoch 0[57/17270] Time:0.235, Train Loss:0.7865555882453918\n",
      "Epoch 0[58/17270] Time:0.237, Train Loss:1.0119819641113281\n",
      "Epoch 0[59/17270] Time:0.233, Train Loss:0.8271455764770508\n",
      "Epoch 0[60/17270] Time:0.286, Train Loss:1.2167936563491821\n",
      "Epoch 0[61/17270] Time:0.23, Train Loss:1.646294116973877\n",
      "Epoch 0[62/17270] Time:0.242, Train Loss:0.6291150450706482\n",
      "Epoch 0[63/17270] Time:0.229, Train Loss:1.2662817239761353\n",
      "Epoch 0[64/17270] Time:0.236, Train Loss:0.73811936378479\n",
      "Epoch 0[65/17270] Time:0.223, Train Loss:1.510644793510437\n",
      "Epoch 0[66/17270] Time:0.242, Train Loss:0.7776274681091309\n",
      "Epoch 0[67/17270] Time:0.23, Train Loss:0.6882753372192383\n",
      "Epoch 0[68/17270] Time:0.23, Train Loss:0.767473042011261\n",
      "Epoch 0[69/17270] Time:0.24, Train Loss:0.6797319054603577\n",
      "Epoch 0[70/17270] Time:0.238, Train Loss:0.7204813957214355\n",
      "Epoch 0[71/17270] Time:0.242, Train Loss:0.9789819717407227\n",
      "Epoch 0[72/17270] Time:0.241, Train Loss:1.3238351345062256\n",
      "Epoch 0[73/17270] Time:0.226, Train Loss:1.16413414478302\n",
      "Epoch 0[74/17270] Time:0.241, Train Loss:0.6849154829978943\n",
      "Epoch 0[75/17270] Time:0.236, Train Loss:0.9973409175872803\n",
      "Epoch 0[76/17270] Time:0.23, Train Loss:1.0588600635528564\n",
      "Epoch 0[77/17270] Time:0.235, Train Loss:0.5965157151222229\n",
      "Epoch 0[78/17270] Time:0.223, Train Loss:0.6707549095153809\n",
      "Epoch 0[79/17270] Time:0.246, Train Loss:1.1836048364639282\n",
      "Epoch 0[80/17270] Time:0.231, Train Loss:1.055708408355713\n",
      "Epoch 0[81/17270] Time:0.234, Train Loss:0.625914990901947\n",
      "Epoch 0[82/17270] Time:0.238, Train Loss:0.7003405094146729\n",
      "Epoch 0[83/17270] Time:0.227, Train Loss:1.6046754121780396\n",
      "Epoch 0[84/17270] Time:0.226, Train Loss:0.8212480545043945\n",
      "Epoch 0[85/17270] Time:0.227, Train Loss:1.3039724826812744\n",
      "Epoch 0[86/17270] Time:0.234, Train Loss:0.9566984176635742\n",
      "Epoch 0[87/17270] Time:0.235, Train Loss:1.0586726665496826\n",
      "Epoch 0[88/17270] Time:0.228, Train Loss:1.1522237062454224\n",
      "Epoch 0[89/17270] Time:0.232, Train Loss:0.9058059453964233\n",
      "Epoch 0[90/17270] Time:0.238, Train Loss:0.6707894802093506\n",
      "Epoch 0[91/17270] Time:0.244, Train Loss:0.7012593150138855\n",
      "Epoch 0[92/17270] Time:0.238, Train Loss:0.734309196472168\n",
      "Epoch 0[93/17270] Time:0.237, Train Loss:1.1566863059997559\n",
      "Epoch 0[94/17270] Time:0.239, Train Loss:1.8016232252120972\n",
      "Epoch 0[95/17270] Time:0.234, Train Loss:1.5597121715545654\n",
      "Epoch 0[96/17270] Time:0.239, Train Loss:1.0702316761016846\n",
      "Epoch 0[97/17270] Time:0.239, Train Loss:1.0081958770751953\n",
      "Epoch 0[98/17270] Time:0.234, Train Loss:0.918938398361206\n",
      "Epoch 0[99/17270] Time:0.236, Train Loss:1.0168135166168213\n",
      "Epoch 0[100/17270] Time:0.233, Train Loss:0.9629367589950562\n",
      "Epoch 0[101/17270] Time:0.236, Train Loss:0.7229313254356384\n",
      "Epoch 0[102/17270] Time:0.234, Train Loss:0.8596412539482117\n",
      "Epoch 0[103/17270] Time:0.225, Train Loss:0.8272712826728821\n",
      "Epoch 0[104/17270] Time:0.238, Train Loss:0.7033601403236389\n",
      "Epoch 0[105/17270] Time:0.235, Train Loss:0.7667149901390076\n",
      "Epoch 0[106/17270] Time:0.236, Train Loss:0.6964036822319031\n",
      "Epoch 0[107/17270] Time:0.229, Train Loss:0.7733160257339478\n",
      "Epoch 0[108/17270] Time:0.223, Train Loss:0.8548610210418701\n",
      "Epoch 0[109/17270] Time:0.239, Train Loss:1.06308114528656\n",
      "Epoch 0[110/17270] Time:0.238, Train Loss:0.8432101607322693\n",
      "Epoch 0[111/17270] Time:0.236, Train Loss:0.6551666855812073\n",
      "Epoch 0[112/17270] Time:0.222, Train Loss:1.045395851135254\n",
      "Epoch 0[113/17270] Time:0.245, Train Loss:0.7259047627449036\n",
      "Epoch 0[114/17270] Time:0.235, Train Loss:0.6252555251121521\n",
      "Epoch 0[115/17270] Time:0.226, Train Loss:0.8375182747840881\n",
      "Epoch 0[116/17270] Time:0.24, Train Loss:0.8466659784317017\n",
      "Epoch 0[117/17270] Time:0.241, Train Loss:0.5465640425682068\n",
      "Epoch 0[118/17270] Time:0.235, Train Loss:1.0742344856262207\n",
      "Epoch 0[119/17270] Time:0.232, Train Loss:1.087174654006958\n",
      "Epoch 0[120/17270] Time:0.247, Train Loss:1.4370667934417725\n",
      "Epoch 0[121/17270] Time:0.234, Train Loss:1.620637059211731\n",
      "Epoch 0[122/17270] Time:0.234, Train Loss:1.0186160802841187\n",
      "Epoch 0[123/17270] Time:0.228, Train Loss:0.8541157841682434\n",
      "Epoch 0[124/17270] Time:0.239, Train Loss:0.8878384232521057\n",
      "Epoch 0[125/17270] Time:0.232, Train Loss:0.9101133346557617\n",
      "Epoch 0[126/17270] Time:0.235, Train Loss:1.170335054397583\n",
      "Epoch 0[127/17270] Time:0.239, Train Loss:0.8010173439979553\n",
      "Epoch 0[128/17270] Time:0.229, Train Loss:0.6433055400848389\n",
      "Epoch 0[129/17270] Time:0.242, Train Loss:1.1565682888031006\n",
      "Epoch 0[130/17270] Time:0.236, Train Loss:0.7773373126983643\n",
      "Epoch 0[131/17270] Time:0.235, Train Loss:0.7329484820365906\n",
      "Epoch 0[132/17270] Time:0.239, Train Loss:0.9548083543777466\n",
      "Epoch 0[133/17270] Time:0.239, Train Loss:1.3046866655349731\n",
      "Epoch 0[134/17270] Time:0.238, Train Loss:0.8305011987686157\n",
      "Epoch 0[135/17270] Time:0.238, Train Loss:0.7549077272415161\n",
      "Epoch 0[136/17270] Time:0.239, Train Loss:0.6883453130722046\n",
      "Epoch 0[137/17270] Time:0.238, Train Loss:1.047472596168518\n",
      "Epoch 0[138/17270] Time:0.238, Train Loss:0.9553502798080444\n",
      "Epoch 0[139/17270] Time:0.251, Train Loss:1.203648567199707\n",
      "Epoch 0[140/17270] Time:0.235, Train Loss:1.0057060718536377\n",
      "Epoch 0[141/17270] Time:0.241, Train Loss:2.3141086101531982\n",
      "Epoch 0[142/17270] Time:0.236, Train Loss:0.7892801761627197\n",
      "Epoch 0[143/17270] Time:0.233, Train Loss:0.9738410711288452\n",
      "Epoch 0[144/17270] Time:0.248, Train Loss:0.9139980673789978\n",
      "Epoch 0[145/17270] Time:0.247, Train Loss:0.5896517038345337\n",
      "Epoch 0[146/17270] Time:0.231, Train Loss:0.8573489785194397\n",
      "Epoch 0[147/17270] Time:0.26, Train Loss:1.0554786920547485\n",
      "Epoch 0[148/17270] Time:0.23, Train Loss:0.7237523794174194\n",
      "Epoch 0[149/17270] Time:0.23, Train Loss:1.233630657196045\n",
      "Epoch 0[150/17270] Time:0.227, Train Loss:1.1676366329193115\n",
      "Epoch 0[151/17270] Time:0.239, Train Loss:0.8754055500030518\n",
      "Epoch 0[152/17270] Time:0.24, Train Loss:1.0279873609542847\n",
      "Epoch 0[153/17270] Time:0.239, Train Loss:0.6924943327903748\n",
      "Epoch 0[154/17270] Time:0.231, Train Loss:0.9260651469230652\n",
      "Epoch 0[155/17270] Time:0.237, Train Loss:0.8636001348495483\n",
      "Epoch 0[156/17270] Time:0.24, Train Loss:1.1269373893737793\n",
      "Epoch 0[157/17270] Time:0.233, Train Loss:0.49705153703689575\n",
      "Epoch 0[158/17270] Time:0.251, Train Loss:0.8505198955535889\n",
      "Epoch 0[159/17270] Time:0.235, Train Loss:1.0456387996673584\n",
      "Epoch 0[160/17270] Time:0.243, Train Loss:0.7622299790382385\n",
      "Epoch 0[161/17270] Time:0.238, Train Loss:0.6902860999107361\n",
      "Epoch 0[162/17270] Time:0.229, Train Loss:1.4535651206970215\n",
      "Epoch 0[163/17270] Time:0.232, Train Loss:1.1306232213974\n",
      "Epoch 0[164/17270] Time:0.232, Train Loss:1.1363799571990967\n",
      "Epoch 0[165/17270] Time:0.244, Train Loss:0.6357683539390564\n",
      "Epoch 0[166/17270] Time:0.233, Train Loss:0.8998368978500366\n",
      "Epoch 0[167/17270] Time:0.233, Train Loss:0.816388726234436\n",
      "Epoch 0[168/17270] Time:0.235, Train Loss:0.7160202860832214\n",
      "Epoch 0[169/17270] Time:0.245, Train Loss:0.6432898640632629\n",
      "Epoch 0[170/17270] Time:0.243, Train Loss:0.9545364379882812\n",
      "Epoch 0[171/17270] Time:0.234, Train Loss:1.2228692770004272\n",
      "Epoch 0[172/17270] Time:0.227, Train Loss:0.7348418831825256\n",
      "Epoch 0[173/17270] Time:0.231, Train Loss:0.8444120287895203\n",
      "Epoch 0[174/17270] Time:0.233, Train Loss:0.7941156625747681\n",
      "Epoch 0[175/17270] Time:0.236, Train Loss:0.7747955322265625\n",
      "Epoch 0[176/17270] Time:0.22, Train Loss:0.5612068772315979\n",
      "Epoch 0[177/17270] Time:0.249, Train Loss:0.8570321798324585\n",
      "Epoch 0[178/17270] Time:0.224, Train Loss:0.9655230045318604\n",
      "Epoch 0[179/17270] Time:0.244, Train Loss:1.0634719133377075\n",
      "Epoch 0[180/17270] Time:0.241, Train Loss:1.1217474937438965\n",
      "Epoch 0[181/17270] Time:0.224, Train Loss:0.6535491943359375\n",
      "Epoch 0[182/17270] Time:0.247, Train Loss:0.8411436080932617\n",
      "Epoch 0[183/17270] Time:0.234, Train Loss:0.6927375197410583\n",
      "Epoch 0[184/17270] Time:0.235, Train Loss:0.7560494542121887\n",
      "Epoch 0[185/17270] Time:0.232, Train Loss:0.6650257706642151\n",
      "Epoch 0[186/17270] Time:0.227, Train Loss:0.6306232213973999\n",
      "Epoch 0[187/17270] Time:0.249, Train Loss:0.5886232256889343\n",
      "Epoch 0[188/17270] Time:0.231, Train Loss:0.6939083337783813\n",
      "Epoch 0[189/17270] Time:0.223, Train Loss:0.7965633869171143\n",
      "Epoch 0[190/17270] Time:0.234, Train Loss:0.8822793364524841\n",
      "Epoch 0[191/17270] Time:0.234, Train Loss:0.680098831653595\n",
      "Epoch 0[192/17270] Time:0.24, Train Loss:0.751453161239624\n",
      "Epoch 0[193/17270] Time:0.238, Train Loss:1.246008276939392\n",
      "Epoch 0[194/17270] Time:0.238, Train Loss:1.0076788663864136\n",
      "Epoch 0[195/17270] Time:0.239, Train Loss:0.6718184351921082\n",
      "Epoch 0[196/17270] Time:0.238, Train Loss:0.6108605861663818\n",
      "Epoch 0[197/17270] Time:0.243, Train Loss:1.468510627746582\n",
      "Epoch 0[198/17270] Time:0.233, Train Loss:0.825639009475708\n",
      "Epoch 0[199/17270] Time:0.235, Train Loss:0.6534042954444885\n",
      "Epoch 0[200/17270] Time:0.234, Train Loss:1.2085031270980835\n",
      "Epoch 0[201/17270] Time:0.229, Train Loss:1.1753222942352295\n",
      "Epoch 0[202/17270] Time:0.228, Train Loss:1.04604172706604\n",
      "Epoch 0[203/17270] Time:0.234, Train Loss:0.7707878947257996\n",
      "Epoch 0[204/17270] Time:0.239, Train Loss:0.6320422291755676\n",
      "Epoch 0[205/17270] Time:0.239, Train Loss:0.7432731986045837\n",
      "Epoch 0[206/17270] Time:0.238, Train Loss:0.7936645150184631\n",
      "Epoch 0[207/17270] Time:0.237, Train Loss:1.1748207807540894\n",
      "Epoch 0[208/17270] Time:0.257, Train Loss:0.7921273708343506\n",
      "Epoch 0[209/17270] Time:0.244, Train Loss:0.8227359652519226\n",
      "Epoch 0[210/17270] Time:0.231, Train Loss:0.6015108227729797\n",
      "Epoch 0[211/17270] Time:0.232, Train Loss:0.7298068404197693\n",
      "Epoch 0[212/17270] Time:0.236, Train Loss:1.2231006622314453\n",
      "Epoch 0[213/17270] Time:0.226, Train Loss:0.9001843929290771\n",
      "Epoch 0[214/17270] Time:0.227, Train Loss:0.793353259563446\n",
      "Epoch 0[215/17270] Time:0.238, Train Loss:0.6978699564933777\n",
      "Epoch 0[216/17270] Time:0.238, Train Loss:1.10932195186615\n",
      "Epoch 0[217/17270] Time:0.238, Train Loss:0.6816186904907227\n",
      "Epoch 0[218/17270] Time:0.233, Train Loss:1.1010127067565918\n",
      "Epoch 0[219/17270] Time:0.229, Train Loss:0.758958101272583\n",
      "Epoch 0[220/17270] Time:0.23, Train Loss:1.072750210762024\n",
      "Epoch 0[221/17270] Time:0.231, Train Loss:1.2611180543899536\n",
      "Epoch 0[222/17270] Time:0.232, Train Loss:0.4896393418312073\n",
      "Epoch 0[223/17270] Time:0.233, Train Loss:0.6048256754875183\n",
      "Epoch 0[224/17270] Time:0.233, Train Loss:0.8204235434532166\n",
      "Epoch 0[225/17270] Time:0.23, Train Loss:0.8709481954574585\n",
      "Epoch 0[226/17270] Time:0.25, Train Loss:0.6883434653282166\n",
      "Epoch 0[227/17270] Time:0.233, Train Loss:0.5720412731170654\n",
      "Epoch 0[228/17270] Time:0.234, Train Loss:0.8115900754928589\n",
      "Epoch 0[229/17270] Time:0.224, Train Loss:0.808903157711029\n",
      "Epoch 0[230/17270] Time:0.262, Train Loss:1.0196318626403809\n",
      "Epoch 0[231/17270] Time:0.231, Train Loss:1.6063793897628784\n",
      "Epoch 0[232/17270] Time:0.235, Train Loss:1.0886954069137573\n",
      "Epoch 0[233/17270] Time:0.233, Train Loss:0.6819049119949341\n",
      "Epoch 0[234/17270] Time:0.239, Train Loss:0.727798342704773\n",
      "Epoch 0[235/17270] Time:0.223, Train Loss:0.7690646648406982\n",
      "Epoch 0[236/17270] Time:0.244, Train Loss:1.2300077676773071\n",
      "Epoch 0[237/17270] Time:0.242, Train Loss:0.7050730586051941\n",
      "Epoch 0[238/17270] Time:0.23, Train Loss:0.8778544068336487\n",
      "Epoch 0[239/17270] Time:0.235, Train Loss:0.7888586521148682\n",
      "Epoch 0[240/17270] Time:0.226, Train Loss:1.2439097166061401\n",
      "Epoch 0[241/17270] Time:0.292, Train Loss:0.7422698140144348\n",
      "Epoch 0[242/17270] Time:0.228, Train Loss:0.8279880881309509\n",
      "Epoch 0[243/17270] Time:0.236, Train Loss:0.8232905268669128\n",
      "Epoch 0[244/17270] Time:0.239, Train Loss:0.6111530661582947\n",
      "Epoch 0[245/17270] Time:0.238, Train Loss:0.6358211636543274\n",
      "Epoch 0[246/17270] Time:0.239, Train Loss:0.8538414239883423\n",
      "Epoch 0[247/17270] Time:0.231, Train Loss:1.1378191709518433\n",
      "Epoch 0[248/17270] Time:0.228, Train Loss:0.7152085304260254\n",
      "Epoch 0[249/17270] Time:0.228, Train Loss:0.5681717991828918\n",
      "Epoch 0[250/17270] Time:0.236, Train Loss:1.0761665105819702\n",
      "Epoch 0[251/17270] Time:0.239, Train Loss:0.7684378623962402\n",
      "Epoch 0[252/17270] Time:0.237, Train Loss:1.527919054031372\n",
      "Epoch 0[253/17270] Time:0.237, Train Loss:0.7803337574005127\n",
      "Epoch 0[254/17270] Time:0.233, Train Loss:0.96537846326828\n",
      "Epoch 0[255/17270] Time:0.238, Train Loss:0.7246845960617065\n",
      "Epoch 0[256/17270] Time:0.239, Train Loss:0.7622313499450684\n",
      "Epoch 0[257/17270] Time:0.23, Train Loss:0.8046057224273682\n",
      "Epoch 0[258/17270] Time:0.24, Train Loss:0.47479048371315\n",
      "Epoch 0[259/17270] Time:0.238, Train Loss:0.5779807567596436\n",
      "Epoch 0[260/17270] Time:0.231, Train Loss:0.5918170809745789\n",
      "Epoch 0[261/17270] Time:0.238, Train Loss:0.6230881810188293\n",
      "Epoch 0[262/17270] Time:0.239, Train Loss:0.6669296622276306\n",
      "Epoch 0[263/17270] Time:0.232, Train Loss:0.6454395055770874\n",
      "Epoch 0[264/17270] Time:0.239, Train Loss:0.46750158071517944\n",
      "Epoch 0[265/17270] Time:0.238, Train Loss:0.9648094177246094\n",
      "Epoch 0[266/17270] Time:0.239, Train Loss:0.5460982322692871\n",
      "Epoch 0[267/17270] Time:0.254, Train Loss:0.8049784302711487\n",
      "Epoch 0[268/17270] Time:0.233, Train Loss:0.3799833059310913\n",
      "Epoch 0[269/17270] Time:0.235, Train Loss:0.7711942791938782\n",
      "Epoch 0[270/17270] Time:0.243, Train Loss:0.41631099581718445\n",
      "Epoch 0[271/17270] Time:0.233, Train Loss:0.6085891127586365\n",
      "Epoch 0[272/17270] Time:0.242, Train Loss:0.6478586196899414\n",
      "Epoch 0[273/17270] Time:0.254, Train Loss:0.7972990274429321\n",
      "Epoch 0[274/17270] Time:0.242, Train Loss:0.5035616159439087\n",
      "Epoch 0[275/17270] Time:0.229, Train Loss:1.8529534339904785\n",
      "Epoch 0[276/17270] Time:0.236, Train Loss:0.539709210395813\n",
      "Epoch 0[277/17270] Time:0.24, Train Loss:0.9171947240829468\n",
      "Epoch 0[278/17270] Time:0.232, Train Loss:1.0018517971038818\n",
      "Epoch 0[279/17270] Time:0.239, Train Loss:1.1104084253311157\n",
      "Epoch 0[280/17270] Time:0.235, Train Loss:1.0304373502731323\n",
      "Epoch 0[281/17270] Time:0.234, Train Loss:0.7137597799301147\n",
      "Epoch 0[282/17270] Time:0.235, Train Loss:0.7264828085899353\n",
      "Epoch 0[283/17270] Time:0.232, Train Loss:1.3421120643615723\n",
      "Epoch 0[284/17270] Time:0.237, Train Loss:0.778809666633606\n",
      "Epoch 0[285/17270] Time:0.25, Train Loss:0.44401928782463074\n",
      "Epoch 0[286/17270] Time:0.229, Train Loss:0.8032782077789307\n",
      "Epoch 0[287/17270] Time:0.229, Train Loss:1.061732530593872\n",
      "Epoch 0[288/17270] Time:0.232, Train Loss:0.6302310228347778\n",
      "Epoch 0[289/17270] Time:0.237, Train Loss:0.663488507270813\n",
      "Epoch 0[290/17270] Time:0.231, Train Loss:0.8868283033370972\n",
      "Epoch 0[291/17270] Time:0.241, Train Loss:0.863460123538971\n",
      "Epoch 0[292/17270] Time:0.233, Train Loss:0.9540258646011353\n",
      "Epoch 0[293/17270] Time:0.231, Train Loss:0.670315146446228\n",
      "Epoch 0[294/17270] Time:0.232, Train Loss:0.878717303276062\n",
      "Epoch 0[295/17270] Time:0.238, Train Loss:0.886448860168457\n",
      "Epoch 0[296/17270] Time:0.239, Train Loss:0.616611897945404\n",
      "Epoch 0[297/17270] Time:0.239, Train Loss:0.7294908761978149\n",
      "Epoch 0[298/17270] Time:0.229, Train Loss:1.087727427482605\n",
      "Epoch 0[299/17270] Time:0.23, Train Loss:0.7408059239387512\n",
      "Epoch 0[300/17270] Time:0.251, Train Loss:1.224677324295044\n",
      "Epoch 0[301/17270] Time:0.234, Train Loss:1.2254999876022339\n",
      "Epoch 0[302/17270] Time:0.231, Train Loss:0.8151794672012329\n",
      "Epoch 0[303/17270] Time:0.235, Train Loss:0.4663652777671814\n",
      "Epoch 0[304/17270] Time:0.233, Train Loss:0.6208787560462952\n",
      "Epoch 0[305/17270] Time:0.241, Train Loss:0.5315911769866943\n",
      "Epoch 0[306/17270] Time:0.234, Train Loss:0.7013933658599854\n",
      "Epoch 0[307/17270] Time:0.225, Train Loss:1.0882105827331543\n",
      "Epoch 0[308/17270] Time:0.236, Train Loss:0.7761064767837524\n",
      "Epoch 0[309/17270] Time:0.236, Train Loss:0.9022903442382812\n",
      "Epoch 0[310/17270] Time:0.224, Train Loss:0.4261500835418701\n",
      "Epoch 0[311/17270] Time:0.232, Train Loss:0.5166952013969421\n",
      "Epoch 0[312/17270] Time:0.244, Train Loss:0.678957998752594\n",
      "Epoch 0[313/17270] Time:0.234, Train Loss:0.5638529062271118\n",
      "Epoch 0[314/17270] Time:0.231, Train Loss:0.8305312991142273\n",
      "Epoch 0[315/17270] Time:0.227, Train Loss:1.0343865156173706\n",
      "Epoch 0[316/17270] Time:0.237, Train Loss:0.4566938877105713\n",
      "Epoch 0[317/17270] Time:0.239, Train Loss:0.726469099521637\n",
      "Epoch 0[318/17270] Time:0.234, Train Loss:1.192049503326416\n",
      "Epoch 0[319/17270] Time:0.233, Train Loss:0.7411934733390808\n",
      "Epoch 0[320/17270] Time:0.23, Train Loss:0.8415389657020569\n",
      "Epoch 0[321/17270] Time:0.235, Train Loss:1.2107876539230347\n",
      "Epoch 0[322/17270] Time:0.229, Train Loss:0.5303238034248352\n",
      "Epoch 0[323/17270] Time:0.235, Train Loss:1.0256984233856201\n",
      "Epoch 0[324/17270] Time:0.232, Train Loss:1.3860913515090942\n",
      "Epoch 0[325/17270] Time:0.249, Train Loss:0.8489858508110046\n",
      "Epoch 0[326/17270] Time:0.236, Train Loss:0.5095906257629395\n",
      "Epoch 0[327/17270] Time:0.242, Train Loss:1.2719385623931885\n",
      "Epoch 0[328/17270] Time:0.229, Train Loss:1.2109163999557495\n",
      "Epoch 0[329/17270] Time:0.242, Train Loss:1.2897028923034668\n",
      "Epoch 0[330/17270] Time:0.235, Train Loss:1.4026225805282593\n",
      "Epoch 0[331/17270] Time:0.231, Train Loss:1.4018304347991943\n",
      "Epoch 0[332/17270] Time:0.229, Train Loss:1.134263277053833\n",
      "Epoch 0[333/17270] Time:0.237, Train Loss:0.9404599666595459\n",
      "Epoch 0[334/17270] Time:0.241, Train Loss:0.7838706970214844\n",
      "Epoch 0[335/17270] Time:0.241, Train Loss:0.8928400874137878\n",
      "Epoch 0[336/17270] Time:0.241, Train Loss:0.7081234455108643\n",
      "Epoch 0[337/17270] Time:0.238, Train Loss:0.642905056476593\n",
      "Epoch 0[338/17270] Time:0.239, Train Loss:0.7656955718994141\n",
      "Epoch 0[339/17270] Time:0.243, Train Loss:1.4717351198196411\n",
      "Epoch 0[340/17270] Time:0.237, Train Loss:0.7571471333503723\n",
      "Epoch 0[341/17270] Time:0.242, Train Loss:1.0954065322875977\n",
      "Epoch 0[342/17270] Time:0.237, Train Loss:0.7128248810768127\n",
      "Epoch 0[343/17270] Time:0.243, Train Loss:0.6615755558013916\n",
      "Epoch 0[344/17270] Time:0.235, Train Loss:0.5325974225997925\n",
      "Epoch 0[345/17270] Time:0.238, Train Loss:0.5849173069000244\n",
      "Epoch 0[346/17270] Time:0.233, Train Loss:1.0105953216552734\n",
      "Epoch 0[347/17270] Time:0.238, Train Loss:0.7814516425132751\n",
      "Epoch 0[348/17270] Time:0.241, Train Loss:0.9753583073616028\n",
      "Epoch 0[349/17270] Time:0.229, Train Loss:0.9033772945404053\n",
      "Epoch 0[350/17270] Time:0.23, Train Loss:1.064530611038208\n",
      "Epoch 0[351/17270] Time:0.235, Train Loss:0.8106118440628052\n",
      "Epoch 0[352/17270] Time:0.232, Train Loss:1.090415120124817\n",
      "Epoch 0[353/17270] Time:0.235, Train Loss:0.8041024804115295\n",
      "Epoch 0[354/17270] Time:0.249, Train Loss:0.712218165397644\n",
      "Epoch 0[355/17270] Time:0.237, Train Loss:1.47611665725708\n",
      "Epoch 0[356/17270] Time:0.234, Train Loss:0.67139732837677\n",
      "Epoch 0[357/17270] Time:0.223, Train Loss:0.7995496392250061\n",
      "Epoch 0[358/17270] Time:0.242, Train Loss:0.7065251469612122\n",
      "Epoch 0[359/17270] Time:0.241, Train Loss:0.771025538444519\n",
      "Epoch 0[360/17270] Time:0.235, Train Loss:1.2910741567611694\n",
      "Epoch 0[361/17270] Time:0.223, Train Loss:0.8331928253173828\n",
      "Epoch 0[362/17270] Time:0.245, Train Loss:0.7894087433815002\n",
      "Epoch 0[363/17270] Time:0.238, Train Loss:0.6958900690078735\n",
      "Epoch 0[364/17270] Time:0.223, Train Loss:1.6223169565200806\n",
      "Epoch 0[365/17270] Time:0.229, Train Loss:0.5872092247009277\n",
      "Epoch 0[366/17270] Time:0.23, Train Loss:1.101772427558899\n",
      "Epoch 0[367/17270] Time:0.228, Train Loss:0.7054900527000427\n",
      "Epoch 0[368/17270] Time:0.233, Train Loss:0.5635585784912109\n",
      "Epoch 0[369/17270] Time:0.241, Train Loss:0.7415751814842224\n",
      "Epoch 0[370/17270] Time:0.236, Train Loss:0.6781594753265381\n",
      "Epoch 0[371/17270] Time:0.229, Train Loss:0.6424320340156555\n",
      "Epoch 0[372/17270] Time:0.239, Train Loss:1.1693023443222046\n",
      "Epoch 0[373/17270] Time:0.235, Train Loss:0.5661827921867371\n",
      "Epoch 0[374/17270] Time:0.231, Train Loss:0.9535293579101562\n",
      "Epoch 0[375/17270] Time:0.236, Train Loss:0.8170997500419617\n",
      "Epoch 0[376/17270] Time:0.229, Train Loss:0.7039181590080261\n",
      "Epoch 0[377/17270] Time:0.229, Train Loss:1.0163756608963013\n",
      "Epoch 0[378/17270] Time:0.235, Train Loss:0.8425857424736023\n",
      "Epoch 0[379/17270] Time:0.227, Train Loss:1.3669487237930298\n",
      "Epoch 0[380/17270] Time:0.232, Train Loss:0.6493147015571594\n",
      "Epoch 0[381/17270] Time:0.237, Train Loss:0.8534462451934814\n",
      "Epoch 0[382/17270] Time:0.222, Train Loss:0.682373046875\n",
      "Epoch 0[383/17270] Time:0.237, Train Loss:0.5981072783470154\n",
      "Epoch 0[384/17270] Time:0.24, Train Loss:0.7163612246513367\n",
      "Epoch 0[385/17270] Time:0.243, Train Loss:0.6281450986862183\n",
      "Epoch 0[386/17270] Time:0.229, Train Loss:0.7575739026069641\n",
      "Epoch 0[387/17270] Time:0.236, Train Loss:0.5712724328041077\n",
      "Epoch 0[388/17270] Time:0.238, Train Loss:1.1078882217407227\n",
      "Epoch 0[389/17270] Time:0.244, Train Loss:0.7422533631324768\n",
      "Epoch 0[390/17270] Time:0.248, Train Loss:0.7475249171257019\n",
      "Epoch 0[391/17270] Time:0.234, Train Loss:0.7247697710990906\n",
      "Epoch 0[392/17270] Time:0.241, Train Loss:0.8764930367469788\n",
      "Epoch 0[393/17270] Time:0.24, Train Loss:0.9139115214347839\n",
      "Epoch 0[394/17270] Time:0.246, Train Loss:1.2388314008712769\n",
      "Epoch 0[395/17270] Time:0.221, Train Loss:0.7925724387168884\n",
      "Epoch 0[396/17270] Time:0.245, Train Loss:0.837424635887146\n",
      "Epoch 0[397/17270] Time:0.242, Train Loss:0.885320246219635\n",
      "Epoch 0[398/17270] Time:0.237, Train Loss:0.6090659499168396\n",
      "Epoch 0[399/17270] Time:0.234, Train Loss:0.7714646458625793\n",
      "Epoch 0[400/17270] Time:0.231, Train Loss:3.5763165950775146\n",
      "Epoch 0[401/17270] Time:0.245, Train Loss:0.7895557284355164\n",
      "Epoch 0[402/17270] Time:0.235, Train Loss:1.0847564935684204\n",
      "Epoch 0[403/17270] Time:0.236, Train Loss:1.0187221765518188\n",
      "Epoch 0[404/17270] Time:0.234, Train Loss:1.0736966133117676\n",
      "Epoch 0[405/17270] Time:0.221, Train Loss:1.277461051940918\n",
      "Epoch 0[406/17270] Time:0.248, Train Loss:1.3776754140853882\n",
      "Epoch 0[407/17270] Time:0.235, Train Loss:1.5372740030288696\n",
      "Epoch 0[408/17270] Time:0.232, Train Loss:1.367997169494629\n",
      "Epoch 0[409/17270] Time:0.222, Train Loss:1.342759370803833\n",
      "Epoch 0[410/17270] Time:0.228, Train Loss:1.5947662591934204\n",
      "Epoch 0[411/17270] Time:0.244, Train Loss:1.221289873123169\n",
      "Epoch 0[412/17270] Time:0.233, Train Loss:1.1306639909744263\n",
      "Epoch 0[413/17270] Time:0.233, Train Loss:1.0485118627548218\n",
      "Epoch 0[414/17270] Time:0.231, Train Loss:0.8943440914154053\n",
      "Epoch 0[415/17270] Time:0.235, Train Loss:0.8924826979637146\n",
      "Epoch 0[416/17270] Time:0.223, Train Loss:1.1163585186004639\n",
      "Epoch 0[417/17270] Time:0.24, Train Loss:1.1249762773513794\n",
      "Epoch 0[418/17270] Time:0.232, Train Loss:0.9488525986671448\n",
      "Epoch 0[419/17270] Time:0.236, Train Loss:0.940089762210846\n",
      "Epoch 0[420/17270] Time:0.227, Train Loss:1.0773125886917114\n",
      "Epoch 0[421/17270] Time:0.245, Train Loss:0.8831491470336914\n",
      "Epoch 0[422/17270] Time:0.235, Train Loss:0.9052560329437256\n",
      "Epoch 0[423/17270] Time:0.227, Train Loss:0.907465398311615\n",
      "Epoch 0[424/17270] Time:0.229, Train Loss:0.5144796371459961\n",
      "Epoch 0[425/17270] Time:0.234, Train Loss:1.1653645038604736\n",
      "Epoch 0[426/17270] Time:0.241, Train Loss:0.6093915700912476\n",
      "Epoch 0[427/17270] Time:0.244, Train Loss:0.8180475234985352\n",
      "Epoch 0[428/17270] Time:0.237, Train Loss:0.7378346920013428\n",
      "Epoch 0[429/17270] Time:0.236, Train Loss:0.6292417645454407\n",
      "Epoch 0[430/17270] Time:0.232, Train Loss:0.6207450032234192\n",
      "Epoch 0[431/17270] Time:0.231, Train Loss:0.7540870904922485\n",
      "Epoch 0[432/17270] Time:0.237, Train Loss:0.8111182451248169\n",
      "Epoch 0[433/17270] Time:0.24, Train Loss:0.7415111660957336\n",
      "Epoch 0[434/17270] Time:0.243, Train Loss:0.7144405841827393\n",
      "Epoch 0[435/17270] Time:0.244, Train Loss:1.024082064628601\n",
      "Epoch 0[436/17270] Time:0.238, Train Loss:0.9124312400817871\n",
      "Epoch 0[437/17270] Time:0.234, Train Loss:0.7678646445274353\n",
      "Epoch 0[438/17270] Time:0.235, Train Loss:0.9737381339073181\n",
      "Epoch 0[439/17270] Time:0.23, Train Loss:0.7004474997520447\n",
      "Epoch 0[440/17270] Time:0.239, Train Loss:0.6052734851837158\n",
      "Epoch 0[441/17270] Time:0.237, Train Loss:0.9592990279197693\n",
      "Epoch 0[442/17270] Time:0.242, Train Loss:0.9027539491653442\n",
      "Epoch 0[443/17270] Time:0.227, Train Loss:0.6950725317001343\n",
      "Epoch 0[444/17270] Time:0.243, Train Loss:1.3669283390045166\n",
      "Epoch 0[445/17270] Time:0.235, Train Loss:0.7380365133285522\n",
      "Epoch 0[446/17270] Time:0.235, Train Loss:0.8646096587181091\n",
      "Epoch 0[447/17270] Time:0.237, Train Loss:0.5888846516609192\n",
      "Epoch 0[448/17270] Time:0.232, Train Loss:0.877777099609375\n",
      "Epoch 0[449/17270] Time:0.234, Train Loss:0.7796215415000916\n",
      "Epoch 0[450/17270] Time:0.225, Train Loss:1.0136573314666748\n",
      "Epoch 0[451/17270] Time:0.24, Train Loss:0.643234372138977\n",
      "Epoch 0[452/17270] Time:0.231, Train Loss:1.0004380941390991\n",
      "Epoch 0[453/17270] Time:0.244, Train Loss:0.5680091977119446\n",
      "Epoch 0[454/17270] Time:0.235, Train Loss:0.46502983570098877\n",
      "Epoch 0[455/17270] Time:0.227, Train Loss:0.677453875541687\n",
      "Epoch 0[456/17270] Time:0.239, Train Loss:0.6717632412910461\n",
      "Epoch 0[457/17270] Time:0.243, Train Loss:1.376320719718933\n",
      "Epoch 0[458/17270] Time:0.233, Train Loss:0.5405286550521851\n",
      "Epoch 0[459/17270] Time:0.234, Train Loss:0.6411906480789185\n",
      "Epoch 0[460/17270] Time:0.223, Train Loss:0.44035419821739197\n",
      "Epoch 0[461/17270] Time:0.239, Train Loss:0.9725857377052307\n",
      "Epoch 0[462/17270] Time:0.244, Train Loss:1.4583266973495483\n",
      "Epoch 0[463/17270] Time:0.223, Train Loss:0.9241895079612732\n",
      "Epoch 0[464/17270] Time:0.232, Train Loss:0.5143035054206848\n",
      "Epoch 0[465/17270] Time:0.24, Train Loss:0.34136688709259033\n",
      "Epoch 0[466/17270] Time:0.247, Train Loss:0.685734212398529\n",
      "Epoch 0[467/17270] Time:0.238, Train Loss:0.660937488079071\n",
      "Epoch 0[468/17270] Time:0.223, Train Loss:0.8066362142562866\n",
      "Epoch 0[469/17270] Time:0.24, Train Loss:0.6238647699356079\n",
      "Epoch 0[470/17270] Time:0.245, Train Loss:0.7303565740585327\n",
      "Epoch 0[471/17270] Time:0.234, Train Loss:0.6910566091537476\n",
      "Epoch 0[472/17270] Time:0.25, Train Loss:0.8803034424781799\n",
      "Epoch 0[473/17270] Time:0.222, Train Loss:0.6673383116722107\n",
      "Epoch 0[474/17270] Time:0.233, Train Loss:0.3781542181968689\n",
      "Epoch 0[475/17270] Time:0.241, Train Loss:0.3989679217338562\n",
      "Epoch 0[476/17270] Time:0.237, Train Loss:0.8288647532463074\n",
      "Epoch 0[477/17270] Time:0.24, Train Loss:0.4842236042022705\n",
      "Epoch 0[478/17270] Time:0.231, Train Loss:1.1207174062728882\n",
      "Epoch 0[479/17270] Time:0.24, Train Loss:0.6695526838302612\n",
      "Epoch 0[480/17270] Time:0.233, Train Loss:1.329555630683899\n",
      "Epoch 0[481/17270] Time:0.236, Train Loss:0.9232788681983948\n",
      "Epoch 0[482/17270] Time:0.225, Train Loss:0.810207724571228\n",
      "Epoch 0[483/17270] Time:0.246, Train Loss:0.7287318706512451\n",
      "Epoch 0[484/17270] Time:0.244, Train Loss:0.9203544855117798\n",
      "Epoch 0[485/17270] Time:0.235, Train Loss:1.39277184009552\n",
      "Epoch 0[486/17270] Time:0.231, Train Loss:0.5591961741447449\n",
      "Epoch 0[487/17270] Time:0.242, Train Loss:0.832014799118042\n",
      "Epoch 0[488/17270] Time:0.238, Train Loss:0.9038182497024536\n",
      "Epoch 0[489/17270] Time:0.237, Train Loss:0.7240065932273865\n",
      "Epoch 0[490/17270] Time:0.227, Train Loss:0.9534786343574524\n",
      "Epoch 0[491/17270] Time:0.231, Train Loss:0.9340898990631104\n",
      "Epoch 0[492/17270] Time:0.234, Train Loss:0.9276504516601562\n",
      "Epoch 0[493/17270] Time:0.231, Train Loss:0.606322169303894\n",
      "Epoch 0[494/17270] Time:0.233, Train Loss:0.6710365414619446\n",
      "Epoch 0[495/17270] Time:0.238, Train Loss:0.6352129578590393\n",
      "Epoch 0[496/17270] Time:0.244, Train Loss:0.768072247505188\n",
      "Epoch 0[497/17270] Time:0.236, Train Loss:0.984158456325531\n",
      "Epoch 0[498/17270] Time:0.242, Train Loss:0.5558145642280579\n",
      "Epoch 0[499/17270] Time:0.235, Train Loss:0.575007438659668\n",
      "Epoch 0[500/17270] Time:0.249, Train Loss:0.6406490802764893\n",
      "Epoch 0[501/17270] Time:0.228, Train Loss:0.7127458453178406\n",
      "Epoch 0[502/17270] Time:0.246, Train Loss:1.052141547203064\n",
      "Epoch 0[503/17270] Time:0.224, Train Loss:0.6840620636940002\n",
      "Epoch 0[504/17270] Time:0.246, Train Loss:0.6069768071174622\n",
      "Epoch 0[505/17270] Time:0.235, Train Loss:0.734032154083252\n",
      "Epoch 0[506/17270] Time:0.222, Train Loss:0.8167599439620972\n",
      "Epoch 0[507/17270] Time:0.23, Train Loss:0.7749404311180115\n",
      "Epoch 0[508/17270] Time:0.245, Train Loss:0.7231065034866333\n",
      "Epoch 0[509/17270] Time:0.224, Train Loss:0.9071543216705322\n",
      "Epoch 0[510/17270] Time:0.232, Train Loss:0.6504047513008118\n",
      "Epoch 0[511/17270] Time:0.237, Train Loss:0.5526176691055298\n",
      "Epoch 0[512/17270] Time:0.229, Train Loss:1.1426966190338135\n",
      "Epoch 0[513/17270] Time:0.266, Train Loss:0.7699944972991943\n",
      "Epoch 0[514/17270] Time:0.231, Train Loss:0.7997992038726807\n",
      "Epoch 0[515/17270] Time:0.233, Train Loss:0.6396803259849548\n",
      "Epoch 0[516/17270] Time:0.248, Train Loss:1.2240068912506104\n",
      "Epoch 0[517/17270] Time:0.232, Train Loss:0.6179737448692322\n",
      "Epoch 0[518/17270] Time:0.226, Train Loss:0.8633853197097778\n",
      "Epoch 0[519/17270] Time:0.238, Train Loss:0.6719363331794739\n",
      "Epoch 0[520/17270] Time:0.24, Train Loss:1.0871086120605469\n",
      "Epoch 0[521/17270] Time:0.226, Train Loss:0.6582530736923218\n",
      "Epoch 0[522/17270] Time:0.232, Train Loss:0.6764435768127441\n",
      "Epoch 0[523/17270] Time:0.24, Train Loss:0.7136471271514893\n",
      "Epoch 0[524/17270] Time:0.226, Train Loss:0.8466125726699829\n",
      "Epoch 0[525/17270] Time:0.236, Train Loss:0.6421672701835632\n",
      "Epoch 0[526/17270] Time:0.24, Train Loss:0.9089325070381165\n",
      "Epoch 0[527/17270] Time:0.242, Train Loss:0.5546792149543762\n",
      "Epoch 0[528/17270] Time:0.238, Train Loss:0.6507607698440552\n",
      "Epoch 0[529/17270] Time:0.233, Train Loss:0.8766663074493408\n",
      "Epoch 0[530/17270] Time:0.239, Train Loss:1.4732255935668945\n",
      "Epoch 0[531/17270] Time:0.23, Train Loss:0.7699092626571655\n",
      "Epoch 0[532/17270] Time:0.245, Train Loss:0.8005948066711426\n",
      "Epoch 0[533/17270] Time:0.225, Train Loss:0.7396056056022644\n",
      "Epoch 0[534/17270] Time:0.231, Train Loss:0.5817468166351318\n",
      "Epoch 0[535/17270] Time:0.234, Train Loss:0.812046229839325\n",
      "Epoch 0[536/17270] Time:0.249, Train Loss:0.5156158208847046\n",
      "Epoch 0[537/17270] Time:0.241, Train Loss:0.7856112718582153\n",
      "Epoch 0[538/17270] Time:0.267, Train Loss:1.1797492504119873\n",
      "Epoch 0[539/17270] Time:0.232, Train Loss:0.8315579295158386\n",
      "Epoch 0[540/17270] Time:0.234, Train Loss:1.0419769287109375\n",
      "Epoch 0[541/17270] Time:0.233, Train Loss:0.705838680267334\n",
      "Epoch 0[542/17270] Time:0.237, Train Loss:0.3465346395969391\n",
      "Epoch 0[543/17270] Time:0.236, Train Loss:0.7722885012626648\n",
      "Epoch 0[544/17270] Time:0.237, Train Loss:0.5594305992126465\n",
      "Epoch 0[545/17270] Time:0.247, Train Loss:1.0137403011322021\n",
      "Epoch 0[546/17270] Time:0.228, Train Loss:0.4305165112018585\n",
      "Epoch 0[547/17270] Time:0.237, Train Loss:0.6732851266860962\n",
      "Epoch 0[548/17270] Time:0.239, Train Loss:1.1819822788238525\n",
      "Epoch 0[549/17270] Time:0.239, Train Loss:0.6180604696273804\n",
      "Epoch 0[550/17270] Time:0.236, Train Loss:1.1502554416656494\n",
      "Epoch 0[551/17270] Time:0.24, Train Loss:0.9302583932876587\n",
      "Epoch 0[552/17270] Time:0.228, Train Loss:0.6686502695083618\n",
      "Epoch 0[553/17270] Time:0.239, Train Loss:0.5574741959571838\n",
      "Epoch 0[554/17270] Time:0.238, Train Loss:0.8210265636444092\n",
      "Epoch 0[555/17270] Time:0.242, Train Loss:0.514116108417511\n",
      "Epoch 0[556/17270] Time:0.235, Train Loss:0.7516229748725891\n",
      "Epoch 0[557/17270] Time:0.223, Train Loss:1.0587077140808105\n",
      "Epoch 0[558/17270] Time:0.247, Train Loss:0.6185836791992188\n",
      "Epoch 0[559/17270] Time:0.236, Train Loss:1.1204277276992798\n",
      "Epoch 0[560/17270] Time:0.229, Train Loss:1.1319150924682617\n",
      "Epoch 0[561/17270] Time:0.238, Train Loss:0.732900083065033\n",
      "Epoch 0[562/17270] Time:0.237, Train Loss:0.5885727405548096\n",
      "Epoch 0[563/17270] Time:0.236, Train Loss:0.8728085160255432\n",
      "Epoch 0[564/17270] Time:0.221, Train Loss:0.6865366697311401\n",
      "Epoch 0[565/17270] Time:0.247, Train Loss:0.716925859451294\n",
      "Epoch 0[566/17270] Time:0.237, Train Loss:0.4711434245109558\n",
      "Epoch 0[567/17270] Time:0.232, Train Loss:1.136813759803772\n",
      "Epoch 0[568/17270] Time:0.237, Train Loss:0.647696852684021\n",
      "Epoch 0[569/17270] Time:0.226, Train Loss:0.695219099521637\n",
      "Epoch 0[570/17270] Time:0.243, Train Loss:0.8413029313087463\n",
      "Epoch 0[571/17270] Time:0.241, Train Loss:0.8239811062812805\n",
      "Epoch 0[572/17270] Time:0.235, Train Loss:0.5892030596733093\n",
      "Epoch 0[573/17270] Time:0.238, Train Loss:0.7047069668769836\n",
      "Epoch 0[574/17270] Time:0.239, Train Loss:0.5363022685050964\n",
      "Epoch 0[575/17270] Time:0.234, Train Loss:0.794775128364563\n",
      "Epoch 0[576/17270] Time:0.238, Train Loss:0.6773079633712769\n",
      "Epoch 0[577/17270] Time:0.238, Train Loss:0.8594874739646912\n",
      "Epoch 0[578/17270] Time:0.238, Train Loss:0.5723460912704468\n",
      "Epoch 0[579/17270] Time:0.236, Train Loss:0.6104719042778015\n",
      "Epoch 0[580/17270] Time:0.245, Train Loss:0.868148684501648\n",
      "Epoch 0[581/17270] Time:0.243, Train Loss:0.5987159609794617\n",
      "Epoch 0[582/17270] Time:0.235, Train Loss:0.652358889579773\n",
      "Epoch 0[583/17270] Time:0.23, Train Loss:2.1113150119781494\n",
      "Epoch 0[584/17270] Time:0.236, Train Loss:1.0824148654937744\n",
      "Epoch 0[585/17270] Time:0.222, Train Loss:0.656080961227417\n",
      "Epoch 0[586/17270] Time:0.245, Train Loss:0.9768614172935486\n",
      "Epoch 0[587/17270] Time:0.236, Train Loss:0.5389513969421387\n",
      "Epoch 0[588/17270] Time:0.241, Train Loss:0.5576809644699097\n",
      "Epoch 0[589/17270] Time:0.235, Train Loss:0.6150166988372803\n",
      "Epoch 0[590/17270] Time:0.227, Train Loss:0.824974000453949\n",
      "Epoch 0[591/17270] Time:0.226, Train Loss:0.7137808799743652\n",
      "Epoch 0[592/17270] Time:0.23, Train Loss:0.5455862879753113\n",
      "Epoch 0[593/17270] Time:0.23, Train Loss:1.292465329170227\n",
      "Epoch 0[594/17270] Time:0.233, Train Loss:1.0840481519699097\n",
      "Epoch 0[595/17270] Time:0.24, Train Loss:0.8972308039665222\n",
      "Epoch 0[596/17270] Time:0.231, Train Loss:0.588600754737854\n",
      "Epoch 0[597/17270] Time:0.23, Train Loss:0.9343141317367554\n",
      "Epoch 0[598/17270] Time:0.231, Train Loss:0.7804368734359741\n",
      "Epoch 0[599/17270] Time:0.238, Train Loss:0.6124418377876282\n",
      "Epoch 0[600/17270] Time:0.243, Train Loss:0.6249877214431763\n",
      "Epoch 0[601/17270] Time:0.236, Train Loss:1.3566343784332275\n",
      "Epoch 0[602/17270] Time:0.231, Train Loss:0.5597878098487854\n",
      "Epoch 0[603/17270] Time:0.226, Train Loss:0.6415592432022095\n",
      "Epoch 0[604/17270] Time:0.238, Train Loss:0.797819972038269\n",
      "Epoch 0[605/17270] Time:0.233, Train Loss:0.6876885294914246\n",
      "Epoch 0[606/17270] Time:0.226, Train Loss:0.5664199590682983\n",
      "Epoch 0[607/17270] Time:0.24, Train Loss:0.9097611308097839\n",
      "Epoch 0[608/17270] Time:0.23, Train Loss:0.8348328471183777\n",
      "Epoch 0[609/17270] Time:0.225, Train Loss:0.7418350577354431\n",
      "Epoch 0[610/17270] Time:0.231, Train Loss:0.6307632327079773\n",
      "Epoch 0[611/17270] Time:0.236, Train Loss:0.6140897870063782\n",
      "Epoch 0[612/17270] Time:0.24, Train Loss:0.7873634696006775\n",
      "Epoch 0[613/17270] Time:0.236, Train Loss:0.8803925514221191\n",
      "Epoch 0[614/17270] Time:0.238, Train Loss:1.1817580461502075\n",
      "Epoch 0[615/17270] Time:0.23, Train Loss:0.7644252777099609\n",
      "Epoch 0[616/17270] Time:0.235, Train Loss:0.5164677500724792\n",
      "Epoch 0[617/17270] Time:0.236, Train Loss:0.8571558594703674\n",
      "Epoch 0[618/17270] Time:0.235, Train Loss:0.5480074286460876\n",
      "Epoch 0[619/17270] Time:0.246, Train Loss:0.6564832925796509\n",
      "Epoch 0[620/17270] Time:0.223, Train Loss:0.8545382022857666\n",
      "Epoch 0[621/17270] Time:0.24, Train Loss:1.0494107007980347\n",
      "Epoch 0[622/17270] Time:0.245, Train Loss:1.753108263015747\n",
      "Epoch 0[623/17270] Time:0.235, Train Loss:0.9951742887496948\n",
      "Epoch 0[624/17270] Time:0.238, Train Loss:0.6282103657722473\n",
      "Epoch 0[625/17270] Time:0.226, Train Loss:0.5564106702804565\n",
      "Epoch 0[626/17270] Time:0.24, Train Loss:1.2231144905090332\n",
      "Epoch 0[627/17270] Time:0.233, Train Loss:0.9438278675079346\n",
      "Epoch 0[628/17270] Time:0.244, Train Loss:0.6201522946357727\n",
      "Epoch 0[629/17270] Time:0.242, Train Loss:0.7092552781105042\n",
      "Epoch 0[630/17270] Time:0.236, Train Loss:0.8556647300720215\n",
      "Epoch 0[631/17270] Time:0.236, Train Loss:1.027858018875122\n",
      "Epoch 0[632/17270] Time:0.225, Train Loss:0.646469235420227\n",
      "Epoch 0[633/17270] Time:0.248, Train Loss:0.5942330360412598\n",
      "Epoch 0[634/17270] Time:0.222, Train Loss:0.6182880997657776\n",
      "Epoch 0[635/17270] Time:0.239, Train Loss:0.9317998886108398\n",
      "Epoch 0[636/17270] Time:0.264, Train Loss:1.1981321573257446\n",
      "Epoch 0[637/17270] Time:0.23, Train Loss:1.088754653930664\n",
      "Epoch 0[638/17270] Time:0.231, Train Loss:0.6568112969398499\n",
      "Epoch 0[639/17270] Time:0.222, Train Loss:1.1205822229385376\n",
      "Epoch 0[640/17270] Time:0.241, Train Loss:1.0045267343521118\n",
      "Epoch 0[641/17270] Time:0.242, Train Loss:0.8388924598693848\n",
      "Epoch 0[642/17270] Time:0.238, Train Loss:0.6891703009605408\n",
      "Epoch 0[643/17270] Time:0.234, Train Loss:0.8910284638404846\n",
      "Epoch 0[644/17270] Time:0.224, Train Loss:0.4308972954750061\n",
      "Epoch 0[645/17270] Time:0.24, Train Loss:0.6739044189453125\n",
      "Epoch 0[646/17270] Time:0.238, Train Loss:0.9352826476097107\n",
      "Epoch 0[647/17270] Time:0.236, Train Loss:0.42109304666519165\n",
      "Epoch 0[648/17270] Time:0.228, Train Loss:0.6293875575065613\n",
      "Epoch 0[649/17270] Time:0.231, Train Loss:0.6722115874290466\n",
      "Epoch 0[650/17270] Time:0.23, Train Loss:0.8403871655464172\n",
      "Epoch 0[651/17270] Time:0.241, Train Loss:1.0373700857162476\n",
      "Epoch 0[652/17270] Time:0.239, Train Loss:1.0893462896347046\n",
      "Epoch 0[653/17270] Time:0.221, Train Loss:0.7226219773292542\n",
      "Epoch 0[654/17270] Time:0.248, Train Loss:0.7807632088661194\n",
      "Epoch 0[655/17270] Time:0.229, Train Loss:0.898151159286499\n",
      "Epoch 0[656/17270] Time:0.248, Train Loss:0.6684722304344177\n",
      "Epoch 0[657/17270] Time:0.239, Train Loss:0.9866234660148621\n",
      "Epoch 0[658/17270] Time:0.224, Train Loss:1.0335791110992432\n",
      "Epoch 0[659/17270] Time:0.247, Train Loss:1.1940350532531738\n",
      "Epoch 0[660/17270] Time:0.243, Train Loss:0.8105562329292297\n",
      "Epoch 0[661/17270] Time:0.236, Train Loss:0.8365967273712158\n",
      "Epoch 0[662/17270] Time:0.22, Train Loss:0.9617337584495544\n",
      "Epoch 0[663/17270] Time:0.233, Train Loss:0.7727940678596497\n",
      "Epoch 0[664/17270] Time:0.251, Train Loss:0.5590286254882812\n",
      "Epoch 0[665/17270] Time:0.225, Train Loss:0.7342838644981384\n",
      "Epoch 0[666/17270] Time:0.236, Train Loss:0.3880714476108551\n",
      "Epoch 0[667/17270] Time:0.233, Train Loss:0.875941276550293\n",
      "Epoch 0[668/17270] Time:0.265, Train Loss:0.9133065342903137\n",
      "Epoch 0[669/17270] Time:0.244, Train Loss:1.0413312911987305\n",
      "Epoch 0[670/17270] Time:0.238, Train Loss:1.015478491783142\n",
      "Epoch 0[671/17270] Time:0.231, Train Loss:0.5770737528800964\n",
      "Epoch 0[672/17270] Time:0.239, Train Loss:1.2698187828063965\n",
      "Epoch 0[673/17270] Time:0.234, Train Loss:0.7946943640708923\n",
      "Epoch 0[674/17270] Time:0.255, Train Loss:0.6706767678260803\n",
      "Epoch 0[675/17270] Time:0.216, Train Loss:0.7482407093048096\n",
      "Epoch 0[676/17270] Time:0.247, Train Loss:1.0719084739685059\n",
      "Epoch 0[677/17270] Time:0.241, Train Loss:0.8434204459190369\n",
      "Epoch 0[678/17270] Time:0.233, Train Loss:0.9615988731384277\n",
      "Epoch 0[679/17270] Time:0.236, Train Loss:1.2622066736221313\n",
      "Epoch 0[680/17270] Time:0.249, Train Loss:0.4850200414657593\n",
      "Epoch 0[681/17270] Time:0.232, Train Loss:0.7405484914779663\n",
      "Epoch 0[682/17270] Time:0.236, Train Loss:0.9044508934020996\n",
      "Epoch 0[683/17270] Time:0.238, Train Loss:0.7237101197242737\n",
      "Epoch 0[684/17270] Time:0.23, Train Loss:0.4958357512950897\n",
      "Epoch 0[685/17270] Time:0.242, Train Loss:0.5978104472160339\n",
      "Epoch 0[686/17270] Time:0.243, Train Loss:0.6514374613761902\n",
      "Epoch 0[687/17270] Time:0.235, Train Loss:0.6760441660881042\n",
      "Epoch 0[688/17270] Time:0.239, Train Loss:0.6858747005462646\n",
      "Epoch 0[689/17270] Time:0.238, Train Loss:0.5588875412940979\n",
      "Epoch 0[690/17270] Time:0.236, Train Loss:0.878777265548706\n",
      "Epoch 0[691/17270] Time:0.235, Train Loss:0.7280146479606628\n",
      "Epoch 0[692/17270] Time:0.222, Train Loss:1.2703993320465088\n",
      "Epoch 0[693/17270] Time:0.244, Train Loss:0.5349177122116089\n",
      "Epoch 0[694/17270] Time:0.233, Train Loss:0.5727878212928772\n",
      "Epoch 0[695/17270] Time:0.226, Train Loss:0.5963090062141418\n",
      "Epoch 0[696/17270] Time:0.246, Train Loss:0.8429070711135864\n",
      "Epoch 0[697/17270] Time:0.235, Train Loss:0.762455403804779\n",
      "Epoch 0[698/17270] Time:0.224, Train Loss:2.5988502502441406\n",
      "Epoch 0[699/17270] Time:0.24, Train Loss:0.7733138203620911\n",
      "Epoch 0[700/17270] Time:0.232, Train Loss:0.7923724055290222\n",
      "Epoch 0[701/17270] Time:0.245, Train Loss:0.6271058917045593\n",
      "Epoch 0[702/17270] Time:0.24, Train Loss:1.264812707901001\n",
      "Epoch 0[703/17270] Time:0.231, Train Loss:0.7559704184532166\n",
      "Epoch 0[704/17270] Time:0.242, Train Loss:0.7957113981246948\n",
      "Epoch 0[705/17270] Time:0.24, Train Loss:1.545507788658142\n",
      "Epoch 0[706/17270] Time:0.235, Train Loss:0.9683264493942261\n",
      "Epoch 0[707/17270] Time:0.239, Train Loss:0.958634078502655\n",
      "Epoch 0[708/17270] Time:0.238, Train Loss:0.7649763226509094\n",
      "Epoch 0[709/17270] Time:0.252, Train Loss:0.7206776738166809\n",
      "Epoch 0[710/17270] Time:0.231, Train Loss:0.8794514536857605\n",
      "Epoch 0[711/17270] Time:0.237, Train Loss:0.7820375561714172\n",
      "Epoch 0[712/17270] Time:0.241, Train Loss:0.831015944480896\n",
      "Epoch 0[713/17270] Time:0.238, Train Loss:0.696905255317688\n",
      "Epoch 0[714/17270] Time:0.258, Train Loss:0.9947680234909058\n",
      "Epoch 0[715/17270] Time:0.227, Train Loss:1.0365684032440186\n",
      "Epoch 0[716/17270] Time:0.235, Train Loss:0.7063629627227783\n",
      "Epoch 0[717/17270] Time:0.232, Train Loss:0.9891933798789978\n",
      "Epoch 0[718/17270] Time:0.24, Train Loss:0.7377151846885681\n",
      "Epoch 0[719/17270] Time:0.231, Train Loss:0.879121720790863\n",
      "Epoch 0[720/17270] Time:0.233, Train Loss:1.1239595413208008\n",
      "Epoch 0[721/17270] Time:0.243, Train Loss:1.0412898063659668\n",
      "Epoch 0[722/17270] Time:0.239, Train Loss:0.5653463006019592\n",
      "Epoch 0[723/17270] Time:0.239, Train Loss:0.660076379776001\n",
      "Epoch 0[724/17270] Time:0.239, Train Loss:0.8977106213569641\n",
      "Epoch 0[725/17270] Time:0.231, Train Loss:1.7346473932266235\n",
      "Epoch 0[726/17270] Time:0.243, Train Loss:0.532939612865448\n",
      "Epoch 0[727/17270] Time:0.236, Train Loss:1.155106544494629\n",
      "Epoch 0[728/17270] Time:0.238, Train Loss:0.758944571018219\n",
      "Epoch 0[729/17270] Time:0.237, Train Loss:1.4593013525009155\n",
      "Epoch 0[730/17270] Time:0.236, Train Loss:0.8626123666763306\n",
      "Epoch 0[731/17270] Time:0.24, Train Loss:1.2387293577194214\n",
      "Epoch 0[732/17270] Time:0.238, Train Loss:1.1421219110488892\n",
      "Epoch 0[733/17270] Time:0.239, Train Loss:1.103189468383789\n",
      "Epoch 0[734/17270] Time:0.241, Train Loss:1.1698951721191406\n",
      "Epoch 0[735/17270] Time:0.231, Train Loss:0.737576961517334\n",
      "Epoch 0[736/17270] Time:0.239, Train Loss:1.1902645826339722\n",
      "Epoch 0[737/17270] Time:0.233, Train Loss:1.0750422477722168\n",
      "Epoch 0[738/17270] Time:0.234, Train Loss:1.0964584350585938\n",
      "Epoch 0[739/17270] Time:0.245, Train Loss:1.1741013526916504\n",
      "Epoch 0[740/17270] Time:0.237, Train Loss:0.8171041011810303\n",
      "Epoch 0[741/17270] Time:0.235, Train Loss:0.9032068252563477\n",
      "Epoch 0[742/17270] Time:0.235, Train Loss:1.1272367238998413\n",
      "Epoch 0[743/17270] Time:0.237, Train Loss:0.5309855937957764\n",
      "Epoch 0[744/17270] Time:0.233, Train Loss:0.5121799111366272\n",
      "Epoch 0[745/17270] Time:0.236, Train Loss:0.6040288805961609\n",
      "Epoch 0[746/17270] Time:0.237, Train Loss:1.0728057622909546\n",
      "Epoch 0[747/17270] Time:0.225, Train Loss:0.4672471880912781\n",
      "Epoch 0[748/17270] Time:0.247, Train Loss:0.9441882371902466\n",
      "Epoch 0[749/17270] Time:0.234, Train Loss:0.5129948854446411\n",
      "Epoch 0[750/17270] Time:0.237, Train Loss:0.6448422074317932\n",
      "Epoch 0[751/17270] Time:0.231, Train Loss:0.5609077215194702\n",
      "Epoch 0[752/17270] Time:0.234, Train Loss:0.6291165947914124\n",
      "Epoch 0[753/17270] Time:0.234, Train Loss:0.8887580037117004\n",
      "Epoch 0[754/17270] Time:0.233, Train Loss:0.6320217251777649\n",
      "Epoch 0[755/17270] Time:0.225, Train Loss:0.7821159958839417\n",
      "Epoch 0[756/17270] Time:0.233, Train Loss:0.8515528440475464\n",
      "Epoch 0[757/17270] Time:0.233, Train Loss:0.5887197256088257\n",
      "Epoch 0[758/17270] Time:0.232, Train Loss:0.8207759261131287\n",
      "Epoch 0[759/17270] Time:0.232, Train Loss:0.560706377029419\n",
      "Epoch 0[760/17270] Time:0.229, Train Loss:0.5872129201889038\n",
      "Epoch 0[761/17270] Time:0.229, Train Loss:0.7712157368659973\n",
      "Epoch 0[762/17270] Time:0.238, Train Loss:0.766146183013916\n",
      "Epoch 0[763/17270] Time:0.238, Train Loss:0.3556971549987793\n",
      "Epoch 0[764/17270] Time:0.241, Train Loss:0.5826826095581055\n",
      "Epoch 0[765/17270] Time:0.231, Train Loss:1.3869765996932983\n",
      "Epoch 0[766/17270] Time:0.24, Train Loss:0.8626273274421692\n",
      "Epoch 0[767/17270] Time:0.235, Train Loss:0.6856099963188171\n",
      "Epoch 0[768/17270] Time:0.247, Train Loss:0.5532730221748352\n",
      "Epoch 0[769/17270] Time:0.234, Train Loss:1.1209585666656494\n",
      "Epoch 0[770/17270] Time:0.233, Train Loss:0.8756147623062134\n",
      "Epoch 0[771/17270] Time:0.236, Train Loss:0.6249933838844299\n",
      "Epoch 0[772/17270] Time:0.24, Train Loss:1.0132472515106201\n",
      "Epoch 0[773/17270] Time:0.232, Train Loss:0.41219818592071533\n",
      "Epoch 0[774/17270] Time:0.243, Train Loss:0.770945131778717\n",
      "Epoch 0[775/17270] Time:0.232, Train Loss:1.0286731719970703\n",
      "Epoch 0[776/17270] Time:0.229, Train Loss:0.3161969780921936\n",
      "Epoch 0[777/17270] Time:0.241, Train Loss:1.1901369094848633\n",
      "Epoch 0[778/17270] Time:0.23, Train Loss:0.7883692383766174\n",
      "Epoch 0[779/17270] Time:0.237, Train Loss:1.1524096727371216\n",
      "Epoch 0[780/17270] Time:0.245, Train Loss:0.5330607891082764\n",
      "Epoch 0[781/17270] Time:0.229, Train Loss:0.5999431014060974\n",
      "Epoch 0[782/17270] Time:0.243, Train Loss:0.5847988724708557\n",
      "Epoch 0[783/17270] Time:0.231, Train Loss:0.6635493040084839\n",
      "Epoch 0[784/17270] Time:0.24, Train Loss:0.5734610557556152\n",
      "Epoch 0[785/17270] Time:0.238, Train Loss:0.8004534244537354\n",
      "Epoch 0[786/17270] Time:0.241, Train Loss:0.6851521730422974\n",
      "Epoch 0[787/17270] Time:0.237, Train Loss:0.6354734301567078\n",
      "Epoch 0[788/17270] Time:0.236, Train Loss:0.47084107995033264\n",
      "Epoch 0[789/17270] Time:0.239, Train Loss:1.2030051946640015\n",
      "Epoch 0[790/17270] Time:0.241, Train Loss:0.7978803515434265\n",
      "Epoch 0[791/17270] Time:0.231, Train Loss:0.5168192386627197\n",
      "Epoch 0[792/17270] Time:0.234, Train Loss:0.895885169506073\n",
      "Epoch 0[793/17270] Time:0.239, Train Loss:0.5539353489875793\n",
      "Epoch 0[794/17270] Time:0.232, Train Loss:0.8196886777877808\n",
      "Epoch 0[795/17270] Time:0.235, Train Loss:0.7241531014442444\n",
      "Epoch 0[796/17270] Time:0.239, Train Loss:0.7927443385124207\n",
      "Epoch 0[797/17270] Time:0.239, Train Loss:0.5361822247505188\n",
      "Epoch 0[798/17270] Time:0.236, Train Loss:1.2925939559936523\n",
      "Epoch 0[799/17270] Time:0.236, Train Loss:0.5470715165138245\n",
      "Epoch 0[800/17270] Time:0.239, Train Loss:1.1696653366088867\n",
      "Epoch 0[801/17270] Time:0.228, Train Loss:0.5859774351119995\n",
      "Epoch 0[802/17270] Time:0.233, Train Loss:0.5302409529685974\n",
      "Epoch 0[803/17270] Time:0.24, Train Loss:0.4401434659957886\n",
      "Epoch 0[804/17270] Time:0.239, Train Loss:0.7933973073959351\n",
      "Epoch 0[805/17270] Time:0.238, Train Loss:0.6702277064323425\n",
      "Epoch 0[806/17270] Time:0.243, Train Loss:0.7809063792228699\n",
      "Epoch 0[807/17270] Time:0.233, Train Loss:0.549804151058197\n",
      "Epoch 0[808/17270] Time:0.233, Train Loss:0.5016800165176392\n",
      "Epoch 0[809/17270] Time:0.231, Train Loss:0.5545064210891724\n",
      "Epoch 0[810/17270] Time:0.233, Train Loss:0.553541898727417\n",
      "Epoch 0[811/17270] Time:0.239, Train Loss:0.5341638922691345\n",
      "Epoch 0[812/17270] Time:0.237, Train Loss:1.163245439529419\n",
      "Epoch 0[813/17270] Time:0.233, Train Loss:0.5706010460853577\n",
      "Epoch 0[814/17270] Time:0.235, Train Loss:0.6837936639785767\n",
      "Epoch 0[815/17270] Time:0.232, Train Loss:0.5704267621040344\n",
      "Epoch 0[816/17270] Time:0.243, Train Loss:0.4853563904762268\n",
      "Epoch 0[817/17270] Time:0.234, Train Loss:0.8678424954414368\n",
      "Epoch 0[818/17270] Time:0.231, Train Loss:0.6524977087974548\n",
      "Epoch 0[819/17270] Time:0.237, Train Loss:0.8894420862197876\n",
      "Epoch 0[820/17270] Time:0.226, Train Loss:1.1401753425598145\n",
      "Epoch 0[821/17270] Time:0.227, Train Loss:0.5430738925933838\n",
      "Epoch 0[822/17270] Time:0.238, Train Loss:0.8580173254013062\n",
      "Epoch 0[823/17270] Time:0.236, Train Loss:1.0012317895889282\n",
      "Epoch 0[824/17270] Time:0.238, Train Loss:0.49314090609550476\n",
      "Epoch 0[825/17270] Time:0.226, Train Loss:0.5934028625488281\n",
      "Epoch 0[826/17270] Time:0.235, Train Loss:0.6385402679443359\n",
      "Epoch 0[827/17270] Time:0.249, Train Loss:0.661368191242218\n",
      "Epoch 0[828/17270] Time:0.237, Train Loss:0.6935024857521057\n",
      "Epoch 0[829/17270] Time:0.235, Train Loss:0.6562553644180298\n",
      "Epoch 0[830/17270] Time:0.225, Train Loss:0.5070592761039734\n",
      "Epoch 0[831/17270] Time:0.242, Train Loss:0.7191352248191833\n",
      "Epoch 0[832/17270] Time:0.236, Train Loss:0.39041954278945923\n",
      "Epoch 0[833/17270] Time:0.224, Train Loss:0.8472101092338562\n",
      "Epoch 0[834/17270] Time:0.235, Train Loss:0.48416659235954285\n",
      "Epoch 0[835/17270] Time:0.233, Train Loss:0.6700173020362854\n",
      "Epoch 0[836/17270] Time:0.232, Train Loss:0.5515069365501404\n",
      "Epoch 0[837/17270] Time:0.235, Train Loss:0.4994485080242157\n",
      "Epoch 0[838/17270] Time:0.231, Train Loss:0.6728193163871765\n",
      "Epoch 0[839/17270] Time:0.239, Train Loss:0.9476466774940491\n",
      "Epoch 0[840/17270] Time:0.238, Train Loss:0.655275821685791\n",
      "Epoch 0[841/17270] Time:0.226, Train Loss:0.47121816873550415\n",
      "Epoch 0[842/17270] Time:0.247, Train Loss:0.6685175895690918\n",
      "Epoch 0[843/17270] Time:0.227, Train Loss:0.6069288849830627\n",
      "Epoch 0[844/17270] Time:0.247, Train Loss:1.5949058532714844\n",
      "Epoch 0[845/17270] Time:0.224, Train Loss:0.6433544158935547\n",
      "Epoch 0[846/17270] Time:0.226, Train Loss:0.4424026608467102\n",
      "Epoch 0[847/17270] Time:0.243, Train Loss:0.7970203161239624\n",
      "Epoch 0[848/17270] Time:0.25, Train Loss:1.1447862386703491\n",
      "Epoch 0[849/17270] Time:0.236, Train Loss:0.6317481398582458\n",
      "Epoch 0[850/17270] Time:0.239, Train Loss:2.653561592102051\n",
      "Epoch 0[851/17270] Time:0.235, Train Loss:0.5345652103424072\n",
      "Epoch 0[852/17270] Time:0.24, Train Loss:1.1856400966644287\n",
      "Epoch 0[853/17270] Time:0.244, Train Loss:0.8238112926483154\n",
      "Epoch 0[854/17270] Time:0.236, Train Loss:0.4257458746433258\n",
      "Epoch 0[855/17270] Time:0.23, Train Loss:0.7237440347671509\n",
      "Epoch 0[856/17270] Time:0.251, Train Loss:0.4170096218585968\n",
      "Epoch 0[857/17270] Time:0.239, Train Loss:0.7570995688438416\n",
      "Epoch 0[858/17270] Time:0.237, Train Loss:0.8957679867744446\n",
      "Epoch 0[859/17270] Time:0.229, Train Loss:0.6958842277526855\n",
      "Epoch 0[860/17270] Time:0.24, Train Loss:0.6694492101669312\n",
      "Epoch 0[861/17270] Time:0.232, Train Loss:1.3221005201339722\n",
      "Epoch 0[862/17270] Time:0.24, Train Loss:0.6778357028961182\n",
      "Epoch 0[863/17270] Time:0.234, Train Loss:1.1552473306655884\n",
      "Epoch 0[864/17270] Time:0.25, Train Loss:0.6877184510231018\n",
      "Epoch 0[865/17270] Time:0.232, Train Loss:0.575894296169281\n",
      "Epoch 0[866/17270] Time:0.233, Train Loss:0.6476565003395081\n",
      "Epoch 0[867/17270] Time:0.231, Train Loss:0.7610622048377991\n",
      "Epoch 0[868/17270] Time:0.233, Train Loss:0.776303231716156\n",
      "Epoch 0[869/17270] Time:0.231, Train Loss:0.8099174499511719\n",
      "Epoch 0[870/17270] Time:0.237, Train Loss:0.8508661389350891\n",
      "Epoch 0[871/17270] Time:0.236, Train Loss:0.6147236824035645\n",
      "Epoch 0[872/17270] Time:0.223, Train Loss:1.0171536207199097\n",
      "Epoch 0[873/17270] Time:0.239, Train Loss:0.8612120747566223\n",
      "Epoch 0[874/17270] Time:0.232, Train Loss:0.6940396428108215\n",
      "Epoch 0[875/17270] Time:0.231, Train Loss:0.8882405161857605\n",
      "Epoch 0[876/17270] Time:0.223, Train Loss:0.8875482678413391\n",
      "Epoch 0[877/17270] Time:0.231, Train Loss:0.3964904248714447\n",
      "Epoch 0[878/17270] Time:0.233, Train Loss:0.7562444806098938\n",
      "Epoch 0[879/17270] Time:0.242, Train Loss:0.7428054809570312\n",
      "Epoch 0[880/17270] Time:0.238, Train Loss:0.5143605470657349\n",
      "Epoch 0[881/17270] Time:0.232, Train Loss:0.59991455078125\n",
      "Epoch 0[882/17270] Time:0.241, Train Loss:0.4174555540084839\n",
      "Epoch 0[883/17270] Time:0.227, Train Loss:1.347843885421753\n",
      "Epoch 0[884/17270] Time:0.237, Train Loss:0.892366886138916\n",
      "Epoch 0[885/17270] Time:0.238, Train Loss:0.9041026830673218\n",
      "Epoch 0[886/17270] Time:0.238, Train Loss:0.6692577600479126\n",
      "Epoch 0[887/17270] Time:0.238, Train Loss:1.199648380279541\n",
      "Epoch 0[888/17270] Time:0.238, Train Loss:0.6021778583526611\n",
      "Epoch 0[889/17270] Time:0.238, Train Loss:1.121293306350708\n",
      "Epoch 0[890/17270] Time:0.237, Train Loss:0.6225478649139404\n",
      "Epoch 0[891/17270] Time:0.225, Train Loss:0.725516140460968\n",
      "Epoch 0[892/17270] Time:0.242, Train Loss:0.7912710905075073\n",
      "Epoch 0[893/17270] Time:0.244, Train Loss:0.6378777623176575\n",
      "Epoch 0[894/17270] Time:0.231, Train Loss:0.7138831615447998\n",
      "Epoch 0[895/17270] Time:0.249, Train Loss:0.5869243741035461\n",
      "Epoch 0[896/17270] Time:0.239, Train Loss:0.6858903765678406\n",
      "Epoch 0[897/17270] Time:0.232, Train Loss:0.6473700404167175\n",
      "Epoch 0[898/17270] Time:0.224, Train Loss:1.0544465780258179\n",
      "Epoch 0[899/17270] Time:0.243, Train Loss:0.6342884302139282\n",
      "Epoch 0[900/17270] Time:0.234, Train Loss:0.7185877561569214\n",
      "Epoch 0[901/17270] Time:0.239, Train Loss:1.0773521661758423\n",
      "Epoch 0[902/17270] Time:0.239, Train Loss:1.053310513496399\n",
      "Epoch 0[903/17270] Time:0.243, Train Loss:0.971507728099823\n",
      "Epoch 0[904/17270] Time:0.234, Train Loss:0.8409832119941711\n",
      "Epoch 0[905/17270] Time:0.224, Train Loss:0.5683541893959045\n",
      "Epoch 0[906/17270] Time:0.242, Train Loss:0.4596976339817047\n",
      "Epoch 0[907/17270] Time:0.231, Train Loss:0.6212793588638306\n",
      "Epoch 0[908/17270] Time:0.237, Train Loss:0.7166614532470703\n",
      "Epoch 0[909/17270] Time:0.253, Train Loss:0.662726879119873\n",
      "Epoch 0[910/17270] Time:0.231, Train Loss:0.6145778298377991\n",
      "Epoch 0[911/17270] Time:0.233, Train Loss:0.6231263875961304\n",
      "Epoch 0[912/17270] Time:0.229, Train Loss:0.8490860462188721\n",
      "Epoch 0[913/17270] Time:0.235, Train Loss:0.6186731457710266\n",
      "Epoch 0[914/17270] Time:0.231, Train Loss:1.3045158386230469\n",
      "Epoch 0[915/17270] Time:0.24, Train Loss:0.9547021389007568\n",
      "Epoch 0[916/17270] Time:0.222, Train Loss:0.960981547832489\n",
      "Epoch 0[917/17270] Time:0.233, Train Loss:0.525536060333252\n",
      "Epoch 0[918/17270] Time:0.247, Train Loss:0.7409353852272034\n",
      "Epoch 0[919/17270] Time:0.232, Train Loss:0.5202573537826538\n",
      "Epoch 0[920/17270] Time:0.237, Train Loss:0.45357653498649597\n",
      "Epoch 0[921/17270] Time:0.243, Train Loss:0.8520386815071106\n",
      "Epoch 0[922/17270] Time:0.243, Train Loss:0.5382363796234131\n",
      "Epoch 0[923/17270] Time:0.233, Train Loss:0.555259108543396\n",
      "Epoch 0[924/17270] Time:0.226, Train Loss:0.5180771946907043\n",
      "Epoch 0[925/17270] Time:0.241, Train Loss:0.6439765691757202\n",
      "Epoch 0[926/17270] Time:0.234, Train Loss:0.5182686448097229\n",
      "Epoch 0[927/17270] Time:0.248, Train Loss:0.5840243697166443\n",
      "Epoch 0[928/17270] Time:0.231, Train Loss:1.0746384859085083\n",
      "Epoch 0[929/17270] Time:0.243, Train Loss:0.3485357463359833\n",
      "Epoch 0[930/17270] Time:0.235, Train Loss:0.5374606251716614\n",
      "Epoch 0[931/17270] Time:0.23, Train Loss:0.4702436923980713\n",
      "Epoch 0[932/17270] Time:0.237, Train Loss:1.1851366758346558\n",
      "Epoch 0[933/17270] Time:0.232, Train Loss:0.4737255275249481\n",
      "Epoch 0[934/17270] Time:0.247, Train Loss:0.9442280530929565\n",
      "Epoch 0[935/17270] Time:0.234, Train Loss:1.121632695198059\n",
      "Epoch 0[936/17270] Time:0.225, Train Loss:0.4561016261577606\n",
      "Epoch 0[937/17270] Time:0.241, Train Loss:0.6602367162704468\n",
      "Epoch 0[938/17270] Time:0.243, Train Loss:0.4183558523654938\n",
      "Epoch 0[939/17270] Time:0.23, Train Loss:0.6845403909683228\n",
      "Epoch 0[940/17270] Time:0.238, Train Loss:0.6265433430671692\n",
      "Epoch 0[941/17270] Time:0.229, Train Loss:0.6562601923942566\n",
      "Epoch 0[942/17270] Time:0.245, Train Loss:0.6459454894065857\n",
      "Epoch 0[943/17270] Time:0.229, Train Loss:0.6063520312309265\n",
      "Epoch 0[944/17270] Time:0.242, Train Loss:0.8372412323951721\n",
      "Epoch 0[945/17270] Time:0.225, Train Loss:0.6433713436126709\n",
      "Epoch 0[946/17270] Time:0.244, Train Loss:0.7916057705879211\n",
      "Epoch 0[947/17270] Time:0.23, Train Loss:0.6444300413131714\n",
      "Epoch 0[948/17270] Time:0.229, Train Loss:1.3213614225387573\n",
      "Epoch 0[949/17270] Time:0.244, Train Loss:0.6708748936653137\n",
      "Epoch 0[950/17270] Time:0.235, Train Loss:0.793842613697052\n",
      "Epoch 0[951/17270] Time:0.229, Train Loss:0.7837599515914917\n",
      "Epoch 0[952/17270] Time:0.244, Train Loss:1.0459848642349243\n",
      "Epoch 0[953/17270] Time:0.233, Train Loss:0.7860516309738159\n",
      "Epoch 0[954/17270] Time:0.241, Train Loss:0.6344099044799805\n",
      "Epoch 0[955/17270] Time:0.23, Train Loss:0.5986190438270569\n",
      "Epoch 0[956/17270] Time:0.243, Train Loss:0.7894607186317444\n",
      "Epoch 0[957/17270] Time:0.225, Train Loss:0.869642436504364\n",
      "Epoch 0[958/17270] Time:0.25, Train Loss:1.3385945558547974\n",
      "Epoch 0[959/17270] Time:0.225, Train Loss:1.0229432582855225\n",
      "Epoch 0[960/17270] Time:0.23, Train Loss:0.8842808604240417\n",
      "Epoch 0[961/17270] Time:0.235, Train Loss:0.515067994594574\n",
      "Epoch 0[962/17270] Time:0.236, Train Loss:0.8560308218002319\n",
      "Epoch 0[963/17270] Time:0.245, Train Loss:1.1589810848236084\n",
      "Epoch 0[964/17270] Time:0.239, Train Loss:0.45199888944625854\n",
      "Epoch 0[965/17270] Time:0.225, Train Loss:0.6756324768066406\n",
      "Epoch 0[966/17270] Time:0.242, Train Loss:0.5291066765785217\n",
      "Epoch 0[967/17270] Time:0.238, Train Loss:0.6387889981269836\n",
      "Epoch 0[968/17270] Time:0.233, Train Loss:0.6600568294525146\n",
      "Epoch 0[969/17270] Time:0.231, Train Loss:0.9154650568962097\n",
      "Epoch 0[970/17270] Time:0.23, Train Loss:0.5244084000587463\n",
      "Epoch 0[971/17270] Time:0.249, Train Loss:0.6199561953544617\n",
      "Epoch 0[972/17270] Time:0.237, Train Loss:0.6698257327079773\n",
      "Epoch 0[973/17270] Time:0.233, Train Loss:0.6443189382553101\n",
      "Epoch 0[974/17270] Time:0.227, Train Loss:0.4974311590194702\n",
      "Epoch 0[975/17270] Time:0.245, Train Loss:0.7879432439804077\n",
      "Epoch 0[976/17270] Time:0.244, Train Loss:0.6100831031799316\n",
      "Epoch 0[977/17270] Time:0.226, Train Loss:0.6841842532157898\n",
      "Epoch 0[978/17270] Time:0.23, Train Loss:1.0918940305709839\n",
      "Epoch 0[979/17270] Time:0.23, Train Loss:0.835279107093811\n",
      "Epoch 0[980/17270] Time:0.241, Train Loss:0.6185467839241028\n",
      "Epoch 0[981/17270] Time:0.23, Train Loss:0.9233070015907288\n",
      "Epoch 0[982/17270] Time:0.227, Train Loss:0.6801413893699646\n",
      "Epoch 0[983/17270] Time:0.246, Train Loss:0.5693734884262085\n",
      "Epoch 0[984/17270] Time:0.235, Train Loss:0.623396635055542\n",
      "Epoch 0[985/17270] Time:0.234, Train Loss:0.597365140914917\n",
      "Epoch 0[986/17270] Time:0.224, Train Loss:0.6940199136734009\n",
      "Epoch 0[987/17270] Time:0.233, Train Loss:0.7010747790336609\n",
      "Epoch 0[988/17270] Time:0.23, Train Loss:0.5781244039535522\n",
      "Epoch 0[989/17270] Time:0.235, Train Loss:0.8700412511825562\n",
      "Epoch 0[990/17270] Time:0.222, Train Loss:1.29151451587677\n",
      "Epoch 0[991/17270] Time:0.247, Train Loss:0.5276800394058228\n",
      "Epoch 0[992/17270] Time:0.239, Train Loss:0.7698588371276855\n",
      "Epoch 0[993/17270] Time:0.228, Train Loss:0.8997350335121155\n",
      "Epoch 0[994/17270] Time:0.233, Train Loss:0.6251117587089539\n",
      "Epoch 0[995/17270] Time:0.228, Train Loss:0.6936162114143372\n",
      "Epoch 0[996/17270] Time:0.237, Train Loss:0.376854807138443\n",
      "Epoch 0[997/17270] Time:0.229, Train Loss:1.091961145401001\n",
      "Epoch 0[998/17270] Time:0.232, Train Loss:0.6930981278419495\n",
      "Epoch 0[999/17270] Time:0.231, Train Loss:1.1358439922332764\n",
      "Epoch 0[1000/17270] Time:0.241, Train Loss:0.6527191400527954\n",
      "Epoch 0[1001/17270] Time:0.237, Train Loss:0.6698606610298157\n",
      "Epoch 0[1002/17270] Time:0.236, Train Loss:0.7338703870773315\n",
      "Epoch 0[1003/17270] Time:0.228, Train Loss:0.6473363637924194\n",
      "Epoch 0[1004/17270] Time:0.237, Train Loss:0.752196729183197\n",
      "Epoch 0[1005/17270] Time:0.226, Train Loss:0.8247694373130798\n",
      "Epoch 0[1006/17270] Time:0.237, Train Loss:0.6742991209030151\n",
      "Epoch 0[1007/17270] Time:0.222, Train Loss:0.5618453025817871\n",
      "Epoch 0[1008/17270] Time:0.236, Train Loss:0.4989699423313141\n",
      "Epoch 0[1009/17270] Time:0.237, Train Loss:0.6547749042510986\n",
      "Epoch 0[1010/17270] Time:0.237, Train Loss:0.786182701587677\n",
      "Epoch 0[1011/17270] Time:0.229, Train Loss:0.48037031292915344\n",
      "Epoch 0[1012/17270] Time:0.234, Train Loss:0.8300782442092896\n",
      "Epoch 0[1013/17270] Time:0.237, Train Loss:0.6053858399391174\n",
      "Epoch 0[1014/17270] Time:0.231, Train Loss:1.000078558921814\n",
      "Epoch 0[1015/17270] Time:0.235, Train Loss:0.36399930715560913\n",
      "Epoch 0[1016/17270] Time:0.239, Train Loss:0.6784243583679199\n",
      "Epoch 0[1017/17270] Time:0.239, Train Loss:0.8190183639526367\n",
      "Epoch 0[1018/17270] Time:0.236, Train Loss:0.3526061773300171\n",
      "Epoch 0[1019/17270] Time:0.234, Train Loss:0.8116251826286316\n",
      "Epoch 0[1020/17270] Time:0.227, Train Loss:1.1590044498443604\n",
      "Epoch 0[1021/17270] Time:0.229, Train Loss:0.6876665353775024\n",
      "Epoch 0[1022/17270] Time:0.233, Train Loss:1.225380539894104\n",
      "Epoch 0[1023/17270] Time:0.229, Train Loss:0.5204864144325256\n",
      "Epoch 0[1024/17270] Time:0.23, Train Loss:0.96708083152771\n",
      "Epoch 0[1025/17270] Time:0.232, Train Loss:0.5819917917251587\n",
      "Epoch 0[1026/17270] Time:0.235, Train Loss:0.9629411101341248\n",
      "Epoch 0[1027/17270] Time:0.246, Train Loss:0.7645989060401917\n",
      "Epoch 0[1028/17270] Time:0.235, Train Loss:0.8737165927886963\n",
      "Epoch 0[1029/17270] Time:0.245, Train Loss:0.8029549717903137\n",
      "Epoch 0[1030/17270] Time:0.231, Train Loss:0.645582914352417\n",
      "Epoch 0[1031/17270] Time:0.245, Train Loss:0.5162224173545837\n",
      "Epoch 0[1032/17270] Time:0.235, Train Loss:0.6250659823417664\n",
      "Epoch 0[1033/17270] Time:0.244, Train Loss:0.5955691337585449\n",
      "Epoch 0[1034/17270] Time:0.228, Train Loss:0.613898515701294\n",
      "Epoch 0[1035/17270] Time:0.242, Train Loss:0.6042450070381165\n",
      "Epoch 0[1036/17270] Time:0.222, Train Loss:0.4889390468597412\n",
      "Epoch 0[1037/17270] Time:0.243, Train Loss:0.889795184135437\n",
      "Epoch 0[1038/17270] Time:0.228, Train Loss:0.8972827196121216\n",
      "Epoch 0[1039/17270] Time:0.229, Train Loss:0.5009720921516418\n",
      "Epoch 0[1040/17270] Time:0.249, Train Loss:0.6595314741134644\n",
      "Epoch 0[1041/17270] Time:0.227, Train Loss:0.941612958908081\n",
      "Epoch 0[1042/17270] Time:0.244, Train Loss:0.4062366485595703\n",
      "Epoch 0[1043/17270] Time:0.226, Train Loss:0.7284862995147705\n",
      "Epoch 0[1044/17270] Time:0.234, Train Loss:0.7430750727653503\n",
      "Epoch 0[1045/17270] Time:0.229, Train Loss:1.3067618608474731\n",
      "Epoch 0[1046/17270] Time:0.239, Train Loss:0.8108941316604614\n",
      "Epoch 0[1047/17270] Time:0.223, Train Loss:0.7004684805870056\n",
      "Epoch 0[1048/17270] Time:0.235, Train Loss:1.368919014930725\n",
      "Epoch 0[1049/17270] Time:0.248, Train Loss:0.5655657649040222\n",
      "Epoch 0[1050/17270] Time:0.239, Train Loss:0.6384072303771973\n",
      "Epoch 0[1051/17270] Time:0.236, Train Loss:0.5308740735054016\n",
      "Epoch 0[1052/17270] Time:0.223, Train Loss:0.6239441633224487\n",
      "Epoch 0[1053/17270] Time:0.24, Train Loss:0.8760547041893005\n",
      "Epoch 0[1054/17270] Time:0.224, Train Loss:0.5883386135101318\n",
      "Epoch 0[1055/17270] Time:0.246, Train Loss:0.8472115993499756\n",
      "Epoch 0[1056/17270] Time:0.239, Train Loss:0.9740915298461914\n",
      "Epoch 0[1057/17270] Time:0.233, Train Loss:0.6389957666397095\n",
      "Epoch 0[1058/17270] Time:0.228, Train Loss:0.6036288738250732\n",
      "Epoch 0[1059/17270] Time:0.24, Train Loss:0.8303733468055725\n",
      "Epoch 0[1060/17270] Time:0.238, Train Loss:0.5519589185714722\n",
      "Epoch 0[1061/17270] Time:0.238, Train Loss:0.5870288014411926\n",
      "Epoch 0[1062/17270] Time:0.234, Train Loss:0.6111146211624146\n",
      "Epoch 0[1063/17270] Time:0.24, Train Loss:0.7515910267829895\n",
      "Epoch 0[1064/17270] Time:0.227, Train Loss:0.570148766040802\n",
      "Epoch 0[1065/17270] Time:0.239, Train Loss:0.8490661382675171\n",
      "Epoch 0[1066/17270] Time:0.227, Train Loss:0.8483577370643616\n",
      "Epoch 0[1067/17270] Time:0.232, Train Loss:1.9320096969604492\n",
      "Epoch 0[1068/17270] Time:0.233, Train Loss:0.6721615195274353\n",
      "Epoch 0[1069/17270] Time:0.241, Train Loss:1.1745376586914062\n",
      "Epoch 0[1070/17270] Time:0.226, Train Loss:0.7795900106430054\n",
      "Epoch 0[1071/17270] Time:0.245, Train Loss:0.7032932043075562\n",
      "Epoch 0[1072/17270] Time:0.228, Train Loss:0.49515673518180847\n",
      "Epoch 0[1073/17270] Time:0.224, Train Loss:0.738170325756073\n",
      "Epoch 0[1074/17270] Time:0.233, Train Loss:0.7156916856765747\n",
      "Epoch 0[1075/17270] Time:0.232, Train Loss:0.686762273311615\n",
      "Epoch 0[1076/17270] Time:0.228, Train Loss:0.6320503950119019\n",
      "Epoch 0[1077/17270] Time:0.233, Train Loss:0.6464902758598328\n",
      "Epoch 0[1078/17270] Time:0.23, Train Loss:0.7191405892372131\n",
      "Epoch 0[1079/17270] Time:0.242, Train Loss:0.7616633772850037\n",
      "Epoch 0[1080/17270] Time:0.236, Train Loss:0.9092456102371216\n",
      "Epoch 0[1081/17270] Time:0.243, Train Loss:0.6676831245422363\n",
      "Epoch 0[1082/17270] Time:0.234, Train Loss:0.6567623019218445\n",
      "Epoch 0[1083/17270] Time:0.243, Train Loss:0.48707541823387146\n",
      "Epoch 0[1084/17270] Time:0.23, Train Loss:0.7999715209007263\n",
      "Epoch 0[1085/17270] Time:0.241, Train Loss:0.710266649723053\n",
      "Epoch 0[1086/17270] Time:0.236, Train Loss:0.7232582569122314\n",
      "Epoch 0[1087/17270] Time:0.241, Train Loss:0.7159075140953064\n",
      "Epoch 0[1088/17270] Time:0.234, Train Loss:0.7687715291976929\n",
      "Epoch 0[1089/17270] Time:0.223, Train Loss:0.5649635195732117\n",
      "Epoch 0[1090/17270] Time:0.231, Train Loss:0.8722747564315796\n",
      "Epoch 0[1091/17270] Time:0.233, Train Loss:0.5577511191368103\n",
      "Epoch 0[1092/17270] Time:0.229, Train Loss:0.4805053770542145\n",
      "Epoch 0[1093/17270] Time:0.247, Train Loss:0.82298743724823\n",
      "Epoch 0[1094/17270] Time:0.233, Train Loss:1.0466232299804688\n",
      "Epoch 0[1095/17270] Time:0.24, Train Loss:0.48970356583595276\n",
      "Epoch 0[1096/17270] Time:0.235, Train Loss:0.46269017457962036\n",
      "Epoch 0[1097/17270] Time:0.231, Train Loss:0.5166624188423157\n",
      "Epoch 0[1098/17270] Time:0.224, Train Loss:0.5880517363548279\n",
      "Epoch 0[1099/17270] Time:0.23, Train Loss:0.5634022355079651\n",
      "Epoch 0[1100/17270] Time:0.234, Train Loss:0.5036799311637878\n",
      "Epoch 0[1101/17270] Time:0.236, Train Loss:0.882243275642395\n",
      "Epoch 0[1102/17270] Time:0.235, Train Loss:0.7682469487190247\n",
      "Epoch 0[1103/17270] Time:0.243, Train Loss:0.5494950413703918\n",
      "Epoch 0[1104/17270] Time:0.228, Train Loss:1.7967225313186646\n",
      "Epoch 0[1105/17270] Time:0.234, Train Loss:0.5560957193374634\n",
      "Epoch 0[1106/17270] Time:0.223, Train Loss:0.44757840037345886\n",
      "Epoch 0[1107/17270] Time:0.229, Train Loss:1.6497591733932495\n",
      "Epoch 0[1108/17270] Time:0.243, Train Loss:1.7292002439498901\n",
      "Epoch 0[1109/17270] Time:0.229, Train Loss:0.67843097448349\n",
      "Epoch 0[1110/17270] Time:0.235, Train Loss:0.47851601243019104\n",
      "Epoch 0[1111/17270] Time:0.249, Train Loss:0.8499314188957214\n",
      "Epoch 0[1112/17270] Time:0.236, Train Loss:0.9156455993652344\n",
      "Epoch 0[1113/17270] Time:0.238, Train Loss:0.743740975856781\n",
      "Epoch 0[1114/17270] Time:0.233, Train Loss:0.7824283242225647\n",
      "Epoch 0[1115/17270] Time:0.243, Train Loss:0.8220310807228088\n",
      "Epoch 0[1116/17270] Time:0.234, Train Loss:0.5640352368354797\n",
      "Epoch 0[1117/17270] Time:0.247, Train Loss:0.5071696639060974\n",
      "Epoch 0[1118/17270] Time:0.226, Train Loss:1.1477560997009277\n",
      "Epoch 0[1119/17270] Time:0.256, Train Loss:0.698546290397644\n",
      "Epoch 0[1120/17270] Time:0.223, Train Loss:0.9129943251609802\n",
      "Epoch 0[1121/17270] Time:0.231, Train Loss:0.740138053894043\n",
      "Epoch 0[1122/17270] Time:0.239, Train Loss:0.6423113942146301\n",
      "Epoch 0[1123/17270] Time:0.245, Train Loss:0.7920348048210144\n",
      "Epoch 0[1124/17270] Time:0.238, Train Loss:0.7907035946846008\n",
      "Epoch 0[1125/17270] Time:0.231, Train Loss:0.8005580902099609\n",
      "Epoch 0[1126/17270] Time:0.235, Train Loss:0.5610088109970093\n",
      "Epoch 0[1127/17270] Time:0.225, Train Loss:0.8074939846992493\n",
      "Epoch 0[1128/17270] Time:0.236, Train Loss:0.729455292224884\n",
      "Epoch 0[1129/17270] Time:0.244, Train Loss:0.4982546865940094\n",
      "Epoch 0[1130/17270] Time:0.231, Train Loss:0.8074224591255188\n",
      "Epoch 0[1131/17270] Time:0.24, Train Loss:0.7160254120826721\n",
      "Epoch 0[1132/17270] Time:0.231, Train Loss:0.5788829326629639\n",
      "Epoch 0[1133/17270] Time:0.242, Train Loss:0.6502499580383301\n",
      "Epoch 0[1134/17270] Time:0.225, Train Loss:0.8657506108283997\n",
      "Epoch 0[1135/17270] Time:0.245, Train Loss:0.8199799060821533\n",
      "Epoch 0[1136/17270] Time:0.246, Train Loss:0.3711114823818207\n",
      "Epoch 0[1137/17270] Time:0.227, Train Loss:2.0155367851257324\n",
      "Epoch 0[1138/17270] Time:0.244, Train Loss:0.7812988758087158\n",
      "Epoch 0[1139/17270] Time:0.231, Train Loss:0.37353289127349854\n",
      "Epoch 0[1140/17270] Time:0.229, Train Loss:0.7733287811279297\n",
      "Epoch 0[1141/17270] Time:0.246, Train Loss:0.5874701142311096\n",
      "Epoch 0[1142/17270] Time:0.235, Train Loss:0.8292810320854187\n",
      "Epoch 0[1143/17270] Time:0.238, Train Loss:0.6094134449958801\n",
      "Epoch 0[1144/17270] Time:0.227, Train Loss:0.5288000106811523\n",
      "Epoch 0[1145/17270] Time:0.237, Train Loss:0.7303228378295898\n",
      "Epoch 0[1146/17270] Time:0.225, Train Loss:0.34286338090896606\n",
      "Epoch 0[1147/17270] Time:0.247, Train Loss:0.7554846405982971\n",
      "Epoch 0[1148/17270] Time:0.228, Train Loss:1.1049104928970337\n",
      "Epoch 0[1149/17270] Time:0.239, Train Loss:1.0666794776916504\n",
      "Epoch 0[1150/17270] Time:0.223, Train Loss:0.5285788178443909\n",
      "Epoch 0[1151/17270] Time:0.241, Train Loss:0.3400384783744812\n",
      "Epoch 0[1152/17270] Time:0.24, Train Loss:0.5458911657333374\n",
      "Epoch 0[1153/17270] Time:0.235, Train Loss:0.5727407336235046\n",
      "Epoch 0[1154/17270] Time:0.223, Train Loss:0.5120459794998169\n",
      "Epoch 0[1155/17270] Time:0.238, Train Loss:0.6560535430908203\n",
      "Epoch 0[1156/17270] Time:0.241, Train Loss:0.5342442393302917\n",
      "Epoch 0[1157/17270] Time:0.235, Train Loss:0.677645206451416\n",
      "Epoch 0[1158/17270] Time:0.22, Train Loss:0.4235900640487671\n",
      "Epoch 0[1159/17270] Time:0.238, Train Loss:0.45852628350257874\n",
      "Epoch 0[1160/17270] Time:0.262, Train Loss:1.1391671895980835\n",
      "Epoch 0[1161/17270] Time:0.231, Train Loss:0.9376369714736938\n",
      "Epoch 0[1162/17270] Time:0.233, Train Loss:0.5605884790420532\n",
      "Epoch 0[1163/17270] Time:0.243, Train Loss:0.5341615676879883\n",
      "Epoch 0[1164/17270] Time:0.221, Train Loss:0.5292211771011353\n",
      "Epoch 0[1165/17270] Time:0.237, Train Loss:0.5982415080070496\n",
      "Epoch 0[1166/17270] Time:0.23, Train Loss:0.421787828207016\n",
      "Epoch 0[1167/17270] Time:0.232, Train Loss:0.615568995475769\n",
      "Epoch 0[1168/17270] Time:0.23, Train Loss:0.7644465565681458\n",
      "Epoch 0[1169/17270] Time:0.255, Train Loss:0.705216109752655\n",
      "Epoch 0[1170/17270] Time:0.23, Train Loss:1.021643042564392\n",
      "Epoch 0[1171/17270] Time:0.244, Train Loss:1.5897856950759888\n",
      "Epoch 0[1172/17270] Time:0.234, Train Loss:0.24046586453914642\n",
      "Epoch 0[1173/17270] Time:0.236, Train Loss:0.7627408504486084\n",
      "Epoch 0[1174/17270] Time:0.236, Train Loss:0.6033835411071777\n",
      "Epoch 0[1175/17270] Time:0.229, Train Loss:0.628748893737793\n",
      "Epoch 0[1176/17270] Time:0.237, Train Loss:0.9081289768218994\n",
      "Epoch 0[1177/17270] Time:0.233, Train Loss:0.4073884189128876\n",
      "Epoch 0[1178/17270] Time:0.248, Train Loss:0.6864879131317139\n",
      "Epoch 0[1179/17270] Time:0.229, Train Loss:0.5017743110656738\n",
      "Epoch 0[1180/17270] Time:0.233, Train Loss:0.7192097306251526\n",
      "Epoch 0[1181/17270] Time:0.233, Train Loss:0.46797946095466614\n",
      "Epoch 0[1182/17270] Time:0.246, Train Loss:1.2105845212936401\n",
      "Epoch 0[1183/17270] Time:0.221, Train Loss:0.7110174894332886\n",
      "Epoch 0[1184/17270] Time:0.239, Train Loss:0.868263840675354\n",
      "Epoch 0[1185/17270] Time:0.239, Train Loss:0.8042851090431213\n",
      "Epoch 0[1186/17270] Time:0.237, Train Loss:0.7146027088165283\n",
      "Epoch 0[1187/17270] Time:0.223, Train Loss:0.5529831051826477\n",
      "Epoch 0[1188/17270] Time:0.228, Train Loss:0.9660549163818359\n",
      "Epoch 0[1189/17270] Time:0.24, Train Loss:0.7380731701850891\n",
      "Epoch 0[1190/17270] Time:0.236, Train Loss:0.592807948589325\n",
      "Epoch 0[1191/17270] Time:0.236, Train Loss:0.740944504737854\n",
      "Epoch 0[1192/17270] Time:0.246, Train Loss:0.6135163903236389\n",
      "Epoch 0[1193/17270] Time:0.226, Train Loss:0.6821673512458801\n",
      "Epoch 0[1194/17270] Time:0.237, Train Loss:0.6997552514076233\n",
      "Epoch 0[1195/17270] Time:0.231, Train Loss:0.8227648735046387\n",
      "Epoch 0[1196/17270] Time:0.227, Train Loss:0.5086526870727539\n",
      "Epoch 0[1197/17270] Time:0.242, Train Loss:0.6792123317718506\n",
      "Epoch 0[1198/17270] Time:0.228, Train Loss:0.47902899980545044\n",
      "Epoch 0[1199/17270] Time:0.227, Train Loss:0.7611691355705261\n",
      "Epoch 0[1200/17270] Time:0.239, Train Loss:0.5380854606628418\n",
      "Epoch 0[1201/17270] Time:0.242, Train Loss:1.1400797367095947\n",
      "Epoch 0[1202/17270] Time:0.228, Train Loss:0.47067752480506897\n",
      "Epoch 0[1203/17270] Time:0.226, Train Loss:0.5284807682037354\n",
      "Epoch 0[1204/17270] Time:0.237, Train Loss:1.640256643295288\n",
      "Epoch 0[1205/17270] Time:0.23, Train Loss:0.4136733114719391\n",
      "Epoch 0[1206/17270] Time:0.231, Train Loss:0.67191082239151\n",
      "Epoch 0[1207/17270] Time:0.236, Train Loss:0.6334848999977112\n",
      "Epoch 0[1208/17270] Time:0.227, Train Loss:0.6765301823616028\n",
      "Epoch 0[1209/17270] Time:0.229, Train Loss:0.6695064306259155\n",
      "Epoch 0[1210/17270] Time:0.239, Train Loss:0.36198890209198\n",
      "Epoch 0[1211/17270] Time:0.232, Train Loss:1.2239261865615845\n",
      "Epoch 0[1212/17270] Time:0.229, Train Loss:0.8130735158920288\n",
      "Epoch 0[1213/17270] Time:0.243, Train Loss:0.4995731711387634\n",
      "Epoch 0[1214/17270] Time:0.22, Train Loss:0.8089186549186707\n",
      "Epoch 0[1215/17270] Time:0.246, Train Loss:1.0119404792785645\n",
      "Epoch 0[1216/17270] Time:0.236, Train Loss:1.0428186655044556\n",
      "Epoch 0[1217/17270] Time:0.243, Train Loss:0.6604889631271362\n",
      "Epoch 0[1218/17270] Time:0.221, Train Loss:0.5771006345748901\n",
      "Epoch 0[1219/17270] Time:0.251, Train Loss:0.34343862533569336\n",
      "Epoch 0[1220/17270] Time:0.227, Train Loss:0.9630475044250488\n",
      "Epoch 0[1221/17270] Time:0.229, Train Loss:1.1704505681991577\n",
      "Epoch 0[1222/17270] Time:0.235, Train Loss:0.6045880317687988\n",
      "Epoch 0[1223/17270] Time:0.226, Train Loss:0.9543129801750183\n",
      "Epoch 0[1224/17270] Time:0.228, Train Loss:0.5653220415115356\n",
      "Epoch 0[1225/17270] Time:0.236, Train Loss:0.7694107294082642\n",
      "Epoch 0[1226/17270] Time:0.242, Train Loss:0.9915503263473511\n",
      "Epoch 0[1227/17270] Time:0.238, Train Loss:0.5403180718421936\n",
      "Epoch 0[1228/17270] Time:0.229, Train Loss:1.1172925233840942\n",
      "Epoch 0[1229/17270] Time:0.233, Train Loss:0.6547526121139526\n",
      "Epoch 0[1230/17270] Time:0.236, Train Loss:0.6445226669311523\n",
      "Epoch 0[1231/17270] Time:0.237, Train Loss:0.5768300890922546\n",
      "Epoch 0[1232/17270] Time:0.243, Train Loss:0.7032015323638916\n",
      "Epoch 0[1233/17270] Time:0.231, Train Loss:0.917166531085968\n",
      "Epoch 0[1234/17270] Time:0.228, Train Loss:0.5850868225097656\n",
      "Epoch 0[1235/17270] Time:0.226, Train Loss:0.7518905997276306\n",
      "Epoch 0[1236/17270] Time:0.234, Train Loss:0.3811708986759186\n",
      "Epoch 0[1237/17270] Time:0.232, Train Loss:1.0111411809921265\n",
      "Epoch 0[1238/17270] Time:0.234, Train Loss:0.8852806687355042\n",
      "Epoch 0[1239/17270] Time:0.249, Train Loss:0.7289935350418091\n",
      "Epoch 0[1240/17270] Time:0.235, Train Loss:1.0103341341018677\n",
      "Epoch 0[1241/17270] Time:0.241, Train Loss:0.8767288327217102\n",
      "Epoch 0[1242/17270] Time:0.218, Train Loss:0.8800455927848816\n",
      "Epoch 0[1243/17270] Time:0.237, Train Loss:1.463480830192566\n",
      "Epoch 0[1244/17270] Time:0.231, Train Loss:0.8918429613113403\n",
      "Epoch 0[1245/17270] Time:0.243, Train Loss:0.6577866077423096\n",
      "Epoch 0[1246/17270] Time:0.25, Train Loss:0.7022484540939331\n",
      "Epoch 0[1247/17270] Time:0.246, Train Loss:0.5648546814918518\n",
      "Epoch 0[1248/17270] Time:0.241, Train Loss:0.6777219176292419\n",
      "Epoch 0[1249/17270] Time:0.238, Train Loss:0.8763766288757324\n",
      "Epoch 0[1250/17270] Time:0.244, Train Loss:0.6533010005950928\n",
      "Epoch 0[1251/17270] Time:0.231, Train Loss:0.6307624578475952\n",
      "Epoch 0[1252/17270] Time:0.246, Train Loss:0.7524242997169495\n",
      "Epoch 0[1253/17270] Time:0.247, Train Loss:0.6897330284118652\n",
      "Epoch 0[1254/17270] Time:0.248, Train Loss:0.9675082564353943\n",
      "Epoch 0[1255/17270] Time:0.242, Train Loss:0.6244892477989197\n",
      "Epoch 0[1256/17270] Time:0.235, Train Loss:0.6330666542053223\n",
      "Epoch 0[1257/17270] Time:0.238, Train Loss:0.6807963848114014\n",
      "Epoch 0[1258/17270] Time:0.238, Train Loss:0.6420754194259644\n",
      "Epoch 0[1259/17270] Time:0.236, Train Loss:0.6498597860336304\n",
      "Epoch 0[1260/17270] Time:0.241, Train Loss:0.7423800826072693\n",
      "Epoch 0[1261/17270] Time:0.235, Train Loss:0.6910147070884705\n",
      "Epoch 0[1262/17270] Time:0.232, Train Loss:0.7462749481201172\n",
      "Epoch 0[1263/17270] Time:0.249, Train Loss:0.8070612549781799\n",
      "Epoch 0[1264/17270] Time:0.233, Train Loss:0.5667378306388855\n",
      "Epoch 0[1265/17270] Time:0.233, Train Loss:0.4451726973056793\n",
      "Epoch 0[1266/17270] Time:0.226, Train Loss:0.8995465636253357\n",
      "Epoch 0[1267/17270] Time:0.247, Train Loss:0.6780161261558533\n",
      "Epoch 0[1268/17270] Time:0.224, Train Loss:0.8631494045257568\n",
      "Epoch 0[1269/17270] Time:0.236, Train Loss:0.666820764541626\n",
      "Epoch 0[1270/17270] Time:0.237, Train Loss:0.568509042263031\n",
      "Epoch 0[1271/17270] Time:0.231, Train Loss:0.8856021165847778\n",
      "Epoch 0[1272/17270] Time:0.232, Train Loss:0.7439646124839783\n",
      "Epoch 0[1273/17270] Time:0.24, Train Loss:1.2110413312911987\n",
      "Epoch 0[1274/17270] Time:0.239, Train Loss:0.5255240797996521\n",
      "Epoch 0[1275/17270] Time:0.243, Train Loss:0.6627978086471558\n",
      "Epoch 0[1276/17270] Time:0.231, Train Loss:0.7404585480690002\n",
      "Epoch 0[1277/17270] Time:0.241, Train Loss:0.5150684118270874\n",
      "Epoch 0[1278/17270] Time:0.221, Train Loss:0.8548065423965454\n",
      "Epoch 0[1279/17270] Time:0.239, Train Loss:0.49141886830329895\n",
      "Epoch 0[1280/17270] Time:0.239, Train Loss:1.0657877922058105\n",
      "Epoch 0[1281/17270] Time:0.23, Train Loss:0.6862199902534485\n",
      "Epoch 0[1282/17270] Time:0.229, Train Loss:0.6280280947685242\n",
      "Epoch 0[1283/17270] Time:0.229, Train Loss:0.7215471267700195\n",
      "Epoch 0[1284/17270] Time:0.235, Train Loss:0.5599942207336426\n",
      "Epoch 0[1285/17270] Time:0.227, Train Loss:0.6095733642578125\n",
      "Epoch 0[1286/17270] Time:0.234, Train Loss:0.7427987456321716\n",
      "Epoch 0[1287/17270] Time:0.228, Train Loss:0.6450326442718506\n",
      "Epoch 0[1288/17270] Time:0.232, Train Loss:0.6941270232200623\n",
      "Epoch 0[1289/17270] Time:0.231, Train Loss:0.903191328048706\n",
      "Epoch 0[1290/17270] Time:0.231, Train Loss:0.573157012462616\n",
      "Epoch 0[1291/17270] Time:0.236, Train Loss:1.0996469259262085\n",
      "Epoch 0[1292/17270] Time:0.24, Train Loss:0.7457488775253296\n",
      "Epoch 0[1293/17270] Time:0.228, Train Loss:1.007651448249817\n",
      "Epoch 0[1294/17270] Time:0.23, Train Loss:0.8441617488861084\n",
      "Epoch 0[1295/17270] Time:0.236, Train Loss:0.49356648325920105\n",
      "Epoch 0[1296/17270] Time:0.224, Train Loss:0.5752436518669128\n",
      "Epoch 0[1297/17270] Time:0.244, Train Loss:0.45199283957481384\n",
      "Epoch 0[1298/17270] Time:0.221, Train Loss:0.5388973951339722\n",
      "Epoch 0[1299/17270] Time:0.237, Train Loss:0.6817863583564758\n",
      "Epoch 0[1300/17270] Time:0.222, Train Loss:0.578464925289154\n",
      "Epoch 0[1301/17270] Time:0.241, Train Loss:0.820893406867981\n",
      "Epoch 0[1302/17270] Time:0.232, Train Loss:0.7338114380836487\n",
      "Epoch 0[1303/17270] Time:0.233, Train Loss:1.228744387626648\n",
      "Epoch 0[1304/17270] Time:0.224, Train Loss:0.47564390301704407\n",
      "Epoch 0[1305/17270] Time:0.23, Train Loss:0.8221595883369446\n",
      "Epoch 0[1306/17270] Time:0.235, Train Loss:0.9476392269134521\n",
      "Epoch 0[1307/17270] Time:0.233, Train Loss:0.8916365504264832\n",
      "Epoch 0[1308/17270] Time:0.23, Train Loss:0.5252229571342468\n",
      "Epoch 0[1309/17270] Time:0.252, Train Loss:0.753826379776001\n",
      "Epoch 0[1310/17270] Time:0.236, Train Loss:0.5292989611625671\n",
      "Epoch 0[1311/17270] Time:0.228, Train Loss:0.5382899641990662\n",
      "Epoch 0[1312/17270] Time:0.234, Train Loss:0.6351167559623718\n",
      "Epoch 0[1313/17270] Time:0.233, Train Loss:0.6793676018714905\n",
      "Epoch 0[1314/17270] Time:0.229, Train Loss:0.5588918924331665\n",
      "Epoch 0[1315/17270] Time:0.232, Train Loss:0.4301862120628357\n",
      "Epoch 0[1316/17270] Time:0.234, Train Loss:0.7499262690544128\n",
      "Epoch 0[1317/17270] Time:0.227, Train Loss:0.579875111579895\n",
      "Epoch 0[1318/17270] Time:0.235, Train Loss:0.6693243384361267\n",
      "Epoch 0[1319/17270] Time:0.232, Train Loss:0.5870228409767151\n",
      "Epoch 0[1320/17270] Time:0.236, Train Loss:0.5743964910507202\n",
      "Epoch 0[1321/17270] Time:0.236, Train Loss:0.6150385141372681\n",
      "Epoch 0[1322/17270] Time:0.228, Train Loss:0.6300950646400452\n",
      "Epoch 0[1323/17270] Time:0.235, Train Loss:0.6471344828605652\n",
      "Epoch 0[1324/17270] Time:0.222, Train Loss:0.39649370312690735\n",
      "Epoch 0[1325/17270] Time:0.244, Train Loss:0.39806416630744934\n",
      "Epoch 0[1326/17270] Time:0.233, Train Loss:0.3702332377433777\n",
      "Epoch 0[1327/17270] Time:0.24, Train Loss:0.745659351348877\n",
      "Epoch 0[1328/17270] Time:0.24, Train Loss:0.6177003979682922\n",
      "Epoch 0[1329/17270] Time:0.239, Train Loss:0.4943072199821472\n",
      "Epoch 0[1330/17270] Time:0.222, Train Loss:0.4964786767959595\n",
      "Epoch 0[1331/17270] Time:0.243, Train Loss:0.9929861426353455\n",
      "Epoch 0[1332/17270] Time:0.234, Train Loss:0.720305860042572\n",
      "Epoch 0[1333/17270] Time:0.246, Train Loss:0.39887139201164246\n",
      "Epoch 0[1334/17270] Time:0.238, Train Loss:0.5501498579978943\n",
      "Epoch 0[1335/17270] Time:0.235, Train Loss:0.5605965256690979\n",
      "Epoch 0[1336/17270] Time:0.233, Train Loss:0.4531165659427643\n",
      "Epoch 0[1337/17270] Time:0.228, Train Loss:1.3011541366577148\n",
      "Epoch 0[1338/17270] Time:0.231, Train Loss:0.8994880318641663\n",
      "Epoch 0[1339/17270] Time:0.23, Train Loss:0.5392976403236389\n",
      "Epoch 0[1340/17270] Time:0.24, Train Loss:0.6739841103553772\n",
      "Epoch 0[1341/17270] Time:0.241, Train Loss:1.1958314180374146\n",
      "Epoch 0[1342/17270] Time:0.246, Train Loss:0.75864577293396\n",
      "Epoch 0[1343/17270] Time:0.233, Train Loss:1.2762962579727173\n",
      "Epoch 0[1344/17270] Time:0.247, Train Loss:0.6197252869606018\n",
      "Epoch 0[1345/17270] Time:0.242, Train Loss:0.31921759247779846\n",
      "Epoch 0[1346/17270] Time:0.235, Train Loss:0.7661741375923157\n",
      "Epoch 0[1347/17270] Time:0.24, Train Loss:0.4994884133338928\n",
      "Epoch 0[1348/17270] Time:0.244, Train Loss:1.5729138851165771\n",
      "Epoch 0[1349/17270] Time:0.238, Train Loss:0.861430287361145\n",
      "Epoch 0[1350/17270] Time:0.236, Train Loss:0.6829874515533447\n",
      "Epoch 0[1351/17270] Time:0.24, Train Loss:0.3960070312023163\n",
      "Epoch 0[1352/17270] Time:0.237, Train Loss:0.705533504486084\n",
      "Epoch 0[1353/17270] Time:0.237, Train Loss:0.5984213948249817\n",
      "Epoch 0[1354/17270] Time:0.24, Train Loss:0.9190833568572998\n",
      "Epoch 0[1355/17270] Time:0.225, Train Loss:0.7226206064224243\n",
      "Epoch 0[1356/17270] Time:0.242, Train Loss:0.7458551526069641\n",
      "Epoch 0[1357/17270] Time:0.235, Train Loss:0.49643853306770325\n",
      "Epoch 0[1358/17270] Time:0.238, Train Loss:0.42319342494010925\n",
      "Epoch 0[1359/17270] Time:0.221, Train Loss:0.7156959176063538\n",
      "Epoch 0[1360/17270] Time:0.229, Train Loss:1.2583240270614624\n",
      "Epoch 0[1361/17270] Time:0.246, Train Loss:0.6815662980079651\n",
      "Epoch 0[1362/17270] Time:0.244, Train Loss:0.4710598886013031\n",
      "Epoch 0[1363/17270] Time:0.224, Train Loss:0.8985260725021362\n",
      "Epoch 0[1364/17270] Time:0.234, Train Loss:0.6344320178031921\n",
      "Epoch 0[1365/17270] Time:0.234, Train Loss:0.661251425743103\n",
      "Epoch 0[1366/17270] Time:0.244, Train Loss:0.5138546228408813\n",
      "Epoch 0[1367/17270] Time:0.232, Train Loss:0.5658462643623352\n",
      "Epoch 0[1368/17270] Time:0.241, Train Loss:0.5812957882881165\n",
      "Epoch 0[1369/17270] Time:0.229, Train Loss:0.3484508991241455\n",
      "Epoch 0[1370/17270] Time:0.241, Train Loss:0.8268841505050659\n",
      "Epoch 0[1371/17270] Time:0.233, Train Loss:0.6984388828277588\n",
      "Epoch 0[1372/17270] Time:0.244, Train Loss:0.7620762586593628\n",
      "Epoch 0[1373/17270] Time:0.231, Train Loss:0.49050620198249817\n",
      "Epoch 0[1374/17270] Time:0.24, Train Loss:0.7180324196815491\n",
      "Epoch 0[1375/17270] Time:0.242, Train Loss:0.6010671257972717\n",
      "Epoch 0[1376/17270] Time:0.227, Train Loss:0.41281554102897644\n",
      "Epoch 0[1377/17270] Time:0.239, Train Loss:0.7720292806625366\n",
      "Epoch 0[1378/17270] Time:0.232, Train Loss:0.9110170602798462\n",
      "Epoch 0[1379/17270] Time:0.236, Train Loss:0.5659372806549072\n",
      "Epoch 0[1380/17270] Time:0.223, Train Loss:0.5526572465896606\n",
      "Epoch 0[1381/17270] Time:0.239, Train Loss:1.0605108737945557\n",
      "Epoch 0[1382/17270] Time:0.241, Train Loss:0.6284214854240417\n",
      "Epoch 0[1383/17270] Time:0.237, Train Loss:0.7093251347541809\n",
      "Epoch 0[1384/17270] Time:0.238, Train Loss:0.5223252177238464\n",
      "Epoch 0[1385/17270] Time:0.239, Train Loss:0.5340801477432251\n",
      "Epoch 0[1386/17270] Time:0.234, Train Loss:0.7858151793479919\n",
      "Epoch 0[1387/17270] Time:0.244, Train Loss:0.8356307744979858\n",
      "Epoch 0[1388/17270] Time:0.232, Train Loss:0.6355036497116089\n",
      "Epoch 0[1389/17270] Time:0.241, Train Loss:0.7665544748306274\n",
      "Epoch 0[1390/17270] Time:0.232, Train Loss:0.5496351718902588\n",
      "Epoch 0[1391/17270] Time:0.242, Train Loss:1.1460329294204712\n",
      "Epoch 0[1392/17270] Time:0.229, Train Loss:0.5522788166999817\n",
      "Epoch 0[1393/17270] Time:0.232, Train Loss:1.2101775407791138\n",
      "Epoch 0[1394/17270] Time:0.234, Train Loss:0.9907790422439575\n",
      "Epoch 0[1395/17270] Time:0.242, Train Loss:0.7661182880401611\n",
      "Epoch 0[1396/17270] Time:0.233, Train Loss:0.5604233741760254\n",
      "Epoch 0[1397/17270] Time:0.243, Train Loss:0.6458873748779297\n",
      "Epoch 0[1398/17270] Time:0.235, Train Loss:0.5786862969398499\n",
      "Epoch 0[1399/17270] Time:0.238, Train Loss:0.6412665247917175\n",
      "Epoch 0[1400/17270] Time:0.225, Train Loss:0.8840447664260864\n",
      "Epoch 0[1401/17270] Time:0.227, Train Loss:0.6519426107406616\n",
      "Epoch 0[1402/17270] Time:0.237, Train Loss:0.3682631254196167\n",
      "Epoch 0[1403/17270] Time:0.237, Train Loss:0.7486374378204346\n",
      "Epoch 0[1404/17270] Time:0.235, Train Loss:0.664944589138031\n",
      "Epoch 0[1405/17270] Time:0.247, Train Loss:0.563385546207428\n",
      "Epoch 0[1406/17270] Time:0.233, Train Loss:0.5858287215232849\n",
      "Epoch 0[1407/17270] Time:0.243, Train Loss:0.6938417553901672\n",
      "Epoch 0[1408/17270] Time:0.235, Train Loss:0.3429858684539795\n",
      "Epoch 0[1409/17270] Time:0.236, Train Loss:0.44385141134262085\n",
      "Epoch 0[1410/17270] Time:0.234, Train Loss:1.0139745473861694\n",
      "Epoch 0[1411/17270] Time:0.235, Train Loss:0.5044575333595276\n",
      "Epoch 0[1412/17270] Time:0.237, Train Loss:0.6729041934013367\n",
      "Epoch 0[1413/17270] Time:0.225, Train Loss:1.3076773881912231\n",
      "Epoch 0[1414/17270] Time:0.233, Train Loss:0.6049355864524841\n",
      "Epoch 0[1415/17270] Time:0.245, Train Loss:0.4549168646335602\n",
      "Epoch 0[1416/17270] Time:0.231, Train Loss:0.439799964427948\n",
      "Epoch 0[1417/17270] Time:0.24, Train Loss:0.5355565547943115\n",
      "Epoch 0[1418/17270] Time:0.238, Train Loss:0.768503725528717\n",
      "Epoch 0[1419/17270] Time:0.233, Train Loss:0.5296453237533569\n",
      "Epoch 0[1420/17270] Time:0.232, Train Loss:0.5849919319152832\n",
      "Epoch 0[1421/17270] Time:0.238, Train Loss:0.8529357314109802\n",
      "Epoch 0[1422/17270] Time:0.225, Train Loss:0.41806560754776\n",
      "Epoch 0[1423/17270] Time:0.246, Train Loss:0.7462430000305176\n",
      "Epoch 0[1424/17270] Time:0.233, Train Loss:1.1029250621795654\n",
      "Epoch 0[1425/17270] Time:0.241, Train Loss:2.0020039081573486\n",
      "Epoch 0[1426/17270] Time:0.239, Train Loss:0.556743860244751\n",
      "Epoch 0[1427/17270] Time:0.233, Train Loss:0.6916782259941101\n",
      "Epoch 0[1428/17270] Time:0.234, Train Loss:0.4450285732746124\n",
      "Epoch 0[1429/17270] Time:0.244, Train Loss:0.6423012018203735\n",
      "Epoch 0[1430/17270] Time:0.239, Train Loss:0.5090078711509705\n",
      "Epoch 0[1431/17270] Time:0.234, Train Loss:1.1935776472091675\n",
      "Epoch 0[1432/17270] Time:0.23, Train Loss:0.745599091053009\n",
      "Epoch 0[1433/17270] Time:0.236, Train Loss:0.5732071399688721\n",
      "Epoch 0[1434/17270] Time:0.238, Train Loss:0.6169030666351318\n",
      "Epoch 0[1435/17270] Time:0.236, Train Loss:1.0714361667633057\n",
      "Epoch 0[1436/17270] Time:0.231, Train Loss:1.0858074426651\n",
      "Epoch 0[1437/17270] Time:0.233, Train Loss:1.3960795402526855\n",
      "Epoch 0[1438/17270] Time:0.232, Train Loss:0.7199328541755676\n",
      "Epoch 0[1439/17270] Time:0.246, Train Loss:0.5330143570899963\n",
      "Epoch 0[1440/17270] Time:0.243, Train Loss:0.5864737033843994\n",
      "Epoch 0[1441/17270] Time:0.239, Train Loss:0.6680908799171448\n",
      "Epoch 0[1442/17270] Time:0.23, Train Loss:0.5712080001831055\n",
      "Epoch 0[1443/17270] Time:0.236, Train Loss:0.6438694000244141\n",
      "Epoch 0[1444/17270] Time:0.234, Train Loss:0.489702045917511\n",
      "Epoch 0[1445/17270] Time:0.24, Train Loss:0.572080671787262\n",
      "Epoch 0[1446/17270] Time:0.223, Train Loss:0.6288841962814331\n",
      "Epoch 0[1447/17270] Time:0.237, Train Loss:0.8418028950691223\n",
      "Epoch 0[1448/17270] Time:0.236, Train Loss:0.653412938117981\n",
      "Epoch 0[1449/17270] Time:0.248, Train Loss:0.42027154564857483\n",
      "Epoch 0[1450/17270] Time:0.234, Train Loss:0.592048704624176\n",
      "Epoch 0[1451/17270] Time:0.226, Train Loss:0.6632329821586609\n",
      "Epoch 0[1452/17270] Time:0.238, Train Loss:1.0519636869430542\n",
      "Epoch 0[1453/17270] Time:0.237, Train Loss:0.734485387802124\n",
      "Epoch 0[1454/17270] Time:0.245, Train Loss:0.7813689708709717\n",
      "Epoch 0[1455/17270] Time:0.224, Train Loss:0.5134838223457336\n",
      "Epoch 0[1456/17270] Time:0.256, Train Loss:1.089789628982544\n",
      "Epoch 0[1457/17270] Time:0.22, Train Loss:0.5932484865188599\n",
      "Epoch 0[1458/17270] Time:0.235, Train Loss:1.360264778137207\n",
      "Epoch 0[1459/17270] Time:0.227, Train Loss:0.33889830112457275\n",
      "Epoch 0[1460/17270] Time:0.23, Train Loss:1.067543864250183\n",
      "Epoch 0[1461/17270] Time:0.229, Train Loss:0.5615895986557007\n",
      "Epoch 0[1462/17270] Time:0.233, Train Loss:0.4236108958721161\n",
      "Epoch 0[1463/17270] Time:0.241, Train Loss:0.771176278591156\n",
      "Epoch 0[1464/17270] Time:0.231, Train Loss:1.1610143184661865\n",
      "Epoch 0[1465/17270] Time:0.243, Train Loss:0.4366033971309662\n",
      "Epoch 0[1466/17270] Time:0.249, Train Loss:0.7403060793876648\n",
      "Epoch 0[1467/17270] Time:0.234, Train Loss:0.5533021092414856\n",
      "Epoch 0[1468/17270] Time:0.222, Train Loss:1.08768892288208\n",
      "Epoch 0[1469/17270] Time:0.249, Train Loss:0.4701448380947113\n",
      "Epoch 0[1470/17270] Time:0.223, Train Loss:0.6807109713554382\n",
      "Epoch 0[1471/17270] Time:0.238, Train Loss:0.8554327487945557\n",
      "Epoch 0[1472/17270] Time:0.227, Train Loss:0.8828105926513672\n",
      "Epoch 0[1473/17270] Time:0.243, Train Loss:0.8234577178955078\n",
      "Epoch 0[1474/17270] Time:0.233, Train Loss:0.814521312713623\n",
      "Epoch 0[1475/17270] Time:0.23, Train Loss:0.5521177053451538\n",
      "Epoch 0[1476/17270] Time:0.238, Train Loss:0.765389621257782\n",
      "Epoch 0[1477/17270] Time:0.248, Train Loss:0.6021068692207336\n",
      "Epoch 0[1478/17270] Time:0.229, Train Loss:0.8297984600067139\n",
      "Epoch 0[1479/17270] Time:0.23, Train Loss:0.9917177557945251\n",
      "Epoch 0[1480/17270] Time:0.229, Train Loss:0.8107976913452148\n",
      "Epoch 0[1481/17270] Time:0.243, Train Loss:0.48020339012145996\n",
      "Epoch 0[1482/17270] Time:0.244, Train Loss:0.7356677055358887\n",
      "Epoch 0[1483/17270] Time:0.231, Train Loss:0.6002429723739624\n",
      "Epoch 0[1484/17270] Time:0.238, Train Loss:0.41286471486091614\n",
      "Epoch 0[1485/17270] Time:0.24, Train Loss:0.6144154071807861\n",
      "Epoch 0[1486/17270] Time:0.231, Train Loss:0.8369191288948059\n",
      "Epoch 0[1487/17270] Time:0.238, Train Loss:0.5043675899505615\n",
      "Epoch 0[1488/17270] Time:0.242, Train Loss:0.4614761471748352\n",
      "Epoch 0[1489/17270] Time:0.24, Train Loss:0.656080961227417\n",
      "Epoch 0[1490/17270] Time:0.239, Train Loss:0.681826651096344\n",
      "Epoch 0[1491/17270] Time:0.23, Train Loss:1.179878830909729\n",
      "Epoch 0[1492/17270] Time:0.234, Train Loss:0.43541404604911804\n",
      "Epoch 0[1493/17270] Time:0.239, Train Loss:0.7469293475151062\n",
      "Epoch 0[1494/17270] Time:0.24, Train Loss:0.8630577921867371\n",
      "Epoch 0[1495/17270] Time:0.24, Train Loss:0.8863457441329956\n",
      "Epoch 0[1496/17270] Time:0.224, Train Loss:0.41638025641441345\n",
      "Epoch 0[1497/17270] Time:0.242, Train Loss:0.6776474118232727\n",
      "Epoch 0[1498/17270] Time:0.229, Train Loss:0.6386722922325134\n",
      "Epoch 0[1499/17270] Time:0.24, Train Loss:0.602387547492981\n",
      "Epoch 0[1500/17270] Time:0.235, Train Loss:0.5911129117012024\n",
      "Epoch 0[1501/17270] Time:0.237, Train Loss:0.9624422192573547\n",
      "Epoch 0[1502/17270] Time:0.235, Train Loss:0.5036954879760742\n",
      "Epoch 0[1503/17270] Time:0.228, Train Loss:0.7610751390457153\n",
      "Epoch 0[1504/17270] Time:0.239, Train Loss:0.837925136089325\n",
      "Epoch 0[1505/17270] Time:0.227, Train Loss:0.8041484951972961\n",
      "Epoch 0[1506/17270] Time:0.242, Train Loss:0.3858770430088043\n",
      "Epoch 0[1507/17270] Time:0.232, Train Loss:0.48008328676223755\n",
      "Epoch 0[1508/17270] Time:0.245, Train Loss:0.5570316910743713\n",
      "Epoch 0[1509/17270] Time:0.234, Train Loss:0.6437148451805115\n",
      "Epoch 0[1510/17270] Time:0.241, Train Loss:0.5765243768692017\n",
      "Epoch 0[1511/17270] Time:0.237, Train Loss:0.8205469250679016\n",
      "Epoch 0[1512/17270] Time:0.229, Train Loss:0.8707665801048279\n",
      "Epoch 0[1513/17270] Time:0.226, Train Loss:0.4779885411262512\n",
      "Epoch 0[1514/17270] Time:0.23, Train Loss:0.5176581740379333\n",
      "Epoch 0[1515/17270] Time:0.231, Train Loss:0.8237993717193604\n",
      "Epoch 0[1516/17270] Time:0.236, Train Loss:0.5332615971565247\n",
      "Epoch 0[1517/17270] Time:0.235, Train Loss:0.47299638390541077\n",
      "Epoch 0[1518/17270] Time:0.245, Train Loss:0.9830130934715271\n",
      "Epoch 0[1519/17270] Time:0.222, Train Loss:0.528118908405304\n",
      "Epoch 0[1520/17270] Time:0.246, Train Loss:0.5791918635368347\n",
      "Epoch 0[1521/17270] Time:0.232, Train Loss:0.879644513130188\n",
      "Epoch 0[1522/17270] Time:0.233, Train Loss:1.1763983964920044\n",
      "Epoch 0[1523/17270] Time:0.229, Train Loss:0.3308349549770355\n",
      "Epoch 0[1524/17270] Time:0.231, Train Loss:0.5929619073867798\n",
      "Epoch 0[1525/17270] Time:0.24, Train Loss:0.7277219295501709\n",
      "Epoch 0[1526/17270] Time:0.231, Train Loss:0.4616105854511261\n",
      "Epoch 0[1527/17270] Time:0.239, Train Loss:0.41325604915618896\n",
      "Epoch 0[1528/17270] Time:0.245, Train Loss:0.862529456615448\n",
      "Epoch 0[1529/17270] Time:0.225, Train Loss:1.0965982675552368\n",
      "Epoch 0[1530/17270] Time:0.245, Train Loss:1.4165178537368774\n",
      "Epoch 0[1531/17270] Time:0.233, Train Loss:0.44316864013671875\n",
      "Epoch 0[1532/17270] Time:0.233, Train Loss:0.4517439305782318\n",
      "Epoch 0[1533/17270] Time:0.237, Train Loss:0.7474647760391235\n",
      "Epoch 0[1534/17270] Time:0.234, Train Loss:0.636533260345459\n",
      "Epoch 0[1535/17270] Time:0.228, Train Loss:0.4912164509296417\n",
      "Epoch 0[1536/17270] Time:0.235, Train Loss:0.7355012893676758\n",
      "Epoch 0[1537/17270] Time:0.229, Train Loss:0.48431023955345154\n",
      "Epoch 0[1538/17270] Time:0.241, Train Loss:0.855603814125061\n",
      "Epoch 0[1539/17270] Time:0.252, Train Loss:0.5172957181930542\n",
      "Epoch 0[1540/17270] Time:0.233, Train Loss:0.38636407256126404\n",
      "Epoch 0[1541/17270] Time:0.226, Train Loss:0.5373942255973816\n",
      "Epoch 0[1542/17270] Time:0.245, Train Loss:1.067054033279419\n",
      "Epoch 0[1543/17270] Time:0.243, Train Loss:1.6648308038711548\n",
      "Epoch 0[1544/17270] Time:0.225, Train Loss:0.9083196520805359\n",
      "Epoch 0[1545/17270] Time:0.226, Train Loss:0.480011910200119\n",
      "Epoch 0[1546/17270] Time:0.255, Train Loss:0.6453215479850769\n",
      "Epoch 0[1547/17270] Time:0.236, Train Loss:0.515715479850769\n",
      "Epoch 0[1548/17270] Time:0.232, Train Loss:0.7653295993804932\n",
      "Epoch 0[1549/17270] Time:0.228, Train Loss:0.8092657923698425\n",
      "Epoch 0[1550/17270] Time:0.242, Train Loss:0.5793558359146118\n",
      "Epoch 0[1551/17270] Time:0.229, Train Loss:0.6585299372673035\n",
      "Epoch 0[1552/17270] Time:0.24, Train Loss:1.1974300146102905\n",
      "Epoch 0[1553/17270] Time:0.239, Train Loss:0.539155900478363\n",
      "Epoch 0[1554/17270] Time:0.246, Train Loss:0.4632074236869812\n",
      "Epoch 0[1555/17270] Time:0.226, Train Loss:0.901687502861023\n",
      "Epoch 0[1556/17270] Time:0.243, Train Loss:0.6916054487228394\n",
      "Epoch 0[1557/17270] Time:0.23, Train Loss:0.5858614444732666\n",
      "Epoch 0[1558/17270] Time:0.232, Train Loss:0.9026713967323303\n",
      "Epoch 0[1559/17270] Time:0.228, Train Loss:0.8027973175048828\n",
      "Epoch 0[1560/17270] Time:0.247, Train Loss:0.7089905142784119\n",
      "Epoch 0[1561/17270] Time:0.223, Train Loss:0.5751461982727051\n",
      "Epoch 0[1562/17270] Time:0.236, Train Loss:0.7682786583900452\n",
      "Epoch 0[1563/17270] Time:0.241, Train Loss:0.9305518269538879\n",
      "Epoch 0[1564/17270] Time:0.229, Train Loss:0.9293404817581177\n",
      "Epoch 0[1565/17270] Time:0.247, Train Loss:0.9650958180427551\n",
      "Epoch 0[1566/17270] Time:0.231, Train Loss:0.47783854603767395\n",
      "Epoch 0[1567/17270] Time:0.235, Train Loss:1.0426150560379028\n",
      "Epoch 0[1568/17270] Time:0.238, Train Loss:0.9249574542045593\n",
      "Epoch 0[1569/17270] Time:0.252, Train Loss:0.8096745610237122\n",
      "Epoch 0[1570/17270] Time:0.228, Train Loss:0.7970545291900635\n",
      "Epoch 0[1571/17270] Time:0.243, Train Loss:0.47341588139533997\n",
      "Epoch 0[1572/17270] Time:0.236, Train Loss:0.645991861820221\n",
      "Epoch 0[1573/17270] Time:0.223, Train Loss:0.8504878878593445\n",
      "Epoch 0[1574/17270] Time:0.24, Train Loss:0.7124800682067871\n",
      "Epoch 0[1575/17270] Time:0.253, Train Loss:0.4231216013431549\n",
      "Epoch 0[1576/17270] Time:0.231, Train Loss:0.443499892950058\n",
      "Epoch 0[1577/17270] Time:0.239, Train Loss:0.8272018432617188\n",
      "Epoch 0[1578/17270] Time:0.238, Train Loss:0.6602783203125\n",
      "Epoch 0[1579/17270] Time:0.23, Train Loss:0.9826031923294067\n",
      "Epoch 0[1580/17270] Time:0.24, Train Loss:0.6360212564468384\n",
      "Epoch 0[1581/17270] Time:0.234, Train Loss:0.6004740595817566\n",
      "Epoch 0[1582/17270] Time:0.232, Train Loss:0.5104264616966248\n",
      "Epoch 0[1583/17270] Time:0.237, Train Loss:0.565534770488739\n",
      "Epoch 0[1584/17270] Time:0.239, Train Loss:0.6393385529518127\n",
      "Epoch 0[1585/17270] Time:0.235, Train Loss:0.7467231750488281\n",
      "Epoch 0[1586/17270] Time:0.237, Train Loss:0.35729625821113586\n",
      "Epoch 0[1587/17270] Time:0.231, Train Loss:0.7831844687461853\n",
      "Epoch 0[1588/17270] Time:0.247, Train Loss:0.47964081168174744\n",
      "Epoch 0[1589/17270] Time:0.229, Train Loss:0.5925807356834412\n",
      "Epoch 0[1590/17270] Time:0.236, Train Loss:0.6828346848487854\n",
      "Epoch 0[1591/17270] Time:0.23, Train Loss:0.478852778673172\n",
      "Epoch 0[1592/17270] Time:0.242, Train Loss:0.6574453711509705\n",
      "Epoch 0[1593/17270] Time:0.224, Train Loss:1.039420485496521\n",
      "Epoch 0[1594/17270] Time:0.239, Train Loss:0.511620819568634\n",
      "Epoch 0[1595/17270] Time:0.226, Train Loss:0.4769730269908905\n",
      "Epoch 0[1596/17270] Time:0.234, Train Loss:0.8988257646560669\n",
      "Epoch 0[1597/17270] Time:0.247, Train Loss:1.0463049411773682\n",
      "Epoch 0[1598/17270] Time:0.233, Train Loss:0.5944805145263672\n",
      "Epoch 0[1599/17270] Time:0.237, Train Loss:0.7143332958221436\n",
      "Epoch 0[1600/17270] Time:0.243, Train Loss:0.6454463601112366\n",
      "Epoch 0[1601/17270] Time:0.227, Train Loss:0.36754465103149414\n",
      "Epoch 0[1602/17270] Time:0.243, Train Loss:0.37909480929374695\n",
      "Epoch 0[1603/17270] Time:0.235, Train Loss:0.4644056558609009\n",
      "Epoch 0[1604/17270] Time:0.245, Train Loss:0.5109972953796387\n",
      "Epoch 0[1605/17270] Time:0.225, Train Loss:0.45206785202026367\n",
      "Epoch 0[1606/17270] Time:0.238, Train Loss:1.3574976921081543\n",
      "Epoch 0[1607/17270] Time:0.25, Train Loss:0.5159380435943604\n",
      "Epoch 0[1608/17270] Time:0.231, Train Loss:0.9299129843711853\n",
      "Epoch 0[1609/17270] Time:0.244, Train Loss:0.8096120357513428\n",
      "Epoch 0[1610/17270] Time:0.225, Train Loss:0.7725310325622559\n",
      "Epoch 0[1611/17270] Time:0.23, Train Loss:0.63791424036026\n",
      "Epoch 0[1612/17270] Time:0.236, Train Loss:0.6346591711044312\n",
      "Epoch 0[1613/17270] Time:0.241, Train Loss:0.845379650592804\n",
      "Epoch 0[1614/17270] Time:0.233, Train Loss:0.3953876495361328\n",
      "Epoch 0[1615/17270] Time:0.248, Train Loss:0.827002763748169\n",
      "Epoch 0[1616/17270] Time:0.235, Train Loss:0.4210933744907379\n",
      "Epoch 0[1617/17270] Time:0.249, Train Loss:1.0635687112808228\n",
      "Epoch 0[1618/17270] Time:0.232, Train Loss:0.4606197774410248\n",
      "Epoch 0[1619/17270] Time:0.243, Train Loss:0.6411027908325195\n",
      "Epoch 0[1620/17270] Time:0.236, Train Loss:0.7405271530151367\n",
      "Epoch 0[1621/17270] Time:0.246, Train Loss:1.4072399139404297\n",
      "Epoch 0[1622/17270] Time:0.239, Train Loss:1.24851393699646\n",
      "Epoch 0[1623/17270] Time:0.234, Train Loss:0.731951117515564\n",
      "Epoch 0[1624/17270] Time:0.245, Train Loss:0.8526796102523804\n",
      "Epoch 0[1625/17270] Time:0.238, Train Loss:1.0230897665023804\n",
      "Epoch 0[1626/17270] Time:0.235, Train Loss:0.628616213798523\n",
      "Epoch 0[1627/17270] Time:0.233, Train Loss:0.6533601880073547\n",
      "Epoch 0[1628/17270] Time:0.242, Train Loss:0.9987085461616516\n",
      "Epoch 0[1629/17270] Time:0.224, Train Loss:0.6029209494590759\n",
      "Epoch 0[1630/17270] Time:0.245, Train Loss:0.5066471695899963\n",
      "Epoch 0[1631/17270] Time:0.224, Train Loss:0.6192951798439026\n",
      "Epoch 0[1632/17270] Time:0.246, Train Loss:0.5194361805915833\n",
      "Epoch 0[1633/17270] Time:0.231, Train Loss:0.4669182002544403\n",
      "Epoch 0[1634/17270] Time:0.246, Train Loss:0.6492511034011841\n",
      "Epoch 0[1635/17270] Time:0.229, Train Loss:0.6189669966697693\n",
      "Epoch 0[1636/17270] Time:0.231, Train Loss:0.7607400417327881\n",
      "Epoch 0[1637/17270] Time:0.225, Train Loss:0.8025877475738525\n",
      "Epoch 0[1638/17270] Time:0.244, Train Loss:0.4298515319824219\n",
      "Epoch 0[1639/17270] Time:0.232, Train Loss:0.6312764883041382\n",
      "Epoch 0[1640/17270] Time:0.233, Train Loss:0.764961838722229\n",
      "Epoch 0[1641/17270] Time:0.235, Train Loss:0.7073133587837219\n",
      "Epoch 0[1642/17270] Time:0.232, Train Loss:0.5215032696723938\n",
      "Epoch 0[1643/17270] Time:0.238, Train Loss:0.5366595387458801\n",
      "Epoch 0[1644/17270] Time:0.243, Train Loss:0.770007312297821\n",
      "Epoch 0[1645/17270] Time:0.235, Train Loss:1.0748209953308105\n",
      "Epoch 0[1646/17270] Time:0.245, Train Loss:0.884878933429718\n",
      "Epoch 0[1647/17270] Time:0.227, Train Loss:0.6589297652244568\n",
      "Epoch 0[1648/17270] Time:0.244, Train Loss:1.0353883504867554\n",
      "Epoch 0[1649/17270] Time:0.224, Train Loss:0.546349823474884\n",
      "Epoch 0[1650/17270] Time:0.243, Train Loss:0.5123594999313354\n",
      "Epoch 0[1651/17270] Time:0.226, Train Loss:0.4803467392921448\n",
      "Epoch 0[1652/17270] Time:0.234, Train Loss:0.7248033285140991\n",
      "Epoch 0[1653/17270] Time:0.234, Train Loss:0.9358111619949341\n",
      "Epoch 0[1654/17270] Time:0.235, Train Loss:0.7441307902336121\n",
      "Epoch 0[1655/17270] Time:0.238, Train Loss:0.7002995014190674\n",
      "Epoch 0[1656/17270] Time:0.228, Train Loss:0.6444748044013977\n",
      "Epoch 0[1657/17270] Time:0.243, Train Loss:0.5363487601280212\n",
      "Epoch 0[1658/17270] Time:0.23, Train Loss:0.7337703108787537\n",
      "Epoch 0[1659/17270] Time:0.234, Train Loss:0.6094052195549011\n",
      "Epoch 0[1660/17270] Time:0.233, Train Loss:0.6274787783622742\n",
      "Epoch 0[1661/17270] Time:0.227, Train Loss:0.577059805393219\n",
      "Epoch 0[1662/17270] Time:0.237, Train Loss:0.8039918541908264\n",
      "Epoch 0[1663/17270] Time:0.238, Train Loss:0.5619912147521973\n",
      "Epoch 0[1664/17270] Time:0.228, Train Loss:0.5707604885101318\n",
      "Epoch 0[1665/17270] Time:0.233, Train Loss:0.5888106822967529\n",
      "Epoch 0[1666/17270] Time:0.242, Train Loss:1.0387483835220337\n",
      "Epoch 0[1667/17270] Time:0.24, Train Loss:0.9390339255332947\n",
      "Epoch 0[1668/17270] Time:0.232, Train Loss:0.6037681102752686\n",
      "Epoch 0[1669/17270] Time:0.223, Train Loss:0.4233531057834625\n",
      "Epoch 0[1670/17270] Time:0.228, Train Loss:0.9140236377716064\n",
      "Epoch 0[1671/17270] Time:0.233, Train Loss:0.505657970905304\n",
      "Epoch 0[1672/17270] Time:0.236, Train Loss:0.4120187759399414\n",
      "Epoch 0[1673/17270] Time:0.225, Train Loss:0.5571788549423218\n",
      "Epoch 0[1674/17270] Time:0.241, Train Loss:1.0614616870880127\n",
      "Epoch 0[1675/17270] Time:0.238, Train Loss:0.5385885834693909\n",
      "Epoch 0[1676/17270] Time:0.228, Train Loss:0.6170176863670349\n",
      "Epoch 0[1677/17270] Time:0.227, Train Loss:0.847386360168457\n",
      "Epoch 0[1678/17270] Time:0.233, Train Loss:0.42156392335891724\n",
      "Epoch 0[1679/17270] Time:0.231, Train Loss:0.697957456111908\n",
      "Epoch 0[1680/17270] Time:0.227, Train Loss:0.5877888202667236\n",
      "Epoch 0[1681/17270] Time:0.233, Train Loss:0.7905701994895935\n",
      "Epoch 0[1682/17270] Time:0.246, Train Loss:0.5482867360115051\n",
      "Epoch 0[1683/17270] Time:0.227, Train Loss:0.36019110679626465\n",
      "Epoch 0[1684/17270] Time:0.233, Train Loss:0.7307396531105042\n",
      "Epoch 0[1685/17270] Time:0.233, Train Loss:0.8566654324531555\n",
      "Epoch 0[1686/17270] Time:0.239, Train Loss:0.5312469601631165\n",
      "Epoch 0[1687/17270] Time:0.231, Train Loss:0.7198726534843445\n",
      "Epoch 0[1688/17270] Time:0.233, Train Loss:0.6405646800994873\n",
      "Epoch 0[1689/17270] Time:0.236, Train Loss:0.4902852773666382\n",
      "Epoch 0[1690/17270] Time:0.235, Train Loss:0.4897177517414093\n",
      "Epoch 0[1691/17270] Time:0.243, Train Loss:0.6476684212684631\n",
      "Epoch 0[1692/17270] Time:0.239, Train Loss:0.8611986041069031\n",
      "Epoch 0[1693/17270] Time:0.238, Train Loss:1.870160698890686\n",
      "Epoch 0[1694/17270] Time:0.223, Train Loss:0.4621036648750305\n",
      "Epoch 0[1695/17270] Time:0.244, Train Loss:0.6845402121543884\n",
      "Epoch 0[1696/17270] Time:0.234, Train Loss:0.5236331820487976\n",
      "Epoch 0[1697/17270] Time:0.231, Train Loss:0.5957639217376709\n",
      "Epoch 0[1698/17270] Time:0.242, Train Loss:0.8852183222770691\n",
      "Epoch 0[1699/17270] Time:0.245, Train Loss:0.7682865262031555\n",
      "Epoch 0[1700/17270] Time:0.23, Train Loss:1.4108240604400635\n",
      "Epoch 0[1701/17270] Time:0.218, Train Loss:0.5137593746185303\n",
      "Epoch 0[1702/17270] Time:0.235, Train Loss:1.335780143737793\n",
      "Epoch 0[1703/17270] Time:0.224, Train Loss:0.6821902394294739\n",
      "Epoch 0[1704/17270] Time:0.234, Train Loss:0.4780736267566681\n",
      "Epoch 0[1705/17270] Time:0.226, Train Loss:0.5794559717178345\n",
      "Epoch 0[1706/17270] Time:0.238, Train Loss:0.6320710778236389\n",
      "Epoch 0[1707/17270] Time:0.243, Train Loss:1.0729299783706665\n",
      "Epoch 0[1708/17270] Time:0.233, Train Loss:0.7800340056419373\n",
      "Epoch 0[1709/17270] Time:0.241, Train Loss:0.509987473487854\n",
      "Epoch 0[1710/17270] Time:0.229, Train Loss:0.511853814125061\n",
      "Epoch 0[1711/17270] Time:0.247, Train Loss:0.5886039733886719\n",
      "Epoch 0[1712/17270] Time:0.231, Train Loss:0.9676670432090759\n",
      "Epoch 0[1713/17270] Time:0.238, Train Loss:1.5416109561920166\n",
      "Epoch 0[1714/17270] Time:0.239, Train Loss:0.7047286033630371\n",
      "Epoch 0[1715/17270] Time:0.23, Train Loss:0.44360750913619995\n",
      "Epoch 0[1716/17270] Time:0.233, Train Loss:0.5964128971099854\n",
      "Epoch 0[1717/17270] Time:0.233, Train Loss:0.4079728424549103\n",
      "Epoch 0[1718/17270] Time:0.233, Train Loss:0.6566332578659058\n",
      "Epoch 0[1719/17270] Time:0.225, Train Loss:0.47550472617149353\n",
      "Epoch 0[1720/17270] Time:0.231, Train Loss:0.9046709537506104\n",
      "Epoch 0[1721/17270] Time:0.245, Train Loss:0.4883924722671509\n",
      "Epoch 0[1722/17270] Time:0.237, Train Loss:0.42100146412849426\n",
      "Epoch 0[1723/17270] Time:0.233, Train Loss:0.6801495552062988\n",
      "Epoch 0[1724/17270] Time:0.224, Train Loss:0.8809468150138855\n",
      "Epoch 0[1725/17270] Time:0.231, Train Loss:0.6541008353233337\n",
      "Epoch 0[1726/17270] Time:0.233, Train Loss:0.5365700721740723\n",
      "Epoch 0[1727/17270] Time:0.231, Train Loss:1.0779930353164673\n",
      "Epoch 0[1728/17270] Time:0.236, Train Loss:0.8043854832649231\n",
      "Epoch 0[1729/17270] Time:0.238, Train Loss:1.2075787782669067\n",
      "Epoch 0[1730/17270] Time:0.244, Train Loss:0.8053336143493652\n",
      "Epoch 0[1731/17270] Time:0.233, Train Loss:0.5204812288284302\n",
      "Epoch 0[1732/17270] Time:0.244, Train Loss:0.9603431224822998\n",
      "Epoch 0[1733/17270] Time:0.224, Train Loss:0.517985463142395\n",
      "Epoch 0[1734/17270] Time:0.242, Train Loss:0.728408694267273\n",
      "Epoch 0[1735/17270] Time:0.234, Train Loss:0.707050085067749\n",
      "Epoch 0[1736/17270] Time:0.238, Train Loss:0.42710959911346436\n",
      "Epoch 0[1737/17270] Time:0.237, Train Loss:0.8013008236885071\n",
      "Epoch 0[1738/17270] Time:0.245, Train Loss:1.140134572982788\n",
      "Epoch 0[1739/17270] Time:0.219, Train Loss:0.44176849722862244\n",
      "Epoch 0[1740/17270] Time:0.236, Train Loss:0.40593838691711426\n",
      "Epoch 0[1741/17270] Time:0.236, Train Loss:0.5856332778930664\n",
      "Epoch 0[1742/17270] Time:0.236, Train Loss:0.8125479221343994\n",
      "Epoch 0[1743/17270] Time:0.221, Train Loss:0.7504122853279114\n",
      "Epoch 0[1744/17270] Time:0.24, Train Loss:0.42037251591682434\n",
      "Epoch 0[1745/17270] Time:0.232, Train Loss:0.5357083082199097\n",
      "Epoch 0[1746/17270] Time:0.226, Train Loss:0.7976134419441223\n",
      "Epoch 0[1747/17270] Time:0.236, Train Loss:0.5493509769439697\n",
      "Epoch 0[1748/17270] Time:0.246, Train Loss:0.7122416496276855\n",
      "Epoch 0[1749/17270] Time:0.228, Train Loss:0.6803552508354187\n",
      "Epoch 0[1750/17270] Time:0.246, Train Loss:0.7036840915679932\n",
      "Epoch 0[1751/17270] Time:0.235, Train Loss:0.35643669962882996\n",
      "Epoch 0[1752/17270] Time:0.231, Train Loss:0.49884167313575745\n",
      "Epoch 0[1753/17270] Time:0.234, Train Loss:0.5099164843559265\n",
      "Epoch 0[1754/17270] Time:0.231, Train Loss:0.4826700985431671\n",
      "Epoch 0[1755/17270] Time:0.236, Train Loss:0.3188129663467407\n",
      "Epoch 0[1756/17270] Time:0.231, Train Loss:0.6849470734596252\n",
      "Epoch 0[1757/17270] Time:0.245, Train Loss:0.5317897796630859\n",
      "Epoch 0[1758/17270] Time:0.238, Train Loss:1.5508489608764648\n",
      "Epoch 0[1759/17270] Time:0.249, Train Loss:0.43151387572288513\n",
      "Epoch 0[1760/17270] Time:0.24, Train Loss:0.5502154231071472\n",
      "Epoch 0[1761/17270] Time:0.233, Train Loss:0.5798584818840027\n",
      "Epoch 0[1762/17270] Time:0.239, Train Loss:0.6700421571731567\n",
      "Epoch 0[1763/17270] Time:0.245, Train Loss:0.5057128667831421\n",
      "Epoch 0[1764/17270] Time:0.244, Train Loss:0.2944931983947754\n",
      "Epoch 0[1765/17270] Time:0.247, Train Loss:0.9738959670066833\n",
      "Epoch 0[1766/17270] Time:0.24, Train Loss:0.7952888607978821\n",
      "Epoch 0[1767/17270] Time:0.242, Train Loss:0.3637405335903168\n",
      "Epoch 0[1768/17270] Time:0.233, Train Loss:0.953883945941925\n",
      "Epoch 0[1769/17270] Time:0.249, Train Loss:0.9137680530548096\n",
      "Epoch 0[1770/17270] Time:0.241, Train Loss:0.9336565136909485\n",
      "Epoch 0[1771/17270] Time:0.241, Train Loss:1.2100236415863037\n",
      "Epoch 0[1772/17270] Time:0.227, Train Loss:1.4214130640029907\n",
      "Epoch 0[1773/17270] Time:0.24, Train Loss:0.6934094429016113\n",
      "Epoch 0[1774/17270] Time:0.229, Train Loss:0.8499767184257507\n",
      "Epoch 0[1775/17270] Time:0.241, Train Loss:0.44734522700309753\n",
      "Epoch 0[1776/17270] Time:0.229, Train Loss:0.6968821883201599\n",
      "Epoch 0[1777/17270] Time:0.242, Train Loss:0.4991323947906494\n",
      "Epoch 0[1778/17270] Time:0.239, Train Loss:0.698030948638916\n",
      "Epoch 0[1779/17270] Time:0.24, Train Loss:0.9710535407066345\n",
      "Epoch 0[1780/17270] Time:0.241, Train Loss:0.7591378688812256\n",
      "Epoch 0[1781/17270] Time:0.236, Train Loss:0.9112447500228882\n",
      "Epoch 0[1782/17270] Time:0.243, Train Loss:0.7550612688064575\n",
      "Epoch 0[1783/17270] Time:0.246, Train Loss:0.730895459651947\n",
      "Epoch 0[1784/17270] Time:0.24, Train Loss:0.7215150594711304\n",
      "Epoch 0[1785/17270] Time:0.24, Train Loss:0.9444096088409424\n",
      "Epoch 0[1786/17270] Time:0.24, Train Loss:0.597716748714447\n",
      "Epoch 0[1787/17270] Time:0.238, Train Loss:0.6095955967903137\n",
      "Epoch 0[1788/17270] Time:0.238, Train Loss:0.7795482873916626\n",
      "Epoch 0[1789/17270] Time:0.242, Train Loss:0.8815938234329224\n",
      "Epoch 0[1790/17270] Time:0.243, Train Loss:0.6267929673194885\n",
      "Epoch 0[1791/17270] Time:0.236, Train Loss:0.40939801931381226\n",
      "Epoch 0[1792/17270] Time:0.237, Train Loss:0.5541791319847107\n",
      "Epoch 0[1793/17270] Time:0.248, Train Loss:0.3961924612522125\n",
      "Epoch 0[1794/17270] Time:0.242, Train Loss:0.49702855944633484\n",
      "Epoch 0[1795/17270] Time:0.235, Train Loss:0.5623350143432617\n",
      "Epoch 0[1796/17270] Time:0.236, Train Loss:0.4685792028903961\n",
      "Epoch 0[1797/17270] Time:0.242, Train Loss:0.4396894872188568\n",
      "Epoch 0[1798/17270] Time:0.244, Train Loss:0.5523126721382141\n",
      "Epoch 0[1799/17270] Time:0.238, Train Loss:0.5501136183738708\n",
      "Epoch 0[1800/17270] Time:0.239, Train Loss:0.658623456954956\n",
      "Epoch 0[1801/17270] Time:0.235, Train Loss:1.0794926881790161\n",
      "Epoch 0[1802/17270] Time:0.237, Train Loss:0.7107276916503906\n",
      "Epoch 0[1803/17270] Time:0.241, Train Loss:0.8941572904586792\n",
      "Epoch 0[1804/17270] Time:0.23, Train Loss:0.6609572768211365\n",
      "Epoch 0[1805/17270] Time:0.25, Train Loss:0.8718002438545227\n",
      "Epoch 0[1806/17270] Time:0.238, Train Loss:0.6516953706741333\n",
      "Epoch 0[1807/17270] Time:0.236, Train Loss:0.7243868708610535\n",
      "Epoch 0[1808/17270] Time:0.236, Train Loss:0.6652789115905762\n",
      "Epoch 0[1809/17270] Time:0.259, Train Loss:0.42269089818000793\n",
      "Epoch 0[1810/17270] Time:0.226, Train Loss:0.4272717535495758\n",
      "Epoch 0[1811/17270] Time:0.229, Train Loss:0.557274341583252\n",
      "Epoch 0[1812/17270] Time:0.243, Train Loss:0.4586055278778076\n",
      "Epoch 0[1813/17270] Time:0.234, Train Loss:0.5302524566650391\n",
      "Epoch 0[1814/17270] Time:0.243, Train Loss:0.6152873635292053\n",
      "Epoch 0[1815/17270] Time:0.234, Train Loss:0.6781243681907654\n",
      "Epoch 0[1816/17270] Time:0.237, Train Loss:0.5725391507148743\n",
      "Epoch 0[1817/17270] Time:0.235, Train Loss:0.5756372809410095\n",
      "Epoch 0[1818/17270] Time:0.242, Train Loss:0.5368828177452087\n",
      "Epoch 0[1819/17270] Time:0.233, Train Loss:1.0141326189041138\n",
      "Epoch 0[1820/17270] Time:0.237, Train Loss:0.8096215128898621\n",
      "Epoch 0[1821/17270] Time:0.232, Train Loss:0.4156966507434845\n",
      "Epoch 0[1822/17270] Time:0.243, Train Loss:0.9931385517120361\n",
      "Epoch 0[1823/17270] Time:0.229, Train Loss:0.4648173749446869\n",
      "Epoch 0[1824/17270] Time:0.242, Train Loss:0.7229042053222656\n",
      "Epoch 0[1825/17270] Time:0.238, Train Loss:0.7040067911148071\n",
      "Epoch 0[1826/17270] Time:0.234, Train Loss:0.9373521208763123\n",
      "Epoch 0[1827/17270] Time:0.228, Train Loss:1.5633275508880615\n",
      "Epoch 0[1828/17270] Time:0.231, Train Loss:1.0667192935943604\n",
      "Epoch 0[1829/17270] Time:0.233, Train Loss:0.4524429142475128\n",
      "Epoch 0[1830/17270] Time:0.229, Train Loss:0.8040210008621216\n",
      "Epoch 0[1831/17270] Time:0.227, Train Loss:0.4355637729167938\n",
      "Epoch 0[1832/17270] Time:0.228, Train Loss:0.7379553914070129\n",
      "Epoch 0[1833/17270] Time:0.243, Train Loss:0.918501615524292\n",
      "Epoch 0[1834/17270] Time:0.235, Train Loss:0.4547950327396393\n",
      "Epoch 0[1835/17270] Time:0.233, Train Loss:0.7939916849136353\n",
      "Epoch 0[1836/17270] Time:0.234, Train Loss:0.5380346775054932\n",
      "Epoch 0[1837/17270] Time:0.246, Train Loss:0.9177995920181274\n",
      "Epoch 0[1838/17270] Time:0.222, Train Loss:0.7227487564086914\n",
      "Epoch 0[1839/17270] Time:0.228, Train Loss:0.6039916276931763\n",
      "Epoch 0[1840/17270] Time:0.237, Train Loss:1.0081207752227783\n",
      "Epoch 0[1841/17270] Time:0.25, Train Loss:0.6548715829849243\n",
      "Epoch 0[1842/17270] Time:0.223, Train Loss:0.49320611357688904\n",
      "Epoch 0[1843/17270] Time:0.233, Train Loss:0.6023819446563721\n",
      "Epoch 0[1844/17270] Time:0.241, Train Loss:0.7135236859321594\n",
      "Epoch 0[1845/17270] Time:0.235, Train Loss:0.5606682300567627\n",
      "Epoch 0[1846/17270] Time:0.241, Train Loss:0.634721040725708\n",
      "Epoch 0[1847/17270] Time:0.238, Train Loss:0.40405479073524475\n",
      "Epoch 0[1848/17270] Time:0.241, Train Loss:0.6281821131706238\n",
      "Epoch 0[1849/17270] Time:0.235, Train Loss:0.6322309970855713\n",
      "Epoch 0[1850/17270] Time:0.257, Train Loss:0.8323789834976196\n",
      "Epoch 0[1851/17270] Time:0.238, Train Loss:1.1925327777862549\n",
      "Epoch 0[1852/17270] Time:0.227, Train Loss:0.681399941444397\n",
      "Epoch 0[1853/17270] Time:0.234, Train Loss:0.49828240275382996\n",
      "Epoch 0[1854/17270] Time:0.242, Train Loss:0.5426602363586426\n",
      "Epoch 0[1855/17270] Time:0.234, Train Loss:0.6390799283981323\n",
      "Epoch 0[1856/17270] Time:0.25, Train Loss:0.9046680927276611\n",
      "Epoch 0[1857/17270] Time:0.233, Train Loss:0.6172822713851929\n",
      "Epoch 0[1858/17270] Time:0.242, Train Loss:1.177524209022522\n",
      "Epoch 0[1859/17270] Time:0.236, Train Loss:0.48581889271736145\n",
      "Epoch 0[1860/17270] Time:0.231, Train Loss:0.8009207844734192\n",
      "Epoch 0[1861/17270] Time:0.233, Train Loss:0.5359753370285034\n",
      "Epoch 0[1862/17270] Time:0.237, Train Loss:0.5721325874328613\n",
      "Epoch 0[1863/17270] Time:0.227, Train Loss:0.7315060496330261\n",
      "Epoch 0[1864/17270] Time:0.238, Train Loss:1.0447953939437866\n",
      "Epoch 0[1865/17270] Time:0.225, Train Loss:0.7315433025360107\n",
      "Epoch 0[1866/17270] Time:0.237, Train Loss:1.014675259590149\n",
      "Epoch 0[1867/17270] Time:0.234, Train Loss:0.4834989905357361\n",
      "Epoch 0[1868/17270] Time:0.236, Train Loss:0.73048996925354\n",
      "Epoch 0[1869/17270] Time:0.238, Train Loss:0.7108215689659119\n",
      "Epoch 0[1870/17270] Time:0.238, Train Loss:0.5474854707717896\n",
      "Epoch 0[1871/17270] Time:0.233, Train Loss:1.15609610080719\n",
      "Epoch 0[1872/17270] Time:0.226, Train Loss:0.7382046580314636\n",
      "Epoch 0[1873/17270] Time:0.239, Train Loss:1.0578398704528809\n",
      "Epoch 0[1874/17270] Time:0.232, Train Loss:0.48776158690452576\n",
      "Epoch 0[1875/17270] Time:0.232, Train Loss:0.5802112817764282\n",
      "Epoch 0[1876/17270] Time:0.245, Train Loss:0.5353871583938599\n",
      "Epoch 0[1877/17270] Time:0.232, Train Loss:0.48485875129699707\n",
      "Epoch 0[1878/17270] Time:0.238, Train Loss:0.7075574398040771\n",
      "Epoch 0[1879/17270] Time:0.235, Train Loss:0.5521715879440308\n",
      "Epoch 0[1880/17270] Time:0.233, Train Loss:0.8774683475494385\n",
      "Epoch 0[1881/17270] Time:0.235, Train Loss:1.1996533870697021\n",
      "Epoch 0[1882/17270] Time:0.242, Train Loss:0.6440739035606384\n",
      "Epoch 0[1883/17270] Time:0.226, Train Loss:1.2128949165344238\n",
      "Epoch 0[1884/17270] Time:0.234, Train Loss:0.48429974913597107\n",
      "Epoch 0[1885/17270] Time:0.227, Train Loss:0.5266201496124268\n",
      "Epoch 0[1886/17270] Time:0.234, Train Loss:0.6076038479804993\n",
      "Epoch 0[1887/17270] Time:0.252, Train Loss:0.5805537104606628\n",
      "Epoch 0[1888/17270] Time:0.221, Train Loss:0.6632756590843201\n",
      "Epoch 0[1889/17270] Time:0.246, Train Loss:0.7569342851638794\n",
      "Epoch 0[1890/17270] Time:0.242, Train Loss:0.9110366106033325\n",
      "Epoch 0[1891/17270] Time:0.242, Train Loss:0.7293567657470703\n",
      "Epoch 0[1892/17270] Time:0.233, Train Loss:0.6722580790519714\n",
      "Epoch 0[1893/17270] Time:0.233, Train Loss:0.7076129913330078\n",
      "Epoch 0[1894/17270] Time:0.246, Train Loss:0.6002135276794434\n",
      "Epoch 0[1895/17270] Time:0.239, Train Loss:0.6028390526771545\n",
      "Epoch 0[1896/17270] Time:0.24, Train Loss:0.6208468675613403\n",
      "Epoch 0[1897/17270] Time:0.233, Train Loss:0.5390864014625549\n",
      "Epoch 0[1898/17270] Time:0.23, Train Loss:0.9742839336395264\n",
      "Epoch 0[1899/17270] Time:0.248, Train Loss:0.7831538915634155\n",
      "Epoch 0[1900/17270] Time:0.246, Train Loss:0.6018244028091431\n",
      "Epoch 0[1901/17270] Time:0.236, Train Loss:0.8274630904197693\n",
      "Epoch 0[1902/17270] Time:0.24, Train Loss:0.8537077307701111\n",
      "Epoch 0[1903/17270] Time:0.246, Train Loss:0.2842271327972412\n",
      "Epoch 0[1904/17270] Time:0.234, Train Loss:0.6520378589630127\n",
      "Epoch 0[1905/17270] Time:0.242, Train Loss:0.8135208487510681\n",
      "Epoch 0[1906/17270] Time:0.237, Train Loss:0.7357978224754333\n",
      "Epoch 0[1907/17270] Time:0.237, Train Loss:0.4538392126560211\n",
      "Epoch 0[1908/17270] Time:0.24, Train Loss:0.5627854466438293\n",
      "Epoch 0[1909/17270] Time:0.249, Train Loss:0.42340078949928284\n",
      "Epoch 0[1910/17270] Time:0.24, Train Loss:0.4868493378162384\n",
      "Epoch 0[1911/17270] Time:0.238, Train Loss:0.48277050256729126\n",
      "Epoch 0[1912/17270] Time:0.223, Train Loss:0.9902521967887878\n",
      "Epoch 0[1913/17270] Time:0.236, Train Loss:0.7098913192749023\n",
      "Epoch 0[1914/17270] Time:0.237, Train Loss:0.586222767829895\n",
      "Epoch 0[1915/17270] Time:0.247, Train Loss:0.5600199699401855\n",
      "Epoch 0[1916/17270] Time:0.241, Train Loss:0.41463756561279297\n",
      "Epoch 0[1917/17270] Time:0.229, Train Loss:0.5936582088470459\n",
      "Epoch 0[1918/17270] Time:0.237, Train Loss:0.48210838437080383\n",
      "Epoch 0[1919/17270] Time:0.241, Train Loss:0.9068851470947266\n",
      "Epoch 0[1920/17270] Time:0.236, Train Loss:0.5043581128120422\n",
      "Epoch 0[1921/17270] Time:0.24, Train Loss:0.42152783274650574\n",
      "Epoch 0[1922/17270] Time:0.222, Train Loss:0.27945515513420105\n",
      "Epoch 0[1923/17270] Time:0.238, Train Loss:1.1653474569320679\n",
      "Epoch 0[1924/17270] Time:0.25, Train Loss:1.152591347694397\n",
      "Epoch 0[1925/17270] Time:0.239, Train Loss:0.560632050037384\n",
      "Epoch 0[1926/17270] Time:0.241, Train Loss:0.727359414100647\n",
      "Epoch 0[1927/17270] Time:0.232, Train Loss:0.3007414638996124\n",
      "Epoch 0[1928/17270] Time:0.238, Train Loss:0.5835290551185608\n",
      "Epoch 0[1929/17270] Time:0.239, Train Loss:0.7812107801437378\n",
      "Epoch 0[1930/17270] Time:0.24, Train Loss:0.6164882779121399\n",
      "Epoch 0[1931/17270] Time:0.226, Train Loss:0.5654566287994385\n",
      "Epoch 0[1932/17270] Time:0.255, Train Loss:0.9805172681808472\n",
      "Epoch 0[1933/17270] Time:0.228, Train Loss:0.9912481307983398\n",
      "Epoch 0[1934/17270] Time:0.228, Train Loss:0.4176085591316223\n",
      "Epoch 0[1935/17270] Time:0.242, Train Loss:0.7369343042373657\n",
      "Epoch 0[1936/17270] Time:0.228, Train Loss:0.42352214455604553\n",
      "Epoch 0[1937/17270] Time:0.263, Train Loss:0.6163277626037598\n",
      "Epoch 0[1938/17270] Time:0.228, Train Loss:1.1578986644744873\n",
      "Epoch 0[1939/17270] Time:0.229, Train Loss:0.8368813395500183\n",
      "Epoch 0[1940/17270] Time:0.24, Train Loss:0.27827468514442444\n",
      "Epoch 0[1941/17270] Time:0.233, Train Loss:0.7270753383636475\n",
      "Epoch 0[1942/17270] Time:0.228, Train Loss:0.3726080656051636\n",
      "Epoch 0[1943/17270] Time:0.245, Train Loss:0.5652137398719788\n",
      "Epoch 0[1944/17270] Time:0.23, Train Loss:0.4112086296081543\n",
      "Epoch 0[1945/17270] Time:0.239, Train Loss:0.40044617652893066\n",
      "Epoch 0[1946/17270] Time:0.248, Train Loss:0.4230358898639679\n",
      "Epoch 0[1947/17270] Time:0.242, Train Loss:0.5980482697486877\n",
      "Epoch 0[1948/17270] Time:0.249, Train Loss:0.6610585451126099\n",
      "Epoch 0[1949/17270] Time:0.234, Train Loss:0.2196478545665741\n",
      "Epoch 0[1950/17270] Time:0.235, Train Loss:0.9017114639282227\n",
      "Epoch 0[1951/17270] Time:0.232, Train Loss:0.8689029216766357\n",
      "Epoch 0[1952/17270] Time:0.225, Train Loss:1.0598783493041992\n",
      "Epoch 0[1953/17270] Time:0.238, Train Loss:0.4958270192146301\n",
      "Epoch 0[1954/17270] Time:0.241, Train Loss:0.4415392577648163\n",
      "Epoch 0[1955/17270] Time:0.24, Train Loss:0.41142788529396057\n",
      "Epoch 0[1956/17270] Time:0.247, Train Loss:0.8546565771102905\n",
      "Epoch 0[1957/17270] Time:0.238, Train Loss:0.6251879930496216\n",
      "Epoch 0[1958/17270] Time:0.239, Train Loss:0.5759525299072266\n",
      "Epoch 0[1959/17270] Time:0.231, Train Loss:1.258037805557251\n",
      "Epoch 0[1960/17270] Time:0.237, Train Loss:0.5245900750160217\n",
      "Epoch 0[1961/17270] Time:0.233, Train Loss:0.4462105631828308\n",
      "Epoch 0[1962/17270] Time:0.244, Train Loss:0.4826728403568268\n",
      "Epoch 0[1963/17270] Time:0.239, Train Loss:0.6189122796058655\n",
      "Epoch 0[1964/17270] Time:0.23, Train Loss:0.7256160974502563\n",
      "Epoch 0[1965/17270] Time:0.251, Train Loss:0.4065527319908142\n",
      "Epoch 0[1966/17270] Time:0.236, Train Loss:0.8072751760482788\n",
      "Epoch 0[1967/17270] Time:0.234, Train Loss:0.4200882613658905\n",
      "Epoch 0[1968/17270] Time:0.242, Train Loss:0.5763179063796997\n",
      "Epoch 0[1969/17270] Time:0.225, Train Loss:0.4534720480442047\n",
      "Epoch 0[1970/17270] Time:0.232, Train Loss:0.35780519247055054\n",
      "Epoch 0[1971/17270] Time:0.238, Train Loss:0.48469433188438416\n",
      "Epoch 0[1972/17270] Time:0.235, Train Loss:0.4263039231300354\n",
      "Epoch 0[1973/17270] Time:0.234, Train Loss:0.33861464262008667\n",
      "Epoch 0[1974/17270] Time:0.24, Train Loss:0.6451444029808044\n",
      "Epoch 0[1975/17270] Time:0.235, Train Loss:0.6795850396156311\n",
      "Epoch 0[1976/17270] Time:0.236, Train Loss:0.6289774179458618\n",
      "Epoch 0[1977/17270] Time:0.243, Train Loss:0.9024327993392944\n",
      "Epoch 0[1978/17270] Time:0.232, Train Loss:0.4252396523952484\n",
      "Epoch 0[1979/17270] Time:0.247, Train Loss:0.9501792788505554\n",
      "Epoch 0[1980/17270] Time:0.236, Train Loss:0.7502930760383606\n",
      "Epoch 0[1981/17270] Time:0.237, Train Loss:0.9738491177558899\n",
      "Epoch 0[1982/17270] Time:0.236, Train Loss:0.5292770266532898\n",
      "Epoch 0[1983/17270] Time:0.244, Train Loss:0.5164508819580078\n",
      "Epoch 0[1984/17270] Time:0.237, Train Loss:0.5809070467948914\n",
      "Epoch 0[1985/17270] Time:0.234, Train Loss:0.42619311809539795\n",
      "Epoch 0[1986/17270] Time:0.236, Train Loss:0.4982827305793762\n",
      "Epoch 0[1987/17270] Time:0.227, Train Loss:0.5958192944526672\n",
      "Epoch 0[1988/17270] Time:0.226, Train Loss:1.442520022392273\n",
      "Epoch 0[1989/17270] Time:0.241, Train Loss:0.5791142582893372\n",
      "Epoch 0[1990/17270] Time:0.233, Train Loss:1.2502776384353638\n",
      "Epoch 0[1991/17270] Time:0.228, Train Loss:0.8155008554458618\n",
      "Epoch 0[1992/17270] Time:0.242, Train Loss:0.6266577839851379\n",
      "Epoch 0[1993/17270] Time:0.239, Train Loss:0.7731777429580688\n",
      "Epoch 0[1994/17270] Time:0.232, Train Loss:1.2418521642684937\n",
      "Epoch 0[1995/17270] Time:0.229, Train Loss:0.7895508408546448\n",
      "Epoch 0[1996/17270] Time:0.236, Train Loss:0.4507714807987213\n",
      "Epoch 0[1997/17270] Time:0.223, Train Loss:0.6665297746658325\n",
      "Epoch 0[1998/17270] Time:0.223, Train Loss:1.285589337348938\n",
      "Epoch 0[1999/17270] Time:0.243, Train Loss:0.7177160382270813\n",
      "Epoch 0[2000/17270] Time:0.224, Train Loss:0.7121908664703369\n",
      "Epoch 0[2001/17270] Time:0.244, Train Loss:0.40324708819389343\n",
      "Epoch 0[2002/17270] Time:0.231, Train Loss:0.5898981690406799\n",
      "Epoch 0[2003/17270] Time:0.245, Train Loss:0.3598271608352661\n",
      "Epoch 0[2004/17270] Time:0.218, Train Loss:0.6422122716903687\n",
      "Epoch 0[2005/17270] Time:0.24, Train Loss:0.7562265396118164\n",
      "Epoch 0[2006/17270] Time:0.239, Train Loss:0.49742963910102844\n",
      "Epoch 0[2007/17270] Time:0.238, Train Loss:0.6066492199897766\n",
      "Epoch 0[2008/17270] Time:0.239, Train Loss:0.5496096611022949\n",
      "Epoch 0[2009/17270] Time:0.236, Train Loss:0.602627694606781\n",
      "Epoch 0[2010/17270] Time:0.237, Train Loss:0.6967822909355164\n",
      "Epoch 0[2011/17270] Time:0.236, Train Loss:0.6660361289978027\n",
      "Epoch 0[2012/17270] Time:0.241, Train Loss:0.6038188934326172\n",
      "Epoch 0[2013/17270] Time:0.237, Train Loss:0.576047956943512\n",
      "Epoch 0[2014/17270] Time:0.237, Train Loss:0.6369088292121887\n",
      "Epoch 0[2015/17270] Time:0.234, Train Loss:0.745197057723999\n",
      "Epoch 0[2016/17270] Time:0.236, Train Loss:0.7370011806488037\n",
      "Epoch 0[2017/17270] Time:0.251, Train Loss:0.899925947189331\n",
      "Epoch 0[2018/17270] Time:0.222, Train Loss:0.3594219982624054\n",
      "Epoch 0[2019/17270] Time:0.243, Train Loss:0.7364771366119385\n",
      "Epoch 0[2020/17270] Time:0.243, Train Loss:0.566734790802002\n",
      "Epoch 0[2021/17270] Time:0.223, Train Loss:2.037137508392334\n",
      "Epoch 0[2022/17270] Time:0.241, Train Loss:0.49910208582878113\n",
      "Epoch 0[2023/17270] Time:0.227, Train Loss:0.9234232902526855\n",
      "Epoch 0[2024/17270] Time:0.234, Train Loss:1.1370419263839722\n",
      "Epoch 0[2025/17270] Time:0.246, Train Loss:1.8369886875152588\n",
      "Epoch 0[2026/17270] Time:0.227, Train Loss:0.5434697270393372\n",
      "Epoch 0[2027/17270] Time:0.24, Train Loss:0.5684221982955933\n",
      "Epoch 0[2028/17270] Time:0.22, Train Loss:0.9260060787200928\n",
      "Epoch 0[2029/17270] Time:0.238, Train Loss:0.7283673286437988\n",
      "Epoch 0[2030/17270] Time:0.227, Train Loss:0.5011676549911499\n",
      "Epoch 0[2031/17270] Time:0.238, Train Loss:0.9588666558265686\n",
      "Epoch 0[2032/17270] Time:0.223, Train Loss:0.7669396996498108\n",
      "Epoch 0[2033/17270] Time:0.244, Train Loss:0.6364725828170776\n",
      "Epoch 0[2034/17270] Time:0.231, Train Loss:0.6607208847999573\n",
      "Epoch 0[2035/17270] Time:0.233, Train Loss:0.9142711162567139\n",
      "Epoch 0[2036/17270] Time:0.227, Train Loss:0.7191129326820374\n",
      "Epoch 0[2037/17270] Time:0.237, Train Loss:0.7174921631813049\n",
      "Epoch 0[2038/17270] Time:0.233, Train Loss:0.6360841989517212\n",
      "Epoch 0[2039/17270] Time:0.228, Train Loss:0.9308754205703735\n",
      "Epoch 0[2040/17270] Time:0.233, Train Loss:0.5659255385398865\n",
      "Epoch 0[2041/17270] Time:0.232, Train Loss:0.6359462141990662\n",
      "Epoch 0[2042/17270] Time:0.243, Train Loss:0.752671480178833\n",
      "Epoch 0[2043/17270] Time:0.235, Train Loss:0.6953677535057068\n",
      "Epoch 0[2044/17270] Time:0.244, Train Loss:0.48440438508987427\n",
      "Epoch 0[2045/17270] Time:0.232, Train Loss:0.6326578855514526\n",
      "Epoch 0[2046/17270] Time:0.242, Train Loss:0.5743082165718079\n",
      "Epoch 0[2047/17270] Time:0.234, Train Loss:0.5251001715660095\n",
      "Epoch 0[2048/17270] Time:0.224, Train Loss:0.7511026263237\n",
      "Epoch 0[2049/17270] Time:0.23, Train Loss:0.6872356534004211\n",
      "Epoch 0[2050/17270] Time:0.234, Train Loss:0.55340975522995\n",
      "Epoch 0[2051/17270] Time:0.229, Train Loss:0.6205973029136658\n",
      "Epoch 0[2052/17270] Time:0.227, Train Loss:0.85940021276474\n",
      "Epoch 0[2053/17270] Time:0.228, Train Loss:0.48591476678848267\n",
      "Epoch 0[2054/17270] Time:0.229, Train Loss:0.7861512303352356\n",
      "Epoch 0[2055/17270] Time:0.234, Train Loss:0.5663290619850159\n",
      "Epoch 0[2056/17270] Time:0.229, Train Loss:0.8066298365592957\n",
      "Epoch 0[2057/17270] Time:0.248, Train Loss:0.8384931683540344\n",
      "Epoch 0[2058/17270] Time:0.229, Train Loss:0.5413388609886169\n",
      "Epoch 0[2059/17270] Time:0.231, Train Loss:0.6538220047950745\n",
      "Epoch 0[2060/17270] Time:0.232, Train Loss:0.7890046834945679\n",
      "Epoch 0[2061/17270] Time:0.246, Train Loss:0.9932530522346497\n",
      "Epoch 0[2062/17270] Time:0.227, Train Loss:1.0941293239593506\n",
      "Epoch 0[2063/17270] Time:0.241, Train Loss:0.6340160369873047\n",
      "Epoch 0[2064/17270] Time:0.232, Train Loss:0.8694671392440796\n",
      "Epoch 0[2065/17270] Time:0.234, Train Loss:0.46364694833755493\n",
      "Epoch 0[2066/17270] Time:0.241, Train Loss:1.0361449718475342\n",
      "Epoch 0[2067/17270] Time:0.231, Train Loss:0.3506488502025604\n",
      "Epoch 0[2068/17270] Time:0.242, Train Loss:0.5943563580513\n",
      "Epoch 0[2069/17270] Time:0.233, Train Loss:0.7645136713981628\n",
      "Epoch 0[2070/17270] Time:0.242, Train Loss:0.7498903274536133\n",
      "Epoch 0[2071/17270] Time:0.234, Train Loss:0.6546990871429443\n",
      "Epoch 0[2072/17270] Time:0.242, Train Loss:0.7459040880203247\n",
      "Epoch 0[2073/17270] Time:0.244, Train Loss:0.5573762655258179\n",
      "Epoch 0[2074/17270] Time:0.24, Train Loss:0.7153693437576294\n",
      "Epoch 0[2075/17270] Time:0.237, Train Loss:0.6374421119689941\n",
      "Epoch 0[2076/17270] Time:0.232, Train Loss:0.6433455348014832\n",
      "Epoch 0[2077/17270] Time:0.238, Train Loss:0.847548246383667\n",
      "Epoch 0[2078/17270] Time:0.221, Train Loss:0.8796908259391785\n",
      "Epoch 0[2079/17270] Time:0.246, Train Loss:1.2374294996261597\n",
      "Epoch 0[2080/17270] Time:0.222, Train Loss:0.5986732840538025\n",
      "Epoch 0[2081/17270] Time:0.243, Train Loss:0.7877209186553955\n",
      "Epoch 0[2082/17270] Time:0.232, Train Loss:0.6709680557250977\n",
      "Epoch 0[2083/17270] Time:0.238, Train Loss:0.614858865737915\n",
      "Epoch 0[2084/17270] Time:0.227, Train Loss:0.5099621415138245\n",
      "Epoch 0[2085/17270] Time:0.237, Train Loss:0.3420312702655792\n",
      "Epoch 0[2086/17270] Time:0.246, Train Loss:0.7937799692153931\n",
      "Epoch 0[2087/17270] Time:0.245, Train Loss:0.6081836223602295\n",
      "Epoch 0[2088/17270] Time:0.242, Train Loss:0.628534197807312\n",
      "Epoch 0[2089/17270] Time:0.235, Train Loss:0.8379536271095276\n",
      "Epoch 0[2090/17270] Time:0.235, Train Loss:0.7460900545120239\n",
      "Epoch 0[2091/17270] Time:0.233, Train Loss:0.4504980444908142\n",
      "Epoch 0[2092/17270] Time:0.251, Train Loss:2.3876984119415283\n",
      "Epoch 0[2093/17270] Time:0.24, Train Loss:0.6055105328559875\n",
      "Epoch 0[2094/17270] Time:0.233, Train Loss:0.6372370719909668\n",
      "Epoch 0[2095/17270] Time:0.239, Train Loss:0.8991547226905823\n",
      "Epoch 0[2096/17270] Time:0.24, Train Loss:0.7943115234375\n",
      "Epoch 0[2097/17270] Time:0.233, Train Loss:0.8294931650161743\n",
      "Epoch 0[2098/17270] Time:0.223, Train Loss:0.6378414630889893\n",
      "Epoch 0[2099/17270] Time:0.244, Train Loss:1.0709798336029053\n",
      "Epoch 0[2100/17270] Time:0.229, Train Loss:0.4252660870552063\n",
      "Epoch 0[2101/17270] Time:0.247, Train Loss:0.9307497143745422\n",
      "Epoch 0[2102/17270] Time:0.232, Train Loss:0.6740549802780151\n",
      "Epoch 0[2103/17270] Time:0.233, Train Loss:0.6512813568115234\n",
      "Epoch 0[2104/17270] Time:0.236, Train Loss:0.9610024690628052\n",
      "Epoch 0[2105/17270] Time:0.225, Train Loss:0.5420286655426025\n",
      "Epoch 0[2106/17270] Time:0.247, Train Loss:0.8641946315765381\n",
      "Epoch 0[2107/17270] Time:0.238, Train Loss:0.4883316457271576\n",
      "Epoch 0[2108/17270] Time:0.237, Train Loss:1.010270118713379\n",
      "Epoch 0[2109/17270] Time:0.239, Train Loss:0.6366629600524902\n",
      "Epoch 0[2110/17270] Time:0.234, Train Loss:0.535984992980957\n",
      "Epoch 0[2111/17270] Time:0.234, Train Loss:0.5453348755836487\n",
      "Epoch 0[2112/17270] Time:0.231, Train Loss:0.43411245942115784\n",
      "Epoch 0[2113/17270] Time:0.238, Train Loss:0.8554961085319519\n",
      "Epoch 0[2114/17270] Time:0.238, Train Loss:0.7433121204376221\n",
      "Epoch 0[2115/17270] Time:0.244, Train Loss:1.235874056816101\n",
      "Epoch 0[2116/17270] Time:0.232, Train Loss:0.5080658793449402\n",
      "Epoch 0[2117/17270] Time:0.233, Train Loss:0.5288791656494141\n",
      "Epoch 0[2118/17270] Time:0.233, Train Loss:0.9963791966438293\n",
      "Epoch 0[2119/17270] Time:0.233, Train Loss:0.8955933451652527\n",
      "Epoch 0[2120/17270] Time:0.247, Train Loss:0.933975875377655\n",
      "Epoch 0[2121/17270] Time:0.234, Train Loss:0.8719223141670227\n",
      "Epoch 0[2122/17270] Time:0.235, Train Loss:0.9762089848518372\n",
      "Epoch 0[2123/17270] Time:0.238, Train Loss:0.6788962483406067\n",
      "Epoch 0[2124/17270] Time:0.235, Train Loss:0.6988986730575562\n",
      "Epoch 0[2125/17270] Time:0.237, Train Loss:0.40317317843437195\n",
      "Epoch 0[2126/17270] Time:0.233, Train Loss:0.6996346712112427\n",
      "Epoch 0[2127/17270] Time:0.242, Train Loss:0.708418607711792\n",
      "Epoch 0[2128/17270] Time:0.238, Train Loss:0.7516076564788818\n",
      "Epoch 0[2129/17270] Time:0.231, Train Loss:0.5643578171730042\n",
      "Epoch 0[2130/17270] Time:0.235, Train Loss:0.6436715126037598\n",
      "Epoch 0[2131/17270] Time:0.253, Train Loss:0.6875590682029724\n",
      "Epoch 0[2132/17270] Time:0.232, Train Loss:0.8297786712646484\n",
      "Epoch 0[2133/17270] Time:0.238, Train Loss:0.5469780564308167\n",
      "Epoch 0[2134/17270] Time:0.234, Train Loss:0.6165927648544312\n",
      "Epoch 0[2135/17270] Time:0.226, Train Loss:1.4511851072311401\n",
      "Epoch 0[2136/17270] Time:0.245, Train Loss:0.5740535855293274\n",
      "Epoch 0[2137/17270] Time:0.235, Train Loss:0.9996969103813171\n",
      "Epoch 0[2138/17270] Time:0.239, Train Loss:1.2902188301086426\n",
      "Epoch 0[2139/17270] Time:0.233, Train Loss:0.9831278920173645\n",
      "Epoch 0[2140/17270] Time:0.238, Train Loss:0.9947168827056885\n",
      "Epoch 0[2141/17270] Time:0.232, Train Loss:0.8538838028907776\n",
      "Epoch 0[2142/17270] Time:0.233, Train Loss:0.7050936818122864\n",
      "Epoch 0[2143/17270] Time:0.24, Train Loss:0.8472820520401001\n",
      "Epoch 0[2144/17270] Time:0.245, Train Loss:0.80641108751297\n",
      "Epoch 0[2145/17270] Time:0.246, Train Loss:1.0675952434539795\n",
      "Epoch 0[2146/17270] Time:0.236, Train Loss:1.4391292333602905\n",
      "Epoch 0[2147/17270] Time:0.234, Train Loss:1.1116739511489868\n",
      "Epoch 0[2148/17270] Time:0.233, Train Loss:0.8028368353843689\n",
      "Epoch 0[2149/17270] Time:0.239, Train Loss:1.0576906204223633\n",
      "Epoch 0[2150/17270] Time:0.238, Train Loss:1.0971429347991943\n",
      "Epoch 0[2151/17270] Time:0.236, Train Loss:1.1715614795684814\n",
      "Epoch 0[2152/17270] Time:0.239, Train Loss:0.7446035742759705\n",
      "Epoch 0[2153/17270] Time:0.233, Train Loss:0.7137557864189148\n",
      "Epoch 0[2154/17270] Time:0.242, Train Loss:0.7527411580085754\n",
      "Epoch 0[2155/17270] Time:0.235, Train Loss:0.9571161866188049\n",
      "Epoch 0[2156/17270] Time:0.249, Train Loss:0.7181869149208069\n",
      "Epoch 0[2157/17270] Time:0.231, Train Loss:0.9204884767532349\n",
      "Epoch 0[2158/17270] Time:0.229, Train Loss:0.8321658372879028\n",
      "Epoch 0[2159/17270] Time:0.233, Train Loss:0.7539904117584229\n",
      "Epoch 0[2160/17270] Time:0.242, Train Loss:0.8314207196235657\n",
      "Epoch 0[2161/17270] Time:0.232, Train Loss:0.40137648582458496\n",
      "Epoch 0[2162/17270] Time:0.248, Train Loss:0.9313372373580933\n",
      "Epoch 0[2163/17270] Time:0.238, Train Loss:0.4079550802707672\n",
      "Epoch 0[2164/17270] Time:0.238, Train Loss:0.6397125720977783\n",
      "Epoch 0[2165/17270] Time:0.228, Train Loss:0.7898674607276917\n",
      "Epoch 0[2166/17270] Time:0.238, Train Loss:0.8961252570152283\n",
      "Epoch 0[2167/17270] Time:0.228, Train Loss:1.0432473421096802\n",
      "Epoch 0[2168/17270] Time:0.241, Train Loss:0.5723938345909119\n",
      "Epoch 0[2169/17270] Time:0.225, Train Loss:0.5880242586135864\n",
      "Epoch 0[2170/17270] Time:0.244, Train Loss:0.7457472085952759\n",
      "Epoch 0[2171/17270] Time:0.236, Train Loss:0.6715934872627258\n",
      "Epoch 0[2172/17270] Time:0.236, Train Loss:0.6035114526748657\n",
      "Epoch 0[2173/17270] Time:0.228, Train Loss:1.3420716524124146\n",
      "Epoch 0[2174/17270] Time:0.238, Train Loss:1.0448920726776123\n",
      "Epoch 0[2175/17270] Time:0.228, Train Loss:0.563098669052124\n",
      "Epoch 0[2176/17270] Time:0.234, Train Loss:0.6149641275405884\n",
      "Epoch 0[2177/17270] Time:0.235, Train Loss:0.5946437120437622\n",
      "Epoch 0[2178/17270] Time:0.23, Train Loss:0.4557951092720032\n",
      "Epoch 0[2179/17270] Time:0.226, Train Loss:0.6637327075004578\n",
      "Epoch 0[2180/17270] Time:0.236, Train Loss:0.5563309788703918\n",
      "Epoch 0[2181/17270] Time:0.226, Train Loss:0.5901004076004028\n",
      "Epoch 0[2182/17270] Time:0.232, Train Loss:0.6000927090644836\n",
      "Epoch 0[2183/17270] Time:0.233, Train Loss:0.5356460809707642\n",
      "Epoch 0[2184/17270] Time:0.242, Train Loss:0.5879846811294556\n",
      "Epoch 0[2185/17270] Time:0.226, Train Loss:0.7731038331985474\n",
      "Epoch 0[2186/17270] Time:0.238, Train Loss:0.6138713359832764\n",
      "Epoch 0[2187/17270] Time:0.232, Train Loss:0.5863682627677917\n",
      "Epoch 0[2188/17270] Time:0.234, Train Loss:0.4502701461315155\n",
      "Epoch 0[2189/17270] Time:0.223, Train Loss:0.6013001799583435\n",
      "Epoch 0[2190/17270] Time:0.238, Train Loss:0.3832353353500366\n",
      "Epoch 0[2191/17270] Time:0.238, Train Loss:0.8326259255409241\n",
      "Epoch 0[2192/17270] Time:0.223, Train Loss:0.7758499979972839\n",
      "Epoch 0[2193/17270] Time:0.233, Train Loss:0.3492181897163391\n",
      "Epoch 0[2194/17270] Time:0.223, Train Loss:0.3849523663520813\n",
      "Epoch 0[2195/17270] Time:0.233, Train Loss:0.9574083685874939\n",
      "Epoch 0[2196/17270] Time:0.23, Train Loss:0.9104207158088684\n",
      "Epoch 0[2197/17270] Time:0.245, Train Loss:0.4018876254558563\n",
      "Epoch 0[2198/17270] Time:0.227, Train Loss:0.7374242544174194\n",
      "Epoch 0[2199/17270] Time:0.24, Train Loss:0.6323578357696533\n",
      "Epoch 0[2200/17270] Time:0.254, Train Loss:0.60384202003479\n",
      "Epoch 0[2201/17270] Time:0.229, Train Loss:0.5326501727104187\n",
      "Epoch 0[2202/17270] Time:0.246, Train Loss:0.6793463826179504\n",
      "Epoch 0[2203/17270] Time:0.239, Train Loss:0.7468990087509155\n",
      "Epoch 0[2204/17270] Time:0.236, Train Loss:0.616343080997467\n",
      "Epoch 0[2205/17270] Time:0.239, Train Loss:0.5825158357620239\n",
      "Epoch 0[2206/17270] Time:0.234, Train Loss:0.6285752058029175\n",
      "Epoch 0[2207/17270] Time:0.242, Train Loss:0.5675199627876282\n",
      "Epoch 0[2208/17270] Time:0.24, Train Loss:0.7773805260658264\n",
      "Epoch 0[2209/17270] Time:0.231, Train Loss:0.6026740670204163\n",
      "Epoch 0[2210/17270] Time:0.245, Train Loss:0.7105174660682678\n",
      "Epoch 0[2211/17270] Time:0.247, Train Loss:0.7681851983070374\n",
      "Epoch 0[2212/17270] Time:0.245, Train Loss:0.3788786232471466\n",
      "Epoch 0[2213/17270] Time:0.242, Train Loss:0.37011370062828064\n",
      "Epoch 0[2214/17270] Time:0.241, Train Loss:0.5891426205635071\n",
      "Epoch 0[2215/17270] Time:0.238, Train Loss:0.6640514135360718\n",
      "Epoch 0[2216/17270] Time:0.249, Train Loss:0.7932901382446289\n",
      "Epoch 0[2217/17270] Time:0.236, Train Loss:0.843548595905304\n",
      "Epoch 0[2218/17270] Time:0.239, Train Loss:2.00998854637146\n",
      "Epoch 0[2219/17270] Time:0.234, Train Loss:0.7737939953804016\n",
      "Epoch 0[2220/17270] Time:0.245, Train Loss:0.7979394793510437\n",
      "Epoch 0[2221/17270] Time:0.233, Train Loss:0.6824289560317993\n",
      "Epoch 0[2222/17270] Time:0.234, Train Loss:0.8189112544059753\n",
      "Epoch 0[2223/17270] Time:0.25, Train Loss:0.6704760193824768\n",
      "Epoch 0[2224/17270] Time:0.239, Train Loss:0.6531707048416138\n",
      "Epoch 0[2225/17270] Time:0.24, Train Loss:0.43676748871803284\n",
      "Epoch 0[2226/17270] Time:0.224, Train Loss:0.3661428689956665\n",
      "Epoch 0[2227/17270] Time:0.244, Train Loss:0.6423845887184143\n",
      "Epoch 0[2228/17270] Time:0.249, Train Loss:0.5550134778022766\n",
      "Epoch 0[2229/17270] Time:0.241, Train Loss:0.5549108982086182\n",
      "Epoch 0[2230/17270] Time:0.239, Train Loss:0.8714550733566284\n",
      "Epoch 0[2231/17270] Time:0.239, Train Loss:0.6741840839385986\n",
      "Epoch 0[2232/17270] Time:0.242, Train Loss:0.6340279579162598\n",
      "Epoch 0[2233/17270] Time:0.235, Train Loss:0.5081156492233276\n",
      "Epoch 0[2234/17270] Time:0.233, Train Loss:0.6263484954833984\n",
      "Epoch 0[2235/17270] Time:0.237, Train Loss:0.7385169863700867\n",
      "Epoch 0[2236/17270] Time:0.237, Train Loss:0.8280213475227356\n",
      "Epoch 0[2237/17270] Time:0.238, Train Loss:0.5603132247924805\n",
      "Epoch 0[2238/17270] Time:0.238, Train Loss:0.7803231477737427\n",
      "Epoch 0[2239/17270] Time:0.231, Train Loss:0.5832411646842957\n",
      "Epoch 0[2240/17270] Time:0.235, Train Loss:1.178662896156311\n",
      "Epoch 0[2241/17270] Time:0.238, Train Loss:0.9427720904350281\n",
      "Epoch 0[2242/17270] Time:0.239, Train Loss:0.4612330198287964\n",
      "Epoch 0[2243/17270] Time:0.23, Train Loss:0.8781083822250366\n",
      "Epoch 0[2244/17270] Time:0.234, Train Loss:0.7431073784828186\n",
      "Epoch 0[2245/17270] Time:0.233, Train Loss:0.7249651551246643\n",
      "Epoch 0[2246/17270] Time:0.238, Train Loss:0.48015084862709045\n",
      "Epoch 0[2247/17270] Time:0.23, Train Loss:0.7257704138755798\n",
      "Epoch 0[2248/17270] Time:0.235, Train Loss:0.9408161640167236\n",
      "Epoch 0[2249/17270] Time:0.238, Train Loss:0.5162926316261292\n",
      "Epoch 0[2250/17270] Time:0.231, Train Loss:0.9751561880111694\n",
      "Epoch 0[2251/17270] Time:0.229, Train Loss:0.8070586323738098\n",
      "Epoch 0[2252/17270] Time:0.232, Train Loss:0.4170554578304291\n",
      "Epoch 0[2253/17270] Time:0.244, Train Loss:0.3090797960758209\n",
      "Epoch 0[2254/17270] Time:0.236, Train Loss:0.6659336090087891\n",
      "Epoch 0[2255/17270] Time:0.241, Train Loss:1.5733776092529297\n",
      "Epoch 0[2256/17270] Time:0.23, Train Loss:0.6325838565826416\n",
      "Epoch 0[2257/17270] Time:0.238, Train Loss:0.34877809882164\n",
      "Epoch 0[2258/17270] Time:0.236, Train Loss:0.768398642539978\n",
      "Epoch 0[2259/17270] Time:0.235, Train Loss:0.8960530757904053\n",
      "Epoch 0[2260/17270] Time:0.232, Train Loss:0.7420075535774231\n",
      "Epoch 0[2261/17270] Time:0.234, Train Loss:1.0380477905273438\n",
      "Epoch 0[2262/17270] Time:0.23, Train Loss:0.6127702593803406\n",
      "Epoch 0[2263/17270] Time:0.245, Train Loss:0.5666040778160095\n",
      "Epoch 0[2264/17270] Time:0.235, Train Loss:0.6921946406364441\n",
      "Epoch 0[2265/17270] Time:0.237, Train Loss:0.6780370473861694\n",
      "Epoch 0[2266/17270] Time:0.235, Train Loss:0.6536562442779541\n",
      "Epoch 0[2267/17270] Time:0.23, Train Loss:0.9253661632537842\n",
      "Epoch 0[2268/17270] Time:0.23, Train Loss:0.6066677570343018\n",
      "Epoch 0[2269/17270] Time:0.239, Train Loss:0.6217077970504761\n",
      "Epoch 0[2270/17270] Time:0.231, Train Loss:0.6156347990036011\n",
      "Epoch 0[2271/17270] Time:0.228, Train Loss:0.7401106953620911\n",
      "Epoch 0[2272/17270] Time:0.248, Train Loss:0.4430941343307495\n",
      "Epoch 0[2273/17270] Time:0.244, Train Loss:0.7277804017066956\n",
      "Epoch 0[2274/17270] Time:0.229, Train Loss:0.6543211936950684\n",
      "Epoch 0[2275/17270] Time:0.228, Train Loss:1.0459980964660645\n",
      "Epoch 0[2276/17270] Time:0.232, Train Loss:0.5708578824996948\n",
      "Epoch 0[2277/17270] Time:0.233, Train Loss:0.6462347507476807\n",
      "Epoch 0[2278/17270] Time:0.242, Train Loss:0.506039023399353\n",
      "Epoch 0[2279/17270] Time:0.234, Train Loss:0.535658597946167\n",
      "Epoch 0[2280/17270] Time:0.242, Train Loss:1.286450743675232\n",
      "Epoch 0[2281/17270] Time:0.225, Train Loss:0.3558948040008545\n",
      "Epoch 0[2282/17270] Time:0.231, Train Loss:0.7070316672325134\n",
      "Epoch 0[2283/17270] Time:0.233, Train Loss:0.7766714692115784\n",
      "Epoch 0[2284/17270] Time:0.248, Train Loss:0.5153064131736755\n",
      "Epoch 0[2285/17270] Time:0.244, Train Loss:0.5418851971626282\n",
      "Epoch 0[2286/17270] Time:0.251, Train Loss:0.8360928893089294\n",
      "Epoch 0[2287/17270] Time:0.222, Train Loss:0.8999664187431335\n",
      "Epoch 0[2288/17270] Time:0.242, Train Loss:1.2064571380615234\n",
      "Epoch 0[2289/17270] Time:0.228, Train Loss:0.6449046730995178\n",
      "Epoch 0[2290/17270] Time:0.241, Train Loss:0.7219486832618713\n",
      "Epoch 0[2291/17270] Time:0.24, Train Loss:0.4447798430919647\n",
      "Epoch 0[2292/17270] Time:0.225, Train Loss:0.8155695796012878\n",
      "Epoch 0[2293/17270] Time:0.23, Train Loss:0.865502119064331\n",
      "Epoch 0[2294/17270] Time:0.233, Train Loss:0.5530192852020264\n",
      "Epoch 0[2295/17270] Time:0.239, Train Loss:0.5792683362960815\n",
      "Epoch 0[2296/17270] Time:0.238, Train Loss:1.0583586692810059\n",
      "Epoch 0[2297/17270] Time:0.253, Train Loss:0.661182701587677\n",
      "Epoch 0[2298/17270] Time:0.225, Train Loss:0.8594593405723572\n",
      "Epoch 0[2299/17270] Time:0.251, Train Loss:0.7328291535377502\n",
      "Epoch 0[2300/17270] Time:0.217, Train Loss:0.6394312977790833\n",
      "Epoch 0[2301/17270] Time:0.24, Train Loss:0.6221484541893005\n",
      "Epoch 0[2302/17270] Time:0.23, Train Loss:0.7861037254333496\n",
      "Epoch 0[2303/17270] Time:0.237, Train Loss:0.667608916759491\n",
      "Epoch 0[2304/17270] Time:0.239, Train Loss:0.8050597310066223\n",
      "Epoch 0[2305/17270] Time:0.235, Train Loss:0.949517548084259\n",
      "Epoch 0[2306/17270] Time:0.245, Train Loss:0.559483528137207\n",
      "Epoch 0[2307/17270] Time:0.237, Train Loss:0.6091520190238953\n",
      "Epoch 0[2308/17270] Time:0.233, Train Loss:0.6584768891334534\n",
      "Epoch 0[2309/17270] Time:0.234, Train Loss:0.925584614276886\n",
      "Epoch 0[2310/17270] Time:0.238, Train Loss:0.529600977897644\n",
      "Epoch 0[2311/17270] Time:0.239, Train Loss:0.9718289971351624\n",
      "Epoch 0[2312/17270] Time:0.226, Train Loss:0.5506405234336853\n",
      "Epoch 0[2313/17270] Time:0.232, Train Loss:0.7368277907371521\n",
      "Epoch 0[2314/17270] Time:0.23, Train Loss:0.949185311794281\n",
      "Epoch 0[2315/17270] Time:0.242, Train Loss:0.789794385433197\n",
      "Epoch 0[2316/17270] Time:0.229, Train Loss:0.6989287734031677\n",
      "Epoch 0[2317/17270] Time:0.231, Train Loss:0.5423187613487244\n",
      "Epoch 0[2318/17270] Time:0.238, Train Loss:0.596310019493103\n",
      "Epoch 0[2319/17270] Time:0.246, Train Loss:0.8898261785507202\n",
      "Epoch 0[2320/17270] Time:0.217, Train Loss:0.6565982699394226\n",
      "Epoch 0[2321/17270] Time:0.24, Train Loss:0.6425306797027588\n",
      "Epoch 0[2322/17270] Time:0.235, Train Loss:0.39084163308143616\n",
      "Epoch 0[2323/17270] Time:0.235, Train Loss:0.6700196266174316\n",
      "Epoch 0[2324/17270] Time:0.234, Train Loss:0.9393553137779236\n",
      "Epoch 0[2325/17270] Time:0.227, Train Loss:0.703509509563446\n",
      "Epoch 0[2326/17270] Time:0.233, Train Loss:0.8532898426055908\n",
      "Epoch 0[2327/17270] Time:0.235, Train Loss:0.49972113966941833\n",
      "Epoch 0[2328/17270] Time:0.231, Train Loss:0.5903488397598267\n",
      "Epoch 0[2329/17270] Time:0.231, Train Loss:0.981341540813446\n",
      "Epoch 0[2330/17270] Time:0.234, Train Loss:0.7658420205116272\n",
      "Epoch 0[2331/17270] Time:0.242, Train Loss:0.5595181584358215\n",
      "Epoch 0[2332/17270] Time:0.242, Train Loss:0.8540212512016296\n",
      "Epoch 0[2333/17270] Time:0.24, Train Loss:1.2192246913909912\n",
      "Epoch 0[2334/17270] Time:0.239, Train Loss:0.4571550488471985\n",
      "Epoch 0[2335/17270] Time:0.241, Train Loss:0.4741649925708771\n",
      "Epoch 0[2336/17270] Time:0.255, Train Loss:0.5954193472862244\n",
      "Epoch 0[2337/17270] Time:0.247, Train Loss:0.5059673190116882\n",
      "Epoch 0[2338/17270] Time:0.244, Train Loss:0.4551892578601837\n",
      "Epoch 0[2339/17270] Time:0.228, Train Loss:0.6453464031219482\n",
      "Epoch 0[2340/17270] Time:0.237, Train Loss:0.5987744331359863\n",
      "Epoch 0[2341/17270] Time:0.242, Train Loss:1.488969087600708\n",
      "Epoch 0[2342/17270] Time:0.242, Train Loss:0.6819926500320435\n",
      "Epoch 0[2343/17270] Time:0.222, Train Loss:0.6143837571144104\n",
      "Epoch 0[2344/17270] Time:0.243, Train Loss:0.7598737478256226\n",
      "Epoch 0[2345/17270] Time:0.239, Train Loss:0.38714665174484253\n",
      "Epoch 0[2346/17270] Time:0.24, Train Loss:0.5240650773048401\n",
      "Epoch 0[2347/17270] Time:0.224, Train Loss:0.4904068410396576\n",
      "Epoch 0[2348/17270] Time:0.246, Train Loss:0.7761174440383911\n",
      "Epoch 0[2349/17270] Time:0.234, Train Loss:0.36516883969306946\n",
      "Epoch 0[2350/17270] Time:0.231, Train Loss:0.745931088924408\n",
      "Epoch 0[2351/17270] Time:0.227, Train Loss:0.7325708270072937\n",
      "Epoch 0[2352/17270] Time:0.245, Train Loss:0.9148146510124207\n",
      "Epoch 0[2353/17270] Time:0.221, Train Loss:0.806263267993927\n",
      "Epoch 0[2354/17270] Time:0.235, Train Loss:0.4964112937450409\n",
      "Epoch 0[2355/17270] Time:0.223, Train Loss:0.9262847304344177\n",
      "Epoch 0[2356/17270] Time:0.24, Train Loss:0.6387794613838196\n",
      "Epoch 0[2357/17270] Time:0.229, Train Loss:0.390163779258728\n",
      "Epoch 0[2358/17270] Time:0.225, Train Loss:1.1213668584823608\n",
      "Epoch 0[2359/17270] Time:0.239, Train Loss:0.7476763725280762\n",
      "Epoch 0[2360/17270] Time:0.239, Train Loss:0.6308049559593201\n",
      "Epoch 0[2361/17270] Time:0.241, Train Loss:0.6985830664634705\n",
      "Epoch 0[2362/17270] Time:0.241, Train Loss:0.42620649933815\n",
      "Epoch 0[2363/17270] Time:0.24, Train Loss:0.4579891562461853\n",
      "Epoch 0[2364/17270] Time:0.241, Train Loss:0.38248902559280396\n",
      "Epoch 0[2365/17270] Time:0.232, Train Loss:0.5741458535194397\n",
      "Epoch 0[2366/17270] Time:0.244, Train Loss:0.8015007376670837\n",
      "Epoch 0[2367/17270] Time:0.239, Train Loss:0.6829623579978943\n",
      "Epoch 0[2368/17270] Time:0.232, Train Loss:0.734363317489624\n",
      "Epoch 0[2369/17270] Time:0.237, Train Loss:0.669367253780365\n",
      "Epoch 0[2370/17270] Time:0.231, Train Loss:0.8244930505752563\n",
      "Epoch 0[2371/17270] Time:0.245, Train Loss:0.4917199909687042\n",
      "Epoch 0[2372/17270] Time:0.233, Train Loss:0.5829934477806091\n",
      "Epoch 0[2373/17270] Time:0.23, Train Loss:0.4597555696964264\n",
      "Epoch 0[2374/17270] Time:0.242, Train Loss:0.5442500710487366\n",
      "Epoch 0[2375/17270] Time:0.221, Train Loss:1.5217485427856445\n",
      "Epoch 0[2376/17270] Time:0.241, Train Loss:0.40502309799194336\n",
      "Epoch 0[2377/17270] Time:0.221, Train Loss:1.4473719596862793\n",
      "Epoch 0[2378/17270] Time:0.242, Train Loss:0.7453020811080933\n",
      "Epoch 0[2379/17270] Time:0.231, Train Loss:0.4236452877521515\n",
      "Epoch 0[2380/17270] Time:0.239, Train Loss:0.6408156752586365\n",
      "Epoch 0[2381/17270] Time:0.241, Train Loss:0.6802276968955994\n",
      "Epoch 0[2382/17270] Time:0.23, Train Loss:0.6954643726348877\n",
      "Epoch 0[2383/17270] Time:0.233, Train Loss:1.1361253261566162\n",
      "Epoch 0[2384/17270] Time:0.24, Train Loss:0.5745898485183716\n",
      "Epoch 0[2385/17270] Time:0.241, Train Loss:0.916845440864563\n",
      "Epoch 0[2386/17270] Time:0.232, Train Loss:1.0615252256393433\n",
      "Epoch 0[2387/17270] Time:0.226, Train Loss:0.4678451716899872\n",
      "Epoch 0[2388/17270] Time:0.229, Train Loss:0.7234375476837158\n",
      "Epoch 0[2389/17270] Time:0.243, Train Loss:1.0334727764129639\n",
      "Epoch 0[2390/17270] Time:0.242, Train Loss:0.8818359971046448\n",
      "Epoch 0[2391/17270] Time:0.224, Train Loss:0.7147160768508911\n",
      "Epoch 0[2392/17270] Time:0.233, Train Loss:0.6615360975265503\n",
      "Epoch 0[2393/17270] Time:0.245, Train Loss:0.44156110286712646\n",
      "Epoch 0[2394/17270] Time:0.233, Train Loss:0.4834710955619812\n",
      "Epoch 0[2395/17270] Time:0.241, Train Loss:0.9909999370574951\n",
      "Epoch 0[2396/17270] Time:0.233, Train Loss:0.7164517641067505\n",
      "Epoch 0[2397/17270] Time:0.231, Train Loss:0.4597715437412262\n",
      "Epoch 0[2398/17270] Time:0.231, Train Loss:0.41794195771217346\n",
      "Epoch 0[2399/17270] Time:0.238, Train Loss:0.878854513168335\n",
      "Epoch 0[2400/17270] Time:0.229, Train Loss:0.8585543632507324\n",
      "Epoch 0[2401/17270] Time:0.238, Train Loss:0.9197384119033813\n",
      "Epoch 0[2402/17270] Time:0.236, Train Loss:0.4730164110660553\n",
      "Epoch 0[2403/17270] Time:0.235, Train Loss:0.7937206029891968\n",
      "Epoch 0[2404/17270] Time:0.233, Train Loss:0.9858821034431458\n",
      "Epoch 0[2405/17270] Time:0.231, Train Loss:0.5749503970146179\n",
      "Epoch 0[2406/17270] Time:0.25, Train Loss:0.41207942366600037\n",
      "Epoch 0[2407/17270] Time:0.243, Train Loss:0.5224987864494324\n",
      "Epoch 0[2408/17270] Time:0.221, Train Loss:0.5428962707519531\n",
      "Epoch 0[2409/17270] Time:0.25, Train Loss:0.6113730669021606\n",
      "Epoch 0[2410/17270] Time:0.227, Train Loss:0.5664790272712708\n",
      "Epoch 0[2411/17270] Time:0.236, Train Loss:0.6402668952941895\n",
      "Epoch 0[2412/17270] Time:0.222, Train Loss:1.3379145860671997\n",
      "Epoch 0[2413/17270] Time:0.244, Train Loss:0.6481689810752869\n",
      "Epoch 0[2414/17270] Time:0.236, Train Loss:1.0277787446975708\n",
      "Epoch 0[2415/17270] Time:0.246, Train Loss:0.4289865493774414\n",
      "Epoch 0[2416/17270] Time:0.238, Train Loss:1.0268502235412598\n",
      "Epoch 0[2417/17270] Time:0.229, Train Loss:0.2932548522949219\n",
      "Epoch 0[2418/17270] Time:0.247, Train Loss:0.9063841700553894\n",
      "Epoch 0[2419/17270] Time:0.231, Train Loss:0.5140216946601868\n",
      "Epoch 0[2420/17270] Time:0.226, Train Loss:0.5699468851089478\n",
      "Epoch 0[2421/17270] Time:0.24, Train Loss:0.7961661219596863\n",
      "Epoch 0[2422/17270] Time:0.239, Train Loss:1.0630733966827393\n",
      "Epoch 0[2423/17270] Time:0.239, Train Loss:0.5740217566490173\n",
      "Epoch 0[2424/17270] Time:0.241, Train Loss:0.47640806436538696\n",
      "Epoch 0[2425/17270] Time:0.234, Train Loss:1.0500495433807373\n",
      "Epoch 0[2426/17270] Time:0.254, Train Loss:0.7530948519706726\n",
      "Epoch 0[2427/17270] Time:0.232, Train Loss:0.6166006326675415\n",
      "Epoch 0[2428/17270] Time:0.231, Train Loss:0.6206179261207581\n",
      "Epoch 0[2429/17270] Time:0.251, Train Loss:0.7504522800445557\n",
      "Epoch 0[2430/17270] Time:0.245, Train Loss:0.8012257218360901\n",
      "Epoch 0[2431/17270] Time:0.234, Train Loss:1.3131119012832642\n",
      "Epoch 0[2432/17270] Time:0.226, Train Loss:0.6810093522071838\n",
      "Epoch 0[2433/17270] Time:0.228, Train Loss:0.5638378262519836\n",
      "Epoch 0[2434/17270] Time:0.244, Train Loss:0.5391412377357483\n",
      "Epoch 0[2435/17270] Time:0.237, Train Loss:0.5360881090164185\n",
      "Epoch 0[2436/17270] Time:0.235, Train Loss:0.6986953020095825\n",
      "Epoch 0[2437/17270] Time:0.235, Train Loss:0.38869014382362366\n",
      "Epoch 0[2438/17270] Time:0.236, Train Loss:0.6962759494781494\n",
      "Epoch 0[2439/17270] Time:0.225, Train Loss:0.9823464751243591\n",
      "Epoch 0[2440/17270] Time:0.226, Train Loss:0.650182843208313\n",
      "Epoch 0[2441/17270] Time:0.231, Train Loss:0.4973374009132385\n",
      "Epoch 0[2442/17270] Time:0.227, Train Loss:0.4560350179672241\n",
      "Epoch 0[2443/17270] Time:0.246, Train Loss:1.0084879398345947\n",
      "Epoch 0[2444/17270] Time:0.234, Train Loss:0.8329131007194519\n",
      "Epoch 0[2445/17270] Time:0.23, Train Loss:1.3526315689086914\n",
      "Epoch 0[2446/17270] Time:0.238, Train Loss:0.6985238790512085\n",
      "Epoch 0[2447/17270] Time:0.242, Train Loss:0.4841567277908325\n",
      "Epoch 0[2448/17270] Time:0.234, Train Loss:1.25517737865448\n",
      "Epoch 0[2449/17270] Time:0.24, Train Loss:0.4558941423892975\n",
      "Epoch 0[2450/17270] Time:0.227, Train Loss:0.9179472923278809\n",
      "Epoch 0[2451/17270] Time:0.242, Train Loss:0.6090170741081238\n",
      "Epoch 0[2452/17270] Time:0.238, Train Loss:0.635155975818634\n",
      "Epoch 0[2453/17270] Time:0.229, Train Loss:0.5194277763366699\n",
      "Epoch 0[2454/17270] Time:0.239, Train Loss:0.48095083236694336\n",
      "Epoch 0[2455/17270] Time:0.232, Train Loss:0.6177614331245422\n",
      "Epoch 0[2456/17270] Time:0.239, Train Loss:0.5359461307525635\n",
      "Epoch 0[2457/17270] Time:0.249, Train Loss:0.6204699873924255\n",
      "Epoch 0[2458/17270] Time:0.232, Train Loss:0.7601796984672546\n",
      "Epoch 0[2459/17270] Time:0.242, Train Loss:0.6427980661392212\n",
      "Epoch 0[2460/17270] Time:0.221, Train Loss:0.4919518530368805\n",
      "Epoch 0[2461/17270] Time:0.236, Train Loss:0.6045902967453003\n",
      "Epoch 0[2462/17270] Time:0.228, Train Loss:1.0670608282089233\n",
      "Epoch 0[2463/17270] Time:0.238, Train Loss:0.6664303541183472\n",
      "Epoch 0[2464/17270] Time:0.233, Train Loss:0.6112537980079651\n",
      "Epoch 0[2465/17270] Time:0.237, Train Loss:0.7476813793182373\n",
      "Epoch 0[2466/17270] Time:0.228, Train Loss:0.9908149838447571\n",
      "Epoch 0[2467/17270] Time:0.237, Train Loss:0.40663856267929077\n",
      "Epoch 0[2468/17270] Time:0.243, Train Loss:0.3539442718029022\n",
      "Epoch 0[2469/17270] Time:0.248, Train Loss:0.6921295523643494\n",
      "Epoch 0[2470/17270] Time:0.229, Train Loss:0.7703882455825806\n",
      "Epoch 0[2471/17270] Time:0.243, Train Loss:0.7741456031799316\n",
      "Epoch 0[2472/17270] Time:0.24, Train Loss:0.5674660801887512\n",
      "Epoch 0[2473/17270] Time:0.239, Train Loss:0.6379203796386719\n",
      "Epoch 0[2474/17270] Time:0.234, Train Loss:0.7352262735366821\n",
      "Epoch 0[2475/17270] Time:0.236, Train Loss:0.867513120174408\n",
      "Epoch 0[2476/17270] Time:0.24, Train Loss:0.5388051271438599\n",
      "Epoch 0[2477/17270] Time:0.23, Train Loss:0.4364246428012848\n",
      "Epoch 0[2478/17270] Time:0.24, Train Loss:0.5949741005897522\n",
      "Epoch 0[2479/17270] Time:0.263, Train Loss:0.5552465319633484\n",
      "Epoch 0[2480/17270] Time:0.24, Train Loss:0.7109531164169312\n",
      "Epoch 0[2481/17270] Time:0.23, Train Loss:0.7336026430130005\n",
      "Epoch 0[2482/17270] Time:0.225, Train Loss:0.5316003561019897\n",
      "Epoch 0[2483/17270] Time:0.241, Train Loss:0.5927367210388184\n",
      "Epoch 0[2484/17270] Time:0.235, Train Loss:0.6314536929130554\n",
      "Epoch 0[2485/17270] Time:0.242, Train Loss:0.5589272379875183\n",
      "Epoch 0[2486/17270] Time:0.234, Train Loss:0.49759477376937866\n",
      "Epoch 0[2487/17270] Time:0.247, Train Loss:0.6358695030212402\n",
      "Epoch 0[2488/17270] Time:0.235, Train Loss:0.6302574872970581\n",
      "Epoch 0[2489/17270] Time:0.24, Train Loss:0.39626404643058777\n",
      "Epoch 0[2490/17270] Time:0.219, Train Loss:0.546360194683075\n",
      "Epoch 0[2491/17270] Time:0.241, Train Loss:0.4560253918170929\n",
      "Epoch 0[2492/17270] Time:0.235, Train Loss:0.5950846076011658\n",
      "Epoch 0[2493/17270] Time:0.234, Train Loss:0.5236083269119263\n",
      "Epoch 0[2494/17270] Time:0.234, Train Loss:1.177858829498291\n",
      "Epoch 0[2495/17270] Time:0.24, Train Loss:0.28432339429855347\n",
      "Epoch 0[2496/17270] Time:0.237, Train Loss:0.6417573094367981\n",
      "Epoch 0[2497/17270] Time:0.234, Train Loss:0.3931739926338196\n",
      "Epoch 0[2498/17270] Time:0.238, Train Loss:0.8492380976676941\n",
      "Epoch 0[2499/17270] Time:0.232, Train Loss:0.6877108812332153\n",
      "Epoch 0[2500/17270] Time:0.235, Train Loss:0.8112941980361938\n",
      "Epoch 0[2501/17270] Time:0.226, Train Loss:0.6067050695419312\n",
      "Epoch 0[2502/17270] Time:0.234, Train Loss:0.5231195688247681\n",
      "Epoch 0[2503/17270] Time:0.23, Train Loss:0.5832529067993164\n",
      "Epoch 0[2504/17270] Time:0.236, Train Loss:0.7139055728912354\n",
      "Epoch 0[2505/17270] Time:0.237, Train Loss:0.8469762206077576\n",
      "Epoch 0[2506/17270] Time:0.222, Train Loss:0.5956752896308899\n",
      "Epoch 0[2507/17270] Time:0.236, Train Loss:0.6736633777618408\n",
      "Epoch 0[2508/17270] Time:0.227, Train Loss:0.6283822655677795\n",
      "Epoch 0[2509/17270] Time:0.234, Train Loss:1.1879618167877197\n",
      "Epoch 0[2510/17270] Time:0.226, Train Loss:0.5858239531517029\n",
      "Epoch 0[2511/17270] Time:0.226, Train Loss:0.6319292783737183\n",
      "Epoch 0[2512/17270] Time:0.249, Train Loss:0.6048446893692017\n",
      "Epoch 0[2513/17270] Time:0.235, Train Loss:0.7109329104423523\n",
      "Epoch 0[2514/17270] Time:0.226, Train Loss:0.4295918345451355\n",
      "Epoch 0[2515/17270] Time:0.233, Train Loss:1.232166051864624\n",
      "Epoch 0[2516/17270] Time:0.237, Train Loss:0.5120093822479248\n",
      "Epoch 0[2517/17270] Time:0.235, Train Loss:0.8751773238182068\n",
      "Epoch 0[2518/17270] Time:0.226, Train Loss:0.4847606420516968\n",
      "Epoch 0[2519/17270] Time:0.236, Train Loss:0.583853542804718\n",
      "Epoch 0[2520/17270] Time:0.251, Train Loss:0.7068681120872498\n",
      "Epoch 0[2521/17270] Time:0.23, Train Loss:1.6073758602142334\n",
      "Epoch 0[2522/17270] Time:0.239, Train Loss:0.9965365529060364\n",
      "Epoch 0[2523/17270] Time:0.244, Train Loss:0.603121817111969\n",
      "Epoch 0[2524/17270] Time:0.227, Train Loss:1.101732850074768\n",
      "Epoch 0[2525/17270] Time:0.238, Train Loss:0.8141891956329346\n",
      "Epoch 0[2526/17270] Time:0.241, Train Loss:0.49824854731559753\n",
      "Epoch 0[2527/17270] Time:0.221, Train Loss:0.5855931639671326\n",
      "Epoch 0[2528/17270] Time:0.245, Train Loss:0.8683757781982422\n",
      "Epoch 0[2529/17270] Time:0.226, Train Loss:0.8623878359794617\n",
      "Epoch 0[2530/17270] Time:0.227, Train Loss:0.7333120703697205\n",
      "Epoch 0[2531/17270] Time:0.235, Train Loss:1.077457070350647\n",
      "Epoch 0[2532/17270] Time:0.235, Train Loss:1.4671626091003418\n",
      "Epoch 0[2533/17270] Time:0.224, Train Loss:0.7418613433837891\n",
      "Epoch 0[2534/17270] Time:0.23, Train Loss:0.9043760895729065\n",
      "Epoch 0[2535/17270] Time:0.247, Train Loss:0.6926758289337158\n",
      "Epoch 0[2536/17270] Time:0.232, Train Loss:0.4479878842830658\n",
      "Epoch 0[2537/17270] Time:0.231, Train Loss:0.673778235912323\n",
      "Epoch 0[2538/17270] Time:0.225, Train Loss:0.5550429821014404\n",
      "Epoch 0[2539/17270] Time:0.247, Train Loss:0.6691574454307556\n",
      "Epoch 0[2540/17270] Time:0.237, Train Loss:0.6960285305976868\n",
      "Epoch 0[2541/17270] Time:0.228, Train Loss:0.4146999418735504\n",
      "Epoch 0[2542/17270] Time:0.232, Train Loss:0.7325102090835571\n",
      "Epoch 0[2543/17270] Time:0.232, Train Loss:0.5386472940444946\n",
      "Epoch 0[2544/17270] Time:0.23, Train Loss:0.4581010639667511\n",
      "Epoch 0[2545/17270] Time:0.237, Train Loss:0.721162736415863\n",
      "Epoch 0[2546/17270] Time:0.231, Train Loss:0.492350310087204\n",
      "Epoch 0[2547/17270] Time:0.251, Train Loss:0.5601165294647217\n",
      "Epoch 0[2548/17270] Time:0.23, Train Loss:0.5702312588691711\n",
      "Epoch 0[2549/17270] Time:0.223, Train Loss:0.7083815336227417\n",
      "Epoch 0[2550/17270] Time:0.247, Train Loss:0.22489787638187408\n",
      "Epoch 0[2551/17270] Time:0.22, Train Loss:0.41936275362968445\n",
      "Epoch 0[2552/17270] Time:0.246, Train Loss:0.529355525970459\n",
      "Epoch 0[2553/17270] Time:0.234, Train Loss:1.4728124141693115\n",
      "Epoch 0[2554/17270] Time:0.234, Train Loss:0.4413885176181793\n",
      "Epoch 0[2555/17270] Time:0.238, Train Loss:1.3336997032165527\n",
      "Epoch 0[2556/17270] Time:0.223, Train Loss:0.6571787595748901\n",
      "Epoch 0[2557/17270] Time:0.254, Train Loss:1.0791120529174805\n",
      "Epoch 0[2558/17270] Time:0.223, Train Loss:0.8093447089195251\n",
      "Epoch 0[2559/17270] Time:0.242, Train Loss:0.7458386421203613\n",
      "Epoch 0[2560/17270] Time:0.238, Train Loss:0.647130012512207\n",
      "Epoch 0[2561/17270] Time:0.237, Train Loss:0.729533851146698\n",
      "Epoch 0[2562/17270] Time:0.25, Train Loss:0.5357200503349304\n",
      "Epoch 0[2563/17270] Time:0.23, Train Loss:0.5349133610725403\n",
      "Epoch 0[2564/17270] Time:0.24, Train Loss:0.7248607873916626\n",
      "Epoch 0[2565/17270] Time:0.249, Train Loss:0.6704561710357666\n",
      "Epoch 0[2566/17270] Time:0.236, Train Loss:0.7565186619758606\n",
      "Epoch 0[2567/17270] Time:0.231, Train Loss:0.5311033129692078\n",
      "Epoch 0[2568/17270] Time:0.247, Train Loss:0.6003901362419128\n",
      "Epoch 0[2569/17270] Time:0.243, Train Loss:0.4611354470252991\n",
      "Epoch 0[2570/17270] Time:0.237, Train Loss:0.3936016261577606\n",
      "Epoch 0[2571/17270] Time:0.244, Train Loss:0.4586915075778961\n",
      "Epoch 0[2572/17270] Time:0.23, Train Loss:0.6585037112236023\n",
      "Epoch 0[2573/17270] Time:0.234, Train Loss:0.495827853679657\n",
      "Epoch 0[2574/17270] Time:0.238, Train Loss:0.5798447132110596\n",
      "Epoch 0[2575/17270] Time:0.237, Train Loss:0.6237988471984863\n",
      "Epoch 0[2576/17270] Time:0.231, Train Loss:0.4789825677871704\n",
      "Epoch 0[2577/17270] Time:0.225, Train Loss:0.5589019060134888\n",
      "Epoch 0[2578/17270] Time:0.229, Train Loss:0.6473420262336731\n",
      "Epoch 0[2579/17270] Time:0.24, Train Loss:0.29194074869155884\n",
      "Epoch 0[2580/17270] Time:0.231, Train Loss:0.8265416622161865\n",
      "Epoch 0[2581/17270] Time:0.229, Train Loss:0.8136413097381592\n",
      "Epoch 0[2582/17270] Time:0.23, Train Loss:0.6454046368598938\n",
      "Epoch 0[2583/17270] Time:0.236, Train Loss:0.4758998453617096\n",
      "Epoch 0[2584/17270] Time:0.236, Train Loss:0.5004022717475891\n",
      "Epoch 0[2585/17270] Time:0.236, Train Loss:0.4444318413734436\n",
      "Epoch 0[2586/17270] Time:0.237, Train Loss:0.8283572196960449\n",
      "Epoch 0[2587/17270] Time:0.23, Train Loss:0.7054007649421692\n",
      "Epoch 0[2588/17270] Time:0.238, Train Loss:0.9027534127235413\n",
      "Epoch 0[2589/17270] Time:0.242, Train Loss:0.7484530210494995\n",
      "Epoch 0[2590/17270] Time:0.231, Train Loss:0.880789577960968\n",
      "Epoch 0[2591/17270] Time:0.236, Train Loss:0.7161199450492859\n",
      "Epoch 0[2592/17270] Time:0.223, Train Loss:0.5155583620071411\n",
      "Epoch 0[2593/17270] Time:0.244, Train Loss:0.39352869987487793\n",
      "Epoch 0[2594/17270] Time:0.235, Train Loss:0.7536371946334839\n",
      "Epoch 0[2595/17270] Time:0.225, Train Loss:0.5945506691932678\n",
      "Epoch 0[2596/17270] Time:0.232, Train Loss:0.5052534341812134\n",
      "Epoch 0[2597/17270] Time:0.243, Train Loss:0.41603928804397583\n",
      "Epoch 0[2598/17270] Time:0.242, Train Loss:0.433149129152298\n",
      "Epoch 0[2599/17270] Time:0.242, Train Loss:0.6354697942733765\n",
      "Epoch 0[2600/17270] Time:0.243, Train Loss:0.543791651725769\n",
      "Epoch 0[2601/17270] Time:0.235, Train Loss:0.5430251955986023\n",
      "Epoch 0[2602/17270] Time:0.244, Train Loss:0.7061640024185181\n",
      "Epoch 0[2603/17270] Time:0.233, Train Loss:0.3634689152240753\n",
      "Epoch 0[2604/17270] Time:0.233, Train Loss:0.4855189919471741\n",
      "Epoch 0[2605/17270] Time:0.231, Train Loss:0.48921945691108704\n",
      "Epoch 0[2606/17270] Time:0.224, Train Loss:0.42709702253341675\n",
      "Epoch 0[2607/17270] Time:0.249, Train Loss:1.1904977560043335\n",
      "Epoch 0[2608/17270] Time:0.232, Train Loss:0.9803802371025085\n",
      "Epoch 0[2609/17270] Time:0.233, Train Loss:1.7868155241012573\n",
      "Epoch 0[2610/17270] Time:0.244, Train Loss:0.5248503088951111\n",
      "Epoch 0[2611/17270] Time:0.227, Train Loss:0.515152096748352\n",
      "Epoch 0[2612/17270] Time:0.242, Train Loss:0.6174591779708862\n",
      "Epoch 0[2613/17270] Time:0.236, Train Loss:1.1893385648727417\n",
      "Epoch 0[2614/17270] Time:0.225, Train Loss:0.6400780081748962\n",
      "Epoch 0[2615/17270] Time:0.223, Train Loss:0.9566993713378906\n",
      "Epoch 0[2616/17270] Time:0.227, Train Loss:0.6903188824653625\n",
      "Epoch 0[2617/17270] Time:0.244, Train Loss:0.7071564793586731\n",
      "Epoch 0[2618/17270] Time:0.232, Train Loss:0.8944702744483948\n",
      "Epoch 0[2619/17270] Time:0.238, Train Loss:0.9659584760665894\n",
      "Epoch 0[2620/17270] Time:0.229, Train Loss:0.39619681239128113\n",
      "Epoch 0[2621/17270] Time:0.235, Train Loss:0.5132216215133667\n",
      "Epoch 0[2622/17270] Time:0.237, Train Loss:0.835021436214447\n",
      "Epoch 0[2623/17270] Time:0.243, Train Loss:1.2399673461914062\n",
      "Epoch 0[2624/17270] Time:0.237, Train Loss:0.4194127321243286\n",
      "Epoch 0[2625/17270] Time:0.236, Train Loss:0.8083922863006592\n",
      "Epoch 0[2626/17270] Time:0.226, Train Loss:0.6481163501739502\n",
      "Epoch 0[2627/17270] Time:0.233, Train Loss:0.9076881408691406\n",
      "Epoch 0[2628/17270] Time:0.234, Train Loss:0.9029404520988464\n",
      "Epoch 0[2629/17270] Time:0.24, Train Loss:0.7441131472587585\n",
      "Epoch 0[2630/17270] Time:0.24, Train Loss:0.7733993530273438\n",
      "Epoch 0[2631/17270] Time:0.236, Train Loss:0.6213099956512451\n",
      "Epoch 0[2632/17270] Time:0.247, Train Loss:0.48594579100608826\n",
      "Epoch 0[2633/17270] Time:0.234, Train Loss:0.5984368324279785\n",
      "Epoch 0[2634/17270] Time:0.236, Train Loss:0.87586510181427\n",
      "Epoch 0[2635/17270] Time:0.238, Train Loss:0.46443691849708557\n",
      "Epoch 0[2636/17270] Time:0.242, Train Loss:0.9045922160148621\n",
      "Epoch 0[2637/17270] Time:0.234, Train Loss:0.3861643970012665\n",
      "Epoch 0[2638/17270] Time:0.24, Train Loss:1.084113597869873\n",
      "Epoch 0[2639/17270] Time:0.234, Train Loss:0.4521065056324005\n",
      "Epoch 0[2640/17270] Time:0.228, Train Loss:0.5507041811943054\n",
      "Epoch 0[2641/17270] Time:0.236, Train Loss:0.9146049618721008\n",
      "Epoch 0[2642/17270] Time:0.227, Train Loss:1.0363624095916748\n",
      "Epoch 0[2643/17270] Time:0.232, Train Loss:0.5164743661880493\n",
      "Epoch 0[2644/17270] Time:0.237, Train Loss:0.7666664719581604\n",
      "Epoch 0[2645/17270] Time:0.236, Train Loss:0.5075596570968628\n",
      "Epoch 0[2646/17270] Time:0.23, Train Loss:0.42716407775878906\n",
      "Epoch 0[2647/17270] Time:0.264, Train Loss:0.6763386130332947\n",
      "Epoch 0[2648/17270] Time:0.234, Train Loss:0.7363787293434143\n",
      "Epoch 0[2649/17270] Time:0.23, Train Loss:0.6291795969009399\n",
      "Epoch 0[2650/17270] Time:0.221, Train Loss:0.46194905042648315\n",
      "Epoch 0[2651/17270] Time:0.255, Train Loss:0.5944212675094604\n",
      "Epoch 0[2652/17270] Time:0.227, Train Loss:0.8019590973854065\n",
      "Epoch 0[2653/17270] Time:0.224, Train Loss:0.7381212711334229\n",
      "Epoch 0[2654/17270] Time:0.237, Train Loss:0.7726302742958069\n",
      "Epoch 0[2655/17270] Time:0.245, Train Loss:0.5234594941139221\n",
      "Epoch 0[2656/17270] Time:0.228, Train Loss:0.4256993234157562\n",
      "Epoch 0[2657/17270] Time:0.238, Train Loss:0.3696151375770569\n",
      "Epoch 0[2658/17270] Time:0.229, Train Loss:1.0927445888519287\n",
      "Epoch 0[2659/17270] Time:0.239, Train Loss:0.5967560410499573\n",
      "Epoch 0[2660/17270] Time:0.233, Train Loss:0.36494192481040955\n",
      "Epoch 0[2661/17270] Time:0.226, Train Loss:0.6319347023963928\n",
      "Epoch 0[2662/17270] Time:0.231, Train Loss:0.6773235201835632\n",
      "Epoch 0[2663/17270] Time:0.248, Train Loss:0.7328293919563293\n",
      "Epoch 0[2664/17270] Time:0.232, Train Loss:0.7358195185661316\n",
      "Epoch 0[2665/17270] Time:0.243, Train Loss:0.7881617546081543\n",
      "Epoch 0[2666/17270] Time:0.23, Train Loss:0.9524738788604736\n",
      "Epoch 0[2667/17270] Time:0.244, Train Loss:1.5701032876968384\n",
      "Epoch 0[2668/17270] Time:0.232, Train Loss:0.3975435197353363\n",
      "Epoch 0[2669/17270] Time:0.242, Train Loss:0.6122985482215881\n",
      "Epoch 0[2670/17270] Time:0.231, Train Loss:0.9064935445785522\n",
      "Epoch 0[2671/17270] Time:0.228, Train Loss:0.6886283159255981\n",
      "Epoch 0[2672/17270] Time:0.235, Train Loss:0.3326534330844879\n",
      "Epoch 0[2673/17270] Time:0.243, Train Loss:0.6800827980041504\n",
      "Epoch 0[2674/17270] Time:0.238, Train Loss:0.9408804178237915\n",
      "Epoch 0[2675/17270] Time:0.232, Train Loss:0.6756207346916199\n",
      "Epoch 0[2676/17270] Time:0.233, Train Loss:0.6975737810134888\n",
      "Epoch 0[2677/17270] Time:0.223, Train Loss:0.48758742213249207\n",
      "Epoch 0[2678/17270] Time:0.248, Train Loss:0.7996006011962891\n",
      "Epoch 0[2679/17270] Time:0.239, Train Loss:1.0193908214569092\n",
      "Epoch 0[2680/17270] Time:0.241, Train Loss:0.6920762062072754\n",
      "Epoch 0[2681/17270] Time:0.237, Train Loss:0.5410303473472595\n",
      "Epoch 0[2682/17270] Time:0.225, Train Loss:0.5874489545822144\n",
      "Epoch 0[2683/17270] Time:0.23, Train Loss:0.5452425479888916\n",
      "Epoch 0[2684/17270] Time:0.24, Train Loss:0.5908111929893494\n",
      "Epoch 0[2685/17270] Time:0.231, Train Loss:0.676997721195221\n",
      "Epoch 0[2686/17270] Time:0.228, Train Loss:0.5198272466659546\n",
      "Epoch 0[2687/17270] Time:0.231, Train Loss:0.6011083722114563\n",
      "Epoch 0[2688/17270] Time:0.25, Train Loss:0.5693344473838806\n",
      "Epoch 0[2689/17270] Time:0.246, Train Loss:0.48348239064216614\n",
      "Epoch 0[2690/17270] Time:0.244, Train Loss:1.2334493398666382\n",
      "Epoch 0[2691/17270] Time:0.242, Train Loss:0.4816356301307678\n",
      "Epoch 0[2692/17270] Time:0.238, Train Loss:1.0327785015106201\n",
      "Epoch 0[2693/17270] Time:0.226, Train Loss:0.4862152338027954\n",
      "Epoch 0[2694/17270] Time:0.232, Train Loss:0.6008061766624451\n",
      "Epoch 0[2695/17270] Time:0.23, Train Loss:0.4832284450531006\n",
      "Epoch 0[2696/17270] Time:0.231, Train Loss:0.9488188624382019\n",
      "Epoch 0[2697/17270] Time:0.227, Train Loss:0.677636444568634\n",
      "Epoch 0[2698/17270] Time:0.229, Train Loss:0.9517102241516113\n",
      "Epoch 0[2699/17270] Time:0.232, Train Loss:0.4278547167778015\n",
      "Epoch 0[2700/17270] Time:0.235, Train Loss:0.9918146729469299\n",
      "Epoch 0[2701/17270] Time:0.238, Train Loss:0.43151989579200745\n",
      "Epoch 0[2702/17270] Time:0.24, Train Loss:0.5847256779670715\n",
      "Epoch 0[2703/17270] Time:0.231, Train Loss:1.2821587324142456\n",
      "Epoch 0[2704/17270] Time:0.246, Train Loss:0.4553697407245636\n",
      "Epoch 0[2705/17270] Time:0.239, Train Loss:0.9173846244812012\n",
      "Epoch 0[2706/17270] Time:0.229, Train Loss:0.36419692635536194\n",
      "Epoch 0[2707/17270] Time:0.237, Train Loss:1.5655415058135986\n",
      "Epoch 0[2708/17270] Time:0.239, Train Loss:1.0552481412887573\n",
      "Epoch 0[2709/17270] Time:0.23, Train Loss:0.7004114985466003\n",
      "Epoch 0[2710/17270] Time:0.23, Train Loss:0.40314677357673645\n",
      "Epoch 0[2711/17270] Time:0.235, Train Loss:1.0408189296722412\n",
      "Epoch 0[2712/17270] Time:0.238, Train Loss:1.1665164232254028\n",
      "Epoch 0[2713/17270] Time:0.237, Train Loss:0.7261563539505005\n",
      "Epoch 0[2714/17270] Time:0.237, Train Loss:1.1415647268295288\n",
      "Epoch 0[2715/17270] Time:0.23, Train Loss:0.5340755581855774\n",
      "Epoch 0[2716/17270] Time:0.228, Train Loss:0.8826505541801453\n",
      "Epoch 0[2717/17270] Time:0.239, Train Loss:0.832930326461792\n",
      "Epoch 0[2718/17270] Time:0.238, Train Loss:0.825554609298706\n",
      "Epoch 0[2719/17270] Time:0.235, Train Loss:1.225188970565796\n",
      "Epoch 0[2720/17270] Time:0.254, Train Loss:0.7475528717041016\n",
      "Epoch 0[2721/17270] Time:0.231, Train Loss:0.5215029120445251\n",
      "Epoch 0[2722/17270] Time:0.233, Train Loss:0.8140482902526855\n",
      "Epoch 0[2723/17270] Time:0.233, Train Loss:0.6844959259033203\n",
      "Epoch 0[2724/17270] Time:0.234, Train Loss:0.8566544055938721\n",
      "Epoch 0[2725/17270] Time:0.242, Train Loss:0.8873524069786072\n",
      "Epoch 0[2726/17270] Time:0.236, Train Loss:0.44658833742141724\n",
      "Epoch 0[2727/17270] Time:0.231, Train Loss:0.6621196269989014\n",
      "Epoch 0[2728/17270] Time:0.232, Train Loss:0.4563673734664917\n",
      "Epoch 0[2729/17270] Time:0.234, Train Loss:0.8810341358184814\n",
      "Epoch 0[2730/17270] Time:0.246, Train Loss:0.6771084070205688\n",
      "Epoch 0[2731/17270] Time:0.233, Train Loss:0.5606448650360107\n",
      "Epoch 0[2732/17270] Time:0.249, Train Loss:1.142122745513916\n",
      "Epoch 0[2733/17270] Time:0.251, Train Loss:0.5455673933029175\n",
      "Epoch 0[2734/17270] Time:0.228, Train Loss:0.5220484137535095\n",
      "Epoch 0[2735/17270] Time:0.235, Train Loss:0.6967136859893799\n",
      "Epoch 0[2736/17270] Time:0.235, Train Loss:0.5806916952133179\n",
      "Epoch 0[2737/17270] Time:0.229, Train Loss:0.5993804335594177\n",
      "Epoch 0[2738/17270] Time:0.229, Train Loss:0.42536962032318115\n",
      "Epoch 0[2739/17270] Time:0.23, Train Loss:0.3405621349811554\n",
      "Epoch 0[2740/17270] Time:0.231, Train Loss:0.5033660531044006\n",
      "Epoch 0[2741/17270] Time:0.235, Train Loss:1.1675002574920654\n",
      "Epoch 0[2742/17270] Time:0.238, Train Loss:0.9291815757751465\n",
      "Epoch 0[2743/17270] Time:0.235, Train Loss:0.4658959209918976\n",
      "Epoch 0[2744/17270] Time:0.246, Train Loss:0.47116410732269287\n",
      "Epoch 0[2745/17270] Time:0.224, Train Loss:0.7948376536369324\n",
      "Epoch 0[2746/17270] Time:0.242, Train Loss:0.926199197769165\n",
      "Epoch 0[2747/17270] Time:0.238, Train Loss:2.05172061920166\n",
      "Epoch 0[2748/17270] Time:0.238, Train Loss:0.5783436894416809\n",
      "Epoch 0[2749/17270] Time:0.227, Train Loss:0.8660860061645508\n",
      "Epoch 0[2750/17270] Time:0.233, Train Loss:1.1578119993209839\n",
      "Epoch 0[2751/17270] Time:0.233, Train Loss:0.9473292231559753\n",
      "Epoch 0[2752/17270] Time:0.241, Train Loss:0.6438148021697998\n",
      "Epoch 0[2753/17270] Time:0.245, Train Loss:0.47687846422195435\n",
      "Epoch 0[2754/17270] Time:0.233, Train Loss:0.845876932144165\n",
      "Epoch 0[2755/17270] Time:0.233, Train Loss:0.45031487941741943\n",
      "Epoch 0[2756/17270] Time:0.226, Train Loss:0.46221596002578735\n",
      "Epoch 0[2757/17270] Time:0.236, Train Loss:0.45747533440589905\n",
      "Epoch 0[2758/17270] Time:0.237, Train Loss:0.8971593379974365\n",
      "Epoch 0[2759/17270] Time:0.25, Train Loss:0.6912642121315002\n",
      "Epoch 0[2760/17270] Time:0.237, Train Loss:1.1049494743347168\n",
      "Epoch 0[2761/17270] Time:0.232, Train Loss:0.7894210815429688\n",
      "Epoch 0[2762/17270] Time:0.234, Train Loss:0.4489953815937042\n",
      "Epoch 0[2763/17270] Time:0.228, Train Loss:0.5724490880966187\n",
      "Epoch 0[2764/17270] Time:0.255, Train Loss:0.5773791670799255\n",
      "Epoch 0[2765/17270] Time:0.242, Train Loss:0.7869757413864136\n",
      "Epoch 0[2766/17270] Time:0.225, Train Loss:0.5104326605796814\n",
      "Epoch 0[2767/17270] Time:0.228, Train Loss:0.7846654057502747\n",
      "Epoch 0[2768/17270] Time:0.237, Train Loss:1.1480118036270142\n",
      "Epoch 0[2769/17270] Time:0.239, Train Loss:0.6787564754486084\n",
      "Epoch 0[2770/17270] Time:0.238, Train Loss:0.5853872299194336\n",
      "Epoch 0[2771/17270] Time:0.242, Train Loss:0.4402587413787842\n",
      "Epoch 0[2772/17270] Time:0.238, Train Loss:0.5464428067207336\n",
      "Epoch 0[2773/17270] Time:0.24, Train Loss:0.5630336999893188\n",
      "Epoch 0[2774/17270] Time:0.244, Train Loss:0.695432186126709\n",
      "Epoch 0[2775/17270] Time:0.236, Train Loss:0.7783735990524292\n",
      "Epoch 0[2776/17270] Time:0.247, Train Loss:1.1140133142471313\n",
      "Epoch 0[2777/17270] Time:0.235, Train Loss:0.429653525352478\n",
      "Epoch 0[2778/17270] Time:0.243, Train Loss:0.610899031162262\n",
      "Epoch 0[2779/17270] Time:0.239, Train Loss:0.5278412103652954\n",
      "Epoch 0[2780/17270] Time:0.235, Train Loss:0.5464178323745728\n",
      "Epoch 0[2781/17270] Time:0.235, Train Loss:0.7526320219039917\n",
      "Epoch 0[2782/17270] Time:0.243, Train Loss:0.444581001996994\n",
      "Epoch 0[2783/17270] Time:0.226, Train Loss:0.2834416925907135\n",
      "Epoch 0[2784/17270] Time:0.24, Train Loss:0.6943963766098022\n",
      "Epoch 0[2785/17270] Time:0.237, Train Loss:0.5766292810440063\n",
      "Epoch 0[2786/17270] Time:0.224, Train Loss:0.5910854935646057\n",
      "Epoch 0[2787/17270] Time:0.243, Train Loss:0.49154648184776306\n",
      "Epoch 0[2788/17270] Time:0.234, Train Loss:0.6744639277458191\n",
      "Epoch 0[2789/17270] Time:0.25, Train Loss:0.7106022238731384\n",
      "Epoch 0[2790/17270] Time:0.223, Train Loss:0.6096272468566895\n",
      "Epoch 0[2791/17270] Time:0.244, Train Loss:0.32689061760902405\n",
      "Epoch 0[2792/17270] Time:0.241, Train Loss:0.7748304009437561\n",
      "Epoch 0[2793/17270] Time:0.237, Train Loss:1.350038766860962\n",
      "Epoch 0[2794/17270] Time:0.237, Train Loss:0.4551026523113251\n",
      "Epoch 0[2795/17270] Time:0.226, Train Loss:0.5851707458496094\n",
      "Epoch 0[2796/17270] Time:0.232, Train Loss:0.5824669599533081\n",
      "Epoch 0[2797/17270] Time:0.235, Train Loss:0.4917559325695038\n",
      "Epoch 0[2798/17270] Time:0.236, Train Loss:0.7631696462631226\n",
      "Epoch 0[2799/17270] Time:0.241, Train Loss:0.5202583074569702\n",
      "Epoch 0[2800/17270] Time:0.229, Train Loss:0.8191556334495544\n",
      "Epoch 0[2801/17270] Time:0.238, Train Loss:0.5857819318771362\n",
      "Epoch 0[2802/17270] Time:0.232, Train Loss:0.6709933876991272\n",
      "Epoch 0[2803/17270] Time:0.226, Train Loss:0.6461697816848755\n",
      "Epoch 0[2804/17270] Time:0.228, Train Loss:0.5040277242660522\n",
      "Epoch 0[2805/17270] Time:0.231, Train Loss:0.47420215606689453\n",
      "Epoch 0[2806/17270] Time:0.236, Train Loss:0.4898896813392639\n",
      "Epoch 0[2807/17270] Time:0.249, Train Loss:0.6965928673744202\n",
      "Epoch 0[2808/17270] Time:0.23, Train Loss:0.5124561786651611\n",
      "Epoch 0[2809/17270] Time:0.23, Train Loss:0.4307708144187927\n",
      "Epoch 0[2810/17270] Time:0.239, Train Loss:0.5305267572402954\n",
      "Epoch 0[2811/17270] Time:0.241, Train Loss:0.4694223701953888\n",
      "Epoch 0[2812/17270] Time:0.241, Train Loss:0.5780808925628662\n",
      "Epoch 0[2813/17270] Time:0.236, Train Loss:0.5711302161216736\n",
      "Epoch 0[2814/17270] Time:0.233, Train Loss:0.36312782764434814\n",
      "Epoch 0[2815/17270] Time:0.231, Train Loss:0.3181527853012085\n",
      "Epoch 0[2816/17270] Time:0.229, Train Loss:0.5719946622848511\n",
      "Epoch 0[2817/17270] Time:0.237, Train Loss:0.4833119213581085\n",
      "Epoch 0[2818/17270] Time:0.236, Train Loss:0.3653670847415924\n",
      "Epoch 0[2819/17270] Time:0.249, Train Loss:0.38232356309890747\n",
      "Epoch 0[2820/17270] Time:0.223, Train Loss:0.3457408845424652\n",
      "Epoch 0[2821/17270] Time:0.237, Train Loss:0.6117979288101196\n",
      "Epoch 0[2822/17270] Time:0.243, Train Loss:0.5974558591842651\n",
      "Epoch 0[2823/17270] Time:0.229, Train Loss:0.4416031241416931\n",
      "Epoch 0[2824/17270] Time:0.24, Train Loss:0.9896559119224548\n",
      "Epoch 0[2825/17270] Time:0.239, Train Loss:1.1648374795913696\n",
      "Epoch 0[2826/17270] Time:0.227, Train Loss:0.5211430191993713\n",
      "Epoch 0[2827/17270] Time:0.234, Train Loss:0.35231536626815796\n",
      "Epoch 0[2828/17270] Time:0.228, Train Loss:0.33095118403434753\n",
      "Epoch 0[2829/17270] Time:0.235, Train Loss:0.6744813323020935\n",
      "Epoch 0[2830/17270] Time:0.251, Train Loss:1.1383554935455322\n",
      "Epoch 0[2831/17270] Time:0.225, Train Loss:0.37200188636779785\n",
      "Epoch 0[2832/17270] Time:0.246, Train Loss:0.9963312745094299\n",
      "Epoch 0[2833/17270] Time:0.222, Train Loss:0.4561954438686371\n",
      "Epoch 0[2834/17270] Time:0.234, Train Loss:0.4449910819530487\n",
      "Epoch 0[2835/17270] Time:0.242, Train Loss:0.9316501617431641\n",
      "Epoch 0[2836/17270] Time:0.236, Train Loss:0.9786497950553894\n",
      "Epoch 0[2837/17270] Time:0.237, Train Loss:0.504059374332428\n",
      "Epoch 0[2838/17270] Time:0.233, Train Loss:0.550230860710144\n",
      "Epoch 0[2839/17270] Time:0.225, Train Loss:0.3994056284427643\n",
      "Epoch 0[2840/17270] Time:0.239, Train Loss:0.7705695033073425\n",
      "Epoch 0[2841/17270] Time:0.241, Train Loss:0.771447479724884\n",
      "Epoch 0[2842/17270] Time:0.227, Train Loss:0.528590977191925\n",
      "Epoch 0[2843/17270] Time:0.229, Train Loss:1.0204617977142334\n",
      "Epoch 0[2844/17270] Time:0.232, Train Loss:0.6685131788253784\n",
      "Epoch 0[2845/17270] Time:0.243, Train Loss:0.7570409774780273\n",
      "Epoch 0[2846/17270] Time:0.241, Train Loss:0.6607794165611267\n",
      "Epoch 0[2847/17270] Time:0.238, Train Loss:0.7039282321929932\n",
      "Epoch 0[2848/17270] Time:0.232, Train Loss:0.7526406049728394\n",
      "Epoch 0[2849/17270] Time:0.225, Train Loss:0.8774579167366028\n",
      "Epoch 0[2850/17270] Time:0.247, Train Loss:0.7596388459205627\n",
      "Epoch 0[2851/17270] Time:0.232, Train Loss:0.9340884685516357\n",
      "Epoch 0[2852/17270] Time:0.234, Train Loss:0.5549159646034241\n",
      "Epoch 0[2853/17270] Time:0.229, Train Loss:0.5490816235542297\n",
      "Epoch 0[2854/17270] Time:0.251, Train Loss:0.4177649915218353\n",
      "Epoch 0[2855/17270] Time:0.234, Train Loss:0.654928982257843\n",
      "Epoch 0[2856/17270] Time:0.226, Train Loss:0.4905288815498352\n",
      "Epoch 0[2857/17270] Time:0.248, Train Loss:0.5080831050872803\n",
      "Epoch 0[2858/17270] Time:0.236, Train Loss:0.3522985875606537\n",
      "Epoch 0[2859/17270] Time:0.235, Train Loss:0.8596052527427673\n",
      "Epoch 0[2860/17270] Time:0.241, Train Loss:0.4342072308063507\n",
      "Epoch 0[2861/17270] Time:0.239, Train Loss:0.6304135322570801\n",
      "Epoch 0[2862/17270] Time:0.233, Train Loss:0.7142499089241028\n",
      "Epoch 0[2863/17270] Time:0.237, Train Loss:1.1927329301834106\n",
      "Epoch 0[2864/17270] Time:0.222, Train Loss:0.6152867078781128\n",
      "Epoch 0[2865/17270] Time:0.229, Train Loss:0.6684404015541077\n",
      "Epoch 0[2866/17270] Time:0.23, Train Loss:0.5946070551872253\n",
      "Epoch 0[2867/17270] Time:0.241, Train Loss:0.6247181296348572\n",
      "Epoch 0[2868/17270] Time:0.234, Train Loss:0.6269686818122864\n",
      "Epoch 0[2869/17270] Time:0.24, Train Loss:0.689687967300415\n",
      "Epoch 0[2870/17270] Time:0.227, Train Loss:1.1120375394821167\n",
      "Epoch 0[2871/17270] Time:0.247, Train Loss:0.5883055329322815\n",
      "Epoch 0[2872/17270] Time:0.226, Train Loss:0.5209327936172485\n",
      "Epoch 0[2873/17270] Time:0.236, Train Loss:1.0455594062805176\n",
      "Epoch 0[2874/17270] Time:0.246, Train Loss:0.39806872606277466\n",
      "Epoch 0[2875/17270] Time:0.224, Train Loss:0.7989830374717712\n",
      "Epoch 0[2876/17270] Time:0.244, Train Loss:0.9065226912498474\n",
      "Epoch 0[2877/17270] Time:0.239, Train Loss:0.7283309698104858\n",
      "Epoch 0[2878/17270] Time:0.236, Train Loss:0.5752621293067932\n",
      "Epoch 0[2879/17270] Time:0.24, Train Loss:0.6713936924934387\n",
      "Epoch 0[2880/17270] Time:0.229, Train Loss:0.6667912602424622\n",
      "Epoch 0[2881/17270] Time:0.243, Train Loss:0.4192184507846832\n",
      "Epoch 0[2882/17270] Time:0.235, Train Loss:0.44406312704086304\n",
      "Epoch 0[2883/17270] Time:0.231, Train Loss:0.7566788792610168\n",
      "Epoch 0[2884/17270] Time:0.236, Train Loss:1.2743635177612305\n",
      "Epoch 0[2885/17270] Time:0.24, Train Loss:0.917320191860199\n",
      "Epoch 0[2886/17270] Time:0.236, Train Loss:0.6788561940193176\n",
      "Epoch 0[2887/17270] Time:0.24, Train Loss:0.6608253717422485\n",
      "Epoch 0[2888/17270] Time:0.225, Train Loss:0.7259879112243652\n",
      "Epoch 0[2889/17270] Time:0.234, Train Loss:0.4904341995716095\n",
      "Epoch 0[2890/17270] Time:0.23, Train Loss:0.4947356581687927\n",
      "Epoch 0[2891/17270] Time:0.232, Train Loss:0.8732860088348389\n",
      "Epoch 0[2892/17270] Time:0.237, Train Loss:0.8333883881568909\n",
      "Epoch 0[2893/17270] Time:0.239, Train Loss:0.6228748559951782\n",
      "Epoch 0[2894/17270] Time:0.237, Train Loss:1.2903070449829102\n",
      "Epoch 0[2895/17270] Time:0.218, Train Loss:0.6198104619979858\n",
      "Epoch 0[2896/17270] Time:0.24, Train Loss:0.4373801052570343\n",
      "Epoch 0[2897/17270] Time:0.23, Train Loss:0.6222468614578247\n",
      "Epoch 0[2898/17270] Time:0.235, Train Loss:0.44977623224258423\n",
      "Epoch 0[2899/17270] Time:0.221, Train Loss:0.5546826720237732\n",
      "Epoch 0[2900/17270] Time:0.238, Train Loss:1.6676899194717407\n",
      "Epoch 0[2901/17270] Time:0.245, Train Loss:0.4813702702522278\n",
      "Epoch 0[2902/17270] Time:0.232, Train Loss:0.5460379719734192\n",
      "Epoch 0[2903/17270] Time:0.23, Train Loss:0.42120054364204407\n",
      "Epoch 0[2904/17270] Time:0.24, Train Loss:1.390152096748352\n",
      "Epoch 0[2905/17270] Time:0.231, Train Loss:0.6770195960998535\n",
      "Epoch 0[2906/17270] Time:0.232, Train Loss:0.3069523274898529\n",
      "Epoch 0[2907/17270] Time:0.233, Train Loss:0.6255302429199219\n",
      "Epoch 0[2908/17270] Time:0.239, Train Loss:0.6253922581672668\n",
      "Epoch 0[2909/17270] Time:0.238, Train Loss:0.6679257750511169\n",
      "Epoch 0[2910/17270] Time:0.235, Train Loss:0.5781707167625427\n",
      "Epoch 0[2911/17270] Time:0.23, Train Loss:0.7408847212791443\n",
      "Epoch 0[2912/17270] Time:0.232, Train Loss:1.2510417699813843\n",
      "Epoch 0[2913/17270] Time:0.244, Train Loss:0.6482700705528259\n",
      "Epoch 0[2914/17270] Time:0.224, Train Loss:0.8765609860420227\n",
      "Epoch 0[2915/17270] Time:0.246, Train Loss:1.0172216892242432\n",
      "Epoch 0[2916/17270] Time:0.235, Train Loss:0.3246404528617859\n",
      "Epoch 0[2917/17270] Time:0.237, Train Loss:0.8586504459381104\n",
      "Epoch 0[2918/17270] Time:0.237, Train Loss:0.5737584233283997\n",
      "Epoch 0[2919/17270] Time:0.24, Train Loss:0.4726925790309906\n",
      "Epoch 0[2920/17270] Time:0.226, Train Loss:0.47849413752555847\n",
      "Epoch 0[2921/17270] Time:0.274, Train Loss:0.892572820186615\n",
      "Epoch 0[2922/17270] Time:0.224, Train Loss:0.7839781045913696\n",
      "Epoch 0[2923/17270] Time:0.251, Train Loss:1.0722194910049438\n",
      "Epoch 0[2924/17270] Time:0.221, Train Loss:0.739609956741333\n",
      "Epoch 0[2925/17270] Time:0.242, Train Loss:0.6101546287536621\n",
      "Epoch 0[2926/17270] Time:0.222, Train Loss:0.9915024042129517\n",
      "Epoch 0[2927/17270] Time:0.241, Train Loss:0.5144668817520142\n",
      "Epoch 0[2928/17270] Time:0.225, Train Loss:0.716613233089447\n",
      "Epoch 0[2929/17270] Time:0.233, Train Loss:1.129746437072754\n",
      "Epoch 0[2930/17270] Time:0.234, Train Loss:0.5565502643585205\n",
      "Epoch 0[2931/17270] Time:0.228, Train Loss:0.35287711024284363\n",
      "Epoch 0[2932/17270] Time:0.239, Train Loss:0.569456934928894\n",
      "Epoch 0[2933/17270] Time:0.24, Train Loss:0.5650283694267273\n",
      "Epoch 0[2934/17270] Time:0.23, Train Loss:0.8306750655174255\n",
      "Epoch 0[2935/17270] Time:0.232, Train Loss:0.6504202485084534\n",
      "Epoch 0[2936/17270] Time:0.241, Train Loss:0.46664485335350037\n",
      "Epoch 0[2937/17270] Time:0.234, Train Loss:0.7911154627799988\n",
      "Epoch 0[2938/17270] Time:0.219, Train Loss:0.4877474904060364\n",
      "Epoch 0[2939/17270] Time:0.241, Train Loss:0.540185272693634\n",
      "Epoch 0[2940/17270] Time:0.232, Train Loss:0.557158887386322\n",
      "Epoch 0[2941/17270] Time:0.232, Train Loss:0.5449336767196655\n",
      "Epoch 0[2942/17270] Time:0.233, Train Loss:1.1799373626708984\n",
      "Epoch 0[2943/17270] Time:0.235, Train Loss:0.6979082226753235\n",
      "Epoch 0[2944/17270] Time:0.22, Train Loss:0.4243331849575043\n",
      "Epoch 0[2945/17270] Time:0.231, Train Loss:0.7911786437034607\n",
      "Epoch 0[2946/17270] Time:0.23, Train Loss:0.8131728768348694\n",
      "Epoch 0[2947/17270] Time:0.229, Train Loss:0.5974777936935425\n",
      "Epoch 0[2948/17270] Time:0.236, Train Loss:0.7475182414054871\n",
      "Epoch 0[2949/17270] Time:0.226, Train Loss:0.5804256796836853\n",
      "Epoch 0[2950/17270] Time:0.231, Train Loss:0.5583390593528748\n",
      "Epoch 0[2951/17270] Time:0.245, Train Loss:1.020033359527588\n",
      "Epoch 0[2952/17270] Time:0.228, Train Loss:0.9664191603660583\n",
      "Epoch 0[2953/17270] Time:0.233, Train Loss:0.8181548118591309\n",
      "Epoch 0[2954/17270] Time:0.243, Train Loss:0.5337820649147034\n",
      "Epoch 0[2955/17270] Time:0.229, Train Loss:0.5769787430763245\n",
      "Epoch 0[2956/17270] Time:0.24, Train Loss:1.2834528684616089\n",
      "Epoch 0[2957/17270] Time:0.226, Train Loss:0.5864771008491516\n",
      "Epoch 0[2958/17270] Time:0.247, Train Loss:0.9393039345741272\n",
      "Epoch 0[2959/17270] Time:0.233, Train Loss:0.4384647607803345\n",
      "Epoch 0[2960/17270] Time:0.245, Train Loss:0.8823798298835754\n",
      "Epoch 0[2961/17270] Time:0.24, Train Loss:0.7091048359870911\n",
      "Epoch 0[2962/17270] Time:0.234, Train Loss:0.6895662546157837\n",
      "Epoch 0[2963/17270] Time:0.232, Train Loss:0.7294194102287292\n",
      "Epoch 0[2964/17270] Time:0.235, Train Loss:0.8840309381484985\n",
      "Epoch 0[2965/17270] Time:0.241, Train Loss:1.2084074020385742\n",
      "Epoch 0[2966/17270] Time:0.237, Train Loss:1.2317167520523071\n",
      "Epoch 0[2967/17270] Time:0.228, Train Loss:0.6824597716331482\n",
      "Epoch 0[2968/17270] Time:0.236, Train Loss:0.49045825004577637\n",
      "Epoch 0[2969/17270] Time:0.232, Train Loss:0.6485162973403931\n",
      "Epoch 0[2970/17270] Time:0.229, Train Loss:0.506194531917572\n",
      "Epoch 0[2971/17270] Time:0.233, Train Loss:0.4271515905857086\n",
      "Epoch 0[2972/17270] Time:0.236, Train Loss:0.6452946662902832\n",
      "Epoch 0[2973/17270] Time:0.24, Train Loss:1.0069814920425415\n",
      "Epoch 0[2974/17270] Time:0.232, Train Loss:0.7109568119049072\n",
      "Epoch 0[2975/17270] Time:0.237, Train Loss:0.5180294513702393\n",
      "Epoch 0[2976/17270] Time:0.23, Train Loss:0.7225283980369568\n",
      "Epoch 0[2977/17270] Time:0.244, Train Loss:0.6080257892608643\n",
      "Epoch 0[2978/17270] Time:0.235, Train Loss:0.6389856338500977\n",
      "Epoch 0[2979/17270] Time:0.237, Train Loss:0.463024765253067\n",
      "Epoch 0[2980/17270] Time:0.238, Train Loss:0.34555584192276\n",
      "Epoch 0[2981/17270] Time:0.238, Train Loss:0.6468783020973206\n",
      "Epoch 0[2982/17270] Time:0.225, Train Loss:0.7680686116218567\n",
      "Epoch 0[2983/17270] Time:0.24, Train Loss:0.5779682397842407\n",
      "Epoch 0[2984/17270] Time:0.234, Train Loss:0.7575169801712036\n",
      "Epoch 0[2985/17270] Time:0.239, Train Loss:0.7982656359672546\n",
      "Epoch 0[2986/17270] Time:0.251, Train Loss:0.8247038722038269\n",
      "Epoch 0[2987/17270] Time:0.232, Train Loss:0.5731745958328247\n",
      "Epoch 0[2988/17270] Time:0.237, Train Loss:0.689189076423645\n",
      "Epoch 0[2989/17270] Time:0.226, Train Loss:0.58066725730896\n",
      "Epoch 0[2990/17270] Time:0.232, Train Loss:0.3845401704311371\n",
      "Epoch 0[2991/17270] Time:0.238, Train Loss:0.460191011428833\n",
      "Epoch 0[2992/17270] Time:0.231, Train Loss:0.633776068687439\n",
      "Epoch 0[2993/17270] Time:0.236, Train Loss:0.7993009090423584\n",
      "Epoch 0[2994/17270] Time:0.233, Train Loss:1.107954502105713\n",
      "Epoch 0[2995/17270] Time:0.241, Train Loss:1.2581889629364014\n",
      "Epoch 0[2996/17270] Time:0.234, Train Loss:0.6350943446159363\n",
      "Epoch 0[2997/17270] Time:0.234, Train Loss:0.9035950899124146\n",
      "Epoch 0[2998/17270] Time:0.229, Train Loss:0.4063139259815216\n",
      "Epoch 0[2999/17270] Time:0.236, Train Loss:0.6017787456512451\n",
      "Epoch 0[3000/17270] Time:0.224, Train Loss:0.9545036554336548\n",
      "Epoch 0[3001/17270] Time:0.237, Train Loss:0.799819827079773\n",
      "Epoch 0[3002/17270] Time:0.239, Train Loss:0.6138467788696289\n",
      "Epoch 0[3003/17270] Time:0.232, Train Loss:0.4957234561443329\n",
      "Epoch 0[3004/17270] Time:0.229, Train Loss:1.0969537496566772\n",
      "Epoch 0[3005/17270] Time:0.241, Train Loss:0.7136768102645874\n",
      "Epoch 0[3006/17270] Time:0.235, Train Loss:0.46794232726097107\n",
      "Epoch 0[3007/17270] Time:0.228, Train Loss:0.8397395014762878\n",
      "Epoch 0[3008/17270] Time:0.237, Train Loss:0.570798933506012\n",
      "Epoch 0[3009/17270] Time:0.249, Train Loss:0.6374017000198364\n",
      "Epoch 0[3010/17270] Time:0.223, Train Loss:0.6550762057304382\n",
      "Epoch 0[3011/17270] Time:0.24, Train Loss:0.620864987373352\n",
      "Epoch 0[3012/17270] Time:0.233, Train Loss:0.766861617565155\n",
      "Epoch 0[3013/17270] Time:0.233, Train Loss:0.4828360080718994\n",
      "Epoch 0[3014/17270] Time:0.238, Train Loss:0.7645936608314514\n",
      "Epoch 0[3015/17270] Time:0.229, Train Loss:0.7063316106796265\n",
      "Epoch 0[3016/17270] Time:0.229, Train Loss:0.7700934410095215\n",
      "Epoch 0[3017/17270] Time:0.233, Train Loss:0.8172942996025085\n",
      "Epoch 0[3018/17270] Time:0.237, Train Loss:0.6861521005630493\n",
      "Epoch 0[3019/17270] Time:0.245, Train Loss:0.7373477220535278\n",
      "Epoch 0[3020/17270] Time:0.226, Train Loss:0.6022021174430847\n",
      "Epoch 0[3021/17270] Time:0.237, Train Loss:2.144608497619629\n",
      "Epoch 0[3022/17270] Time:0.22, Train Loss:0.4957164525985718\n",
      "Epoch 0[3023/17270] Time:0.242, Train Loss:0.4685485363006592\n",
      "Epoch 0[3024/17270] Time:0.227, Train Loss:0.6519898772239685\n",
      "Epoch 0[3025/17270] Time:0.235, Train Loss:0.46922796964645386\n",
      "Epoch 0[3026/17270] Time:0.242, Train Loss:0.5886897444725037\n",
      "Epoch 0[3027/17270] Time:0.243, Train Loss:0.5860841870307922\n",
      "Epoch 0[3028/17270] Time:0.241, Train Loss:1.09660005569458\n",
      "Epoch 0[3029/17270] Time:0.236, Train Loss:1.0583488941192627\n",
      "Epoch 0[3030/17270] Time:0.231, Train Loss:0.6827459931373596\n",
      "Epoch 0[3031/17270] Time:0.233, Train Loss:1.001694917678833\n",
      "Epoch 0[3032/17270] Time:0.259, Train Loss:0.5846034288406372\n",
      "Epoch 0[3033/17270] Time:0.232, Train Loss:0.7531706690788269\n",
      "Epoch 0[3034/17270] Time:0.249, Train Loss:0.5457869172096252\n",
      "Epoch 0[3035/17270] Time:0.236, Train Loss:0.5912014245986938\n",
      "Epoch 0[3036/17270] Time:0.239, Train Loss:0.6812582015991211\n",
      "Epoch 0[3037/17270] Time:0.235, Train Loss:0.6479248404502869\n",
      "Epoch 0[3038/17270] Time:0.236, Train Loss:1.0122880935668945\n",
      "Epoch 0[3039/17270] Time:0.238, Train Loss:0.6921710968017578\n",
      "Epoch 0[3040/17270] Time:0.237, Train Loss:0.771801769733429\n",
      "Epoch 0[3041/17270] Time:0.232, Train Loss:0.8297694325447083\n",
      "Epoch 0[3042/17270] Time:0.229, Train Loss:0.5350295901298523\n",
      "Epoch 0[3043/17270] Time:0.234, Train Loss:0.575272262096405\n",
      "Epoch 0[3044/17270] Time:0.237, Train Loss:0.5439847111701965\n",
      "Epoch 0[3045/17270] Time:0.236, Train Loss:0.5261849164962769\n",
      "Epoch 0[3046/17270] Time:0.238, Train Loss:0.7136844992637634\n",
      "Epoch 0[3047/17270] Time:0.232, Train Loss:0.5381817817687988\n",
      "Epoch 0[3048/17270] Time:0.237, Train Loss:0.6877798438072205\n",
      "Epoch 0[3049/17270] Time:0.234, Train Loss:0.7087798118591309\n",
      "Epoch 0[3050/17270] Time:0.231, Train Loss:0.6202177405357361\n",
      "Epoch 0[3051/17270] Time:0.23, Train Loss:1.296007752418518\n",
      "Epoch 0[3052/17270] Time:0.252, Train Loss:0.8286096453666687\n",
      "Epoch 0[3053/17270] Time:0.223, Train Loss:1.0911767482757568\n",
      "Epoch 0[3054/17270] Time:0.225, Train Loss:0.4170742332935333\n",
      "Epoch 0[3055/17270] Time:0.228, Train Loss:0.6190714240074158\n",
      "Epoch 0[3056/17270] Time:0.233, Train Loss:0.7883873581886292\n",
      "Epoch 0[3057/17270] Time:0.24, Train Loss:0.8793079853057861\n",
      "Epoch 0[3058/17270] Time:0.239, Train Loss:0.7602810263633728\n",
      "Epoch 0[3059/17270] Time:0.236, Train Loss:0.5722794532775879\n",
      "Epoch 0[3060/17270] Time:0.23, Train Loss:0.3356925845146179\n",
      "Epoch 0[3061/17270] Time:0.231, Train Loss:0.9093992710113525\n",
      "Epoch 0[3062/17270] Time:0.228, Train Loss:0.7572824954986572\n",
      "Epoch 0[3063/17270] Time:0.23, Train Loss:1.6129117012023926\n",
      "Epoch 0[3064/17270] Time:0.239, Train Loss:0.5312146544456482\n",
      "Epoch 0[3065/17270] Time:0.235, Train Loss:0.750608503818512\n",
      "Epoch 0[3066/17270] Time:0.237, Train Loss:0.7572584748268127\n",
      "Epoch 0[3067/17270] Time:0.234, Train Loss:0.6600034236907959\n",
      "Epoch 0[3068/17270] Time:0.221, Train Loss:0.6562200784683228\n",
      "Epoch 0[3069/17270] Time:0.238, Train Loss:0.4198993742465973\n",
      "Epoch 0[3070/17270] Time:0.228, Train Loss:0.8085700869560242\n",
      "Epoch 0[3071/17270] Time:0.244, Train Loss:0.9367427825927734\n",
      "Epoch 0[3072/17270] Time:0.239, Train Loss:0.7627770900726318\n",
      "Epoch 0[3073/17270] Time:0.233, Train Loss:1.0009260177612305\n",
      "Epoch 0[3074/17270] Time:0.239, Train Loss:0.73918616771698\n",
      "Epoch 0[3075/17270] Time:0.218, Train Loss:0.5663679838180542\n",
      "Epoch 0[3076/17270] Time:0.243, Train Loss:0.8032433390617371\n",
      "Epoch 0[3077/17270] Time:0.231, Train Loss:0.7097330689430237\n",
      "Epoch 0[3078/17270] Time:0.241, Train Loss:0.3782757818698883\n",
      "Epoch 0[3079/17270] Time:0.232, Train Loss:0.39380937814712524\n",
      "Epoch 0[3080/17270] Time:0.222, Train Loss:0.5655868053436279\n",
      "Epoch 0[3081/17270] Time:0.235, Train Loss:0.3350611627101898\n",
      "Epoch 0[3082/17270] Time:0.237, Train Loss:0.569279670715332\n",
      "Epoch 0[3083/17270] Time:0.231, Train Loss:0.49843907356262207\n",
      "Epoch 0[3084/17270] Time:0.24, Train Loss:0.9961429238319397\n",
      "Epoch 0[3085/17270] Time:0.232, Train Loss:0.5515696406364441\n",
      "Epoch 0[3086/17270] Time:0.239, Train Loss:0.6019521355628967\n",
      "Epoch 0[3087/17270] Time:0.229, Train Loss:0.5460994243621826\n",
      "Epoch 0[3088/17270] Time:0.24, Train Loss:0.6487885117530823\n",
      "Epoch 0[3089/17270] Time:0.24, Train Loss:0.6023480296134949\n",
      "Epoch 0[3090/17270] Time:0.229, Train Loss:0.9253264665603638\n",
      "Epoch 0[3091/17270] Time:0.244, Train Loss:0.6805418729782104\n",
      "Epoch 0[3092/17270] Time:0.223, Train Loss:0.4115197956562042\n",
      "Epoch 0[3093/17270] Time:0.248, Train Loss:0.4946669638156891\n",
      "Epoch 0[3094/17270] Time:0.23, Train Loss:0.36303141713142395\n",
      "Epoch 0[3095/17270] Time:0.232, Train Loss:0.8743594288825989\n",
      "Epoch 0[3096/17270] Time:0.235, Train Loss:0.8165987730026245\n",
      "Epoch 0[3097/17270] Time:0.238, Train Loss:0.3936828672885895\n",
      "Epoch 0[3098/17270] Time:0.239, Train Loss:0.9269339442253113\n",
      "Epoch 0[3099/17270] Time:0.239, Train Loss:0.6497169137001038\n",
      "Epoch 0[3100/17270] Time:0.242, Train Loss:0.47516587376594543\n",
      "Epoch 0[3101/17270] Time:0.25, Train Loss:0.5469635725021362\n",
      "Epoch 0[3102/17270] Time:0.237, Train Loss:0.41880813241004944\n",
      "Epoch 0[3103/17270] Time:0.23, Train Loss:0.63896644115448\n",
      "Epoch 0[3104/17270] Time:0.238, Train Loss:0.5628113746643066\n",
      "Epoch 0[3105/17270] Time:0.236, Train Loss:1.1210265159606934\n",
      "Epoch 0[3106/17270] Time:0.23, Train Loss:0.6829007267951965\n",
      "Epoch 0[3107/17270] Time:0.246, Train Loss:0.5494750738143921\n",
      "Epoch 0[3108/17270] Time:0.224, Train Loss:0.9065284729003906\n",
      "Epoch 0[3109/17270] Time:0.247, Train Loss:0.5180764198303223\n",
      "Epoch 0[3110/17270] Time:0.229, Train Loss:0.5555750131607056\n",
      "Epoch 0[3111/17270] Time:0.238, Train Loss:0.5436439514160156\n",
      "Epoch 0[3112/17270] Time:0.235, Train Loss:0.3279606103897095\n",
      "Epoch 0[3113/17270] Time:0.246, Train Loss:2.1929121017456055\n",
      "Epoch 0[3114/17270] Time:0.23, Train Loss:0.3373788893222809\n",
      "Epoch 0[3115/17270] Time:0.23, Train Loss:0.621630072593689\n",
      "Epoch 0[3116/17270] Time:0.232, Train Loss:0.6840150356292725\n",
      "Epoch 0[3117/17270] Time:0.231, Train Loss:0.546562671661377\n",
      "Epoch 0[3118/17270] Time:0.238, Train Loss:0.5365599393844604\n",
      "Epoch 0[3119/17270] Time:0.238, Train Loss:0.49744388461112976\n",
      "Epoch 0[3120/17270] Time:0.234, Train Loss:1.4748048782348633\n",
      "Epoch 0[3121/17270] Time:0.227, Train Loss:0.7987586855888367\n",
      "Epoch 0[3122/17270] Time:0.245, Train Loss:0.5147393941879272\n",
      "Epoch 0[3123/17270] Time:0.238, Train Loss:0.58067387342453\n",
      "Epoch 0[3124/17270] Time:0.223, Train Loss:0.6141787767410278\n",
      "Epoch 0[3125/17270] Time:0.245, Train Loss:0.427916020154953\n",
      "Epoch 0[3126/17270] Time:0.238, Train Loss:0.4812512695789337\n",
      "Epoch 0[3127/17270] Time:0.232, Train Loss:0.6867075562477112\n",
      "Epoch 0[3128/17270] Time:0.234, Train Loss:0.5260328650474548\n",
      "Epoch 0[3129/17270] Time:0.232, Train Loss:0.8623665571212769\n",
      "Epoch 0[3130/17270] Time:0.243, Train Loss:0.4759565591812134\n",
      "Epoch 0[3131/17270] Time:0.234, Train Loss:0.8731712102890015\n",
      "Epoch 0[3132/17270] Time:0.234, Train Loss:0.5349324345588684\n",
      "Epoch 0[3133/17270] Time:0.233, Train Loss:0.630806028842926\n",
      "Epoch 0[3134/17270] Time:0.239, Train Loss:1.421348214149475\n",
      "Epoch 0[3135/17270] Time:0.235, Train Loss:0.5645638704299927\n",
      "Epoch 0[3136/17270] Time:0.233, Train Loss:0.4993774890899658\n",
      "Epoch 0[3137/17270] Time:0.226, Train Loss:1.1190454959869385\n",
      "Epoch 0[3138/17270] Time:0.23, Train Loss:0.743947446346283\n",
      "Epoch 0[3139/17270] Time:0.239, Train Loss:0.5923577547073364\n",
      "Epoch 0[3140/17270] Time:0.227, Train Loss:0.6057209372520447\n",
      "Epoch 0[3141/17270] Time:0.238, Train Loss:0.6288581490516663\n",
      "Epoch 0[3142/17270] Time:0.237, Train Loss:0.47599855065345764\n",
      "Epoch 0[3143/17270] Time:0.23, Train Loss:0.7783798575401306\n",
      "Epoch 0[3144/17270] Time:0.238, Train Loss:0.41726014018058777\n",
      "Epoch 0[3145/17270] Time:0.232, Train Loss:0.5853632092475891\n",
      "Epoch 0[3146/17270] Time:0.253, Train Loss:0.5094447731971741\n",
      "Epoch 0[3147/17270] Time:0.225, Train Loss:0.7593013644218445\n",
      "Epoch 0[3148/17270] Time:0.242, Train Loss:0.5616576671600342\n",
      "Epoch 0[3149/17270] Time:0.241, Train Loss:0.8695526719093323\n",
      "Epoch 0[3150/17270] Time:0.237, Train Loss:0.5212275385856628\n",
      "Epoch 0[3151/17270] Time:0.231, Train Loss:0.4845251142978668\n",
      "Epoch 0[3152/17270] Time:0.238, Train Loss:0.703330934047699\n",
      "Epoch 0[3153/17270] Time:0.252, Train Loss:1.6060041189193726\n",
      "Epoch 0[3154/17270] Time:0.244, Train Loss:0.8411598801612854\n",
      "Epoch 0[3155/17270] Time:0.235, Train Loss:0.3595592975616455\n",
      "Epoch 0[3156/17270] Time:0.233, Train Loss:0.9483920335769653\n",
      "Epoch 0[3157/17270] Time:0.237, Train Loss:0.5941926836967468\n",
      "Epoch 0[3158/17270] Time:0.236, Train Loss:0.5171794295310974\n",
      "Epoch 0[3159/17270] Time:0.223, Train Loss:0.3335818648338318\n",
      "Epoch 0[3160/17270] Time:0.229, Train Loss:0.7283241748809814\n",
      "Epoch 0[3161/17270] Time:0.237, Train Loss:0.6585791110992432\n",
      "Epoch 0[3162/17270] Time:0.237, Train Loss:0.4796338975429535\n",
      "Epoch 0[3163/17270] Time:0.229, Train Loss:1.0694948434829712\n",
      "Epoch 0[3164/17270] Time:0.233, Train Loss:0.6356223225593567\n",
      "Epoch 0[3165/17270] Time:0.229, Train Loss:0.6120815873146057\n",
      "Epoch 0[3166/17270] Time:0.23, Train Loss:0.7662047743797302\n",
      "Epoch 0[3167/17270] Time:0.228, Train Loss:0.6170753240585327\n",
      "Epoch 0[3168/17270] Time:0.237, Train Loss:0.7602764368057251\n",
      "Epoch 0[3169/17270] Time:0.229, Train Loss:0.5305449366569519\n",
      "Epoch 0[3170/17270] Time:0.236, Train Loss:0.60265052318573\n",
      "Epoch 0[3171/17270] Time:0.239, Train Loss:0.6676815748214722\n",
      "Epoch 0[3172/17270] Time:0.232, Train Loss:0.5814446806907654\n",
      "Epoch 0[3173/17270] Time:0.226, Train Loss:0.8750243186950684\n",
      "Epoch 0[3174/17270] Time:0.232, Train Loss:0.4611627161502838\n",
      "Epoch 0[3175/17270] Time:0.233, Train Loss:0.3518223166465759\n",
      "Epoch 0[3176/17270] Time:0.232, Train Loss:0.5619242787361145\n",
      "Epoch 0[3177/17270] Time:0.235, Train Loss:0.40813201665878296\n",
      "Epoch 0[3178/17270] Time:0.231, Train Loss:0.42243853211402893\n",
      "Epoch 0[3179/17270] Time:0.231, Train Loss:0.6659196019172668\n",
      "Epoch 0[3180/17270] Time:0.234, Train Loss:1.0060523748397827\n",
      "Epoch 0[3181/17270] Time:0.24, Train Loss:0.9918053150177002\n",
      "Epoch 0[3182/17270] Time:0.247, Train Loss:1.7038744688034058\n",
      "Epoch 0[3183/17270] Time:0.237, Train Loss:0.43111997842788696\n",
      "Epoch 0[3184/17270] Time:0.242, Train Loss:0.40767237544059753\n",
      "Epoch 0[3185/17270] Time:0.228, Train Loss:0.8301894664764404\n",
      "Epoch 0[3186/17270] Time:0.243, Train Loss:0.9161151051521301\n",
      "Epoch 0[3187/17270] Time:0.233, Train Loss:1.1051318645477295\n",
      "Epoch 0[3188/17270] Time:0.25, Train Loss:0.6395257711410522\n",
      "Epoch 0[3189/17270] Time:0.216, Train Loss:0.4398992359638214\n",
      "Epoch 0[3190/17270] Time:0.229, Train Loss:0.7158908247947693\n",
      "Epoch 0[3191/17270] Time:0.239, Train Loss:0.6163178086280823\n",
      "Epoch 0[3192/17270] Time:0.24, Train Loss:0.9714645743370056\n",
      "Epoch 0[3193/17270] Time:0.241, Train Loss:0.41793835163116455\n",
      "Epoch 0[3194/17270] Time:0.24, Train Loss:0.6366510987281799\n",
      "Epoch 0[3195/17270] Time:0.239, Train Loss:0.4631156623363495\n",
      "Epoch 0[3196/17270] Time:0.234, Train Loss:0.6116124987602234\n",
      "Epoch 0[3197/17270] Time:0.224, Train Loss:1.0406931638717651\n",
      "Epoch 0[3198/17270] Time:0.234, Train Loss:0.6680987477302551\n",
      "Epoch 0[3199/17270] Time:0.239, Train Loss:0.5328188538551331\n",
      "Epoch 0[3200/17270] Time:0.247, Train Loss:0.5253557562828064\n",
      "Epoch 0[3201/17270] Time:0.228, Train Loss:1.0619539022445679\n",
      "Epoch 0[3202/17270] Time:0.227, Train Loss:0.4629681706428528\n",
      "Epoch 0[3203/17270] Time:0.232, Train Loss:0.9709970951080322\n",
      "Epoch 0[3204/17270] Time:0.244, Train Loss:1.1327110528945923\n",
      "Epoch 0[3205/17270] Time:0.233, Train Loss:0.690345287322998\n",
      "Epoch 0[3206/17270] Time:0.236, Train Loss:0.5730642676353455\n",
      "Epoch 0[3207/17270] Time:0.227, Train Loss:0.7871453762054443\n",
      "Epoch 0[3208/17270] Time:0.229, Train Loss:0.48844441771507263\n",
      "Epoch 0[3209/17270] Time:0.239, Train Loss:0.7880932092666626\n",
      "Epoch 0[3210/17270] Time:0.229, Train Loss:0.7992995381355286\n",
      "Epoch 0[3211/17270] Time:0.235, Train Loss:0.8045562505722046\n",
      "Epoch 0[3212/17270] Time:0.231, Train Loss:0.48920005559921265\n",
      "Epoch 0[3213/17270] Time:0.234, Train Loss:0.6173169612884521\n",
      "Epoch 0[3214/17270] Time:0.237, Train Loss:0.6239404082298279\n",
      "Epoch 0[3215/17270] Time:0.234, Train Loss:0.5339670181274414\n",
      "Epoch 0[3216/17270] Time:0.234, Train Loss:0.6888263821601868\n",
      "Epoch 0[3217/17270] Time:0.232, Train Loss:0.6740750074386597\n",
      "Epoch 0[3218/17270] Time:0.235, Train Loss:0.5517572164535522\n",
      "Epoch 0[3219/17270] Time:0.235, Train Loss:0.5695859789848328\n",
      "Epoch 0[3220/17270] Time:0.243, Train Loss:0.5437049865722656\n",
      "Epoch 0[3221/17270] Time:0.236, Train Loss:0.4781234860420227\n",
      "Epoch 0[3222/17270] Time:0.244, Train Loss:0.8647696375846863\n",
      "Epoch 0[3223/17270] Time:0.24, Train Loss:0.6163891553878784\n",
      "Epoch 0[3224/17270] Time:0.228, Train Loss:0.6876625418663025\n",
      "Epoch 0[3225/17270] Time:0.236, Train Loss:0.5666037797927856\n",
      "Epoch 0[3226/17270] Time:0.23, Train Loss:0.7439295649528503\n",
      "Epoch 0[3227/17270] Time:0.233, Train Loss:1.1150555610656738\n",
      "Epoch 0[3228/17270] Time:0.222, Train Loss:0.4762875735759735\n",
      "Epoch 0[3229/17270] Time:0.231, Train Loss:0.7453370690345764\n",
      "Epoch 0[3230/17270] Time:0.24, Train Loss:0.4876047670841217\n",
      "Epoch 0[3231/17270] Time:0.225, Train Loss:0.9622591137886047\n",
      "Epoch 0[3232/17270] Time:0.245, Train Loss:0.6257959008216858\n",
      "Epoch 0[3233/17270] Time:0.233, Train Loss:0.6072830557823181\n",
      "Epoch 0[3234/17270] Time:0.226, Train Loss:0.4917839765548706\n",
      "Epoch 0[3235/17270] Time:0.243, Train Loss:0.7114527821540833\n",
      "Epoch 0[3236/17270] Time:0.237, Train Loss:0.9333596229553223\n",
      "Epoch 0[3237/17270] Time:0.239, Train Loss:0.6176272630691528\n",
      "Epoch 0[3238/17270] Time:0.234, Train Loss:0.446204274892807\n",
      "Epoch 0[3239/17270] Time:0.226, Train Loss:0.7837217450141907\n",
      "Epoch 0[3240/17270] Time:0.235, Train Loss:0.44046568870544434\n",
      "Epoch 0[3241/17270] Time:0.222, Train Loss:0.34915825724601746\n",
      "Epoch 0[3242/17270] Time:0.242, Train Loss:0.5418897271156311\n",
      "Epoch 0[3243/17270] Time:0.231, Train Loss:0.7099660038948059\n",
      "Epoch 0[3244/17270] Time:0.232, Train Loss:0.5311868786811829\n",
      "Epoch 0[3245/17270] Time:0.239, Train Loss:1.1223621368408203\n",
      "Epoch 0[3246/17270] Time:0.22, Train Loss:0.7364293336868286\n",
      "Epoch 0[3247/17270] Time:0.239, Train Loss:0.8922632336616516\n",
      "Epoch 0[3248/17270] Time:0.239, Train Loss:0.8805866241455078\n",
      "Epoch 0[3249/17270] Time:0.234, Train Loss:0.683411180973053\n",
      "Epoch 0[3250/17270] Time:0.228, Train Loss:0.9522430300712585\n",
      "Epoch 0[3251/17270] Time:0.23, Train Loss:0.5442649126052856\n",
      "Epoch 0[3252/17270] Time:0.238, Train Loss:0.8815992474555969\n",
      "Epoch 0[3253/17270] Time:0.235, Train Loss:0.6601898670196533\n",
      "Epoch 0[3254/17270] Time:0.238, Train Loss:0.3609359860420227\n",
      "Epoch 0[3255/17270] Time:0.238, Train Loss:0.6051920056343079\n",
      "Epoch 0[3256/17270] Time:0.242, Train Loss:0.5825532078742981\n",
      "Epoch 0[3257/17270] Time:0.251, Train Loss:1.2953652143478394\n",
      "Epoch 0[3258/17270] Time:0.23, Train Loss:0.6065799593925476\n",
      "Epoch 0[3259/17270] Time:0.241, Train Loss:0.5582306385040283\n",
      "Epoch 0[3260/17270] Time:0.222, Train Loss:0.9373033046722412\n",
      "Epoch 0[3261/17270] Time:0.228, Train Loss:0.5077738165855408\n",
      "Epoch 0[3262/17270] Time:0.238, Train Loss:0.5963543653488159\n",
      "Epoch 0[3263/17270] Time:0.237, Train Loss:0.732192873954773\n",
      "Epoch 0[3264/17270] Time:0.239, Train Loss:0.7845981121063232\n",
      "Epoch 0[3265/17270] Time:0.236, Train Loss:0.5972442030906677\n",
      "Epoch 0[3266/17270] Time:0.245, Train Loss:0.986372709274292\n",
      "Epoch 0[3267/17270] Time:0.234, Train Loss:0.5657384991645813\n",
      "Epoch 0[3268/17270] Time:0.235, Train Loss:0.7728811502456665\n",
      "Epoch 0[3269/17270] Time:0.239, Train Loss:0.9283602833747864\n",
      "Epoch 0[3270/17270] Time:0.233, Train Loss:0.45131126046180725\n",
      "Epoch 0[3271/17270] Time:0.232, Train Loss:0.9309539794921875\n",
      "Epoch 0[3272/17270] Time:0.236, Train Loss:0.5297806859016418\n",
      "Epoch 0[3273/17270] Time:0.235, Train Loss:0.655002772808075\n",
      "Epoch 0[3274/17270] Time:0.233, Train Loss:0.4262336790561676\n",
      "Epoch 0[3275/17270] Time:0.245, Train Loss:1.339957356452942\n",
      "Epoch 0[3276/17270] Time:0.245, Train Loss:1.2991753816604614\n",
      "Epoch 0[3277/17270] Time:0.224, Train Loss:0.6682600378990173\n",
      "Epoch 0[3278/17270] Time:0.238, Train Loss:0.7133920788764954\n",
      "Epoch 0[3279/17270] Time:0.228, Train Loss:0.499728798866272\n",
      "Epoch 0[3280/17270] Time:0.231, Train Loss:0.9611538648605347\n",
      "Epoch 0[3281/17270] Time:0.243, Train Loss:0.349412739276886\n",
      "Epoch 0[3282/17270] Time:0.232, Train Loss:0.5275495052337646\n",
      "Epoch 0[3283/17270] Time:0.245, Train Loss:0.660740315914154\n",
      "Epoch 0[3284/17270] Time:0.239, Train Loss:0.44240689277648926\n",
      "Epoch 0[3285/17270] Time:0.234, Train Loss:0.7155658602714539\n",
      "Epoch 0[3286/17270] Time:0.238, Train Loss:0.5587521195411682\n",
      "Epoch 0[3287/17270] Time:0.25, Train Loss:0.5926033854484558\n",
      "Epoch 0[3288/17270] Time:0.232, Train Loss:0.6298690438270569\n",
      "Epoch 0[3289/17270] Time:0.233, Train Loss:0.6646892428398132\n",
      "Epoch 0[3290/17270] Time:0.234, Train Loss:0.6018862724304199\n",
      "Epoch 0[3291/17270] Time:0.234, Train Loss:0.5911265015602112\n",
      "Epoch 0[3292/17270] Time:0.243, Train Loss:0.59830242395401\n",
      "Epoch 0[3293/17270] Time:0.235, Train Loss:0.7280790209770203\n",
      "Epoch 0[3294/17270] Time:0.236, Train Loss:0.6285778284072876\n",
      "Epoch 0[3295/17270] Time:0.235, Train Loss:0.4894171953201294\n",
      "Epoch 0[3296/17270] Time:0.234, Train Loss:0.4872882068157196\n",
      "Epoch 0[3297/17270] Time:0.237, Train Loss:0.4402731657028198\n",
      "Epoch 0[3298/17270] Time:0.236, Train Loss:0.41797947883605957\n",
      "Epoch 0[3299/17270] Time:0.223, Train Loss:0.5883795022964478\n",
      "Epoch 0[3300/17270] Time:0.236, Train Loss:0.46108266711235046\n",
      "Epoch 0[3301/17270] Time:0.236, Train Loss:0.5565769672393799\n",
      "Epoch 0[3302/17270] Time:0.24, Train Loss:0.3323638141155243\n",
      "Epoch 0[3303/17270] Time:0.232, Train Loss:0.47283756732940674\n",
      "Epoch 0[3304/17270] Time:0.237, Train Loss:0.38694673776626587\n",
      "Epoch 0[3305/17270] Time:0.23, Train Loss:0.7151194214820862\n",
      "Epoch 0[3306/17270] Time:0.23, Train Loss:0.680682897567749\n",
      "Epoch 0[3307/17270] Time:0.232, Train Loss:0.8729894757270813\n",
      "Epoch 0[3308/17270] Time:0.241, Train Loss:1.1412285566329956\n",
      "Epoch 0[3309/17270] Time:0.225, Train Loss:0.4625564217567444\n",
      "Epoch 0[3310/17270] Time:0.241, Train Loss:0.7011527419090271\n",
      "Epoch 0[3311/17270] Time:0.23, Train Loss:0.5008812546730042\n",
      "Epoch 0[3312/17270] Time:0.229, Train Loss:1.1611214876174927\n",
      "Epoch 0[3313/17270] Time:0.243, Train Loss:1.065205693244934\n",
      "Epoch 0[3314/17270] Time:0.22, Train Loss:1.2418665885925293\n",
      "Epoch 0[3315/17270] Time:0.234, Train Loss:0.41435977816581726\n",
      "Epoch 0[3316/17270] Time:0.23, Train Loss:0.5348368287086487\n",
      "Epoch 0[3317/17270] Time:0.237, Train Loss:0.40179717540740967\n",
      "Epoch 0[3318/17270] Time:0.228, Train Loss:0.8436711430549622\n",
      "Epoch 0[3319/17270] Time:0.23, Train Loss:0.8949370980262756\n",
      "Epoch 0[3320/17270] Time:0.231, Train Loss:0.4836100935935974\n",
      "Epoch 0[3321/17270] Time:0.237, Train Loss:0.39046546816825867\n",
      "Epoch 0[3322/17270] Time:0.231, Train Loss:0.8283201456069946\n",
      "Epoch 0[3323/17270] Time:0.23, Train Loss:0.6661929488182068\n",
      "Epoch 0[3324/17270] Time:0.239, Train Loss:0.6740881204605103\n",
      "Epoch 0[3325/17270] Time:0.227, Train Loss:0.6103141903877258\n",
      "Epoch 0[3326/17270] Time:0.229, Train Loss:1.0266139507293701\n",
      "Epoch 0[3327/17270] Time:0.233, Train Loss:0.73226398229599\n",
      "Epoch 0[3328/17270] Time:0.23, Train Loss:0.8412047028541565\n",
      "Epoch 0[3329/17270] Time:0.241, Train Loss:0.46423327922821045\n",
      "Epoch 0[3330/17270] Time:0.23, Train Loss:0.7391619086265564\n",
      "Epoch 0[3331/17270] Time:0.238, Train Loss:0.4554528295993805\n",
      "Epoch 0[3332/17270] Time:0.237, Train Loss:1.1192340850830078\n",
      "Epoch 0[3333/17270] Time:0.241, Train Loss:0.7303393483161926\n",
      "Epoch 0[3334/17270] Time:0.242, Train Loss:0.6203574538230896\n",
      "Epoch 0[3335/17270] Time:0.237, Train Loss:0.5851304531097412\n",
      "Epoch 0[3336/17270] Time:0.233, Train Loss:0.7710815072059631\n",
      "Epoch 0[3337/17270] Time:0.233, Train Loss:0.8375580310821533\n",
      "Epoch 0[3338/17270] Time:0.238, Train Loss:1.1462700366973877\n",
      "Epoch 0[3339/17270] Time:0.231, Train Loss:0.44991737604141235\n",
      "Epoch 0[3340/17270] Time:0.235, Train Loss:0.8325848579406738\n",
      "Epoch 0[3341/17270] Time:0.23, Train Loss:0.5689826607704163\n",
      "Epoch 0[3342/17270] Time:0.239, Train Loss:0.5337702035903931\n",
      "Epoch 0[3343/17270] Time:0.229, Train Loss:0.6545239090919495\n",
      "Epoch 0[3344/17270] Time:0.225, Train Loss:0.5461171269416809\n",
      "Epoch 0[3345/17270] Time:0.228, Train Loss:0.6701452732086182\n",
      "Epoch 0[3346/17270] Time:0.228, Train Loss:0.6444056034088135\n",
      "Epoch 0[3347/17270] Time:0.227, Train Loss:0.8400261402130127\n",
      "Epoch 0[3348/17270] Time:0.237, Train Loss:0.5283456444740295\n",
      "Epoch 0[3349/17270] Time:0.229, Train Loss:0.531879723072052\n",
      "Epoch 0[3350/17270] Time:0.23, Train Loss:1.0519530773162842\n",
      "Epoch 0[3351/17270] Time:0.228, Train Loss:0.4019967317581177\n",
      "Epoch 0[3352/17270] Time:0.238, Train Loss:1.2289079427719116\n",
      "Epoch 0[3353/17270] Time:0.23, Train Loss:0.5661092400550842\n",
      "Epoch 0[3354/17270] Time:0.233, Train Loss:0.7068085670471191\n",
      "Epoch 0[3355/17270] Time:0.23, Train Loss:0.6413080096244812\n",
      "Epoch 0[3356/17270] Time:0.231, Train Loss:0.398579478263855\n",
      "Epoch 0[3357/17270] Time:0.229, Train Loss:0.6536985039710999\n",
      "Epoch 0[3358/17270] Time:0.237, Train Loss:0.5680570602416992\n",
      "Epoch 0[3359/17270] Time:0.237, Train Loss:0.7563517093658447\n",
      "Epoch 0[3360/17270] Time:0.235, Train Loss:0.782615065574646\n",
      "Epoch 0[3361/17270] Time:0.236, Train Loss:0.8271211385726929\n",
      "Epoch 0[3362/17270] Time:0.232, Train Loss:0.506835401058197\n",
      "Epoch 0[3363/17270] Time:0.231, Train Loss:0.6293089389801025\n",
      "Epoch 0[3364/17270] Time:0.236, Train Loss:1.2960717678070068\n",
      "Epoch 0[3365/17270] Time:0.243, Train Loss:0.561358630657196\n",
      "Epoch 0[3366/17270] Time:0.23, Train Loss:1.2219722270965576\n",
      "Epoch 0[3367/17270] Time:0.222, Train Loss:1.0134962797164917\n",
      "Epoch 0[3368/17270] Time:0.24, Train Loss:0.6219992637634277\n",
      "Epoch 0[3369/17270] Time:0.239, Train Loss:0.7566444873809814\n",
      "Epoch 0[3370/17270] Time:0.235, Train Loss:0.5419623255729675\n",
      "Epoch 0[3371/17270] Time:0.234, Train Loss:0.5954360365867615\n",
      "Epoch 0[3372/17270] Time:0.242, Train Loss:0.7869578003883362\n",
      "Epoch 0[3373/17270] Time:0.23, Train Loss:0.5029494762420654\n",
      "Epoch 0[3374/17270] Time:0.238, Train Loss:0.8996979594230652\n",
      "Epoch 0[3375/17270] Time:0.217, Train Loss:0.6171703338623047\n",
      "Epoch 0[3376/17270] Time:0.232, Train Loss:0.6305372714996338\n",
      "Epoch 0[3377/17270] Time:0.226, Train Loss:0.5057121515274048\n",
      "Epoch 0[3378/17270] Time:0.231, Train Loss:0.8872939944267273\n",
      "Epoch 0[3379/17270] Time:0.24, Train Loss:0.5124115943908691\n",
      "Epoch 0[3380/17270] Time:0.225, Train Loss:1.007156252861023\n",
      "Epoch 0[3381/17270] Time:0.239, Train Loss:0.5151288509368896\n",
      "Epoch 0[3382/17270] Time:0.227, Train Loss:0.5381311774253845\n",
      "Epoch 0[3383/17270] Time:0.246, Train Loss:0.43311217427253723\n",
      "Epoch 0[3384/17270] Time:0.227, Train Loss:0.5753648281097412\n",
      "Epoch 0[3385/17270] Time:0.237, Train Loss:0.6655903458595276\n",
      "Epoch 0[3386/17270] Time:0.24, Train Loss:0.6863523721694946\n",
      "Epoch 0[3387/17270] Time:0.244, Train Loss:0.8767350912094116\n",
      "Epoch 0[3388/17270] Time:0.239, Train Loss:0.526419460773468\n",
      "Epoch 0[3389/17270] Time:0.226, Train Loss:0.6502175331115723\n",
      "Epoch 0[3390/17270] Time:0.236, Train Loss:1.1447317600250244\n",
      "Epoch 0[3391/17270] Time:0.238, Train Loss:0.6340309977531433\n",
      "Epoch 0[3392/17270] Time:0.23, Train Loss:0.5771880745887756\n",
      "Epoch 0[3393/17270] Time:0.231, Train Loss:0.42977213859558105\n",
      "Epoch 0[3394/17270] Time:0.227, Train Loss:0.6367332935333252\n",
      "Epoch 0[3395/17270] Time:0.232, Train Loss:0.315125972032547\n",
      "Epoch 0[3396/17270] Time:0.233, Train Loss:0.7794529795646667\n",
      "Epoch 0[3397/17270] Time:0.229, Train Loss:0.580646812915802\n",
      "Epoch 0[3398/17270] Time:0.23, Train Loss:0.6504367589950562\n",
      "Epoch 0[3399/17270] Time:0.229, Train Loss:0.6854438185691833\n",
      "Epoch 0[3400/17270] Time:0.229, Train Loss:0.9177480936050415\n",
      "Epoch 0[3401/17270] Time:0.238, Train Loss:0.47439292073249817\n",
      "Epoch 0[3402/17270] Time:0.238, Train Loss:0.468549907207489\n",
      "Epoch 0[3403/17270] Time:0.23, Train Loss:0.6965846419334412\n",
      "Epoch 0[3404/17270] Time:0.236, Train Loss:0.6614022850990295\n",
      "Epoch 0[3405/17270] Time:0.238, Train Loss:1.1834717988967896\n",
      "Epoch 0[3406/17270] Time:0.237, Train Loss:0.5977914333343506\n",
      "Epoch 0[3407/17270] Time:0.231, Train Loss:0.7451906800270081\n",
      "Epoch 0[3408/17270] Time:0.23, Train Loss:0.6883335113525391\n",
      "Epoch 0[3409/17270] Time:0.237, Train Loss:0.478041410446167\n",
      "Epoch 0[3410/17270] Time:0.239, Train Loss:0.45739755034446716\n",
      "Epoch 0[3411/17270] Time:0.227, Train Loss:1.6205233335494995\n",
      "Epoch 0[3412/17270] Time:0.233, Train Loss:1.133293867111206\n",
      "Epoch 0[3413/17270] Time:0.233, Train Loss:0.9229861497879028\n",
      "Epoch 0[3414/17270] Time:0.238, Train Loss:0.9382495880126953\n",
      "Epoch 0[3415/17270] Time:0.232, Train Loss:0.7146908640861511\n",
      "Epoch 0[3416/17270] Time:0.23, Train Loss:0.4678701162338257\n",
      "Epoch 0[3417/17270] Time:0.238, Train Loss:0.6102792024612427\n",
      "Epoch 0[3418/17270] Time:0.231, Train Loss:0.686919093132019\n",
      "Epoch 0[3419/17270] Time:0.238, Train Loss:0.8532630205154419\n",
      "Epoch 0[3420/17270] Time:0.238, Train Loss:0.6713697910308838\n",
      "Epoch 0[3421/17270] Time:0.232, Train Loss:0.5961399078369141\n",
      "Epoch 0[3422/17270] Time:0.237, Train Loss:0.45240578055381775\n",
      "Epoch 0[3423/17270] Time:0.241, Train Loss:0.616653561592102\n",
      "Epoch 0[3424/17270] Time:0.233, Train Loss:0.6292775869369507\n",
      "Epoch 0[3425/17270] Time:0.233, Train Loss:0.5241296291351318\n",
      "Epoch 0[3426/17270] Time:0.242, Train Loss:0.5702236294746399\n",
      "Epoch 0[3427/17270] Time:0.234, Train Loss:0.8056302070617676\n",
      "Epoch 0[3428/17270] Time:0.223, Train Loss:1.0193872451782227\n",
      "Epoch 0[3429/17270] Time:0.24, Train Loss:0.49487683176994324\n",
      "Epoch 0[3430/17270] Time:0.232, Train Loss:1.7471202611923218\n",
      "Epoch 0[3431/17270] Time:0.24, Train Loss:0.8105953931808472\n",
      "Epoch 0[3432/17270] Time:0.238, Train Loss:0.8376173973083496\n",
      "Epoch 0[3433/17270] Time:0.238, Train Loss:0.6322691440582275\n",
      "Epoch 0[3434/17270] Time:0.236, Train Loss:0.5378313064575195\n",
      "Epoch 0[3435/17270] Time:0.228, Train Loss:0.6532315015792847\n",
      "Epoch 0[3436/17270] Time:0.232, Train Loss:0.699470579624176\n",
      "Epoch 0[3437/17270] Time:0.231, Train Loss:1.5480821132659912\n",
      "Epoch 0[3438/17270] Time:0.231, Train Loss:0.6957510709762573\n",
      "Epoch 0[3439/17270] Time:0.237, Train Loss:0.5534924268722534\n",
      "Epoch 0[3440/17270] Time:0.237, Train Loss:0.5252211689949036\n",
      "Epoch 0[3441/17270] Time:0.235, Train Loss:0.5592018365859985\n",
      "Epoch 0[3442/17270] Time:0.226, Train Loss:0.5913650393486023\n",
      "Epoch 0[3443/17270] Time:0.23, Train Loss:0.7919149994850159\n",
      "Epoch 0[3444/17270] Time:0.234, Train Loss:0.7007582187652588\n",
      "Epoch 0[3445/17270] Time:0.239, Train Loss:0.7141416072845459\n",
      "Epoch 0[3446/17270] Time:0.235, Train Loss:1.202349305152893\n",
      "Epoch 0[3447/17270] Time:0.233, Train Loss:0.5440422892570496\n",
      "Epoch 0[3448/17270] Time:0.236, Train Loss:0.5327738523483276\n",
      "Epoch 0[3449/17270] Time:0.231, Train Loss:0.5699137449264526\n",
      "Epoch 0[3450/17270] Time:0.234, Train Loss:0.5458970665931702\n",
      "Epoch 0[3451/17270] Time:0.23, Train Loss:0.6483144760131836\n",
      "Epoch 0[3452/17270] Time:0.227, Train Loss:0.9953205585479736\n",
      "Epoch 0[3453/17270] Time:0.229, Train Loss:0.9834495782852173\n",
      "Epoch 0[3454/17270] Time:0.228, Train Loss:0.8447757959365845\n",
      "Epoch 0[3455/17270] Time:0.229, Train Loss:0.5801931023597717\n",
      "Epoch 0[3456/17270] Time:0.235, Train Loss:0.5875690579414368\n",
      "Epoch 0[3457/17270] Time:0.235, Train Loss:0.579810619354248\n",
      "Epoch 0[3458/17270] Time:0.232, Train Loss:0.46160411834716797\n",
      "Epoch 0[3459/17270] Time:0.257, Train Loss:0.6503952145576477\n",
      "Epoch 0[3460/17270] Time:0.23, Train Loss:0.5362734198570251\n",
      "Epoch 0[3461/17270] Time:0.233, Train Loss:1.254938006401062\n",
      "Epoch 0[3462/17270] Time:0.226, Train Loss:0.3622663617134094\n",
      "Epoch 0[3463/17270] Time:0.234, Train Loss:0.44528481364250183\n",
      "Epoch 0[3464/17270] Time:0.229, Train Loss:0.5758985877037048\n",
      "Epoch 0[3465/17270] Time:0.235, Train Loss:0.5174364447593689\n",
      "Epoch 0[3466/17270] Time:0.227, Train Loss:1.1944202184677124\n",
      "Epoch 0[3467/17270] Time:0.23, Train Loss:0.8071739673614502\n",
      "Epoch 0[3468/17270] Time:0.228, Train Loss:0.492477148771286\n",
      "Epoch 0[3469/17270] Time:0.232, Train Loss:0.5726903080940247\n",
      "Epoch 0[3470/17270] Time:0.236, Train Loss:0.5686575174331665\n",
      "Epoch 0[3471/17270] Time:0.236, Train Loss:0.3873807489871979\n",
      "Epoch 0[3472/17270] Time:0.234, Train Loss:0.9707798361778259\n",
      "Epoch 0[3473/17270] Time:0.247, Train Loss:0.3951015770435333\n",
      "Epoch 0[3474/17270] Time:0.235, Train Loss:0.6056969165802002\n",
      "Epoch 0[3475/17270] Time:0.234, Train Loss:0.6539378762245178\n",
      "Epoch 0[3476/17270] Time:0.239, Train Loss:0.45850929617881775\n",
      "Epoch 0[3477/17270] Time:0.237, Train Loss:0.4419202208518982\n",
      "Epoch 0[3478/17270] Time:0.237, Train Loss:0.6393890380859375\n",
      "Epoch 0[3479/17270] Time:0.238, Train Loss:0.712419867515564\n",
      "Epoch 0[3480/17270] Time:0.232, Train Loss:1.0558654069900513\n",
      "Epoch 0[3481/17270] Time:0.227, Train Loss:0.5529659986495972\n",
      "Epoch 0[3482/17270] Time:0.229, Train Loss:0.552809476852417\n",
      "Epoch 0[3483/17270] Time:0.233, Train Loss:1.0697932243347168\n",
      "Epoch 0[3484/17270] Time:0.231, Train Loss:0.6534190773963928\n",
      "Epoch 0[3485/17270] Time:0.237, Train Loss:0.5458711385726929\n",
      "Epoch 0[3486/17270] Time:0.229, Train Loss:0.4866062104701996\n",
      "Epoch 0[3487/17270] Time:0.236, Train Loss:0.6914355158805847\n",
      "Epoch 0[3488/17270] Time:0.237, Train Loss:0.5998656153678894\n",
      "Epoch 0[3489/17270] Time:0.236, Train Loss:0.6913933753967285\n",
      "Epoch 0[3490/17270] Time:0.236, Train Loss:0.35599908232688904\n",
      "Epoch 0[3491/17270] Time:0.229, Train Loss:0.7663944363594055\n",
      "Epoch 0[3492/17270] Time:0.229, Train Loss:0.758724570274353\n",
      "Epoch 0[3493/17270] Time:0.232, Train Loss:0.4825533926486969\n",
      "Epoch 0[3494/17270] Time:0.238, Train Loss:0.6831751465797424\n",
      "Epoch 0[3495/17270] Time:0.228, Train Loss:0.6258636713027954\n",
      "Epoch 0[3496/17270] Time:0.233, Train Loss:0.7140153050422668\n",
      "Epoch 0[3497/17270] Time:0.226, Train Loss:0.4347391426563263\n",
      "Epoch 0[3498/17270] Time:0.237, Train Loss:0.43031948804855347\n",
      "Epoch 0[3499/17270] Time:0.237, Train Loss:0.4818834364414215\n",
      "Epoch 0[3500/17270] Time:0.238, Train Loss:0.3677356243133545\n",
      "Epoch 0[3501/17270] Time:0.228, Train Loss:0.566079318523407\n",
      "Epoch 0[3502/17270] Time:0.227, Train Loss:0.5190784931182861\n",
      "Epoch 0[3503/17270] Time:0.233, Train Loss:0.6271523237228394\n",
      "Epoch 0[3504/17270] Time:0.237, Train Loss:0.5030490159988403\n",
      "Epoch 0[3505/17270] Time:0.236, Train Loss:0.46824219822883606\n",
      "Epoch 0[3506/17270] Time:0.238, Train Loss:0.40020930767059326\n",
      "Epoch 0[3507/17270] Time:0.232, Train Loss:0.7055392265319824\n",
      "Epoch 0[3508/17270] Time:0.23, Train Loss:0.445929616689682\n",
      "Epoch 0[3509/17270] Time:0.25, Train Loss:0.3105478584766388\n",
      "Epoch 0[3510/17270] Time:0.233, Train Loss:0.5333686470985413\n",
      "Epoch 0[3511/17270] Time:0.228, Train Loss:0.5569764375686646\n",
      "Epoch 0[3512/17270] Time:0.234, Train Loss:0.6003110408782959\n",
      "Epoch 0[3513/17270] Time:0.227, Train Loss:0.8073883056640625\n",
      "Epoch 0[3514/17270] Time:0.24, Train Loss:0.5866938233375549\n",
      "Epoch 0[3515/17270] Time:0.24, Train Loss:0.6851522326469421\n",
      "Epoch 0[3516/17270] Time:0.248, Train Loss:0.523893415927887\n",
      "Epoch 0[3517/17270] Time:0.237, Train Loss:0.5433601140975952\n",
      "Epoch 0[3518/17270] Time:0.245, Train Loss:2.012148141860962\n",
      "Epoch 0[3519/17270] Time:0.225, Train Loss:0.6030052900314331\n",
      "Epoch 0[3520/17270] Time:0.231, Train Loss:1.4155197143554688\n",
      "Epoch 0[3521/17270] Time:0.231, Train Loss:0.45479702949523926\n",
      "Epoch 0[3522/17270] Time:0.23, Train Loss:0.4876365065574646\n",
      "Epoch 0[3523/17270] Time:0.241, Train Loss:1.3358635902404785\n",
      "Epoch 0[3524/17270] Time:0.241, Train Loss:0.7216676473617554\n",
      "Epoch 0[3525/17270] Time:0.232, Train Loss:0.9451345801353455\n",
      "Epoch 0[3526/17270] Time:0.224, Train Loss:0.6656225919723511\n",
      "Epoch 0[3527/17270] Time:0.238, Train Loss:0.6540701985359192\n",
      "Epoch 0[3528/17270] Time:0.235, Train Loss:0.5938525199890137\n",
      "Epoch 0[3529/17270] Time:0.231, Train Loss:0.6513211131095886\n",
      "Epoch 0[3530/17270] Time:0.234, Train Loss:0.9692974090576172\n",
      "Epoch 0[3531/17270] Time:0.223, Train Loss:0.40353137254714966\n",
      "Epoch 0[3532/17270] Time:0.245, Train Loss:0.9129052758216858\n",
      "Epoch 0[3533/17270] Time:0.23, Train Loss:0.773785412311554\n",
      "Epoch 0[3534/17270] Time:0.238, Train Loss:0.7180681228637695\n",
      "Epoch 0[3535/17270] Time:0.23, Train Loss:0.6835938692092896\n",
      "Epoch 0[3536/17270] Time:0.237, Train Loss:0.34729626774787903\n",
      "Epoch 0[3537/17270] Time:0.234, Train Loss:0.5490431189537048\n",
      "Epoch 0[3538/17270] Time:0.256, Train Loss:0.410517156124115\n",
      "Epoch 0[3539/17270] Time:0.234, Train Loss:0.4403945803642273\n",
      "Epoch 0[3540/17270] Time:0.23, Train Loss:0.6753299236297607\n",
      "Epoch 0[3541/17270] Time:0.236, Train Loss:0.7095668315887451\n",
      "Epoch 0[3542/17270] Time:0.246, Train Loss:0.4234618842601776\n",
      "Epoch 0[3543/17270] Time:0.224, Train Loss:0.5491811037063599\n",
      "Epoch 0[3544/17270] Time:0.235, Train Loss:0.7146580815315247\n",
      "Epoch 0[3545/17270] Time:0.236, Train Loss:0.7371913194656372\n",
      "Epoch 0[3546/17270] Time:0.24, Train Loss:0.3998674750328064\n",
      "Epoch 0[3547/17270] Time:0.225, Train Loss:0.5863563418388367\n",
      "Epoch 0[3548/17270] Time:0.239, Train Loss:1.0806914567947388\n",
      "Epoch 0[3549/17270] Time:0.247, Train Loss:0.4146289825439453\n",
      "Epoch 0[3550/17270] Time:0.239, Train Loss:0.7541926503181458\n",
      "Epoch 0[3551/17270] Time:0.228, Train Loss:0.7379686832427979\n",
      "Epoch 0[3552/17270] Time:0.238, Train Loss:0.6685839891433716\n",
      "Epoch 0[3553/17270] Time:0.233, Train Loss:0.9406600594520569\n",
      "Epoch 0[3554/17270] Time:0.233, Train Loss:0.7263597249984741\n",
      "Epoch 0[3555/17270] Time:0.226, Train Loss:0.7712208032608032\n",
      "Epoch 0[3556/17270] Time:0.241, Train Loss:0.6584827303886414\n",
      "Epoch 0[3557/17270] Time:0.232, Train Loss:0.8315959572792053\n",
      "Epoch 0[3558/17270] Time:0.237, Train Loss:1.0209529399871826\n",
      "Epoch 0[3559/17270] Time:0.23, Train Loss:0.7266852855682373\n",
      "Epoch 0[3560/17270] Time:0.229, Train Loss:0.5860067009925842\n",
      "Epoch 0[3561/17270] Time:0.245, Train Loss:0.621938943862915\n",
      "Epoch 0[3562/17270] Time:0.243, Train Loss:1.0231959819793701\n",
      "Epoch 0[3563/17270] Time:0.239, Train Loss:0.6641113758087158\n",
      "Epoch 0[3564/17270] Time:0.227, Train Loss:0.9732763767242432\n",
      "Epoch 0[3565/17270] Time:0.233, Train Loss:1.0695210695266724\n",
      "Epoch 0[3566/17270] Time:0.225, Train Loss:1.4244999885559082\n",
      "Epoch 0[3567/17270] Time:0.246, Train Loss:1.149734616279602\n",
      "Epoch 0[3568/17270] Time:0.232, Train Loss:0.7704257369041443\n",
      "Epoch 0[3569/17270] Time:0.24, Train Loss:0.8234575390815735\n",
      "Epoch 0[3570/17270] Time:0.237, Train Loss:0.7961949706077576\n",
      "Epoch 0[3571/17270] Time:0.243, Train Loss:0.7561793923377991\n",
      "Epoch 0[3572/17270] Time:0.234, Train Loss:0.8033193945884705\n",
      "Epoch 0[3573/17270] Time:0.234, Train Loss:1.0032261610031128\n",
      "Epoch 0[3574/17270] Time:0.236, Train Loss:0.5847241282463074\n",
      "Epoch 0[3575/17270] Time:0.229, Train Loss:0.7478014230728149\n",
      "Epoch 0[3576/17270] Time:0.236, Train Loss:0.6488211750984192\n",
      "Epoch 0[3577/17270] Time:0.241, Train Loss:0.611636221408844\n",
      "Epoch 0[3578/17270] Time:0.244, Train Loss:0.6298924088478088\n",
      "Epoch 0[3579/17270] Time:0.243, Train Loss:0.6760496497154236\n",
      "Epoch 0[3580/17270] Time:0.241, Train Loss:1.024351716041565\n",
      "Epoch 0[3581/17270] Time:0.237, Train Loss:0.6912481784820557\n",
      "Epoch 0[3582/17270] Time:0.24, Train Loss:0.9004106521606445\n",
      "Epoch 0[3583/17270] Time:0.24, Train Loss:0.7552819848060608\n",
      "Epoch 0[3584/17270] Time:0.239, Train Loss:0.7318891882896423\n",
      "Epoch 0[3585/17270] Time:0.233, Train Loss:0.7054873704910278\n",
      "Epoch 0[3586/17270] Time:0.245, Train Loss:0.7792174220085144\n",
      "Epoch 0[3587/17270] Time:0.23, Train Loss:1.2280058860778809\n",
      "Epoch 0[3588/17270] Time:0.242, Train Loss:0.6537411212921143\n",
      "Epoch 0[3589/17270] Time:0.235, Train Loss:0.6236428022384644\n",
      "Epoch 0[3590/17270] Time:0.235, Train Loss:0.7436543703079224\n",
      "Epoch 0[3591/17270] Time:0.226, Train Loss:0.6868626475334167\n",
      "Epoch 0[3592/17270] Time:0.231, Train Loss:0.5745252966880798\n",
      "Epoch 0[3593/17270] Time:0.236, Train Loss:0.8347131609916687\n",
      "Epoch 0[3594/17270] Time:0.238, Train Loss:0.6602702140808105\n",
      "Epoch 0[3595/17270] Time:0.224, Train Loss:0.6948965787887573\n",
      "Epoch 0[3596/17270] Time:0.241, Train Loss:0.6011987924575806\n",
      "Epoch 0[3597/17270] Time:0.221, Train Loss:0.6949669718742371\n",
      "Epoch 0[3598/17270] Time:0.232, Train Loss:0.3658443093299866\n",
      "Epoch 0[3599/17270] Time:0.228, Train Loss:0.9636666774749756\n",
      "Epoch 0[3600/17270] Time:0.231, Train Loss:0.8367111682891846\n",
      "Epoch 0[3601/17270] Time:0.227, Train Loss:1.1288642883300781\n",
      "Epoch 0[3602/17270] Time:0.239, Train Loss:0.7041895985603333\n",
      "Epoch 0[3603/17270] Time:0.25, Train Loss:0.6683531999588013\n",
      "Epoch 0[3604/17270] Time:0.225, Train Loss:0.71794593334198\n",
      "Epoch 0[3605/17270] Time:0.244, Train Loss:0.6341148614883423\n",
      "Epoch 0[3606/17270] Time:0.242, Train Loss:0.4511214792728424\n",
      "Epoch 0[3607/17270] Time:0.238, Train Loss:0.7269570231437683\n",
      "Epoch 0[3608/17270] Time:0.238, Train Loss:0.5304259657859802\n",
      "Epoch 0[3609/17270] Time:0.227, Train Loss:0.6440950036048889\n",
      "Epoch 0[3610/17270] Time:0.232, Train Loss:0.39489689469337463\n",
      "Epoch 0[3611/17270] Time:0.237, Train Loss:0.5589222311973572\n",
      "Epoch 0[3612/17270] Time:0.232, Train Loss:0.495511531829834\n",
      "Epoch 0[3613/17270] Time:0.232, Train Loss:0.4441569745540619\n",
      "Epoch 0[3614/17270] Time:0.237, Train Loss:0.6433416604995728\n",
      "Epoch 0[3615/17270] Time:0.235, Train Loss:0.8875478506088257\n",
      "Epoch 0[3616/17270] Time:0.238, Train Loss:0.9283701777458191\n",
      "Epoch 0[3617/17270] Time:0.239, Train Loss:0.6837103962898254\n",
      "Epoch 0[3618/17270] Time:0.232, Train Loss:0.6130738854408264\n",
      "Epoch 0[3619/17270] Time:0.228, Train Loss:0.5363835096359253\n",
      "Epoch 0[3620/17270] Time:0.251, Train Loss:0.47314369678497314\n",
      "Epoch 0[3621/17270] Time:0.239, Train Loss:0.419415146112442\n",
      "Epoch 0[3622/17270] Time:0.236, Train Loss:0.8096122741699219\n",
      "Epoch 0[3623/17270] Time:0.238, Train Loss:0.36909663677215576\n",
      "Epoch 0[3624/17270] Time:0.235, Train Loss:0.9657045006752014\n",
      "Epoch 0[3625/17270] Time:0.239, Train Loss:0.9683360457420349\n",
      "Epoch 0[3626/17270] Time:0.227, Train Loss:0.9458882808685303\n",
      "Epoch 0[3627/17270] Time:0.25, Train Loss:1.132622241973877\n",
      "Epoch 0[3628/17270] Time:0.24, Train Loss:0.47212669253349304\n",
      "Epoch 0[3629/17270] Time:0.239, Train Loss:0.8160827159881592\n",
      "Epoch 0[3630/17270] Time:0.216, Train Loss:0.8076617121696472\n",
      "Epoch 0[3631/17270] Time:0.234, Train Loss:1.1172969341278076\n",
      "Epoch 0[3632/17270] Time:0.244, Train Loss:0.6657631397247314\n",
      "Epoch 0[3633/17270] Time:0.24, Train Loss:0.514065146446228\n",
      "Epoch 0[3634/17270] Time:0.237, Train Loss:0.2996065318584442\n",
      "Epoch 0[3635/17270] Time:0.225, Train Loss:1.5378384590148926\n",
      "Epoch 0[3636/17270] Time:0.237, Train Loss:0.5192170739173889\n",
      "Epoch 0[3637/17270] Time:0.247, Train Loss:0.8064696788787842\n",
      "Epoch 0[3638/17270] Time:0.236, Train Loss:0.7266910672187805\n",
      "Epoch 0[3639/17270] Time:0.232, Train Loss:0.8662850260734558\n",
      "Epoch 0[3640/17270] Time:0.242, Train Loss:0.7836799621582031\n",
      "Epoch 0[3641/17270] Time:0.235, Train Loss:0.682977020740509\n",
      "Epoch 0[3642/17270] Time:0.232, Train Loss:0.4062858521938324\n",
      "Epoch 0[3643/17270] Time:0.225, Train Loss:0.63671875\n",
      "Epoch 0[3644/17270] Time:0.253, Train Loss:0.8831052184104919\n",
      "Epoch 0[3645/17270] Time:0.226, Train Loss:0.6289386749267578\n",
      "Epoch 0[3646/17270] Time:0.233, Train Loss:0.8886750340461731\n",
      "Epoch 0[3647/17270] Time:0.236, Train Loss:0.7813827991485596\n",
      "Epoch 0[3648/17270] Time:0.236, Train Loss:0.5978357791900635\n",
      "Epoch 0[3649/17270] Time:0.241, Train Loss:0.4886265695095062\n",
      "Epoch 0[3650/17270] Time:0.226, Train Loss:1.0246245861053467\n",
      "Epoch 0[3651/17270] Time:0.232, Train Loss:0.9156975150108337\n",
      "Epoch 0[3652/17270] Time:0.236, Train Loss:0.43928077816963196\n",
      "Epoch 0[3653/17270] Time:0.248, Train Loss:0.5065364241600037\n",
      "Epoch 0[3654/17270] Time:0.235, Train Loss:0.5923516750335693\n",
      "Epoch 0[3655/17270] Time:0.242, Train Loss:0.5081173777580261\n",
      "Epoch 0[3656/17270] Time:0.229, Train Loss:0.6598847508430481\n",
      "Epoch 0[3657/17270] Time:0.235, Train Loss:1.189967155456543\n",
      "Epoch 0[3658/17270] Time:0.228, Train Loss:0.6333665251731873\n",
      "Epoch 0[3659/17270] Time:0.246, Train Loss:0.36203402280807495\n",
      "Epoch 0[3660/17270] Time:0.239, Train Loss:0.7486485838890076\n",
      "Epoch 0[3661/17270] Time:0.235, Train Loss:0.787375271320343\n",
      "Epoch 0[3662/17270] Time:0.244, Train Loss:0.595906674861908\n",
      "Epoch 0[3663/17270] Time:0.239, Train Loss:0.6479114294052124\n",
      "Epoch 0[3664/17270] Time:0.237, Train Loss:0.6844158172607422\n",
      "Epoch 0[3665/17270] Time:0.24, Train Loss:0.6871549487113953\n",
      "Epoch 0[3666/17270] Time:0.234, Train Loss:0.48147720098495483\n",
      "Epoch 0[3667/17270] Time:0.238, Train Loss:0.4308479130268097\n",
      "Epoch 0[3668/17270] Time:0.248, Train Loss:0.6346492767333984\n",
      "Epoch 0[3669/17270] Time:0.238, Train Loss:0.4552018642425537\n",
      "Epoch 0[3670/17270] Time:0.236, Train Loss:0.5570803284645081\n",
      "Epoch 0[3671/17270] Time:0.232, Train Loss:0.5906869173049927\n",
      "Epoch 0[3672/17270] Time:0.254, Train Loss:0.5856119990348816\n",
      "Epoch 0[3673/17270] Time:0.244, Train Loss:0.5462725758552551\n",
      "Epoch 0[3674/17270] Time:0.241, Train Loss:0.627006471157074\n",
      "Epoch 0[3675/17270] Time:0.241, Train Loss:0.6097163558006287\n",
      "Epoch 0[3676/17270] Time:0.24, Train Loss:0.47694098949432373\n",
      "Epoch 0[3677/17270] Time:0.245, Train Loss:0.46986839175224304\n",
      "Epoch 0[3678/17270] Time:0.234, Train Loss:0.5875465869903564\n",
      "Epoch 0[3679/17270] Time:0.249, Train Loss:0.6155978441238403\n",
      "Epoch 0[3680/17270] Time:0.237, Train Loss:1.5096503496170044\n",
      "Epoch 0[3681/17270] Time:0.252, Train Loss:0.4686250388622284\n",
      "Epoch 0[3682/17270] Time:0.226, Train Loss:0.6319091320037842\n",
      "Epoch 0[3683/17270] Time:0.234, Train Loss:0.37950679659843445\n",
      "Epoch 0[3684/17270] Time:0.232, Train Loss:0.4752412438392639\n",
      "Epoch 0[3685/17270] Time:0.228, Train Loss:0.4153467118740082\n",
      "Epoch 0[3686/17270] Time:0.232, Train Loss:1.0994856357574463\n",
      "Epoch 0[3687/17270] Time:0.242, Train Loss:0.42580944299697876\n",
      "Epoch 0[3688/17270] Time:0.24, Train Loss:0.6889367699623108\n",
      "Epoch 0[3689/17270] Time:0.241, Train Loss:0.9364160895347595\n",
      "Epoch 0[3690/17270] Time:0.232, Train Loss:0.45874980092048645\n",
      "Epoch 0[3691/17270] Time:0.234, Train Loss:0.7739545106887817\n",
      "Epoch 0[3692/17270] Time:0.231, Train Loss:0.34176018834114075\n",
      "Epoch 0[3693/17270] Time:0.237, Train Loss:0.5265291929244995\n",
      "Epoch 0[3694/17270] Time:0.234, Train Loss:0.48295915126800537\n",
      "Epoch 0[3695/17270] Time:0.231, Train Loss:0.8482926487922668\n",
      "Epoch 0[3696/17270] Time:0.225, Train Loss:0.6557825803756714\n",
      "Epoch 0[3697/17270] Time:0.247, Train Loss:0.6757687926292419\n",
      "Epoch 0[3698/17270] Time:0.227, Train Loss:0.5264258980751038\n",
      "Epoch 0[3699/17270] Time:0.231, Train Loss:0.5860057473182678\n",
      "Epoch 0[3700/17270] Time:0.237, Train Loss:0.6672472953796387\n",
      "Epoch 0[3701/17270] Time:0.237, Train Loss:0.5746554732322693\n",
      "Epoch 0[3702/17270] Time:0.239, Train Loss:0.6489864587783813\n",
      "Epoch 0[3703/17270] Time:0.23, Train Loss:1.275940179824829\n",
      "Epoch 0[3704/17270] Time:0.24, Train Loss:0.4838235676288605\n",
      "Epoch 0[3705/17270] Time:0.23, Train Loss:0.5208892226219177\n",
      "Epoch 0[3706/17270] Time:0.24, Train Loss:0.4974953532218933\n",
      "Epoch 0[3707/17270] Time:0.237, Train Loss:0.5831608176231384\n",
      "Epoch 0[3708/17270] Time:0.235, Train Loss:0.9745742678642273\n",
      "Epoch 0[3709/17270] Time:0.24, Train Loss:0.7290701270103455\n",
      "Epoch 0[3710/17270] Time:0.241, Train Loss:0.4280669689178467\n",
      "Epoch 0[3711/17270] Time:0.227, Train Loss:0.5604789853096008\n",
      "Epoch 0[3712/17270] Time:0.23, Train Loss:0.6286026239395142\n",
      "Epoch 0[3713/17270] Time:0.239, Train Loss:0.657569408416748\n",
      "Epoch 0[3714/17270] Time:0.243, Train Loss:0.3904159963130951\n",
      "Epoch 0[3715/17270] Time:0.234, Train Loss:0.29429253935813904\n",
      "Epoch 0[3716/17270] Time:0.233, Train Loss:0.49493494629859924\n",
      "Epoch 0[3717/17270] Time:0.233, Train Loss:0.7684248089790344\n",
      "Epoch 0[3718/17270] Time:0.231, Train Loss:0.776687502861023\n",
      "Epoch 0[3719/17270] Time:0.234, Train Loss:0.7301504015922546\n",
      "Epoch 0[3720/17270] Time:0.235, Train Loss:0.5167078375816345\n",
      "Epoch 0[3721/17270] Time:0.247, Train Loss:0.4791148602962494\n",
      "Epoch 0[3722/17270] Time:0.237, Train Loss:0.5879867672920227\n",
      "Epoch 0[3723/17270] Time:0.235, Train Loss:1.157189130783081\n",
      "Epoch 0[3724/17270] Time:0.234, Train Loss:0.7218081951141357\n",
      "Epoch 0[3725/17270] Time:0.237, Train Loss:0.4311683475971222\n",
      "Epoch 0[3726/17270] Time:0.235, Train Loss:0.48042216897010803\n",
      "Epoch 0[3727/17270] Time:0.234, Train Loss:0.46227824687957764\n",
      "Epoch 0[3728/17270] Time:0.234, Train Loss:0.7505618333816528\n",
      "Epoch 0[3729/17270] Time:0.25, Train Loss:0.4790455996990204\n",
      "Epoch 0[3730/17270] Time:0.227, Train Loss:0.7129052877426147\n",
      "Epoch 0[3731/17270] Time:0.24, Train Loss:0.5898001194000244\n",
      "Epoch 0[3732/17270] Time:0.237, Train Loss:2.025158405303955\n",
      "Epoch 0[3733/17270] Time:0.242, Train Loss:1.35996413230896\n",
      "Epoch 0[3734/17270] Time:0.25, Train Loss:0.4455663859844208\n",
      "Epoch 0[3735/17270] Time:0.236, Train Loss:0.37912338972091675\n",
      "Epoch 0[3736/17270] Time:0.236, Train Loss:0.7241308093070984\n",
      "Epoch 0[3737/17270] Time:0.234, Train Loss:0.659979522228241\n",
      "Epoch 0[3738/17270] Time:0.221, Train Loss:0.9036554098129272\n",
      "Epoch 0[3739/17270] Time:0.23, Train Loss:0.49607664346694946\n",
      "Epoch 0[3740/17270] Time:0.232, Train Loss:0.7081067562103271\n",
      "Epoch 0[3741/17270] Time:0.231, Train Loss:0.5139676332473755\n",
      "Epoch 0[3742/17270] Time:0.232, Train Loss:0.7027609348297119\n",
      "Epoch 0[3743/17270] Time:0.234, Train Loss:1.3530142307281494\n",
      "Epoch 0[3744/17270] Time:0.229, Train Loss:0.6400813460350037\n",
      "Epoch 0[3745/17270] Time:0.226, Train Loss:0.4656340777873993\n",
      "Epoch 0[3746/17270] Time:0.239, Train Loss:0.5685860514640808\n",
      "Epoch 0[3747/17270] Time:0.229, Train Loss:0.8623272776603699\n",
      "Epoch 0[3748/17270] Time:0.238, Train Loss:0.48123422265052795\n",
      "Epoch 0[3749/17270] Time:0.23, Train Loss:0.6524792313575745\n",
      "Epoch 0[3750/17270] Time:0.244, Train Loss:0.4822142720222473\n",
      "Epoch 0[3751/17270] Time:0.228, Train Loss:0.7279477119445801\n",
      "Epoch 0[3752/17270] Time:0.252, Train Loss:0.6599259972572327\n",
      "Epoch 0[3753/17270] Time:0.243, Train Loss:0.5646750926971436\n",
      "Epoch 0[3754/17270] Time:0.229, Train Loss:0.6584227085113525\n",
      "Epoch 0[3755/17270] Time:0.234, Train Loss:0.5591197609901428\n",
      "Epoch 0[3756/17270] Time:0.226, Train Loss:0.5156260132789612\n",
      "Epoch 0[3757/17270] Time:0.234, Train Loss:0.5962619781494141\n",
      "Epoch 0[3758/17270] Time:0.232, Train Loss:0.8717113137245178\n",
      "Epoch 0[3759/17270] Time:0.239, Train Loss:1.2180002927780151\n",
      "Epoch 0[3760/17270] Time:0.233, Train Loss:0.5768339037895203\n",
      "Epoch 0[3761/17270] Time:0.234, Train Loss:0.4986600875854492\n",
      "Epoch 0[3762/17270] Time:0.233, Train Loss:0.6163105964660645\n",
      "Epoch 0[3763/17270] Time:0.234, Train Loss:0.3017197549343109\n",
      "Epoch 0[3764/17270] Time:0.232, Train Loss:0.6517229080200195\n",
      "Epoch 0[3765/17270] Time:0.235, Train Loss:0.4801086187362671\n",
      "Epoch 0[3766/17270] Time:0.233, Train Loss:0.43554574251174927\n",
      "Epoch 0[3767/17270] Time:0.226, Train Loss:0.7540354132652283\n",
      "Epoch 0[3768/17270] Time:0.236, Train Loss:0.5378316044807434\n",
      "Epoch 0[3769/17270] Time:0.232, Train Loss:0.5013312101364136\n",
      "Epoch 0[3770/17270] Time:0.23, Train Loss:0.5043092370033264\n",
      "Epoch 0[3771/17270] Time:0.251, Train Loss:0.4212719798088074\n",
      "Epoch 0[3772/17270] Time:0.225, Train Loss:1.1571584939956665\n",
      "Epoch 0[3773/17270] Time:0.228, Train Loss:0.46074679493904114\n",
      "Epoch 0[3774/17270] Time:0.229, Train Loss:0.505755603313446\n",
      "Epoch 0[3775/17270] Time:0.229, Train Loss:1.0158272981643677\n",
      "Epoch 0[3776/17270] Time:0.228, Train Loss:0.6097345948219299\n",
      "Epoch 0[3777/17270] Time:0.234, Train Loss:0.6432861685752869\n",
      "Epoch 0[3778/17270] Time:0.236, Train Loss:0.5661993026733398\n",
      "Epoch 0[3779/17270] Time:0.234, Train Loss:0.5565796494483948\n",
      "Epoch 0[3780/17270] Time:0.243, Train Loss:0.5284518003463745\n",
      "Epoch 0[3781/17270] Time:0.222, Train Loss:0.5795198678970337\n",
      "Epoch 0[3782/17270] Time:0.245, Train Loss:0.5938347578048706\n",
      "Epoch 0[3783/17270] Time:0.222, Train Loss:0.5847482681274414\n",
      "Epoch 0[3784/17270] Time:0.245, Train Loss:0.8312727212905884\n",
      "Epoch 0[3785/17270] Time:0.232, Train Loss:0.694133460521698\n",
      "Epoch 0[3786/17270] Time:0.239, Train Loss:0.42618879675865173\n",
      "Epoch 0[3787/17270] Time:0.235, Train Loss:0.45939090847969055\n",
      "Epoch 0[3788/17270] Time:0.235, Train Loss:1.887546420097351\n",
      "Epoch 0[3789/17270] Time:0.234, Train Loss:1.3533896207809448\n",
      "Epoch 0[3790/17270] Time:0.235, Train Loss:0.969108521938324\n",
      "Epoch 0[3791/17270] Time:0.247, Train Loss:0.7396925687789917\n",
      "Epoch 0[3792/17270] Time:0.215, Train Loss:0.750178337097168\n",
      "Epoch 0[3793/17270] Time:0.242, Train Loss:0.5091676712036133\n",
      "Epoch 0[3794/17270] Time:0.235, Train Loss:0.4287099838256836\n",
      "Epoch 0[3795/17270] Time:0.235, Train Loss:0.5810955166816711\n",
      "Epoch 0[3796/17270] Time:0.239, Train Loss:0.9899287223815918\n",
      "Epoch 0[3797/17270] Time:0.235, Train Loss:0.5014292001724243\n",
      "Epoch 0[3798/17270] Time:0.236, Train Loss:0.9758865833282471\n",
      "Epoch 0[3799/17270] Time:0.238, Train Loss:0.7783989906311035\n",
      "Epoch 0[3800/17270] Time:0.238, Train Loss:0.7317036390304565\n",
      "Epoch 0[3801/17270] Time:0.229, Train Loss:0.5922324657440186\n",
      "Epoch 0[3802/17270] Time:0.235, Train Loss:0.3588542640209198\n",
      "Epoch 0[3803/17270] Time:0.231, Train Loss:0.5561204552650452\n",
      "Epoch 0[3804/17270] Time:0.231, Train Loss:1.21250319480896\n",
      "Epoch 0[3805/17270] Time:0.238, Train Loss:0.5293111205101013\n",
      "Epoch 0[3806/17270] Time:0.231, Train Loss:0.7425862550735474\n",
      "Epoch 0[3807/17270] Time:0.228, Train Loss:0.8167771100997925\n",
      "Epoch 0[3808/17270] Time:0.227, Train Loss:0.6575145721435547\n",
      "Epoch 0[3809/17270] Time:0.232, Train Loss:0.51582932472229\n",
      "Epoch 0[3810/17270] Time:0.24, Train Loss:0.9228116869926453\n",
      "Epoch 0[3811/17270] Time:0.236, Train Loss:0.5663219690322876\n",
      "Epoch 0[3812/17270] Time:0.239, Train Loss:1.2698642015457153\n",
      "Epoch 0[3813/17270] Time:0.228, Train Loss:0.6184807419776917\n",
      "Epoch 0[3814/17270] Time:0.232, Train Loss:1.0478190183639526\n",
      "Epoch 0[3815/17270] Time:0.237, Train Loss:0.5721887350082397\n",
      "Epoch 0[3816/17270] Time:0.235, Train Loss:0.605143666267395\n",
      "Epoch 0[3817/17270] Time:0.228, Train Loss:0.6290569305419922\n",
      "Epoch 0[3818/17270] Time:0.231, Train Loss:0.48935404419898987\n",
      "Epoch 0[3819/17270] Time:0.227, Train Loss:0.937811017036438\n",
      "Epoch 0[3820/17270] Time:0.23, Train Loss:0.5934101343154907\n",
      "Epoch 0[3821/17270] Time:0.226, Train Loss:0.45739468932151794\n",
      "Epoch 0[3822/17270] Time:0.24, Train Loss:0.424687922000885\n",
      "Epoch 0[3823/17270] Time:0.241, Train Loss:0.4312968850135803\n",
      "Epoch 0[3824/17270] Time:0.235, Train Loss:0.5201526880264282\n",
      "Epoch 0[3825/17270] Time:0.251, Train Loss:0.49115046858787537\n",
      "Epoch 0[3826/17270] Time:0.236, Train Loss:0.644993245601654\n",
      "Epoch 0[3827/17270] Time:0.235, Train Loss:0.5184011459350586\n",
      "Epoch 0[3828/17270] Time:0.232, Train Loss:0.5981771945953369\n",
      "Epoch 0[3829/17270] Time:0.237, Train Loss:0.48558810353279114\n",
      "Epoch 0[3830/17270] Time:0.227, Train Loss:0.4386531710624695\n",
      "Epoch 0[3831/17270] Time:0.23, Train Loss:0.3646768629550934\n",
      "Epoch 0[3832/17270] Time:0.248, Train Loss:1.0782314538955688\n",
      "Epoch 0[3833/17270] Time:0.235, Train Loss:0.5373269319534302\n",
      "Epoch 0[3834/17270] Time:0.251, Train Loss:0.522732675075531\n",
      "Epoch 0[3835/17270] Time:0.222, Train Loss:0.9247856736183167\n",
      "Epoch 0[3836/17270] Time:0.245, Train Loss:0.5891211628913879\n",
      "Epoch 0[3837/17270] Time:0.233, Train Loss:0.5866198539733887\n",
      "Epoch 0[3838/17270] Time:0.237, Train Loss:0.7560312151908875\n",
      "Epoch 0[3839/17270] Time:0.222, Train Loss:0.7265693545341492\n",
      "Epoch 0[3840/17270] Time:0.256, Train Loss:1.4217774868011475\n",
      "Epoch 0[3841/17270] Time:0.228, Train Loss:0.6729474067687988\n",
      "Epoch 0[3842/17270] Time:0.237, Train Loss:0.4714270234107971\n",
      "Epoch 0[3843/17270] Time:0.221, Train Loss:0.6009457111358643\n",
      "Epoch 0[3844/17270] Time:0.236, Train Loss:0.7117290496826172\n",
      "Epoch 0[3845/17270] Time:0.239, Train Loss:1.141554355621338\n",
      "Epoch 0[3846/17270] Time:0.228, Train Loss:0.6780858635902405\n",
      "Epoch 0[3847/17270] Time:0.237, Train Loss:0.4774962067604065\n",
      "Epoch 0[3848/17270] Time:0.232, Train Loss:0.6083268523216248\n",
      "Epoch 0[3849/17270] Time:0.232, Train Loss:0.44107064604759216\n",
      "Epoch 0[3850/17270] Time:0.229, Train Loss:0.7561603784561157\n",
      "Epoch 0[3851/17270] Time:0.236, Train Loss:0.5406442284584045\n",
      "Epoch 0[3852/17270] Time:0.239, Train Loss:0.4145295321941376\n",
      "Epoch 0[3853/17270] Time:0.242, Train Loss:0.3837551176548004\n",
      "Epoch 0[3854/17270] Time:0.221, Train Loss:0.7295289039611816\n",
      "Epoch 0[3855/17270] Time:0.232, Train Loss:1.2837228775024414\n",
      "Epoch 0[3856/17270] Time:0.228, Train Loss:0.5798464417457581\n",
      "Epoch 0[3857/17270] Time:0.256, Train Loss:0.5051172971725464\n",
      "Epoch 0[3858/17270] Time:0.23, Train Loss:0.7743356227874756\n",
      "Epoch 0[3859/17270] Time:0.239, Train Loss:0.35944655537605286\n",
      "Epoch 0[3860/17270] Time:0.238, Train Loss:0.3987788259983063\n",
      "Epoch 0[3861/17270] Time:0.239, Train Loss:0.8877946734428406\n",
      "Epoch 0[3862/17270] Time:0.234, Train Loss:0.4336136281490326\n",
      "Epoch 0[3863/17270] Time:0.238, Train Loss:0.3257571756839752\n",
      "Epoch 0[3864/17270] Time:0.228, Train Loss:0.7413691282272339\n",
      "Epoch 0[3865/17270] Time:0.229, Train Loss:0.5220239162445068\n",
      "Epoch 0[3866/17270] Time:0.241, Train Loss:0.6439117193222046\n",
      "Epoch 0[3867/17270] Time:0.247, Train Loss:0.47722089290618896\n",
      "Epoch 0[3868/17270] Time:0.235, Train Loss:0.5344328284263611\n",
      "Epoch 0[3869/17270] Time:0.235, Train Loss:0.6409720182418823\n",
      "Epoch 0[3870/17270] Time:0.233, Train Loss:0.470829039812088\n",
      "Epoch 0[3871/17270] Time:0.234, Train Loss:0.5694608688354492\n",
      "Epoch 0[3872/17270] Time:0.231, Train Loss:0.47973817586898804\n",
      "Epoch 0[3873/17270] Time:0.222, Train Loss:0.6749230027198792\n",
      "Epoch 0[3874/17270] Time:0.254, Train Loss:0.9282225370407104\n",
      "Epoch 0[3875/17270] Time:0.229, Train Loss:0.4974977672100067\n",
      "Epoch 0[3876/17270] Time:0.224, Train Loss:0.7414758205413818\n",
      "Epoch 0[3877/17270] Time:0.232, Train Loss:1.0137367248535156\n",
      "Epoch 0[3878/17270] Time:0.24, Train Loss:0.2641301453113556\n",
      "Epoch 0[3879/17270] Time:0.226, Train Loss:0.5788984298706055\n",
      "Epoch 0[3880/17270] Time:0.234, Train Loss:0.44631463289260864\n",
      "Epoch 0[3881/17270] Time:0.235, Train Loss:0.8074373006820679\n",
      "Epoch 0[3882/17270] Time:0.238, Train Loss:0.5155162215232849\n",
      "Epoch 0[3883/17270] Time:0.237, Train Loss:0.6317175030708313\n",
      "Epoch 0[3884/17270] Time:0.227, Train Loss:0.4274004399776459\n",
      "Epoch 0[3885/17270] Time:0.243, Train Loss:0.6781027913093567\n",
      "Epoch 0[3886/17270] Time:0.234, Train Loss:0.9442977905273438\n",
      "Epoch 0[3887/17270] Time:0.232, Train Loss:0.8443937301635742\n",
      "Epoch 0[3888/17270] Time:0.232, Train Loss:0.4392692446708679\n",
      "Epoch 0[3889/17270] Time:0.238, Train Loss:0.39443260431289673\n",
      "Epoch 0[3890/17270] Time:0.238, Train Loss:0.5639878511428833\n",
      "Epoch 0[3891/17270] Time:0.232, Train Loss:0.5729473829269409\n",
      "Epoch 0[3892/17270] Time:0.23, Train Loss:0.4674217104911804\n",
      "Epoch 0[3893/17270] Time:0.246, Train Loss:0.5442780256271362\n",
      "Epoch 0[3894/17270] Time:0.235, Train Loss:0.9283658266067505\n",
      "Epoch 0[3895/17270] Time:0.227, Train Loss:1.1594500541687012\n",
      "Epoch 0[3896/17270] Time:0.244, Train Loss:1.2814607620239258\n",
      "Epoch 0[3897/17270] Time:0.236, Train Loss:0.5514171123504639\n",
      "Epoch 0[3898/17270] Time:0.237, Train Loss:1.4525563716888428\n",
      "Epoch 0[3899/17270] Time:0.235, Train Loss:0.47741398215293884\n",
      "Epoch 0[3900/17270] Time:0.234, Train Loss:0.4985663890838623\n",
      "Epoch 0[3901/17270] Time:0.226, Train Loss:0.8244414329528809\n",
      "Epoch 0[3902/17270] Time:0.231, Train Loss:0.8656021356582642\n",
      "Epoch 0[3903/17270] Time:0.247, Train Loss:0.47070062160491943\n",
      "Epoch 0[3904/17270] Time:0.226, Train Loss:0.5591557025909424\n",
      "Epoch 0[3905/17270] Time:0.23, Train Loss:0.5191783308982849\n",
      "Epoch 0[3906/17270] Time:0.23, Train Loss:0.5970024466514587\n",
      "Epoch 0[3907/17270] Time:0.234, Train Loss:0.7483346462249756\n",
      "Epoch 0[3908/17270] Time:0.226, Train Loss:0.411225289106369\n",
      "Epoch 0[3909/17270] Time:0.231, Train Loss:0.9078218340873718\n",
      "Epoch 0[3910/17270] Time:0.23, Train Loss:0.4958891272544861\n",
      "Epoch 0[3911/17270] Time:0.242, Train Loss:0.40211716294288635\n",
      "Epoch 0[3912/17270] Time:0.233, Train Loss:0.47878941893577576\n",
      "Epoch 0[3913/17270] Time:0.229, Train Loss:0.6131078600883484\n",
      "Epoch 0[3914/17270] Time:0.236, Train Loss:0.7332290410995483\n",
      "Epoch 0[3915/17270] Time:0.23, Train Loss:0.9013475775718689\n",
      "Epoch 0[3916/17270] Time:0.236, Train Loss:1.2170113325119019\n",
      "Epoch 0[3917/17270] Time:0.244, Train Loss:0.6282283067703247\n",
      "Epoch 0[3918/17270] Time:0.227, Train Loss:0.4003395736217499\n",
      "Epoch 0[3919/17270] Time:0.236, Train Loss:0.45524153113365173\n",
      "Epoch 0[3920/17270] Time:0.223, Train Loss:0.7630555629730225\n",
      "Epoch 0[3921/17270] Time:0.246, Train Loss:0.48186206817626953\n",
      "Epoch 0[3922/17270] Time:0.238, Train Loss:0.6428220272064209\n",
      "Epoch 0[3923/17270] Time:0.229, Train Loss:0.6851179599761963\n",
      "Epoch 0[3924/17270] Time:0.24, Train Loss:0.4610460102558136\n",
      "Epoch 0[3925/17270] Time:0.231, Train Loss:0.5598945617675781\n",
      "Epoch 0[3926/17270] Time:0.226, Train Loss:0.4618574380874634\n",
      "Epoch 0[3927/17270] Time:0.24, Train Loss:0.5975876450538635\n",
      "Epoch 0[3928/17270] Time:0.231, Train Loss:0.5558037161827087\n",
      "Epoch 0[3929/17270] Time:0.229, Train Loss:0.6200052499771118\n",
      "Epoch 0[3930/17270] Time:0.246, Train Loss:0.5750452280044556\n",
      "Epoch 0[3931/17270] Time:0.242, Train Loss:0.8697580099105835\n",
      "Epoch 0[3932/17270] Time:0.226, Train Loss:1.3731300830841064\n",
      "Epoch 0[3933/17270] Time:0.238, Train Loss:0.5141479969024658\n",
      "Epoch 0[3934/17270] Time:0.226, Train Loss:0.42383432388305664\n",
      "Epoch 0[3935/17270] Time:0.223, Train Loss:0.838598906993866\n",
      "Epoch 0[3936/17270] Time:0.241, Train Loss:0.28593096137046814\n",
      "Epoch 0[3937/17270] Time:0.232, Train Loss:1.2745120525360107\n",
      "Epoch 0[3938/17270] Time:0.238, Train Loss:0.93270343542099\n",
      "Epoch 0[3939/17270] Time:0.235, Train Loss:1.2948095798492432\n",
      "Epoch 0[3940/17270] Time:0.235, Train Loss:0.9030528664588928\n",
      "Epoch 0[3941/17270] Time:0.237, Train Loss:1.3359501361846924\n",
      "Epoch 0[3942/17270] Time:0.232, Train Loss:0.5725780725479126\n",
      "Epoch 0[3943/17270] Time:0.245, Train Loss:0.722004234790802\n",
      "Epoch 0[3944/17270] Time:0.234, Train Loss:0.5805889964103699\n",
      "Epoch 0[3945/17270] Time:0.233, Train Loss:0.6455371975898743\n",
      "Epoch 0[3946/17270] Time:0.236, Train Loss:0.40676361322402954\n",
      "Epoch 0[3947/17270] Time:0.233, Train Loss:0.7015511393547058\n",
      "Epoch 0[3948/17270] Time:0.236, Train Loss:0.790710985660553\n",
      "Epoch 0[3949/17270] Time:0.236, Train Loss:0.5784713625907898\n",
      "Epoch 0[3950/17270] Time:0.236, Train Loss:0.37043797969818115\n",
      "Epoch 0[3951/17270] Time:0.239, Train Loss:0.4786228835582733\n",
      "Epoch 0[3952/17270] Time:0.232, Train Loss:0.6624272465705872\n",
      "Epoch 0[3953/17270] Time:0.248, Train Loss:0.39584195613861084\n",
      "Epoch 0[3954/17270] Time:0.224, Train Loss:0.599129855632782\n",
      "Epoch 0[3955/17270] Time:0.237, Train Loss:0.7209610342979431\n",
      "Epoch 0[3956/17270] Time:0.243, Train Loss:0.5848528146743774\n",
      "Epoch 0[3957/17270] Time:0.226, Train Loss:0.6202075481414795\n",
      "Epoch 0[3958/17270] Time:0.243, Train Loss:1.0035589933395386\n",
      "Epoch 0[3959/17270] Time:0.244, Train Loss:0.3935677111148834\n",
      "Epoch 0[3960/17270] Time:0.234, Train Loss:0.564087986946106\n",
      "Epoch 0[3961/17270] Time:0.225, Train Loss:0.4791446924209595\n",
      "Epoch 0[3962/17270] Time:0.245, Train Loss:0.8212285041809082\n",
      "Epoch 0[3963/17270] Time:0.234, Train Loss:0.5106063485145569\n",
      "Epoch 0[3964/17270] Time:0.231, Train Loss:0.7976517677307129\n",
      "Epoch 0[3965/17270] Time:0.235, Train Loss:1.236158013343811\n",
      "Epoch 0[3966/17270] Time:0.232, Train Loss:0.47679227590560913\n",
      "Epoch 0[3967/17270] Time:0.232, Train Loss:0.862652599811554\n",
      "Epoch 0[3968/17270] Time:0.233, Train Loss:0.7871423363685608\n",
      "Epoch 0[3969/17270] Time:0.233, Train Loss:0.5157078504562378\n",
      "Epoch 0[3970/17270] Time:0.233, Train Loss:0.7197851538658142\n",
      "Epoch 0[3971/17270] Time:0.24, Train Loss:0.47061479091644287\n",
      "Epoch 0[3972/17270] Time:0.237, Train Loss:0.5072488784790039\n",
      "Epoch 0[3973/17270] Time:0.237, Train Loss:0.7614523768424988\n",
      "Epoch 0[3974/17270] Time:0.229, Train Loss:0.6957222819328308\n",
      "Epoch 0[3975/17270] Time:0.237, Train Loss:0.6527013778686523\n",
      "Epoch 0[3976/17270] Time:0.233, Train Loss:0.6615467667579651\n",
      "Epoch 0[3977/17270] Time:0.236, Train Loss:1.0933241844177246\n",
      "Epoch 0[3978/17270] Time:0.234, Train Loss:0.46351659297943115\n",
      "Epoch 0[3979/17270] Time:0.225, Train Loss:0.41047871112823486\n",
      "Epoch 0[3980/17270] Time:0.237, Train Loss:0.6394103765487671\n",
      "Epoch 0[3981/17270] Time:0.232, Train Loss:1.1436790227890015\n",
      "Epoch 0[3982/17270] Time:0.232, Train Loss:0.589662492275238\n",
      "Epoch 0[3983/17270] Time:0.23, Train Loss:0.38567468523979187\n",
      "Epoch 0[3984/17270] Time:0.232, Train Loss:0.4310601055622101\n",
      "Epoch 0[3985/17270] Time:0.236, Train Loss:0.465928852558136\n",
      "Epoch 0[3986/17270] Time:0.24, Train Loss:0.8617703318595886\n",
      "Epoch 0[3987/17270] Time:0.229, Train Loss:1.0829718112945557\n",
      "Epoch 0[3988/17270] Time:0.235, Train Loss:1.5358469486236572\n",
      "Epoch 0[3989/17270] Time:0.23, Train Loss:1.2651138305664062\n",
      "Epoch 0[3990/17270] Time:0.228, Train Loss:1.0560648441314697\n",
      "Epoch 0[3991/17270] Time:0.238, Train Loss:0.695498526096344\n",
      "Epoch 0[3992/17270] Time:0.232, Train Loss:0.47257667779922485\n",
      "Epoch 0[3993/17270] Time:0.238, Train Loss:0.4072330892086029\n",
      "Epoch 0[3994/17270] Time:0.235, Train Loss:0.7911158204078674\n",
      "Epoch 0[3995/17270] Time:0.236, Train Loss:0.4993176758289337\n",
      "Epoch 0[3996/17270] Time:0.233, Train Loss:0.5052441358566284\n",
      "Epoch 0[3997/17270] Time:0.238, Train Loss:1.0846951007843018\n",
      "Epoch 0[3998/17270] Time:0.23, Train Loss:0.5499374866485596\n",
      "Epoch 0[3999/17270] Time:0.234, Train Loss:0.6427140235900879\n",
      "Epoch 0[4000/17270] Time:0.234, Train Loss:0.3402666449546814\n",
      "Epoch 0[4001/17270] Time:0.229, Train Loss:0.5550354719161987\n",
      "Epoch 0[4002/17270] Time:0.231, Train Loss:0.8624038696289062\n",
      "Epoch 0[4003/17270] Time:0.229, Train Loss:0.8370745778083801\n",
      "Epoch 0[4004/17270] Time:0.25, Train Loss:0.5095424652099609\n",
      "Epoch 0[4005/17270] Time:0.229, Train Loss:0.6803408265113831\n",
      "Epoch 0[4006/17270] Time:0.243, Train Loss:0.4728262424468994\n",
      "Epoch 0[4007/17270] Time:0.229, Train Loss:0.9997057914733887\n",
      "Epoch 0[4008/17270] Time:0.243, Train Loss:0.5462045669555664\n",
      "Epoch 0[4009/17270] Time:0.239, Train Loss:0.4840356707572937\n",
      "Epoch 0[4010/17270] Time:0.24, Train Loss:0.6574397683143616\n",
      "Epoch 0[4011/17270] Time:0.235, Train Loss:0.5645566582679749\n",
      "Epoch 0[4012/17270] Time:0.239, Train Loss:0.5288739800453186\n",
      "Epoch 0[4013/17270] Time:0.238, Train Loss:0.8556183576583862\n",
      "Epoch 0[4014/17270] Time:0.242, Train Loss:0.9775390625\n",
      "Epoch 0[4015/17270] Time:0.227, Train Loss:0.8628579378128052\n",
      "Epoch 0[4016/17270] Time:0.231, Train Loss:0.5259588956832886\n",
      "Epoch 0[4017/17270] Time:0.241, Train Loss:0.6271103620529175\n",
      "Epoch 0[4018/17270] Time:0.226, Train Loss:0.5856821537017822\n",
      "Epoch 0[4019/17270] Time:0.237, Train Loss:0.4090266823768616\n",
      "Epoch 0[4020/17270] Time:0.223, Train Loss:0.8245866894721985\n",
      "Epoch 0[4021/17270] Time:0.252, Train Loss:0.6485720276832581\n",
      "Epoch 0[4022/17270] Time:0.238, Train Loss:0.629520833492279\n",
      "Epoch 0[4023/17270] Time:0.236, Train Loss:0.5657272934913635\n",
      "Epoch 0[4024/17270] Time:0.235, Train Loss:0.6953945159912109\n",
      "Epoch 0[4025/17270] Time:0.226, Train Loss:0.7408649325370789\n",
      "Epoch 0[4026/17270] Time:0.233, Train Loss:0.6033791899681091\n",
      "Epoch 0[4027/17270] Time:0.232, Train Loss:0.5132461190223694\n",
      "Epoch 0[4028/17270] Time:0.232, Train Loss:1.8166494369506836\n",
      "Epoch 0[4029/17270] Time:0.233, Train Loss:0.547332227230072\n",
      "Epoch 0[4030/17270] Time:0.222, Train Loss:0.5306580066680908\n",
      "Epoch 0[4031/17270] Time:0.243, Train Loss:0.5435183048248291\n",
      "Epoch 0[4032/17270] Time:0.223, Train Loss:0.7078726291656494\n",
      "Epoch 0[4033/17270] Time:0.228, Train Loss:0.6493170857429504\n",
      "Epoch 0[4034/17270] Time:0.247, Train Loss:0.5971513390541077\n",
      "Epoch 0[4035/17270] Time:0.231, Train Loss:0.4058379530906677\n",
      "Epoch 0[4036/17270] Time:0.221, Train Loss:0.6156695485115051\n",
      "Epoch 0[4037/17270] Time:0.234, Train Loss:0.4277399182319641\n",
      "Epoch 0[4038/17270] Time:0.226, Train Loss:0.4142880439758301\n",
      "Epoch 0[4039/17270] Time:0.245, Train Loss:0.4803164005279541\n",
      "Epoch 0[4040/17270] Time:0.239, Train Loss:0.7308475971221924\n",
      "Epoch 0[4041/17270] Time:0.227, Train Loss:0.6768167018890381\n",
      "Epoch 0[4042/17270] Time:0.235, Train Loss:0.5256872773170471\n",
      "Epoch 0[4043/17270] Time:0.251, Train Loss:0.5863890051841736\n",
      "Epoch 0[4044/17270] Time:0.236, Train Loss:0.6081466674804688\n",
      "Epoch 0[4045/17270] Time:0.232, Train Loss:0.427032470703125\n",
      "Epoch 0[4046/17270] Time:0.226, Train Loss:0.7617166638374329\n",
      "Epoch 0[4047/17270] Time:0.229, Train Loss:0.7692198753356934\n",
      "Epoch 0[4048/17270] Time:0.233, Train Loss:0.969512939453125\n",
      "Epoch 0[4049/17270] Time:0.231, Train Loss:0.5164317488670349\n",
      "Epoch 0[4050/17270] Time:0.234, Train Loss:0.7417087554931641\n",
      "Epoch 0[4051/17270] Time:0.23, Train Loss:0.575003981590271\n",
      "Epoch 0[4052/17270] Time:0.237, Train Loss:1.419811487197876\n",
      "Epoch 0[4053/17270] Time:0.245, Train Loss:0.7322489619255066\n",
      "Epoch 0[4054/17270] Time:0.233, Train Loss:0.6010833978652954\n",
      "Epoch 0[4055/17270] Time:0.24, Train Loss:0.6166431307792664\n",
      "Epoch 0[4056/17270] Time:0.24, Train Loss:1.240871548652649\n",
      "Epoch 0[4057/17270] Time:0.236, Train Loss:0.5858118534088135\n",
      "Epoch 0[4058/17270] Time:0.237, Train Loss:0.6998015642166138\n",
      "Epoch 0[4059/17270] Time:0.233, Train Loss:0.8718768358230591\n",
      "Epoch 0[4060/17270] Time:0.235, Train Loss:0.6515361070632935\n",
      "Epoch 0[4061/17270] Time:0.231, Train Loss:0.571677565574646\n",
      "Epoch 0[4062/17270] Time:0.226, Train Loss:0.658481776714325\n",
      "Epoch 0[4063/17270] Time:0.226, Train Loss:0.7410680651664734\n",
      "Epoch 0[4064/17270] Time:0.229, Train Loss:0.4063328504562378\n",
      "Epoch 0[4065/17270] Time:0.232, Train Loss:0.4188883602619171\n",
      "Epoch 0[4066/17270] Time:0.24, Train Loss:0.56239253282547\n",
      "Epoch 0[4067/17270] Time:0.229, Train Loss:0.6478168368339539\n",
      "Epoch 0[4068/17270] Time:0.237, Train Loss:0.5953435897827148\n",
      "Epoch 0[4069/17270] Time:0.239, Train Loss:0.6103242039680481\n",
      "Epoch 0[4070/17270] Time:0.231, Train Loss:0.7514795064926147\n",
      "Epoch 0[4071/17270] Time:0.253, Train Loss:0.380420982837677\n",
      "Epoch 0[4072/17270] Time:0.231, Train Loss:0.6646206378936768\n",
      "Epoch 0[4073/17270] Time:0.222, Train Loss:0.622269868850708\n",
      "Epoch 0[4074/17270] Time:0.233, Train Loss:0.518770158290863\n",
      "Epoch 0[4075/17270] Time:0.242, Train Loss:0.8294773697853088\n",
      "Epoch 0[4076/17270] Time:0.236, Train Loss:0.7412651181221008\n",
      "Epoch 0[4077/17270] Time:0.24, Train Loss:0.6978138089179993\n",
      "Epoch 0[4078/17270] Time:0.228, Train Loss:0.748127281665802\n",
      "Epoch 0[4079/17270] Time:0.245, Train Loss:0.5418293476104736\n",
      "Epoch 0[4080/17270] Time:0.251, Train Loss:0.876682460308075\n",
      "Epoch 0[4081/17270] Time:0.234, Train Loss:0.504828691482544\n",
      "Epoch 0[4082/17270] Time:0.236, Train Loss:0.5351045727729797\n",
      "Epoch 0[4083/17270] Time:0.234, Train Loss:0.4915824234485626\n",
      "Epoch 0[4084/17270] Time:0.23, Train Loss:0.5970219373703003\n",
      "Epoch 0[4085/17270] Time:0.23, Train Loss:0.4775081276893616\n",
      "Epoch 0[4086/17270] Time:0.249, Train Loss:0.651298999786377\n",
      "Epoch 0[4087/17270] Time:0.225, Train Loss:0.8064700961112976\n",
      "Epoch 0[4088/17270] Time:0.242, Train Loss:0.42554914951324463\n",
      "Epoch 0[4089/17270] Time:0.24, Train Loss:0.7217840552330017\n",
      "Epoch 0[4090/17270] Time:0.244, Train Loss:0.3874814212322235\n",
      "Epoch 0[4091/17270] Time:0.241, Train Loss:0.6062242984771729\n",
      "Epoch 0[4092/17270] Time:0.232, Train Loss:0.2856579124927521\n",
      "Epoch 0[4093/17270] Time:0.233, Train Loss:0.6263806819915771\n",
      "Epoch 0[4094/17270] Time:0.221, Train Loss:0.6377280950546265\n",
      "Epoch 0[4095/17270] Time:0.238, Train Loss:0.8443506360054016\n",
      "Epoch 0[4096/17270] Time:0.254, Train Loss:0.7503350377082825\n",
      "Epoch 0[4097/17270] Time:0.228, Train Loss:0.506804347038269\n",
      "Epoch 0[4098/17270] Time:0.228, Train Loss:0.41287216544151306\n",
      "Epoch 0[4099/17270] Time:0.234, Train Loss:1.2493598461151123\n",
      "Epoch 0[4100/17270] Time:0.232, Train Loss:0.35754403471946716\n",
      "Epoch 0[4101/17270] Time:0.236, Train Loss:0.5330931544303894\n",
      "Epoch 0[4102/17270] Time:0.238, Train Loss:0.7260178923606873\n",
      "Epoch 0[4103/17270] Time:0.238, Train Loss:0.5402440428733826\n",
      "Epoch 0[4104/17270] Time:0.237, Train Loss:0.7298911809921265\n",
      "Epoch 0[4105/17270] Time:0.225, Train Loss:0.6421431303024292\n",
      "Epoch 0[4106/17270] Time:0.232, Train Loss:0.3814886212348938\n",
      "Epoch 0[4107/17270] Time:0.239, Train Loss:0.4800020158290863\n",
      "Epoch 0[4108/17270] Time:0.236, Train Loss:0.3345337212085724\n",
      "Epoch 0[4109/17270] Time:0.239, Train Loss:0.5645115971565247\n",
      "Epoch 0[4110/17270] Time:0.237, Train Loss:1.0517174005508423\n",
      "Epoch 0[4111/17270] Time:0.234, Train Loss:0.3625527322292328\n",
      "Epoch 0[4112/17270] Time:0.237, Train Loss:0.5987187027931213\n",
      "Epoch 0[4113/17270] Time:0.232, Train Loss:1.4811656475067139\n",
      "Epoch 0[4114/17270] Time:0.243, Train Loss:0.5246782302856445\n",
      "Epoch 0[4115/17270] Time:0.226, Train Loss:0.40232551097869873\n",
      "Epoch 0[4116/17270] Time:0.239, Train Loss:0.8402543663978577\n",
      "Epoch 0[4117/17270] Time:0.23, Train Loss:0.731921911239624\n",
      "Epoch 0[4118/17270] Time:0.239, Train Loss:2.011148452758789\n",
      "Epoch 0[4119/17270] Time:0.237, Train Loss:0.8133488297462463\n",
      "Epoch 0[4120/17270] Time:0.243, Train Loss:0.5211300253868103\n",
      "Epoch 0[4121/17270] Time:0.235, Train Loss:0.6477566361427307\n",
      "Epoch 0[4122/17270] Time:0.228, Train Loss:0.417214959859848\n",
      "Epoch 0[4123/17270] Time:0.239, Train Loss:0.5790553092956543\n",
      "Epoch 0[4124/17270] Time:0.236, Train Loss:0.8233903050422668\n",
      "Epoch 0[4125/17270] Time:0.24, Train Loss:0.9275063872337341\n",
      "Epoch 0[4126/17270] Time:0.236, Train Loss:0.6083060503005981\n",
      "Epoch 0[4127/17270] Time:0.23, Train Loss:0.46709057688713074\n",
      "Epoch 0[4128/17270] Time:0.234, Train Loss:0.5108485221862793\n",
      "Epoch 0[4129/17270] Time:0.221, Train Loss:0.47363463044166565\n",
      "Epoch 0[4130/17270] Time:0.241, Train Loss:0.3521053194999695\n",
      "Epoch 0[4131/17270] Time:0.226, Train Loss:0.4866827726364136\n",
      "Epoch 0[4132/17270] Time:0.237, Train Loss:0.6997095346450806\n",
      "Epoch 0[4133/17270] Time:0.236, Train Loss:0.6955075860023499\n",
      "Epoch 0[4134/17270] Time:0.241, Train Loss:0.42367279529571533\n",
      "Epoch 0[4135/17270] Time:0.23, Train Loss:0.6244516372680664\n",
      "Epoch 0[4136/17270] Time:0.249, Train Loss:0.4364599585533142\n",
      "Epoch 0[4137/17270] Time:0.229, Train Loss:0.4713760316371918\n",
      "Epoch 0[4138/17270] Time:0.228, Train Loss:1.2170472145080566\n",
      "Epoch 0[4139/17270] Time:0.226, Train Loss:0.4816296696662903\n",
      "Epoch 0[4140/17270] Time:0.227, Train Loss:0.5408011674880981\n",
      "Epoch 0[4141/17270] Time:0.233, Train Loss:0.4771536886692047\n",
      "Epoch 0[4142/17270] Time:0.236, Train Loss:0.5343484282493591\n",
      "Epoch 0[4143/17270] Time:0.23, Train Loss:0.5013039708137512\n",
      "Epoch 0[4144/17270] Time:0.233, Train Loss:1.2423232793807983\n",
      "Epoch 0[4145/17270] Time:0.228, Train Loss:0.49223679304122925\n",
      "Epoch 0[4146/17270] Time:0.231, Train Loss:0.4599565863609314\n",
      "Epoch 0[4147/17270] Time:0.226, Train Loss:0.6687604784965515\n",
      "Epoch 0[4148/17270] Time:0.228, Train Loss:0.9600038528442383\n",
      "Epoch 0[4149/17270] Time:0.232, Train Loss:0.48193880915641785\n",
      "Epoch 0[4150/17270] Time:0.228, Train Loss:0.4357181191444397\n",
      "Epoch 0[4151/17270] Time:0.229, Train Loss:0.32423779368400574\n",
      "Epoch 0[4152/17270] Time:0.228, Train Loss:0.7187402248382568\n",
      "Epoch 0[4153/17270] Time:0.239, Train Loss:0.4545397460460663\n",
      "Epoch 0[4154/17270] Time:0.247, Train Loss:0.8470026254653931\n",
      "Epoch 0[4155/17270] Time:0.246, Train Loss:1.3844884634017944\n",
      "Epoch 0[4156/17270] Time:0.239, Train Loss:0.5578498840332031\n",
      "Epoch 0[4157/17270] Time:0.238, Train Loss:0.5072154998779297\n",
      "Epoch 0[4158/17270] Time:0.239, Train Loss:0.5524789690971375\n",
      "Epoch 0[4159/17270] Time:0.235, Train Loss:0.45014992356300354\n",
      "Epoch 0[4160/17270] Time:0.238, Train Loss:0.7342298030853271\n",
      "Epoch 0[4161/17270] Time:0.241, Train Loss:0.8472743630409241\n",
      "Epoch 0[4162/17270] Time:0.232, Train Loss:0.43624594807624817\n",
      "Epoch 0[4163/17270] Time:0.242, Train Loss:0.9197866320610046\n",
      "Epoch 0[4164/17270] Time:0.228, Train Loss:0.7238451242446899\n",
      "Epoch 0[4165/17270] Time:0.225, Train Loss:0.4201284646987915\n",
      "Epoch 0[4166/17270] Time:0.231, Train Loss:0.4167226552963257\n",
      "Epoch 0[4167/17270] Time:0.23, Train Loss:0.5474358797073364\n",
      "Epoch 0[4168/17270] Time:0.243, Train Loss:1.7488232851028442\n",
      "Epoch 0[4169/17270] Time:0.227, Train Loss:0.49430522322654724\n",
      "Epoch 0[4170/17270] Time:0.226, Train Loss:0.9145716428756714\n",
      "Epoch 0[4171/17270] Time:0.231, Train Loss:0.5888198018074036\n",
      "Epoch 0[4172/17270] Time:0.238, Train Loss:0.5382719039916992\n",
      "Epoch 0[4173/17270] Time:0.235, Train Loss:0.6317581534385681\n",
      "Epoch 0[4174/17270] Time:0.227, Train Loss:0.5253328680992126\n",
      "Epoch 0[4175/17270] Time:0.246, Train Loss:0.4390987753868103\n",
      "Epoch 0[4176/17270] Time:0.237, Train Loss:0.6349535584449768\n",
      "Epoch 0[4177/17270] Time:0.221, Train Loss:0.546576201915741\n",
      "Epoch 0[4178/17270] Time:0.239, Train Loss:0.6179989576339722\n",
      "Epoch 0[4179/17270] Time:0.232, Train Loss:1.2234854698181152\n",
      "Epoch 0[4180/17270] Time:0.232, Train Loss:0.6880894303321838\n",
      "Epoch 0[4181/17270] Time:0.231, Train Loss:0.6195105314254761\n",
      "Epoch 0[4182/17270] Time:0.226, Train Loss:0.5818249583244324\n",
      "Epoch 0[4183/17270] Time:0.251, Train Loss:0.709142804145813\n",
      "Epoch 0[4184/17270] Time:0.228, Train Loss:0.5257922410964966\n",
      "Epoch 0[4185/17270] Time:0.227, Train Loss:0.5521761178970337\n",
      "Epoch 0[4186/17270] Time:0.231, Train Loss:0.5286209583282471\n",
      "Epoch 0[4187/17270] Time:0.231, Train Loss:0.8957995176315308\n",
      "Epoch 0[4188/17270] Time:0.228, Train Loss:0.6459097862243652\n",
      "Epoch 0[4189/17270] Time:0.235, Train Loss:0.8473334312438965\n",
      "Epoch 0[4190/17270] Time:0.236, Train Loss:0.3376860022544861\n",
      "Epoch 0[4191/17270] Time:0.238, Train Loss:0.6629571914672852\n",
      "Epoch 0[4192/17270] Time:0.229, Train Loss:1.3887128829956055\n",
      "Epoch 0[4193/17270] Time:0.238, Train Loss:0.6692408323287964\n",
      "Epoch 0[4194/17270] Time:0.234, Train Loss:0.5102894306182861\n",
      "Epoch 0[4195/17270] Time:0.239, Train Loss:0.6562610864639282\n",
      "Epoch 0[4196/17270] Time:0.229, Train Loss:0.38663381338119507\n",
      "Epoch 0[4197/17270] Time:0.238, Train Loss:1.3429075479507446\n",
      "Epoch 0[4198/17270] Time:0.232, Train Loss:0.9299992918968201\n",
      "Epoch 0[4199/17270] Time:0.236, Train Loss:0.49958017468452454\n",
      "Epoch 0[4200/17270] Time:0.236, Train Loss:1.0870044231414795\n",
      "Epoch 0[4201/17270] Time:0.24, Train Loss:0.8066668510437012\n",
      "Epoch 0[4202/17270] Time:0.232, Train Loss:1.3299490213394165\n",
      "Epoch 0[4203/17270] Time:0.231, Train Loss:0.8538566827774048\n",
      "Epoch 0[4204/17270] Time:0.232, Train Loss:0.624306857585907\n",
      "Epoch 0[4205/17270] Time:0.245, Train Loss:0.6308881044387817\n",
      "Epoch 0[4206/17270] Time:0.234, Train Loss:0.6146665215492249\n",
      "Epoch 0[4207/17270] Time:0.225, Train Loss:0.634304404258728\n",
      "Epoch 0[4208/17270] Time:0.248, Train Loss:0.5499355792999268\n",
      "Epoch 0[4209/17270] Time:0.231, Train Loss:0.9287634491920471\n",
      "Epoch 0[4210/17270] Time:0.236, Train Loss:0.651753306388855\n",
      "Epoch 0[4211/17270] Time:0.248, Train Loss:1.0427523851394653\n",
      "Epoch 0[4212/17270] Time:0.244, Train Loss:0.7163363099098206\n",
      "Epoch 0[4213/17270] Time:0.248, Train Loss:0.4296211004257202\n",
      "Epoch 0[4214/17270] Time:0.234, Train Loss:0.43707844614982605\n",
      "Epoch 0[4215/17270] Time:0.238, Train Loss:0.4983031749725342\n",
      "Epoch 0[4216/17270] Time:0.232, Train Loss:0.815295934677124\n",
      "Epoch 0[4217/17270] Time:0.234, Train Loss:0.7097718119621277\n",
      "Epoch 0[4218/17270] Time:0.237, Train Loss:0.6405603885650635\n",
      "Epoch 0[4219/17270] Time:0.23, Train Loss:0.8777090311050415\n",
      "Epoch 0[4220/17270] Time:0.23, Train Loss:1.105605959892273\n",
      "Epoch 0[4221/17270] Time:0.234, Train Loss:0.6173787117004395\n",
      "Epoch 0[4222/17270] Time:0.232, Train Loss:0.5310457348823547\n",
      "Epoch 0[4223/17270] Time:0.238, Train Loss:0.7144133448600769\n",
      "Epoch 0[4224/17270] Time:0.227, Train Loss:0.5663791298866272\n",
      "Epoch 0[4225/17270] Time:0.228, Train Loss:0.6077765822410583\n",
      "Epoch 0[4226/17270] Time:0.238, Train Loss:0.7270532250404358\n",
      "Epoch 0[4227/17270] Time:0.229, Train Loss:0.4108580946922302\n",
      "Epoch 0[4228/17270] Time:0.24, Train Loss:0.3810705244541168\n",
      "Epoch 0[4229/17270] Time:0.227, Train Loss:0.5979154706001282\n",
      "Epoch 0[4230/17270] Time:0.238, Train Loss:0.5399563908576965\n",
      "Epoch 0[4231/17270] Time:0.239, Train Loss:1.6717525720596313\n",
      "Epoch 0[4232/17270] Time:0.231, Train Loss:0.5178199410438538\n",
      "Epoch 0[4233/17270] Time:0.24, Train Loss:0.7024337649345398\n",
      "Epoch 0[4234/17270] Time:0.233, Train Loss:0.5387009382247925\n",
      "Epoch 0[4235/17270] Time:0.223, Train Loss:0.41737377643585205\n",
      "Epoch 0[4236/17270] Time:0.229, Train Loss:0.39201515913009644\n",
      "Epoch 0[4237/17270] Time:0.227, Train Loss:0.6569807529449463\n",
      "Epoch 0[4238/17270] Time:0.233, Train Loss:1.0735130310058594\n",
      "Epoch 0[4239/17270] Time:0.23, Train Loss:0.5145227909088135\n",
      "Epoch 0[4240/17270] Time:0.237, Train Loss:0.44145384430885315\n",
      "Epoch 0[4241/17270] Time:0.235, Train Loss:0.7186278700828552\n",
      "Epoch 0[4242/17270] Time:0.227, Train Loss:0.5573499798774719\n",
      "Epoch 0[4243/17270] Time:0.238, Train Loss:0.7062354683876038\n",
      "Epoch 0[4244/17270] Time:0.234, Train Loss:0.6201649308204651\n",
      "Epoch 0[4245/17270] Time:0.238, Train Loss:0.8978075385093689\n",
      "Epoch 0[4246/17270] Time:0.232, Train Loss:0.8837085366249084\n",
      "Epoch 0[4247/17270] Time:0.23, Train Loss:0.7411136627197266\n",
      "Epoch 0[4248/17270] Time:0.232, Train Loss:0.6256312131881714\n",
      "Epoch 0[4249/17270] Time:0.234, Train Loss:0.49036523699760437\n",
      "Epoch 0[4250/17270] Time:0.238, Train Loss:0.541043758392334\n",
      "Epoch 0[4251/17270] Time:0.231, Train Loss:0.5133096575737\n",
      "Epoch 0[4252/17270] Time:0.23, Train Loss:0.6164106130599976\n",
      "Epoch 0[4253/17270] Time:0.228, Train Loss:0.4242640733718872\n",
      "Epoch 0[4254/17270] Time:0.228, Train Loss:0.5779998302459717\n",
      "Epoch 0[4255/17270] Time:0.232, Train Loss:0.6598265171051025\n",
      "Epoch 0[4256/17270] Time:0.236, Train Loss:0.7197198867797852\n",
      "Epoch 0[4257/17270] Time:0.238, Train Loss:0.8481209874153137\n",
      "Epoch 0[4258/17270] Time:0.231, Train Loss:0.5170174837112427\n",
      "Epoch 0[4259/17270] Time:0.23, Train Loss:0.6342759728431702\n",
      "Epoch 0[4260/17270] Time:0.227, Train Loss:0.47719836235046387\n",
      "Epoch 0[4261/17270] Time:0.231, Train Loss:1.360973596572876\n",
      "Epoch 0[4262/17270] Time:0.229, Train Loss:0.5539245009422302\n",
      "Epoch 0[4263/17270] Time:0.231, Train Loss:0.5664038062095642\n",
      "Epoch 0[4264/17270] Time:0.239, Train Loss:0.519889235496521\n",
      "Epoch 0[4265/17270] Time:0.239, Train Loss:0.34538373351097107\n",
      "Epoch 0[4266/17270] Time:0.242, Train Loss:1.6223317384719849\n",
      "Epoch 0[4267/17270] Time:0.237, Train Loss:0.4965520203113556\n",
      "Epoch 0[4268/17270] Time:0.24, Train Loss:0.6461793780326843\n",
      "Epoch 0[4269/17270] Time:0.232, Train Loss:0.5571601390838623\n",
      "Epoch 0[4270/17270] Time:0.239, Train Loss:0.8828280568122864\n",
      "Epoch 0[4271/17270] Time:0.234, Train Loss:1.1621010303497314\n",
      "Epoch 0[4272/17270] Time:0.24, Train Loss:0.7523018717765808\n",
      "Epoch 0[4273/17270] Time:0.23, Train Loss:0.3975379467010498\n",
      "Epoch 0[4274/17270] Time:0.231, Train Loss:0.6762664914131165\n",
      "Epoch 0[4275/17270] Time:0.227, Train Loss:0.3787415325641632\n",
      "Epoch 0[4276/17270] Time:0.237, Train Loss:0.8208941221237183\n",
      "Epoch 0[4277/17270] Time:0.237, Train Loss:0.6314411759376526\n",
      "Epoch 0[4278/17270] Time:0.238, Train Loss:0.5134742259979248\n",
      "Epoch 0[4279/17270] Time:0.231, Train Loss:0.7744559049606323\n",
      "Epoch 0[4280/17270] Time:0.229, Train Loss:1.1991196870803833\n",
      "Epoch 0[4281/17270] Time:0.23, Train Loss:0.5924689173698425\n",
      "Epoch 0[4282/17270] Time:0.23, Train Loss:0.36886322498321533\n",
      "Epoch 0[4283/17270] Time:0.237, Train Loss:0.44440290331840515\n",
      "Epoch 0[4284/17270] Time:0.24, Train Loss:0.6979978680610657\n",
      "Epoch 0[4285/17270] Time:0.228, Train Loss:1.1202590465545654\n",
      "Epoch 0[4286/17270] Time:0.231, Train Loss:0.46756914258003235\n",
      "Epoch 0[4287/17270] Time:0.234, Train Loss:0.38240087032318115\n",
      "Epoch 0[4288/17270] Time:0.243, Train Loss:0.7209696769714355\n",
      "Epoch 0[4289/17270] Time:0.242, Train Loss:0.6138654947280884\n",
      "Epoch 0[4290/17270] Time:0.242, Train Loss:0.4173324406147003\n",
      "Epoch 0[4291/17270] Time:0.217, Train Loss:0.3973241448402405\n",
      "Epoch 0[4292/17270] Time:0.24, Train Loss:0.5809763073921204\n",
      "Epoch 0[4293/17270] Time:0.238, Train Loss:0.6781700849533081\n",
      "Epoch 0[4294/17270] Time:0.217, Train Loss:0.64323490858078\n",
      "Epoch 0[4295/17270] Time:0.236, Train Loss:0.5204352736473083\n",
      "Epoch 0[4296/17270] Time:0.225, Train Loss:0.41970160603523254\n",
      "Epoch 0[4297/17270] Time:0.232, Train Loss:0.3482798933982849\n",
      "Epoch 0[4298/17270] Time:0.248, Train Loss:0.7138193249702454\n",
      "Epoch 0[4299/17270] Time:0.236, Train Loss:0.7228697538375854\n",
      "Epoch 0[4300/17270] Time:0.233, Train Loss:1.296548843383789\n",
      "Epoch 0[4301/17270] Time:0.239, Train Loss:0.8780667781829834\n",
      "Epoch 0[4302/17270] Time:0.226, Train Loss:0.5370468497276306\n",
      "Epoch 0[4303/17270] Time:0.229, Train Loss:0.9308754801750183\n",
      "Epoch 0[4304/17270] Time:0.238, Train Loss:0.6765029430389404\n",
      "Epoch 0[4305/17270] Time:0.243, Train Loss:0.895375669002533\n",
      "Epoch 0[4306/17270] Time:0.242, Train Loss:0.4370807111263275\n",
      "Epoch 0[4307/17270] Time:0.224, Train Loss:0.5720997452735901\n",
      "Epoch 0[4308/17270] Time:0.241, Train Loss:0.6589590907096863\n",
      "Epoch 0[4309/17270] Time:0.238, Train Loss:1.5763105154037476\n",
      "Epoch 0[4310/17270] Time:0.23, Train Loss:0.9261712431907654\n",
      "Epoch 0[4311/17270] Time:0.228, Train Loss:0.4275478422641754\n",
      "Epoch 0[4312/17270] Time:0.237, Train Loss:0.5304163694381714\n",
      "Epoch 0[4313/17270] Time:0.231, Train Loss:0.4587438106536865\n",
      "Epoch 0[4314/17270] Time:0.232, Train Loss:0.7443886399269104\n",
      "Epoch 0[4315/17270] Time:0.23, Train Loss:0.3160507082939148\n",
      "Epoch 0[4316/17270] Time:0.235, Train Loss:0.6787025332450867\n",
      "Epoch 0[4317/17270] Time:0.239, Train Loss:0.771894633769989\n",
      "Epoch 0[4318/17270] Time:0.233, Train Loss:0.7027896046638489\n",
      "Epoch 0[4319/17270] Time:0.239, Train Loss:1.0379114151000977\n",
      "Epoch 0[4320/17270] Time:0.244, Train Loss:0.47534146904945374\n",
      "Epoch 0[4321/17270] Time:0.233, Train Loss:0.547031819820404\n",
      "Epoch 0[4322/17270] Time:0.25, Train Loss:0.8379486203193665\n",
      "Epoch 0[4323/17270] Time:0.232, Train Loss:0.4181838035583496\n",
      "Epoch 0[4324/17270] Time:0.24, Train Loss:1.4819493293762207\n",
      "Epoch 0[4325/17270] Time:0.219, Train Loss:0.41535869240760803\n",
      "Epoch 0[4326/17270] Time:0.232, Train Loss:0.7169301509857178\n",
      "Epoch 0[4327/17270] Time:0.238, Train Loss:0.6134704351425171\n",
      "Epoch 0[4328/17270] Time:0.234, Train Loss:1.1033357381820679\n",
      "Epoch 0[4329/17270] Time:0.231, Train Loss:0.44131967425346375\n",
      "Epoch 0[4330/17270] Time:0.237, Train Loss:1.3088759183883667\n",
      "Epoch 0[4331/17270] Time:0.248, Train Loss:0.6166536808013916\n",
      "Epoch 0[4332/17270] Time:0.239, Train Loss:0.643602728843689\n",
      "Epoch 0[4333/17270] Time:0.229, Train Loss:0.6056833863258362\n",
      "Epoch 0[4334/17270] Time:0.238, Train Loss:0.7148603200912476\n",
      "Epoch 0[4335/17270] Time:0.237, Train Loss:0.5963754057884216\n",
      "Epoch 0[4336/17270] Time:0.243, Train Loss:1.278824806213379\n",
      "Epoch 0[4337/17270] Time:0.231, Train Loss:0.8166447877883911\n",
      "Epoch 0[4338/17270] Time:0.232, Train Loss:0.45384109020233154\n",
      "Epoch 0[4339/17270] Time:0.237, Train Loss:0.4791482388973236\n",
      "Epoch 0[4340/17270] Time:0.238, Train Loss:0.9274958372116089\n",
      "Epoch 0[4341/17270] Time:0.24, Train Loss:0.6607442498207092\n",
      "Epoch 0[4342/17270] Time:0.239, Train Loss:0.5791590213775635\n",
      "Epoch 0[4343/17270] Time:0.228, Train Loss:0.5739593505859375\n",
      "Epoch 0[4344/17270] Time:0.24, Train Loss:0.3960213363170624\n",
      "Epoch 0[4345/17270] Time:0.233, Train Loss:0.5836763381958008\n",
      "Epoch 0[4346/17270] Time:0.23, Train Loss:0.5300229787826538\n",
      "Epoch 0[4347/17270] Time:0.229, Train Loss:0.48213398456573486\n",
      "Epoch 0[4348/17270] Time:0.223, Train Loss:0.5388697385787964\n",
      "Epoch 0[4349/17270] Time:0.236, Train Loss:0.3832038342952728\n",
      "Epoch 0[4350/17270] Time:0.238, Train Loss:0.420148104429245\n",
      "Epoch 0[4351/17270] Time:0.23, Train Loss:0.5180521607398987\n",
      "Epoch 0[4352/17270] Time:0.232, Train Loss:0.6308058500289917\n",
      "Epoch 0[4353/17270] Time:0.236, Train Loss:0.36546269059181213\n",
      "Epoch 0[4354/17270] Time:0.238, Train Loss:0.6952428221702576\n",
      "Epoch 0[4355/17270] Time:0.236, Train Loss:0.6636320352554321\n",
      "Epoch 0[4356/17270] Time:0.24, Train Loss:0.45470890402793884\n",
      "Epoch 0[4357/17270] Time:0.234, Train Loss:0.5836228132247925\n",
      "Epoch 0[4358/17270] Time:0.231, Train Loss:1.4457662105560303\n",
      "Epoch 0[4359/17270] Time:0.23, Train Loss:0.7070713043212891\n",
      "Epoch 0[4360/17270] Time:0.238, Train Loss:0.5249853134155273\n",
      "Epoch 0[4361/17270] Time:0.232, Train Loss:0.4529992341995239\n",
      "Epoch 0[4362/17270] Time:0.231, Train Loss:0.5564282536506653\n",
      "Epoch 0[4363/17270] Time:0.237, Train Loss:0.43219682574272156\n",
      "Epoch 0[4364/17270] Time:0.233, Train Loss:0.6984079480171204\n",
      "Epoch 0[4365/17270] Time:0.228, Train Loss:0.36756351590156555\n",
      "Epoch 0[4366/17270] Time:0.236, Train Loss:0.6520193219184875\n",
      "Epoch 0[4367/17270] Time:0.236, Train Loss:0.43037745356559753\n",
      "Epoch 0[4368/17270] Time:0.236, Train Loss:0.4233406186103821\n",
      "Epoch 0[4369/17270] Time:0.227, Train Loss:0.978524386882782\n",
      "Epoch 0[4370/17270] Time:0.239, Train Loss:0.4927820563316345\n",
      "Epoch 0[4371/17270] Time:0.222, Train Loss:0.5663455724716187\n",
      "Epoch 0[4372/17270] Time:0.24, Train Loss:0.44850954413414\n",
      "Epoch 0[4373/17270] Time:0.232, Train Loss:0.27832335233688354\n",
      "Epoch 0[4374/17270] Time:0.23, Train Loss:0.4513515532016754\n",
      "Epoch 0[4375/17270] Time:0.228, Train Loss:0.461913138628006\n",
      "Epoch 0[4376/17270] Time:0.238, Train Loss:0.8963921070098877\n",
      "Epoch 0[4377/17270] Time:0.234, Train Loss:0.4213351011276245\n",
      "Epoch 0[4378/17270] Time:0.229, Train Loss:1.2717962265014648\n",
      "Epoch 0[4379/17270] Time:0.24, Train Loss:0.48848944902420044\n",
      "Epoch 0[4380/17270] Time:0.228, Train Loss:0.5492581725120544\n",
      "Epoch 0[4381/17270] Time:0.248, Train Loss:0.9646598100662231\n",
      "Epoch 0[4382/17270] Time:0.24, Train Loss:0.5498418211936951\n",
      "Epoch 0[4383/17270] Time:0.242, Train Loss:0.4579724371433258\n",
      "Epoch 0[4384/17270] Time:0.243, Train Loss:0.7242010831832886\n",
      "Epoch 0[4385/17270] Time:0.231, Train Loss:0.3925669491291046\n",
      "Epoch 0[4386/17270] Time:0.239, Train Loss:0.547236442565918\n",
      "Epoch 0[4387/17270] Time:0.238, Train Loss:0.5345267653465271\n",
      "Epoch 0[4388/17270] Time:0.234, Train Loss:1.205487608909607\n",
      "Epoch 0[4389/17270] Time:0.239, Train Loss:2.149139404296875\n",
      "Epoch 0[4390/17270] Time:0.236, Train Loss:0.49771595001220703\n",
      "Epoch 0[4391/17270] Time:0.235, Train Loss:0.623161256313324\n",
      "Epoch 0[4392/17270] Time:0.239, Train Loss:0.5469695329666138\n",
      "Epoch 0[4393/17270] Time:0.237, Train Loss:0.9318735003471375\n",
      "Epoch 0[4394/17270] Time:0.227, Train Loss:0.7476677894592285\n",
      "Epoch 0[4395/17270] Time:0.238, Train Loss:0.6214798092842102\n",
      "Epoch 0[4396/17270] Time:0.244, Train Loss:0.6858375072479248\n",
      "Epoch 0[4397/17270] Time:0.235, Train Loss:0.6147244572639465\n",
      "Epoch 0[4398/17270] Time:0.236, Train Loss:0.6500429511070251\n",
      "Epoch 0[4399/17270] Time:0.237, Train Loss:0.5963501334190369\n",
      "Epoch 0[4400/17270] Time:0.227, Train Loss:0.49104630947113037\n",
      "Epoch 0[4401/17270] Time:0.232, Train Loss:0.6306005120277405\n",
      "Epoch 0[4402/17270] Time:0.239, Train Loss:1.1918227672576904\n",
      "Epoch 0[4403/17270] Time:0.239, Train Loss:0.854924738407135\n",
      "Epoch 0[4404/17270] Time:0.232, Train Loss:0.4897138774394989\n",
      "Epoch 0[4405/17270] Time:0.233, Train Loss:0.5397397875785828\n",
      "Epoch 0[4406/17270] Time:0.238, Train Loss:0.4115552604198456\n",
      "Epoch 0[4407/17270] Time:0.233, Train Loss:0.6614344716072083\n",
      "Epoch 0[4408/17270] Time:0.23, Train Loss:0.697724461555481\n",
      "Epoch 0[4409/17270] Time:0.252, Train Loss:0.607003390789032\n",
      "Epoch 0[4410/17270] Time:0.24, Train Loss:0.7631247043609619\n",
      "Epoch 0[4411/17270] Time:0.222, Train Loss:0.3312516510486603\n",
      "Epoch 0[4412/17270] Time:0.226, Train Loss:0.6073254942893982\n",
      "Epoch 0[4413/17270] Time:0.257, Train Loss:0.536059558391571\n",
      "Epoch 0[4414/17270] Time:0.231, Train Loss:0.7317948341369629\n",
      "Epoch 0[4415/17270] Time:0.231, Train Loss:0.8552144169807434\n",
      "Epoch 0[4416/17270] Time:0.241, Train Loss:1.2491140365600586\n",
      "Epoch 0[4417/17270] Time:0.243, Train Loss:0.4730219542980194\n",
      "Epoch 0[4418/17270] Time:0.248, Train Loss:0.4565243124961853\n",
      "Epoch 0[4419/17270] Time:0.237, Train Loss:0.3798690438270569\n",
      "Epoch 0[4420/17270] Time:0.238, Train Loss:0.7140592336654663\n",
      "Epoch 0[4421/17270] Time:0.238, Train Loss:0.6770090460777283\n",
      "Epoch 0[4422/17270] Time:0.25, Train Loss:0.5699220895767212\n",
      "Epoch 0[4423/17270] Time:0.242, Train Loss:0.5120792984962463\n",
      "Epoch 0[4424/17270] Time:0.241, Train Loss:0.9235161542892456\n",
      "Epoch 0[4425/17270] Time:0.237, Train Loss:0.647930383682251\n",
      "Epoch 0[4426/17270] Time:0.231, Train Loss:1.516640067100525\n",
      "Epoch 0[4427/17270] Time:0.235, Train Loss:0.5823591947555542\n",
      "Epoch 0[4428/17270] Time:0.23, Train Loss:0.5304260849952698\n",
      "Epoch 0[4429/17270] Time:0.228, Train Loss:0.5912688374519348\n",
      "Epoch 0[4430/17270] Time:0.236, Train Loss:1.6257158517837524\n",
      "Epoch 0[4431/17270] Time:0.237, Train Loss:0.38095512986183167\n",
      "Epoch 0[4432/17270] Time:0.232, Train Loss:0.7734941244125366\n",
      "Epoch 0[4433/17270] Time:0.232, Train Loss:0.7127606868743896\n",
      "Epoch 0[4434/17270] Time:0.258, Train Loss:0.7204177379608154\n",
      "Epoch 0[4435/17270] Time:0.234, Train Loss:0.46670931577682495\n",
      "Epoch 0[4436/17270] Time:0.224, Train Loss:0.4355505108833313\n",
      "Epoch 0[4437/17270] Time:0.224, Train Loss:0.6334394216537476\n",
      "Epoch 0[4438/17270] Time:0.231, Train Loss:0.5353536605834961\n",
      "Epoch 0[4439/17270] Time:0.238, Train Loss:0.6006839275360107\n",
      "Epoch 0[4440/17270] Time:0.239, Train Loss:0.765762448310852\n",
      "Epoch 0[4441/17270] Time:0.23, Train Loss:0.8027439713478088\n",
      "Epoch 0[4442/17270] Time:0.239, Train Loss:0.4919274151325226\n",
      "Epoch 0[4443/17270] Time:0.232, Train Loss:0.6271281838417053\n",
      "Epoch 0[4444/17270] Time:0.237, Train Loss:0.2512975335121155\n",
      "Epoch 0[4445/17270] Time:0.238, Train Loss:0.5421352386474609\n",
      "Epoch 0[4446/17270] Time:0.231, Train Loss:0.8550116419792175\n",
      "Epoch 0[4447/17270] Time:0.24, Train Loss:0.5313290357589722\n",
      "Epoch 0[4448/17270] Time:0.231, Train Loss:0.3451717793941498\n",
      "Epoch 0[4449/17270] Time:0.239, Train Loss:1.1371914148330688\n",
      "Epoch 0[4450/17270] Time:0.236, Train Loss:0.6091163754463196\n",
      "Epoch 0[4451/17270] Time:0.237, Train Loss:0.3790230453014374\n",
      "Epoch 0[4452/17270] Time:0.235, Train Loss:0.46441420912742615\n",
      "Epoch 0[4453/17270] Time:0.231, Train Loss:0.3393917977809906\n",
      "Epoch 0[4454/17270] Time:0.237, Train Loss:0.37498968839645386\n",
      "Epoch 0[4455/17270] Time:0.227, Train Loss:0.5168062448501587\n",
      "Epoch 0[4456/17270] Time:0.236, Train Loss:0.3506280183792114\n",
      "Epoch 0[4457/17270] Time:0.245, Train Loss:0.8907290101051331\n",
      "Epoch 0[4458/17270] Time:0.241, Train Loss:0.32708612084388733\n",
      "Epoch 0[4459/17270] Time:0.244, Train Loss:0.7230181097984314\n",
      "Epoch 0[4460/17270] Time:0.229, Train Loss:0.39940395951271057\n",
      "Epoch 0[4461/17270] Time:0.241, Train Loss:1.056604266166687\n",
      "Epoch 0[4462/17270] Time:0.235, Train Loss:0.2911240756511688\n",
      "Epoch 0[4463/17270] Time:0.228, Train Loss:0.7052590250968933\n",
      "Epoch 0[4464/17270] Time:0.238, Train Loss:0.6749289631843567\n",
      "Epoch 0[4465/17270] Time:0.237, Train Loss:0.8722664713859558\n",
      "Epoch 0[4466/17270] Time:0.245, Train Loss:0.5333670377731323\n",
      "Epoch 0[4467/17270] Time:0.23, Train Loss:0.4787282943725586\n",
      "Epoch 0[4468/17270] Time:0.228, Train Loss:0.6608909368515015\n",
      "Epoch 0[4469/17270] Time:0.232, Train Loss:0.44657453894615173\n",
      "Epoch 0[4470/17270] Time:0.237, Train Loss:0.838959276676178\n",
      "Epoch 0[4471/17270] Time:0.232, Train Loss:0.5323526263237\n",
      "Epoch 0[4472/17270] Time:0.239, Train Loss:0.852416455745697\n",
      "Epoch 0[4473/17270] Time:0.233, Train Loss:0.8670997619628906\n",
      "Epoch 0[4474/17270] Time:0.237, Train Loss:0.607093334197998\n",
      "Epoch 0[4475/17270] Time:0.24, Train Loss:0.7324435114860535\n",
      "Epoch 0[4476/17270] Time:0.242, Train Loss:0.6652336716651917\n",
      "Epoch 0[4477/17270] Time:0.232, Train Loss:0.642884373664856\n",
      "Epoch 0[4478/17270] Time:0.238, Train Loss:0.5503246784210205\n",
      "Epoch 0[4479/17270] Time:0.23, Train Loss:0.7932475209236145\n",
      "Epoch 0[4480/17270] Time:0.235, Train Loss:0.4005643427371979\n",
      "Epoch 0[4481/17270] Time:0.244, Train Loss:0.6349387168884277\n",
      "Epoch 0[4482/17270] Time:0.227, Train Loss:0.3229711949825287\n",
      "Epoch 0[4483/17270] Time:0.24, Train Loss:0.5159038305282593\n",
      "Epoch 0[4484/17270] Time:0.237, Train Loss:0.6121195554733276\n",
      "Epoch 0[4485/17270] Time:0.23, Train Loss:0.5094800591468811\n",
      "Epoch 0[4486/17270] Time:0.237, Train Loss:0.30735161900520325\n",
      "Epoch 0[4487/17270] Time:0.237, Train Loss:0.3714514970779419\n",
      "Epoch 0[4488/17270] Time:0.234, Train Loss:1.280658483505249\n",
      "Epoch 0[4489/17270] Time:0.238, Train Loss:1.0792415142059326\n",
      "Epoch 0[4490/17270] Time:0.234, Train Loss:0.5911145210266113\n",
      "Epoch 0[4491/17270] Time:0.249, Train Loss:0.460786372423172\n",
      "Epoch 0[4492/17270] Time:0.232, Train Loss:0.5911557078361511\n",
      "Epoch 0[4493/17270] Time:0.23, Train Loss:0.3639559745788574\n",
      "Epoch 0[4494/17270] Time:0.238, Train Loss:0.43280020356178284\n",
      "Epoch 0[4495/17270] Time:0.242, Train Loss:0.6828855872154236\n",
      "Epoch 0[4496/17270] Time:0.223, Train Loss:0.46855056285858154\n",
      "Epoch 0[4497/17270] Time:0.238, Train Loss:0.9808716773986816\n",
      "Epoch 0[4498/17270] Time:0.241, Train Loss:0.5128641128540039\n",
      "Epoch 0[4499/17270] Time:0.239, Train Loss:1.0015907287597656\n",
      "Epoch 0[4500/17270] Time:0.242, Train Loss:0.6556614637374878\n",
      "Epoch 0[4501/17270] Time:0.239, Train Loss:0.4585089087486267\n",
      "Epoch 0[4502/17270] Time:0.236, Train Loss:0.4107244312763214\n",
      "Epoch 0[4503/17270] Time:0.237, Train Loss:0.7339026927947998\n",
      "Epoch 0[4504/17270] Time:0.219, Train Loss:0.5964068174362183\n",
      "Epoch 0[4505/17270] Time:0.232, Train Loss:0.36834171414375305\n",
      "Epoch 0[4506/17270] Time:0.225, Train Loss:0.8018538951873779\n",
      "Epoch 0[4507/17270] Time:0.229, Train Loss:0.5435051321983337\n",
      "Epoch 0[4508/17270] Time:0.232, Train Loss:2.2454068660736084\n",
      "Epoch 0[4509/17270] Time:0.237, Train Loss:0.7084457278251648\n",
      "Epoch 0[4510/17270] Time:0.235, Train Loss:1.0399435758590698\n",
      "Epoch 0[4511/17270] Time:0.239, Train Loss:1.321581244468689\n",
      "Epoch 0[4512/17270] Time:0.23, Train Loss:0.7195300459861755\n",
      "Epoch 0[4513/17270] Time:0.228, Train Loss:0.5969013571739197\n",
      "Epoch 0[4514/17270] Time:0.224, Train Loss:0.5878709554672241\n",
      "Epoch 0[4515/17270] Time:0.234, Train Loss:0.6777289509773254\n",
      "Epoch 0[4516/17270] Time:0.234, Train Loss:0.714836061000824\n",
      "Epoch 0[4517/17270] Time:0.233, Train Loss:0.6655274033546448\n",
      "Epoch 0[4518/17270] Time:0.232, Train Loss:0.7291886806488037\n",
      "Epoch 0[4519/17270] Time:0.231, Train Loss:0.47151443362236023\n",
      "Epoch 0[4520/17270] Time:0.233, Train Loss:1.1847208738327026\n",
      "Epoch 0[4521/17270] Time:0.234, Train Loss:0.5565727353096008\n",
      "Epoch 0[4522/17270] Time:0.232, Train Loss:0.3901260495185852\n",
      "Epoch 0[4523/17270] Time:0.233, Train Loss:0.6381427049636841\n",
      "Epoch 0[4524/17270] Time:0.233, Train Loss:0.5351341962814331\n",
      "Epoch 0[4525/17270] Time:0.232, Train Loss:0.5594304203987122\n",
      "Epoch 0[4526/17270] Time:0.239, Train Loss:0.5488423109054565\n",
      "Epoch 0[4527/17270] Time:0.24, Train Loss:0.5241591334342957\n",
      "Epoch 0[4528/17270] Time:0.23, Train Loss:0.5492616295814514\n",
      "Epoch 0[4529/17270] Time:0.238, Train Loss:0.5094923973083496\n",
      "Epoch 0[4530/17270] Time:0.233, Train Loss:0.7799543142318726\n",
      "Epoch 0[4531/17270] Time:0.224, Train Loss:0.9040660858154297\n",
      "Epoch 0[4532/17270] Time:0.238, Train Loss:0.5981593132019043\n",
      "Epoch 0[4533/17270] Time:0.238, Train Loss:0.8003843426704407\n",
      "Epoch 0[4534/17270] Time:0.232, Train Loss:0.5747103095054626\n",
      "Epoch 0[4535/17270] Time:0.233, Train Loss:0.5557284951210022\n",
      "Epoch 0[4536/17270] Time:0.238, Train Loss:0.861375629901886\n",
      "Epoch 0[4537/17270] Time:0.236, Train Loss:0.5215920209884644\n",
      "Epoch 0[4538/17270] Time:0.227, Train Loss:0.4402441084384918\n",
      "Epoch 0[4539/17270] Time:0.23, Train Loss:0.47508955001831055\n",
      "Epoch 0[4540/17270] Time:0.228, Train Loss:0.4885208010673523\n",
      "Epoch 0[4541/17270] Time:0.239, Train Loss:0.8300909399986267\n",
      "Epoch 0[4542/17270] Time:0.228, Train Loss:0.7849474549293518\n",
      "Epoch 0[4543/17270] Time:0.237, Train Loss:0.8095364570617676\n",
      "Epoch 0[4544/17270] Time:0.234, Train Loss:0.7955119609832764\n",
      "Epoch 0[4545/17270] Time:0.235, Train Loss:1.0309888124465942\n",
      "Epoch 0[4546/17270] Time:0.237, Train Loss:0.5728594660758972\n",
      "Epoch 0[4547/17270] Time:0.227, Train Loss:0.6504997611045837\n",
      "Epoch 0[4548/17270] Time:0.229, Train Loss:0.7225852012634277\n",
      "Epoch 0[4549/17270] Time:0.228, Train Loss:0.8783904314041138\n",
      "Epoch 0[4550/17270] Time:0.236, Train Loss:0.7091608643531799\n",
      "Epoch 0[4551/17270] Time:0.238, Train Loss:1.1585066318511963\n",
      "Epoch 0[4552/17270] Time:0.231, Train Loss:0.43553435802459717\n",
      "Epoch 0[4553/17270] Time:0.238, Train Loss:0.43620365858078003\n",
      "Epoch 0[4554/17270] Time:0.227, Train Loss:0.9970386624336243\n",
      "Epoch 0[4555/17270] Time:0.244, Train Loss:0.7755460739135742\n",
      "Epoch 0[4556/17270] Time:0.239, Train Loss:0.5673393607139587\n",
      "Epoch 0[4557/17270] Time:0.229, Train Loss:0.5477850437164307\n",
      "Epoch 0[4558/17270] Time:0.232, Train Loss:0.8576602339744568\n",
      "Epoch 0[4559/17270] Time:0.238, Train Loss:0.319601446390152\n",
      "Epoch 0[4560/17270] Time:0.222, Train Loss:0.6044926643371582\n",
      "Epoch 0[4561/17270] Time:0.229, Train Loss:0.6129418015480042\n",
      "Epoch 0[4562/17270] Time:0.234, Train Loss:0.5545350313186646\n",
      "Epoch 0[4563/17270] Time:0.229, Train Loss:0.5462000966072083\n",
      "Epoch 0[4564/17270] Time:0.23, Train Loss:0.4857849180698395\n",
      "Epoch 0[4565/17270] Time:0.239, Train Loss:0.5131617188453674\n",
      "Epoch 0[4566/17270] Time:0.238, Train Loss:0.3906995952129364\n",
      "Epoch 0[4567/17270] Time:0.231, Train Loss:0.49988895654678345\n",
      "Epoch 0[4568/17270] Time:0.237, Train Loss:0.5189276337623596\n",
      "Epoch 0[4569/17270] Time:0.23, Train Loss:1.0187724828720093\n",
      "Epoch 0[4570/17270] Time:0.238, Train Loss:0.5455935001373291\n",
      "Epoch 0[4571/17270] Time:0.237, Train Loss:0.4218737781047821\n",
      "Epoch 0[4572/17270] Time:0.23, Train Loss:1.2121413946151733\n",
      "Epoch 0[4573/17270] Time:0.256, Train Loss:0.8972060680389404\n",
      "Epoch 0[4574/17270] Time:0.238, Train Loss:0.39946576952934265\n",
      "Epoch 0[4575/17270] Time:0.239, Train Loss:0.4801124036312103\n",
      "Epoch 0[4576/17270] Time:0.229, Train Loss:0.6126193404197693\n",
      "Epoch 0[4577/17270] Time:0.234, Train Loss:0.3995209336280823\n",
      "Epoch 0[4578/17270] Time:0.251, Train Loss:0.5625113248825073\n",
      "Epoch 0[4579/17270] Time:0.238, Train Loss:0.3861081898212433\n",
      "Epoch 0[4580/17270] Time:0.227, Train Loss:0.5363496541976929\n",
      "Epoch 0[4581/17270] Time:0.232, Train Loss:0.5690123438835144\n",
      "Epoch 0[4582/17270] Time:0.23, Train Loss:1.0446407794952393\n",
      "Epoch 0[4583/17270] Time:0.236, Train Loss:0.6349286437034607\n",
      "Epoch 0[4584/17270] Time:0.235, Train Loss:0.7585095167160034\n",
      "Epoch 0[4585/17270] Time:0.229, Train Loss:0.8315144181251526\n",
      "Epoch 0[4586/17270] Time:0.239, Train Loss:0.7007812261581421\n",
      "Epoch 0[4587/17270] Time:0.23, Train Loss:0.450215220451355\n",
      "Epoch 0[4588/17270] Time:0.228, Train Loss:0.7725313901901245\n",
      "Epoch 0[4589/17270] Time:0.231, Train Loss:0.3464468717575073\n",
      "Epoch 0[4590/17270] Time:0.229, Train Loss:0.3714015483856201\n",
      "Epoch 0[4591/17270] Time:0.238, Train Loss:0.7596180438995361\n",
      "Epoch 0[4592/17270] Time:0.237, Train Loss:0.5339124798774719\n",
      "Epoch 0[4593/17270] Time:0.229, Train Loss:0.6965312361717224\n",
      "Epoch 0[4594/17270] Time:0.236, Train Loss:1.0946437120437622\n",
      "Epoch 0[4595/17270] Time:0.242, Train Loss:0.45804649591445923\n",
      "Epoch 0[4596/17270] Time:0.221, Train Loss:0.7488683462142944\n",
      "Epoch 0[4597/17270] Time:0.236, Train Loss:0.7059751749038696\n",
      "Epoch 0[4598/17270] Time:0.235, Train Loss:0.5855371356010437\n",
      "Epoch 0[4599/17270] Time:0.232, Train Loss:0.7096676826477051\n",
      "Epoch 0[4600/17270] Time:0.236, Train Loss:0.6549365520477295\n",
      "Epoch 0[4601/17270] Time:0.228, Train Loss:0.8621838688850403\n",
      "Epoch 0[4602/17270] Time:0.237, Train Loss:0.9774314761161804\n",
      "Epoch 0[4603/17270] Time:0.229, Train Loss:0.5575138330459595\n",
      "Epoch 0[4604/17270] Time:0.238, Train Loss:0.6041572690010071\n",
      "Epoch 0[4605/17270] Time:0.239, Train Loss:0.4108515977859497\n",
      "Epoch 0[4606/17270] Time:0.237, Train Loss:0.36698538064956665\n",
      "Epoch 0[4607/17270] Time:0.236, Train Loss:0.5069642663002014\n",
      "Epoch 0[4608/17270] Time:0.23, Train Loss:1.0419737100601196\n",
      "Epoch 0[4609/17270] Time:0.229, Train Loss:0.32257071137428284\n",
      "Epoch 0[4610/17270] Time:0.238, Train Loss:0.45626264810562134\n",
      "Epoch 0[4611/17270] Time:0.231, Train Loss:0.589512825012207\n",
      "Epoch 0[4612/17270] Time:0.238, Train Loss:0.7045941948890686\n",
      "Epoch 0[4613/17270] Time:0.238, Train Loss:0.4736163020133972\n",
      "Epoch 0[4614/17270] Time:0.226, Train Loss:1.2282196283340454\n",
      "Epoch 0[4615/17270] Time:0.231, Train Loss:0.31834936141967773\n",
      "Epoch 0[4616/17270] Time:0.238, Train Loss:0.6548016667366028\n",
      "Epoch 0[4617/17270] Time:0.232, Train Loss:0.6140353083610535\n",
      "Epoch 0[4618/17270] Time:0.226, Train Loss:0.6866478323936462\n",
      "Epoch 0[4619/17270] Time:0.23, Train Loss:0.7247627377510071\n",
      "Epoch 0[4620/17270] Time:0.238, Train Loss:0.6550471782684326\n",
      "Epoch 0[4621/17270] Time:0.229, Train Loss:0.25239717960357666\n",
      "Epoch 0[4622/17270] Time:0.233, Train Loss:0.858942449092865\n",
      "Epoch 0[4623/17270] Time:0.227, Train Loss:0.6445090770721436\n",
      "Epoch 0[4624/17270] Time:0.24, Train Loss:0.7622566819190979\n",
      "Epoch 0[4625/17270] Time:0.238, Train Loss:0.6005740761756897\n",
      "Epoch 0[4626/17270] Time:0.236, Train Loss:0.44597816467285156\n",
      "Epoch 0[4627/17270] Time:0.236, Train Loss:0.34139755368232727\n",
      "Epoch 0[4628/17270] Time:0.237, Train Loss:0.5127201080322266\n",
      "Epoch 0[4629/17270] Time:0.239, Train Loss:0.42636606097221375\n",
      "Epoch 0[4630/17270] Time:0.239, Train Loss:0.45756134390830994\n",
      "Epoch 0[4631/17270] Time:0.229, Train Loss:0.7523902058601379\n",
      "Epoch 0[4632/17270] Time:0.236, Train Loss:0.2858324348926544\n",
      "Epoch 0[4633/17270] Time:0.248, Train Loss:1.1481835842132568\n",
      "Epoch 0[4634/17270] Time:0.248, Train Loss:0.5235924124717712\n",
      "Epoch 0[4635/17270] Time:0.237, Train Loss:0.6457822322845459\n",
      "Epoch 0[4636/17270] Time:0.231, Train Loss:0.5371920466423035\n",
      "Epoch 0[4637/17270] Time:0.238, Train Loss:0.5463163256645203\n",
      "Epoch 0[4638/17270] Time:0.229, Train Loss:1.0297002792358398\n",
      "Epoch 0[4639/17270] Time:0.24, Train Loss:0.4468548595905304\n",
      "Epoch 0[4640/17270] Time:0.245, Train Loss:0.9232521653175354\n",
      "Epoch 0[4641/17270] Time:0.224, Train Loss:0.9160174131393433\n",
      "Epoch 0[4642/17270] Time:0.246, Train Loss:1.0013039112091064\n",
      "Epoch 0[4643/17270] Time:0.231, Train Loss:0.36093464493751526\n",
      "Epoch 0[4644/17270] Time:0.232, Train Loss:0.9003168344497681\n",
      "Epoch 0[4645/17270] Time:0.235, Train Loss:0.5015232563018799\n",
      "Epoch 0[4646/17270] Time:0.231, Train Loss:0.5928043723106384\n",
      "Epoch 0[4647/17270] Time:0.237, Train Loss:0.36726123094558716\n",
      "Epoch 0[4648/17270] Time:0.235, Train Loss:0.4709482789039612\n",
      "Epoch 0[4649/17270] Time:0.224, Train Loss:0.8553590774536133\n",
      "Epoch 0[4650/17270] Time:0.235, Train Loss:0.4779492914676666\n",
      "Epoch 0[4651/17270] Time:0.245, Train Loss:0.5616366863250732\n",
      "Epoch 0[4652/17270] Time:0.234, Train Loss:0.6274587512016296\n",
      "Epoch 0[4653/17270] Time:0.227, Train Loss:0.6120131611824036\n",
      "Epoch 0[4654/17270] Time:0.228, Train Loss:0.5437597036361694\n",
      "Epoch 0[4655/17270] Time:0.231, Train Loss:0.5403070449829102\n",
      "Epoch 0[4656/17270] Time:0.25, Train Loss:0.6616374850273132\n",
      "Epoch 0[4657/17270] Time:0.24, Train Loss:1.1980892419815063\n",
      "Epoch 0[4658/17270] Time:0.232, Train Loss:0.5886979699134827\n",
      "Epoch 0[4659/17270] Time:0.23, Train Loss:1.3952950239181519\n",
      "Epoch 0[4660/17270] Time:0.244, Train Loss:0.5880959630012512\n",
      "Epoch 0[4661/17270] Time:0.239, Train Loss:0.8561527729034424\n",
      "Epoch 0[4662/17270] Time:0.228, Train Loss:0.6555690765380859\n",
      "Epoch 0[4663/17270] Time:0.228, Train Loss:0.9989498257637024\n",
      "Epoch 0[4664/17270] Time:0.233, Train Loss:0.9662533402442932\n",
      "Epoch 0[4665/17270] Time:0.239, Train Loss:0.6772347092628479\n",
      "Epoch 0[4666/17270] Time:0.233, Train Loss:0.578216016292572\n",
      "Epoch 0[4667/17270] Time:0.233, Train Loss:0.29954907298088074\n",
      "Epoch 0[4668/17270] Time:0.231, Train Loss:0.6900018453598022\n",
      "Epoch 0[4669/17270] Time:0.244, Train Loss:0.6243206858634949\n",
      "Epoch 0[4670/17270] Time:0.242, Train Loss:0.3558894991874695\n",
      "Epoch 0[4671/17270] Time:0.228, Train Loss:1.2712081670761108\n",
      "Epoch 0[4672/17270] Time:0.233, Train Loss:0.7880825400352478\n",
      "Epoch 0[4673/17270] Time:0.232, Train Loss:0.4690811038017273\n",
      "Epoch 0[4674/17270] Time:0.241, Train Loss:0.40929096937179565\n",
      "Epoch 0[4675/17270] Time:0.23, Train Loss:0.7136895060539246\n",
      "Epoch 0[4676/17270] Time:0.232, Train Loss:0.4924573004245758\n",
      "Epoch 0[4677/17270] Time:0.238, Train Loss:0.5109124779701233\n",
      "Epoch 0[4678/17270] Time:0.233, Train Loss:0.437762588262558\n",
      "Epoch 0[4679/17270] Time:0.235, Train Loss:0.738021194934845\n",
      "Epoch 0[4680/17270] Time:0.231, Train Loss:0.4250012934207916\n",
      "Epoch 0[4681/17270] Time:0.231, Train Loss:0.37551450729370117\n",
      "Epoch 0[4682/17270] Time:0.23, Train Loss:0.7266656756401062\n",
      "Epoch 0[4683/17270] Time:0.226, Train Loss:0.5085783004760742\n",
      "Epoch 0[4684/17270] Time:0.232, Train Loss:0.45582717657089233\n",
      "Epoch 0[4685/17270] Time:0.255, Train Loss:0.5246874094009399\n",
      "Epoch 0[4686/17270] Time:0.242, Train Loss:0.7900163531303406\n",
      "Epoch 0[4687/17270] Time:0.241, Train Loss:0.8830083012580872\n",
      "Epoch 0[4688/17270] Time:0.237, Train Loss:0.4346149265766144\n",
      "Epoch 0[4689/17270] Time:0.238, Train Loss:0.45210590958595276\n",
      "Epoch 0[4690/17270] Time:0.24, Train Loss:0.4157463312149048\n",
      "Epoch 0[4691/17270] Time:0.234, Train Loss:0.5216565728187561\n",
      "Epoch 0[4692/17270] Time:0.235, Train Loss:0.5039195418357849\n",
      "Epoch 0[4693/17270] Time:0.237, Train Loss:0.6500999927520752\n",
      "Epoch 0[4694/17270] Time:0.236, Train Loss:0.487808495759964\n",
      "Epoch 0[4695/17270] Time:0.227, Train Loss:1.430612564086914\n",
      "Epoch 0[4696/17270] Time:0.231, Train Loss:0.3842772841453552\n",
      "Epoch 0[4697/17270] Time:0.237, Train Loss:0.5681169629096985\n",
      "Epoch 0[4698/17270] Time:0.226, Train Loss:0.31719401478767395\n",
      "Epoch 0[4699/17270] Time:0.225, Train Loss:0.5862342715263367\n",
      "Epoch 0[4700/17270] Time:0.232, Train Loss:0.7244966626167297\n",
      "Epoch 0[4701/17270] Time:0.238, Train Loss:0.6500824093818665\n",
      "Epoch 0[4702/17270] Time:0.24, Train Loss:1.181008219718933\n",
      "Epoch 0[4703/17270] Time:0.237, Train Loss:0.7946358323097229\n",
      "Epoch 0[4704/17270] Time:0.227, Train Loss:0.7440899610519409\n",
      "Epoch 0[4705/17270] Time:0.236, Train Loss:0.47992128133773804\n",
      "Epoch 0[4706/17270] Time:0.238, Train Loss:1.2335968017578125\n",
      "Epoch 0[4707/17270] Time:0.236, Train Loss:0.44367536902427673\n",
      "Epoch 0[4708/17270] Time:0.231, Train Loss:0.39290982484817505\n",
      "Epoch 0[4709/17270] Time:0.228, Train Loss:0.9597985148429871\n",
      "Epoch 0[4710/17270] Time:0.238, Train Loss:0.8165385723114014\n",
      "Epoch 0[4711/17270] Time:0.239, Train Loss:0.3824838697910309\n",
      "Epoch 0[4712/17270] Time:0.229, Train Loss:0.4299045205116272\n",
      "Epoch 0[4713/17270] Time:0.231, Train Loss:0.5210195779800415\n",
      "Epoch 0[4714/17270] Time:0.229, Train Loss:0.8309728503227234\n",
      "Epoch 0[4715/17270] Time:0.239, Train Loss:1.4480546712875366\n",
      "Epoch 0[4716/17270] Time:0.238, Train Loss:0.41992199420928955\n",
      "Epoch 0[4717/17270] Time:0.227, Train Loss:0.6397631168365479\n",
      "Epoch 0[4718/17270] Time:0.227, Train Loss:0.41647636890411377\n",
      "Epoch 0[4719/17270] Time:0.233, Train Loss:0.4192390739917755\n",
      "Epoch 0[4720/17270] Time:0.231, Train Loss:0.7028083205223083\n",
      "Epoch 0[4721/17270] Time:0.238, Train Loss:0.5555521845817566\n",
      "Epoch 0[4722/17270] Time:0.23, Train Loss:0.6756488084793091\n",
      "Epoch 0[4723/17270] Time:0.231, Train Loss:0.6805974245071411\n",
      "Epoch 0[4724/17270] Time:0.236, Train Loss:0.8562224507331848\n",
      "Epoch 0[4725/17270] Time:0.23, Train Loss:0.3978501260280609\n",
      "Epoch 0[4726/17270] Time:0.232, Train Loss:0.45031124353408813\n",
      "Epoch 0[4727/17270] Time:0.247, Train Loss:1.305877447128296\n",
      "Epoch 0[4728/17270] Time:0.231, Train Loss:0.6738956570625305\n",
      "Epoch 0[4729/17270] Time:0.233, Train Loss:0.7046360373497009\n",
      "Epoch 0[4730/17270] Time:0.229, Train Loss:0.5105352401733398\n",
      "Epoch 0[4731/17270] Time:0.231, Train Loss:0.5604525208473206\n",
      "Epoch 0[4732/17270] Time:0.234, Train Loss:0.4475296437740326\n",
      "Epoch 0[4733/17270] Time:0.226, Train Loss:0.4969428777694702\n",
      "Epoch 0[4734/17270] Time:0.229, Train Loss:0.3805789649486542\n",
      "Epoch 0[4735/17270] Time:0.234, Train Loss:0.5157310962677002\n",
      "Epoch 0[4736/17270] Time:0.239, Train Loss:1.2152771949768066\n",
      "Epoch 0[4737/17270] Time:0.228, Train Loss:0.4497211277484894\n",
      "Epoch 0[4738/17270] Time:0.231, Train Loss:0.6742737889289856\n",
      "Epoch 0[4739/17270] Time:0.235, Train Loss:0.5577667951583862\n",
      "Epoch 0[4740/17270] Time:0.233, Train Loss:0.35862380266189575\n",
      "Epoch 0[4741/17270] Time:0.24, Train Loss:0.6111086010932922\n",
      "Epoch 0[4742/17270] Time:0.23, Train Loss:0.4587255120277405\n",
      "Epoch 0[4743/17270] Time:0.226, Train Loss:0.6408616900444031\n",
      "Epoch 0[4744/17270] Time:0.236, Train Loss:0.7850958108901978\n",
      "Epoch 0[4745/17270] Time:0.236, Train Loss:0.40497025847435\n",
      "Epoch 0[4746/17270] Time:0.229, Train Loss:0.5212424397468567\n",
      "Epoch 0[4747/17270] Time:0.232, Train Loss:0.5161314010620117\n",
      "Epoch 0[4748/17270] Time:0.237, Train Loss:0.49774718284606934\n",
      "Epoch 0[4749/17270] Time:0.236, Train Loss:0.5562112331390381\n",
      "Epoch 0[4750/17270] Time:0.236, Train Loss:0.85552579164505\n",
      "Epoch 0[4751/17270] Time:0.23, Train Loss:0.7636117935180664\n",
      "Epoch 0[4752/17270] Time:0.236, Train Loss:0.4526154696941376\n",
      "Epoch 0[4753/17270] Time:0.225, Train Loss:0.9609754681587219\n",
      "Epoch 0[4754/17270] Time:0.238, Train Loss:0.6304875612258911\n",
      "Epoch 0[4755/17270] Time:0.227, Train Loss:0.9527009725570679\n",
      "Epoch 0[4756/17270] Time:0.231, Train Loss:0.8101686239242554\n",
      "Epoch 0[4757/17270] Time:0.231, Train Loss:0.634597659111023\n",
      "Epoch 0[4758/17270] Time:0.242, Train Loss:0.316349595785141\n",
      "Epoch 0[4759/17270] Time:0.235, Train Loss:1.0667402744293213\n",
      "Epoch 0[4760/17270] Time:0.225, Train Loss:0.5316199660301208\n",
      "Epoch 0[4761/17270] Time:0.237, Train Loss:0.41505080461502075\n",
      "Epoch 0[4762/17270] Time:0.236, Train Loss:0.43975022435188293\n",
      "Epoch 0[4763/17270] Time:0.243, Train Loss:0.6786420941352844\n",
      "Epoch 0[4764/17270] Time:0.239, Train Loss:0.5100507140159607\n",
      "Epoch 0[4765/17270] Time:0.243, Train Loss:0.47128573060035706\n",
      "Epoch 0[4766/17270] Time:0.23, Train Loss:0.5012879967689514\n",
      "Epoch 0[4767/17270] Time:0.235, Train Loss:0.3090342879295349\n",
      "Epoch 0[4768/17270] Time:0.232, Train Loss:0.4532027542591095\n",
      "Epoch 0[4769/17270] Time:0.247, Train Loss:0.9184689521789551\n",
      "Epoch 0[4770/17270] Time:0.233, Train Loss:1.1114963293075562\n",
      "Epoch 0[4771/17270] Time:0.235, Train Loss:0.5566126108169556\n",
      "Epoch 0[4772/17270] Time:0.231, Train Loss:0.6076943278312683\n",
      "Epoch 0[4773/17270] Time:0.234, Train Loss:0.697717010974884\n",
      "Epoch 0[4774/17270] Time:0.238, Train Loss:0.709531843662262\n",
      "Epoch 0[4775/17270] Time:0.239, Train Loss:0.9438115954399109\n",
      "Epoch 0[4776/17270] Time:0.23, Train Loss:0.6000934839248657\n",
      "Epoch 0[4777/17270] Time:0.234, Train Loss:0.5904750823974609\n",
      "Epoch 0[4778/17270] Time:0.228, Train Loss:0.36860400438308716\n",
      "Epoch 0[4779/17270] Time:0.233, Train Loss:0.48617780208587646\n",
      "Epoch 0[4780/17270] Time:0.231, Train Loss:0.6704069375991821\n",
      "Epoch 0[4781/17270] Time:0.23, Train Loss:1.1595627069473267\n",
      "Epoch 0[4782/17270] Time:0.234, Train Loss:0.8903214335441589\n",
      "Epoch 0[4783/17270] Time:0.236, Train Loss:0.5561999082565308\n",
      "Epoch 0[4784/17270] Time:0.238, Train Loss:0.6986032724380493\n",
      "Epoch 0[4785/17270] Time:0.228, Train Loss:0.912368893623352\n",
      "Epoch 0[4786/17270] Time:0.23, Train Loss:0.6667563319206238\n",
      "Epoch 0[4787/17270] Time:0.23, Train Loss:0.6998828649520874\n",
      "Epoch 0[4788/17270] Time:0.231, Train Loss:0.5879871845245361\n",
      "Epoch 0[4789/17270] Time:0.232, Train Loss:0.892703115940094\n",
      "Epoch 0[4790/17270] Time:0.231, Train Loss:0.6312873363494873\n",
      "Epoch 0[4791/17270] Time:0.229, Train Loss:0.595614492893219\n",
      "Epoch 0[4792/17270] Time:0.231, Train Loss:0.5220176577568054\n",
      "Epoch 0[4793/17270] Time:0.246, Train Loss:0.5478929877281189\n",
      "Epoch 0[4794/17270] Time:0.224, Train Loss:0.6273683905601501\n",
      "Epoch 0[4795/17270] Time:0.224, Train Loss:0.6251821517944336\n",
      "Epoch 0[4796/17270] Time:0.226, Train Loss:0.3659347891807556\n",
      "Epoch 0[4797/17270] Time:0.229, Train Loss:0.655992329120636\n",
      "Epoch 0[4798/17270] Time:0.238, Train Loss:0.5791710615158081\n",
      "Epoch 0[4799/17270] Time:0.229, Train Loss:0.9999281764030457\n",
      "Epoch 0[4800/17270] Time:0.233, Train Loss:0.5746820569038391\n",
      "Epoch 0[4801/17270] Time:0.227, Train Loss:0.30165034532546997\n",
      "Epoch 0[4802/17270] Time:0.237, Train Loss:0.6115654110908508\n",
      "Epoch 0[4803/17270] Time:0.239, Train Loss:0.6896464824676514\n",
      "Epoch 0[4804/17270] Time:0.227, Train Loss:1.4606109857559204\n",
      "Epoch 0[4805/17270] Time:0.236, Train Loss:0.5446527004241943\n",
      "Epoch 0[4806/17270] Time:0.234, Train Loss:0.6816093325614929\n",
      "Epoch 0[4807/17270] Time:0.227, Train Loss:0.8184288740158081\n",
      "Epoch 0[4808/17270] Time:0.237, Train Loss:0.5275830030441284\n",
      "Epoch 0[4809/17270] Time:0.231, Train Loss:1.3105578422546387\n",
      "Epoch 0[4810/17270] Time:0.232, Train Loss:0.9408817887306213\n",
      "Epoch 0[4811/17270] Time:0.235, Train Loss:0.5038229823112488\n",
      "Epoch 0[4812/17270] Time:0.237, Train Loss:0.5343692302703857\n",
      "Epoch 0[4813/17270] Time:0.23, Train Loss:0.5887178778648376\n",
      "Epoch 0[4814/17270] Time:0.233, Train Loss:0.6184040307998657\n",
      "Epoch 0[4815/17270] Time:0.227, Train Loss:0.6095011234283447\n",
      "Epoch 0[4816/17270] Time:0.238, Train Loss:0.5630398988723755\n",
      "Epoch 0[4817/17270] Time:0.231, Train Loss:0.6004469990730286\n",
      "Epoch 0[4818/17270] Time:0.231, Train Loss:1.081723928451538\n",
      "Epoch 0[4819/17270] Time:0.233, Train Loss:0.4587751626968384\n",
      "Epoch 0[4820/17270] Time:0.232, Train Loss:0.8557698726654053\n",
      "Epoch 0[4821/17270] Time:0.234, Train Loss:0.319027304649353\n",
      "Epoch 0[4822/17270] Time:0.233, Train Loss:0.5964425802230835\n",
      "Epoch 0[4823/17270] Time:0.238, Train Loss:0.5799193382263184\n",
      "Epoch 0[4824/17270] Time:0.232, Train Loss:0.5866851806640625\n",
      "Epoch 0[4825/17270] Time:0.25, Train Loss:1.1631247997283936\n",
      "Epoch 0[4826/17270] Time:0.23, Train Loss:0.7294079065322876\n",
      "Epoch 0[4827/17270] Time:0.234, Train Loss:0.6133154630661011\n",
      "Epoch 0[4828/17270] Time:0.229, Train Loss:0.9908322095870972\n",
      "Epoch 0[4829/17270] Time:0.238, Train Loss:0.824099063873291\n",
      "Epoch 0[4830/17270] Time:0.233, Train Loss:0.5141140222549438\n",
      "Epoch 0[4831/17270] Time:0.234, Train Loss:0.6288353204727173\n",
      "Epoch 0[4832/17270] Time:0.236, Train Loss:0.494746595621109\n",
      "Epoch 0[4833/17270] Time:0.239, Train Loss:0.38526761531829834\n",
      "Epoch 0[4834/17270] Time:0.229, Train Loss:0.6929164528846741\n",
      "Epoch 0[4835/17270] Time:0.234, Train Loss:0.9107860326766968\n",
      "Epoch 0[4836/17270] Time:0.231, Train Loss:0.43879151344299316\n",
      "Epoch 0[4837/17270] Time:0.232, Train Loss:0.7590903043746948\n",
      "Epoch 0[4838/17270] Time:0.228, Train Loss:0.5090869069099426\n",
      "Epoch 0[4839/17270] Time:0.232, Train Loss:0.3008568584918976\n",
      "Epoch 0[4840/17270] Time:0.231, Train Loss:0.5102682113647461\n",
      "Epoch 0[4841/17270] Time:0.234, Train Loss:1.4113572835922241\n",
      "Epoch 0[4842/17270] Time:0.237, Train Loss:0.910423219203949\n",
      "Epoch 0[4843/17270] Time:0.231, Train Loss:0.5595585703849792\n",
      "Epoch 0[4844/17270] Time:0.232, Train Loss:0.462714821100235\n",
      "Epoch 0[4845/17270] Time:0.232, Train Loss:0.6454590559005737\n",
      "Epoch 0[4846/17270] Time:0.226, Train Loss:0.757194995880127\n",
      "Epoch 0[4847/17270] Time:0.23, Train Loss:0.7516093850135803\n",
      "Epoch 0[4848/17270] Time:0.234, Train Loss:0.9645542502403259\n",
      "Epoch 0[4849/17270] Time:0.234, Train Loss:0.6021274924278259\n",
      "Epoch 0[4850/17270] Time:0.236, Train Loss:1.5547317266464233\n",
      "Epoch 0[4851/17270] Time:0.24, Train Loss:0.6643120646476746\n",
      "Epoch 0[4852/17270] Time:0.236, Train Loss:0.39917558431625366\n",
      "Epoch 0[4853/17270] Time:0.231, Train Loss:0.7174776792526245\n",
      "Epoch 0[4854/17270] Time:0.233, Train Loss:1.2807718515396118\n",
      "Epoch 0[4855/17270] Time:0.237, Train Loss:0.44772157073020935\n",
      "Epoch 0[4856/17270] Time:0.237, Train Loss:0.568647563457489\n",
      "Epoch 0[4857/17270] Time:0.232, Train Loss:0.8464900851249695\n",
      "Epoch 0[4858/17270] Time:0.233, Train Loss:0.6465358138084412\n",
      "Epoch 0[4859/17270] Time:0.228, Train Loss:0.5179241299629211\n",
      "Epoch 0[4860/17270] Time:0.254, Train Loss:0.724425196647644\n",
      "Epoch 0[4861/17270] Time:0.225, Train Loss:0.6312178373336792\n",
      "Epoch 0[4862/17270] Time:0.237, Train Loss:0.5960550308227539\n",
      "Epoch 0[4863/17270] Time:0.243, Train Loss:1.3753552436828613\n",
      "Epoch 0[4864/17270] Time:0.236, Train Loss:0.6226511597633362\n",
      "Epoch 0[4865/17270] Time:0.238, Train Loss:0.7330542206764221\n",
      "Epoch 0[4866/17270] Time:0.229, Train Loss:0.45501503348350525\n",
      "Epoch 0[4867/17270] Time:0.232, Train Loss:0.4906567335128784\n",
      "Epoch 0[4868/17270] Time:0.232, Train Loss:0.7215784788131714\n",
      "Epoch 0[4869/17270] Time:0.236, Train Loss:0.5173399448394775\n",
      "Epoch 0[4870/17270] Time:0.232, Train Loss:1.1167999505996704\n",
      "Epoch 0[4871/17270] Time:0.233, Train Loss:0.6120389103889465\n",
      "Epoch 0[4872/17270] Time:0.238, Train Loss:0.6215519309043884\n",
      "Epoch 0[4873/17270] Time:0.229, Train Loss:0.7476066946983337\n",
      "Epoch 0[4874/17270] Time:0.239, Train Loss:0.6066924929618835\n",
      "Epoch 0[4875/17270] Time:0.232, Train Loss:1.0570429563522339\n",
      "Epoch 0[4876/17270] Time:0.232, Train Loss:0.4517286419868469\n",
      "Epoch 0[4877/17270] Time:0.237, Train Loss:0.5415661334991455\n",
      "Epoch 0[4878/17270] Time:0.234, Train Loss:0.5737854838371277\n",
      "Epoch 0[4879/17270] Time:0.236, Train Loss:0.9959859848022461\n",
      "Epoch 0[4880/17270] Time:0.234, Train Loss:0.6937007308006287\n",
      "Epoch 0[4881/17270] Time:0.238, Train Loss:0.8082436919212341\n",
      "Epoch 0[4882/17270] Time:0.232, Train Loss:0.8803744316101074\n",
      "Epoch 0[4883/17270] Time:0.231, Train Loss:0.5462056994438171\n",
      "Epoch 0[4884/17270] Time:0.23, Train Loss:0.407981276512146\n",
      "Epoch 0[4885/17270] Time:0.232, Train Loss:0.538470983505249\n",
      "Epoch 0[4886/17270] Time:0.231, Train Loss:0.9454792141914368\n",
      "Epoch 0[4887/17270] Time:0.238, Train Loss:0.578893780708313\n",
      "Epoch 0[4888/17270] Time:0.23, Train Loss:0.506325364112854\n",
      "Epoch 0[4889/17270] Time:0.23, Train Loss:0.700831413269043\n",
      "Epoch 0[4890/17270] Time:0.234, Train Loss:0.34751272201538086\n",
      "Epoch 0[4891/17270] Time:0.244, Train Loss:0.570906937122345\n",
      "Epoch 0[4892/17270] Time:0.233, Train Loss:0.5110145211219788\n",
      "Epoch 0[4893/17270] Time:0.237, Train Loss:0.611794650554657\n",
      "Epoch 0[4894/17270] Time:0.237, Train Loss:0.7585964202880859\n",
      "Epoch 0[4895/17270] Time:0.23, Train Loss:1.4336912631988525\n",
      "Epoch 0[4896/17270] Time:0.233, Train Loss:0.35078051686286926\n",
      "Epoch 0[4897/17270] Time:0.233, Train Loss:0.6885021924972534\n",
      "Epoch 0[4898/17270] Time:0.232, Train Loss:0.6082669496536255\n",
      "Epoch 0[4899/17270] Time:0.232, Train Loss:1.0243600606918335\n",
      "Epoch 0[4900/17270] Time:0.236, Train Loss:0.5027767419815063\n",
      "Epoch 0[4901/17270] Time:0.231, Train Loss:0.6646947860717773\n",
      "Epoch 0[4902/17270] Time:0.235, Train Loss:0.6405438780784607\n",
      "Epoch 0[4903/17270] Time:0.23, Train Loss:0.5327494144439697\n",
      "Epoch 0[4904/17270] Time:0.237, Train Loss:0.6556575298309326\n",
      "Epoch 0[4905/17270] Time:0.227, Train Loss:0.5974050760269165\n",
      "Epoch 0[4906/17270] Time:0.229, Train Loss:0.5426221489906311\n",
      "Epoch 0[4907/17270] Time:0.233, Train Loss:0.6831337213516235\n",
      "Epoch 0[4908/17270] Time:0.236, Train Loss:0.9255073070526123\n",
      "Epoch 0[4909/17270] Time:0.243, Train Loss:1.261824607849121\n",
      "Epoch 0[4910/17270] Time:0.238, Train Loss:0.5676863789558411\n",
      "Epoch 0[4911/17270] Time:0.237, Train Loss:0.3316652178764343\n",
      "Epoch 0[4912/17270] Time:0.247, Train Loss:0.5829741954803467\n",
      "Epoch 0[4913/17270] Time:0.239, Train Loss:0.8125654458999634\n",
      "Epoch 0[4914/17270] Time:0.228, Train Loss:0.7807040810585022\n",
      "Epoch 0[4915/17270] Time:0.227, Train Loss:0.696697473526001\n",
      "Epoch 0[4916/17270] Time:0.224, Train Loss:1.1342982053756714\n",
      "Epoch 0[4917/17270] Time:0.237, Train Loss:0.586489737033844\n",
      "Epoch 0[4918/17270] Time:0.237, Train Loss:0.7668650150299072\n",
      "Epoch 0[4919/17270] Time:0.232, Train Loss:0.68239426612854\n",
      "Epoch 0[4920/17270] Time:0.237, Train Loss:0.7709599733352661\n",
      "Epoch 0[4921/17270] Time:0.238, Train Loss:0.629078209400177\n",
      "Epoch 0[4922/17270] Time:0.246, Train Loss:0.5793017745018005\n",
      "Epoch 0[4923/17270] Time:0.237, Train Loss:0.46950867772102356\n",
      "Epoch 0[4924/17270] Time:0.232, Train Loss:0.4352250397205353\n",
      "Epoch 0[4925/17270] Time:0.243, Train Loss:0.646421492099762\n",
      "Epoch 0[4926/17270] Time:0.223, Train Loss:0.3815862238407135\n",
      "Epoch 0[4927/17270] Time:0.249, Train Loss:0.39256396889686584\n",
      "Epoch 0[4928/17270] Time:0.235, Train Loss:0.48976415395736694\n",
      "Epoch 0[4929/17270] Time:0.239, Train Loss:0.64081871509552\n",
      "Epoch 0[4930/17270] Time:0.241, Train Loss:0.7863374352455139\n",
      "Epoch 0[4931/17270] Time:0.239, Train Loss:0.6469956040382385\n",
      "Epoch 0[4932/17270] Time:0.238, Train Loss:0.6629823446273804\n",
      "Epoch 0[4933/17270] Time:0.239, Train Loss:0.45791614055633545\n",
      "Epoch 0[4934/17270] Time:0.236, Train Loss:0.7550998330116272\n",
      "Epoch 0[4935/17270] Time:0.237, Train Loss:0.7637848854064941\n",
      "Epoch 0[4936/17270] Time:0.236, Train Loss:0.8672627210617065\n",
      "Epoch 0[4937/17270] Time:0.24, Train Loss:0.5732495784759521\n",
      "Epoch 0[4938/17270] Time:0.234, Train Loss:0.5058355331420898\n",
      "Epoch 0[4939/17270] Time:0.243, Train Loss:0.7415647506713867\n",
      "Epoch 0[4940/17270] Time:0.228, Train Loss:0.9717093110084534\n",
      "Epoch 0[4941/17270] Time:0.239, Train Loss:0.571193277835846\n",
      "Epoch 0[4942/17270] Time:0.23, Train Loss:0.4913052022457123\n",
      "Epoch 0[4943/17270] Time:0.246, Train Loss:0.7563415169715881\n",
      "Epoch 0[4944/17270] Time:0.236, Train Loss:0.7828061580657959\n",
      "Epoch 0[4945/17270] Time:0.236, Train Loss:0.4065752923488617\n",
      "Epoch 0[4946/17270] Time:0.244, Train Loss:0.5118538737297058\n",
      "Epoch 0[4947/17270] Time:0.23, Train Loss:1.2265288829803467\n",
      "Epoch 0[4948/17270] Time:0.229, Train Loss:1.226749300956726\n",
      "Epoch 0[4949/17270] Time:0.24, Train Loss:0.6756424903869629\n",
      "Epoch 0[4950/17270] Time:0.241, Train Loss:0.6937612295150757\n",
      "Epoch 0[4951/17270] Time:0.235, Train Loss:0.6275641918182373\n",
      "Epoch 0[4952/17270] Time:0.224, Train Loss:0.5488064289093018\n",
      "Epoch 0[4953/17270] Time:0.253, Train Loss:0.6589686274528503\n",
      "Epoch 0[4954/17270] Time:0.234, Train Loss:0.6169118881225586\n",
      "Epoch 0[4955/17270] Time:0.234, Train Loss:0.36539286375045776\n",
      "Epoch 0[4956/17270] Time:0.234, Train Loss:0.3700460195541382\n",
      "Epoch 0[4957/17270] Time:0.229, Train Loss:0.6841884255409241\n",
      "Epoch 0[4958/17270] Time:0.228, Train Loss:0.6533451676368713\n",
      "Epoch 0[4959/17270] Time:0.24, Train Loss:0.4199921488761902\n",
      "Epoch 0[4960/17270] Time:0.243, Train Loss:0.6088112592697144\n",
      "Epoch 0[4961/17270] Time:0.238, Train Loss:0.6524455547332764\n",
      "Epoch 0[4962/17270] Time:0.231, Train Loss:0.6797850131988525\n",
      "Epoch 0[4963/17270] Time:0.236, Train Loss:0.9060499668121338\n",
      "Epoch 0[4964/17270] Time:0.229, Train Loss:0.5877955555915833\n",
      "Epoch 0[4965/17270] Time:0.243, Train Loss:0.4927721321582794\n",
      "Epoch 0[4966/17270] Time:0.237, Train Loss:0.8851905465126038\n",
      "Epoch 0[4967/17270] Time:0.232, Train Loss:0.4262228012084961\n",
      "Epoch 0[4968/17270] Time:0.236, Train Loss:0.5031688809394836\n",
      "Epoch 0[4969/17270] Time:0.238, Train Loss:0.6728371977806091\n",
      "Epoch 0[4970/17270] Time:0.237, Train Loss:0.46933287382125854\n",
      "Epoch 0[4971/17270] Time:0.237, Train Loss:1.3219852447509766\n",
      "Epoch 0[4972/17270] Time:0.239, Train Loss:0.4770866632461548\n",
      "Epoch 0[4973/17270] Time:0.226, Train Loss:0.47160103917121887\n",
      "Epoch 0[4974/17270] Time:0.232, Train Loss:0.6415560245513916\n",
      "Epoch 0[4975/17270] Time:0.233, Train Loss:0.4955653250217438\n",
      "Epoch 0[4976/17270] Time:0.229, Train Loss:0.3977816104888916\n",
      "Epoch 0[4977/17270] Time:0.238, Train Loss:0.6922740340232849\n",
      "Epoch 0[4978/17270] Time:0.23, Train Loss:0.515079140663147\n",
      "Epoch 0[4979/17270] Time:0.232, Train Loss:0.6703904271125793\n",
      "Epoch 0[4980/17270] Time:0.239, Train Loss:0.6325109004974365\n",
      "Epoch 0[4981/17270] Time:0.236, Train Loss:0.5154556632041931\n",
      "Epoch 0[4982/17270] Time:0.234, Train Loss:0.8213496804237366\n",
      "Epoch 0[4983/17270] Time:0.237, Train Loss:0.9257645010948181\n",
      "Epoch 0[4984/17270] Time:0.239, Train Loss:0.7395293116569519\n",
      "Epoch 0[4985/17270] Time:0.239, Train Loss:0.5115910768508911\n",
      "Epoch 0[4986/17270] Time:0.224, Train Loss:0.3160528242588043\n",
      "Epoch 0[4987/17270] Time:0.236, Train Loss:0.3283388018608093\n",
      "Epoch 0[4988/17270] Time:0.225, Train Loss:0.5689960718154907\n",
      "Epoch 0[4989/17270] Time:0.238, Train Loss:0.5132185816764832\n",
      "Epoch 0[4990/17270] Time:0.238, Train Loss:0.4083092510700226\n",
      "Epoch 0[4991/17270] Time:0.232, Train Loss:0.3791501820087433\n",
      "Epoch 0[4992/17270] Time:0.236, Train Loss:0.6007864475250244\n",
      "Epoch 0[4993/17270] Time:0.237, Train Loss:0.5120009183883667\n",
      "Epoch 0[4994/17270] Time:0.228, Train Loss:0.5590950846672058\n",
      "Epoch 0[4995/17270] Time:0.237, Train Loss:0.8096975684165955\n",
      "Epoch 0[4996/17270] Time:0.239, Train Loss:0.8039860129356384\n",
      "Epoch 0[4997/17270] Time:0.23, Train Loss:0.5328208804130554\n",
      "Epoch 0[4998/17270] Time:0.237, Train Loss:0.5431955456733704\n",
      "Epoch 0[4999/17270] Time:0.238, Train Loss:1.4676769971847534\n",
      "Epoch 0[5000/17270] Time:0.238, Train Loss:0.6717779636383057\n",
      "Epoch 0[5001/17270] Time:0.237, Train Loss:0.5593763589859009\n",
      "Epoch 0[5002/17270] Time:0.23, Train Loss:0.5645195245742798\n",
      "Epoch 0[5003/17270] Time:0.238, Train Loss:0.5693156123161316\n",
      "Epoch 0[5004/17270] Time:0.237, Train Loss:0.47162380814552307\n",
      "Epoch 0[5005/17270] Time:0.237, Train Loss:0.690399706363678\n",
      "Epoch 0[5006/17270] Time:0.238, Train Loss:0.5386790037155151\n",
      "Epoch 0[5007/17270] Time:0.228, Train Loss:0.7278690338134766\n",
      "Epoch 0[5008/17270] Time:0.229, Train Loss:0.42404821515083313\n",
      "Epoch 0[5009/17270] Time:0.237, Train Loss:0.6676639318466187\n",
      "Epoch 0[5010/17270] Time:0.229, Train Loss:0.416158527135849\n",
      "Epoch 0[5011/17270] Time:0.226, Train Loss:0.48718178272247314\n",
      "Epoch 0[5012/17270] Time:0.228, Train Loss:0.6472213864326477\n",
      "Epoch 0[5013/17270] Time:0.226, Train Loss:0.5519754886627197\n",
      "Epoch 0[5014/17270] Time:0.231, Train Loss:0.4024694859981537\n",
      "Epoch 0[5015/17270] Time:0.238, Train Loss:0.45766234397888184\n",
      "Epoch 0[5016/17270] Time:0.237, Train Loss:1.4592427015304565\n",
      "Epoch 0[5017/17270] Time:0.229, Train Loss:1.1406017541885376\n",
      "Epoch 0[5018/17270] Time:0.234, Train Loss:0.4458765983581543\n",
      "Epoch 0[5019/17270] Time:0.229, Train Loss:0.506236732006073\n",
      "Epoch 0[5020/17270] Time:0.236, Train Loss:0.2760314345359802\n",
      "Epoch 0[5021/17270] Time:0.228, Train Loss:1.367473840713501\n",
      "Epoch 0[5022/17270] Time:0.232, Train Loss:0.28561776876449585\n",
      "Epoch 0[5023/17270] Time:0.236, Train Loss:0.6558218002319336\n",
      "Epoch 0[5024/17270] Time:0.239, Train Loss:0.9921233654022217\n",
      "Epoch 0[5025/17270] Time:0.242, Train Loss:0.42954230308532715\n",
      "Epoch 0[5026/17270] Time:0.241, Train Loss:0.8340135812759399\n",
      "Epoch 0[5027/17270] Time:0.231, Train Loss:0.7363948225975037\n",
      "Epoch 0[5028/17270] Time:0.23, Train Loss:0.3640286326408386\n",
      "Epoch 0[5029/17270] Time:0.236, Train Loss:0.8419334888458252\n",
      "Epoch 0[5030/17270] Time:0.24, Train Loss:0.4644927680492401\n",
      "Epoch 0[5031/17270] Time:0.228, Train Loss:0.5875844955444336\n",
      "Epoch 0[5032/17270] Time:0.235, Train Loss:0.6213226318359375\n",
      "Epoch 0[5033/17270] Time:0.23, Train Loss:1.0082776546478271\n",
      "Epoch 0[5034/17270] Time:0.227, Train Loss:0.5819177627563477\n",
      "Epoch 0[5035/17270] Time:0.229, Train Loss:0.9313914179801941\n",
      "Epoch 0[5036/17270] Time:0.236, Train Loss:0.5221655368804932\n",
      "Epoch 0[5037/17270] Time:0.235, Train Loss:0.5187756419181824\n",
      "Epoch 0[5038/17270] Time:0.241, Train Loss:0.29338765144348145\n",
      "Epoch 0[5039/17270] Time:0.229, Train Loss:0.6079900860786438\n",
      "Epoch 0[5040/17270] Time:0.238, Train Loss:0.8285218477249146\n",
      "Epoch 0[5041/17270] Time:0.238, Train Loss:0.4937559962272644\n",
      "Epoch 0[5042/17270] Time:0.232, Train Loss:0.46931636333465576\n",
      "Epoch 0[5043/17270] Time:0.229, Train Loss:0.5363375544548035\n",
      "Epoch 0[5044/17270] Time:0.229, Train Loss:0.6047216057777405\n",
      "Epoch 0[5045/17270] Time:0.238, Train Loss:0.5748426914215088\n",
      "Epoch 0[5046/17270] Time:0.231, Train Loss:0.5912916660308838\n",
      "Epoch 0[5047/17270] Time:0.231, Train Loss:1.0431324243545532\n",
      "Epoch 0[5048/17270] Time:0.232, Train Loss:0.34473541378974915\n",
      "Epoch 0[5049/17270] Time:0.233, Train Loss:0.47958996891975403\n",
      "Epoch 0[5050/17270] Time:0.238, Train Loss:0.7772606015205383\n",
      "Epoch 0[5051/17270] Time:0.236, Train Loss:0.5832561254501343\n",
      "Epoch 0[5052/17270] Time:0.24, Train Loss:0.5617592930793762\n",
      "Epoch 0[5053/17270] Time:0.219, Train Loss:0.41679248213768005\n",
      "Epoch 0[5054/17270] Time:0.239, Train Loss:0.37301236391067505\n",
      "Epoch 0[5055/17270] Time:0.231, Train Loss:0.7922471165657043\n",
      "Epoch 0[5056/17270] Time:0.231, Train Loss:0.416798859834671\n",
      "Epoch 0[5057/17270] Time:0.233, Train Loss:0.6080693602561951\n",
      "Epoch 0[5058/17270] Time:0.236, Train Loss:0.4051641821861267\n",
      "Epoch 0[5059/17270] Time:0.238, Train Loss:0.5944182276725769\n",
      "Epoch 0[5060/17270] Time:0.231, Train Loss:0.2728832960128784\n",
      "Epoch 0[5061/17270] Time:0.23, Train Loss:0.5755487680435181\n",
      "Epoch 0[5062/17270] Time:0.233, Train Loss:0.9363940954208374\n",
      "Epoch 0[5063/17270] Time:0.228, Train Loss:0.4225430190563202\n",
      "Epoch 0[5064/17270] Time:0.236, Train Loss:0.5454471111297607\n",
      "Epoch 0[5065/17270] Time:0.237, Train Loss:0.5558363199234009\n",
      "Epoch 0[5066/17270] Time:0.238, Train Loss:0.8686831593513489\n",
      "Epoch 0[5067/17270] Time:0.23, Train Loss:0.6648145914077759\n",
      "Epoch 0[5068/17270] Time:0.247, Train Loss:0.5456089973449707\n",
      "Epoch 0[5069/17270] Time:0.231, Train Loss:0.48831331729888916\n",
      "Epoch 0[5070/17270] Time:0.238, Train Loss:0.6643670201301575\n",
      "Epoch 0[5071/17270] Time:0.235, Train Loss:0.32699474692344666\n",
      "Epoch 0[5072/17270] Time:0.229, Train Loss:0.4893118143081665\n",
      "Epoch 0[5073/17270] Time:0.232, Train Loss:1.880592942237854\n",
      "Epoch 0[5074/17270] Time:0.239, Train Loss:0.5222733616828918\n",
      "Epoch 0[5075/17270] Time:0.236, Train Loss:0.5250632762908936\n",
      "Epoch 0[5076/17270] Time:0.238, Train Loss:0.7049871683120728\n",
      "Epoch 0[5077/17270] Time:0.238, Train Loss:0.3863111734390259\n",
      "Epoch 0[5078/17270] Time:0.239, Train Loss:0.513020932674408\n",
      "Epoch 0[5079/17270] Time:0.232, Train Loss:0.4292256534099579\n",
      "Epoch 0[5080/17270] Time:0.232, Train Loss:0.600787341594696\n",
      "Epoch 0[5081/17270] Time:0.265, Train Loss:0.4721584916114807\n",
      "Epoch 0[5082/17270] Time:0.233, Train Loss:1.3441784381866455\n",
      "Epoch 0[5083/17270] Time:0.234, Train Loss:1.0354621410369873\n",
      "Epoch 0[5084/17270] Time:0.232, Train Loss:0.5691513419151306\n",
      "Epoch 0[5085/17270] Time:0.233, Train Loss:0.7822198271751404\n",
      "Epoch 0[5086/17270] Time:0.238, Train Loss:0.475885808467865\n",
      "Epoch 0[5087/17270] Time:0.234, Train Loss:0.45694994926452637\n",
      "Epoch 0[5088/17270] Time:0.25, Train Loss:0.500099241733551\n",
      "Epoch 0[5089/17270] Time:0.235, Train Loss:0.7659324407577515\n",
      "Epoch 0[5090/17270] Time:0.23, Train Loss:0.47534066438674927\n",
      "Epoch 0[5091/17270] Time:0.229, Train Loss:1.1039406061172485\n",
      "Epoch 0[5092/17270] Time:0.24, Train Loss:0.6305344104766846\n",
      "Epoch 0[5093/17270] Time:0.234, Train Loss:0.4078046977519989\n",
      "Epoch 0[5094/17270] Time:0.24, Train Loss:0.5284249782562256\n",
      "Epoch 0[5095/17270] Time:0.24, Train Loss:0.569942831993103\n",
      "Epoch 0[5096/17270] Time:0.239, Train Loss:0.43921610713005066\n",
      "Epoch 0[5097/17270] Time:0.231, Train Loss:0.3505252003669739\n",
      "Epoch 0[5098/17270] Time:0.231, Train Loss:0.40858176350593567\n",
      "Epoch 0[5099/17270] Time:0.241, Train Loss:0.55922931432724\n",
      "Epoch 0[5100/17270] Time:0.237, Train Loss:0.657842218875885\n",
      "Epoch 0[5101/17270] Time:0.231, Train Loss:0.6157342791557312\n",
      "Epoch 0[5102/17270] Time:0.234, Train Loss:0.7000564336776733\n",
      "Epoch 0[5103/17270] Time:0.229, Train Loss:0.8736647367477417\n",
      "Epoch 0[5104/17270] Time:0.238, Train Loss:0.5501415729522705\n",
      "Epoch 0[5105/17270] Time:0.238, Train Loss:0.6705129742622375\n",
      "Epoch 0[5106/17270] Time:0.231, Train Loss:0.3730979263782501\n",
      "Epoch 0[5107/17270] Time:0.229, Train Loss:0.5009176135063171\n",
      "Epoch 0[5108/17270] Time:0.23, Train Loss:0.33767202496528625\n",
      "Epoch 0[5109/17270] Time:0.242, Train Loss:0.7084128856658936\n",
      "Epoch 0[5110/17270] Time:0.231, Train Loss:0.5826961994171143\n",
      "Epoch 0[5111/17270] Time:0.235, Train Loss:0.769207775592804\n",
      "Epoch 0[5112/17270] Time:0.229, Train Loss:1.1478006839752197\n",
      "Epoch 0[5113/17270] Time:0.237, Train Loss:0.4433683156967163\n",
      "Epoch 0[5114/17270] Time:0.238, Train Loss:0.387317419052124\n",
      "Epoch 0[5115/17270] Time:0.237, Train Loss:0.44605696201324463\n",
      "Epoch 0[5116/17270] Time:0.231, Train Loss:0.5151762962341309\n",
      "Epoch 0[5117/17270] Time:0.231, Train Loss:0.2670965790748596\n",
      "Epoch 0[5118/17270] Time:0.234, Train Loss:0.4095453917980194\n",
      "Epoch 0[5119/17270] Time:0.233, Train Loss:1.2086997032165527\n",
      "Epoch 0[5120/17270] Time:0.234, Train Loss:0.9475441575050354\n",
      "Epoch 0[5121/17270] Time:0.237, Train Loss:0.7482984662055969\n",
      "Epoch 0[5122/17270] Time:0.237, Train Loss:0.7278353571891785\n",
      "Epoch 0[5123/17270] Time:0.23, Train Loss:0.5535877346992493\n",
      "Epoch 0[5124/17270] Time:0.232, Train Loss:1.0801657438278198\n",
      "Epoch 0[5125/17270] Time:0.237, Train Loss:0.6961386799812317\n",
      "Epoch 0[5126/17270] Time:0.232, Train Loss:0.9418343901634216\n",
      "Epoch 0[5127/17270] Time:0.228, Train Loss:0.3500461280345917\n",
      "Epoch 0[5128/17270] Time:0.239, Train Loss:0.5957632660865784\n",
      "Epoch 0[5129/17270] Time:0.236, Train Loss:0.5555925369262695\n",
      "Epoch 0[5130/17270] Time:0.231, Train Loss:0.6723880171775818\n",
      "Epoch 0[5131/17270] Time:0.229, Train Loss:1.1273568868637085\n",
      "Epoch 0[5132/17270] Time:0.237, Train Loss:0.6949146389961243\n",
      "Epoch 0[5133/17270] Time:0.237, Train Loss:0.5986962914466858\n",
      "Epoch 0[5134/17270] Time:0.237, Train Loss:0.5162543654441833\n",
      "Epoch 0[5135/17270] Time:0.231, Train Loss:0.24998018145561218\n",
      "Epoch 0[5136/17270] Time:0.23, Train Loss:0.6728342175483704\n",
      "Epoch 0[5137/17270] Time:0.237, Train Loss:0.4947458505630493\n",
      "Epoch 0[5138/17270] Time:0.238, Train Loss:0.4062196612358093\n",
      "Epoch 0[5139/17270] Time:0.251, Train Loss:0.6526240110397339\n",
      "Epoch 0[5140/17270] Time:0.231, Train Loss:0.6584015488624573\n",
      "Epoch 0[5141/17270] Time:0.233, Train Loss:0.7830396890640259\n",
      "Epoch 0[5142/17270] Time:0.236, Train Loss:0.4212031662464142\n",
      "Epoch 0[5143/17270] Time:0.236, Train Loss:0.9265995025634766\n",
      "Epoch 0[5144/17270] Time:0.239, Train Loss:0.6284115314483643\n",
      "Epoch 0[5145/17270] Time:0.233, Train Loss:0.44358569383621216\n",
      "Epoch 0[5146/17270] Time:0.238, Train Loss:1.1379141807556152\n",
      "Epoch 0[5147/17270] Time:0.234, Train Loss:0.5500735640525818\n",
      "Epoch 0[5148/17270] Time:0.228, Train Loss:0.40873828530311584\n",
      "Epoch 0[5149/17270] Time:0.239, Train Loss:0.4904446601867676\n",
      "Epoch 0[5150/17270] Time:0.235, Train Loss:0.6475663185119629\n",
      "Epoch 0[5151/17270] Time:0.234, Train Loss:0.5331152677536011\n",
      "Epoch 0[5152/17270] Time:0.237, Train Loss:0.6479895114898682\n",
      "Epoch 0[5153/17270] Time:0.237, Train Loss:0.9645919799804688\n",
      "Epoch 0[5154/17270] Time:0.237, Train Loss:0.5853719115257263\n",
      "Epoch 0[5155/17270] Time:0.236, Train Loss:0.4716474711894989\n",
      "Epoch 0[5156/17270] Time:0.231, Train Loss:0.9802364110946655\n",
      "Epoch 0[5157/17270] Time:0.237, Train Loss:0.47827738523483276\n",
      "Epoch 0[5158/17270] Time:0.231, Train Loss:0.44419145584106445\n",
      "Epoch 0[5159/17270] Time:0.224, Train Loss:0.6303620338439941\n",
      "Epoch 0[5160/17270] Time:0.231, Train Loss:0.48815637826919556\n",
      "Epoch 0[5161/17270] Time:0.229, Train Loss:0.6628901958465576\n",
      "Epoch 0[5162/17270] Time:0.229, Train Loss:1.2851641178131104\n",
      "Epoch 0[5163/17270] Time:0.237, Train Loss:0.5706839561462402\n",
      "Epoch 0[5164/17270] Time:0.239, Train Loss:0.5180124044418335\n",
      "Epoch 0[5165/17270] Time:0.232, Train Loss:0.6328397393226624\n",
      "Epoch 0[5166/17270] Time:0.23, Train Loss:0.6105482578277588\n",
      "Epoch 0[5167/17270] Time:0.232, Train Loss:0.7977715730667114\n",
      "Epoch 0[5168/17270] Time:0.231, Train Loss:0.5268378257751465\n",
      "Epoch 0[5169/17270] Time:0.239, Train Loss:1.0018718242645264\n",
      "Epoch 0[5170/17270] Time:0.23, Train Loss:0.5348739624023438\n",
      "Epoch 0[5171/17270] Time:0.23, Train Loss:0.4100550711154938\n",
      "Epoch 0[5172/17270] Time:0.248, Train Loss:0.838624119758606\n",
      "Epoch 0[5173/17270] Time:0.226, Train Loss:0.6528233885765076\n",
      "Epoch 0[5174/17270] Time:0.238, Train Loss:0.5587731003761292\n",
      "Epoch 0[5175/17270] Time:0.236, Train Loss:0.6935840845108032\n",
      "Epoch 0[5176/17270] Time:0.237, Train Loss:0.7510649561882019\n",
      "Epoch 0[5177/17270] Time:0.228, Train Loss:0.5951430201530457\n",
      "Epoch 0[5178/17270] Time:0.238, Train Loss:0.3787987530231476\n",
      "Epoch 0[5179/17270] Time:0.237, Train Loss:0.5237532258033752\n",
      "Epoch 0[5180/17270] Time:0.235, Train Loss:1.0359210968017578\n",
      "Epoch 0[5181/17270] Time:0.234, Train Loss:0.49232983589172363\n",
      "Epoch 0[5182/17270] Time:0.237, Train Loss:0.42884716391563416\n",
      "Epoch 0[5183/17270] Time:0.233, Train Loss:0.44280970096588135\n",
      "Epoch 0[5184/17270] Time:0.227, Train Loss:0.46871188282966614\n",
      "Epoch 0[5185/17270] Time:0.223, Train Loss:0.41603943705558777\n",
      "Epoch 0[5186/17270] Time:0.23, Train Loss:0.630443274974823\n",
      "Epoch 0[5187/17270] Time:0.23, Train Loss:0.561506450176239\n",
      "Epoch 0[5188/17270] Time:0.237, Train Loss:0.4266144633293152\n",
      "Epoch 0[5189/17270] Time:0.231, Train Loss:0.5545171499252319\n",
      "Epoch 0[5190/17270] Time:0.239, Train Loss:0.5775539875030518\n",
      "Epoch 0[5191/17270] Time:0.232, Train Loss:0.873939037322998\n",
      "Epoch 0[5192/17270] Time:0.238, Train Loss:0.3602631986141205\n",
      "Epoch 0[5193/17270] Time:0.239, Train Loss:0.510107159614563\n",
      "Epoch 0[5194/17270] Time:0.227, Train Loss:0.6043612360954285\n",
      "Epoch 0[5195/17270] Time:0.249, Train Loss:0.475689560174942\n",
      "Epoch 0[5196/17270] Time:0.242, Train Loss:0.6871441602706909\n",
      "Epoch 0[5197/17270] Time:0.237, Train Loss:0.35543838143348694\n",
      "Epoch 0[5198/17270] Time:0.231, Train Loss:1.2958989143371582\n",
      "Epoch 0[5199/17270] Time:0.235, Train Loss:0.802104115486145\n",
      "Epoch 0[5200/17270] Time:0.226, Train Loss:0.8712599873542786\n",
      "Epoch 0[5201/17270] Time:0.236, Train Loss:0.8943381309509277\n",
      "Epoch 0[5202/17270] Time:0.237, Train Loss:0.3657347857952118\n",
      "Epoch 0[5203/17270] Time:0.235, Train Loss:1.017625331878662\n",
      "Epoch 0[5204/17270] Time:0.237, Train Loss:0.6365287899971008\n",
      "Epoch 0[5205/17270] Time:0.238, Train Loss:0.4757002592086792\n",
      "Epoch 0[5206/17270] Time:0.228, Train Loss:0.8202989101409912\n",
      "Epoch 0[5207/17270] Time:0.234, Train Loss:0.7695577144622803\n",
      "Epoch 0[5208/17270] Time:0.24, Train Loss:0.30934083461761475\n",
      "Epoch 0[5209/17270] Time:0.23, Train Loss:0.49726542830467224\n",
      "Epoch 0[5210/17270] Time:0.238, Train Loss:0.7423915863037109\n",
      "Epoch 0[5211/17270] Time:0.238, Train Loss:0.3770741820335388\n",
      "Epoch 0[5212/17270] Time:0.231, Train Loss:0.6126670837402344\n",
      "Epoch 0[5213/17270] Time:0.239, Train Loss:0.6066228747367859\n",
      "Epoch 0[5214/17270] Time:0.231, Train Loss:0.5122766494750977\n",
      "Epoch 0[5215/17270] Time:0.233, Train Loss:0.7197146415710449\n",
      "Epoch 0[5216/17270] Time:0.227, Train Loss:0.948284924030304\n",
      "Epoch 0[5217/17270] Time:0.229, Train Loss:0.9512091279029846\n",
      "Epoch 0[5218/17270] Time:0.228, Train Loss:1.2930290699005127\n",
      "Epoch 0[5219/17270] Time:0.226, Train Loss:0.4734693169593811\n",
      "Epoch 0[5220/17270] Time:0.238, Train Loss:0.7950344681739807\n",
      "Epoch 0[5221/17270] Time:0.249, Train Loss:0.6867843270301819\n",
      "Epoch 0[5222/17270] Time:0.241, Train Loss:1.640834927558899\n",
      "Epoch 0[5223/17270] Time:0.235, Train Loss:0.4540797173976898\n",
      "Epoch 0[5224/17270] Time:0.238, Train Loss:0.6291022300720215\n",
      "Epoch 0[5225/17270] Time:0.235, Train Loss:0.4885188937187195\n",
      "Epoch 0[5226/17270] Time:0.244, Train Loss:0.5545278787612915\n",
      "Epoch 0[5227/17270] Time:0.229, Train Loss:0.7002264857292175\n",
      "Epoch 0[5228/17270] Time:0.226, Train Loss:0.4384978115558624\n",
      "Epoch 0[5229/17270] Time:0.241, Train Loss:0.5874563455581665\n",
      "Epoch 0[5230/17270] Time:0.232, Train Loss:0.5852367877960205\n",
      "Epoch 0[5231/17270] Time:0.246, Train Loss:0.673634946346283\n",
      "Epoch 0[5232/17270] Time:0.234, Train Loss:0.8345059752464294\n",
      "Epoch 0[5233/17270] Time:0.244, Train Loss:0.2945706844329834\n",
      "Epoch 0[5234/17270] Time:0.234, Train Loss:0.6160576939582825\n",
      "Epoch 0[5235/17270] Time:0.249, Train Loss:0.5246228575706482\n",
      "Epoch 0[5236/17270] Time:0.238, Train Loss:0.6433194875717163\n",
      "Epoch 0[5237/17270] Time:0.242, Train Loss:0.41690441966056824\n",
      "Epoch 0[5238/17270] Time:0.248, Train Loss:0.5421313047409058\n",
      "Epoch 0[5239/17270] Time:0.245, Train Loss:0.6230716705322266\n",
      "Epoch 0[5240/17270] Time:0.235, Train Loss:1.1891720294952393\n",
      "Epoch 0[5241/17270] Time:0.24, Train Loss:0.7964147925376892\n",
      "Epoch 0[5242/17270] Time:0.234, Train Loss:0.6649613976478577\n",
      "Epoch 0[5243/17270] Time:0.236, Train Loss:0.44394171237945557\n",
      "Epoch 0[5244/17270] Time:0.239, Train Loss:0.6196768879890442\n",
      "Epoch 0[5245/17270] Time:0.237, Train Loss:0.640206515789032\n",
      "Epoch 0[5246/17270] Time:0.231, Train Loss:0.5431073904037476\n",
      "Epoch 0[5247/17270] Time:0.236, Train Loss:0.9403052926063538\n",
      "Epoch 0[5248/17270] Time:0.231, Train Loss:0.4830693304538727\n",
      "Epoch 0[5249/17270] Time:0.231, Train Loss:0.41750410199165344\n",
      "Epoch 0[5250/17270] Time:0.233, Train Loss:0.4062662422657013\n",
      "Epoch 0[5251/17270] Time:0.232, Train Loss:1.329332709312439\n",
      "Epoch 0[5252/17270] Time:0.231, Train Loss:0.43692925572395325\n",
      "Epoch 0[5253/17270] Time:0.239, Train Loss:0.5806416273117065\n",
      "Epoch 0[5254/17270] Time:0.234, Train Loss:0.4228183925151825\n",
      "Epoch 0[5255/17270] Time:0.23, Train Loss:0.5773416757583618\n",
      "Epoch 0[5256/17270] Time:0.233, Train Loss:0.3780772387981415\n",
      "Epoch 0[5257/17270] Time:0.237, Train Loss:1.080723524093628\n",
      "Epoch 0[5258/17270] Time:0.238, Train Loss:0.4606444239616394\n",
      "Epoch 0[5259/17270] Time:0.23, Train Loss:0.4079980254173279\n",
      "Epoch 0[5260/17270] Time:0.23, Train Loss:0.6282380223274231\n",
      "Epoch 0[5261/17270] Time:0.237, Train Loss:0.8248884081840515\n",
      "Epoch 0[5262/17270] Time:0.237, Train Loss:0.4372475743293762\n",
      "Epoch 0[5263/17270] Time:0.236, Train Loss:0.5697933435440063\n",
      "Epoch 0[5264/17270] Time:0.237, Train Loss:0.42245155572891235\n",
      "Epoch 0[5265/17270] Time:0.225, Train Loss:0.3756607174873352\n",
      "Epoch 0[5266/17270] Time:0.227, Train Loss:0.2911314070224762\n",
      "Epoch 0[5267/17270] Time:0.237, Train Loss:1.313262701034546\n",
      "Epoch 0[5268/17270] Time:0.238, Train Loss:0.42366331815719604\n",
      "Epoch 0[5269/17270] Time:0.238, Train Loss:0.6371331810951233\n",
      "Epoch 0[5270/17270] Time:0.238, Train Loss:1.018947958946228\n",
      "Epoch 0[5271/17270] Time:0.238, Train Loss:0.5761017799377441\n",
      "Epoch 0[5272/17270] Time:0.236, Train Loss:0.6038359999656677\n",
      "Epoch 0[5273/17270] Time:0.239, Train Loss:0.42106494307518005\n",
      "Epoch 0[5274/17270] Time:0.246, Train Loss:0.313970685005188\n",
      "Epoch 0[5275/17270] Time:0.238, Train Loss:0.49606820940971375\n",
      "Epoch 0[5276/17270] Time:0.231, Train Loss:0.2928650975227356\n",
      "Epoch 0[5277/17270] Time:0.241, Train Loss:0.9336400628089905\n",
      "Epoch 0[5278/17270] Time:0.231, Train Loss:1.0140695571899414\n",
      "Epoch 0[5279/17270] Time:0.236, Train Loss:0.7115344405174255\n",
      "Epoch 0[5280/17270] Time:0.231, Train Loss:0.7844232320785522\n",
      "Epoch 0[5281/17270] Time:0.226, Train Loss:0.5566880106925964\n",
      "Epoch 0[5282/17270] Time:0.245, Train Loss:0.49514028429985046\n",
      "Epoch 0[5283/17270] Time:0.235, Train Loss:0.8601009249687195\n",
      "Epoch 0[5284/17270] Time:0.239, Train Loss:0.6196381449699402\n",
      "Epoch 0[5285/17270] Time:0.231, Train Loss:0.4672062397003174\n",
      "Epoch 0[5286/17270] Time:0.227, Train Loss:0.9400286078453064\n",
      "Epoch 0[5287/17270] Time:0.23, Train Loss:0.4693491756916046\n",
      "Epoch 0[5288/17270] Time:0.239, Train Loss:0.2987641990184784\n",
      "Epoch 0[5289/17270] Time:0.235, Train Loss:0.45756545662879944\n",
      "Epoch 0[5290/17270] Time:0.241, Train Loss:0.6799002885818481\n",
      "Epoch 0[5291/17270] Time:0.236, Train Loss:0.537914514541626\n",
      "Epoch 0[5292/17270] Time:0.24, Train Loss:0.7716939449310303\n",
      "Epoch 0[5293/17270] Time:0.232, Train Loss:0.5528298616409302\n",
      "Epoch 0[5294/17270] Time:0.235, Train Loss:0.4318145215511322\n",
      "Epoch 0[5295/17270] Time:0.234, Train Loss:0.8582169413566589\n",
      "Epoch 0[5296/17270] Time:0.249, Train Loss:0.4095894396305084\n",
      "Epoch 0[5297/17270] Time:0.228, Train Loss:0.47056812047958374\n",
      "Epoch 0[5298/17270] Time:0.246, Train Loss:1.6938132047653198\n",
      "Epoch 0[5299/17270] Time:0.246, Train Loss:0.5079496502876282\n",
      "Epoch 0[5300/17270] Time:0.231, Train Loss:0.623757541179657\n",
      "Epoch 0[5301/17270] Time:0.24, Train Loss:0.510796070098877\n",
      "Epoch 0[5302/17270] Time:0.241, Train Loss:0.24135185778141022\n",
      "Epoch 0[5303/17270] Time:0.23, Train Loss:0.4970709979534149\n",
      "Epoch 0[5304/17270] Time:0.241, Train Loss:0.7720277309417725\n",
      "Epoch 0[5305/17270] Time:0.238, Train Loss:0.9497755169868469\n",
      "Epoch 0[5306/17270] Time:0.233, Train Loss:0.8193551301956177\n",
      "Epoch 0[5307/17270] Time:0.235, Train Loss:0.5323552489280701\n",
      "Epoch 0[5308/17270] Time:0.232, Train Loss:0.6696056723594666\n",
      "Epoch 0[5309/17270] Time:0.235, Train Loss:0.952173113822937\n",
      "Epoch 0[5310/17270] Time:0.252, Train Loss:0.5847628116607666\n",
      "Epoch 0[5311/17270] Time:0.223, Train Loss:0.49703991413116455\n",
      "Epoch 0[5312/17270] Time:0.231, Train Loss:0.7544152140617371\n",
      "Epoch 0[5313/17270] Time:0.241, Train Loss:0.587419867515564\n",
      "Epoch 0[5314/17270] Time:0.236, Train Loss:0.6430137753486633\n",
      "Epoch 0[5315/17270] Time:0.235, Train Loss:0.5134848952293396\n",
      "Epoch 0[5316/17270] Time:0.233, Train Loss:0.4389473497867584\n",
      "Epoch 0[5317/17270] Time:0.226, Train Loss:0.7108403444290161\n",
      "Epoch 0[5318/17270] Time:0.233, Train Loss:0.738813042640686\n",
      "Epoch 0[5319/17270] Time:0.23, Train Loss:1.0225759744644165\n",
      "Epoch 0[5320/17270] Time:0.239, Train Loss:0.3203721046447754\n",
      "Epoch 0[5321/17270] Time:0.237, Train Loss:0.4844624698162079\n",
      "Epoch 0[5322/17270] Time:0.23, Train Loss:0.7902794480323792\n",
      "Epoch 0[5323/17270] Time:0.237, Train Loss:0.7665925621986389\n",
      "Epoch 0[5324/17270] Time:0.231, Train Loss:0.6820113658905029\n",
      "Epoch 0[5325/17270] Time:0.231, Train Loss:0.5925853252410889\n",
      "Epoch 0[5326/17270] Time:0.238, Train Loss:1.1361093521118164\n",
      "Epoch 0[5327/17270] Time:0.245, Train Loss:0.6068722009658813\n",
      "Epoch 0[5328/17270] Time:0.229, Train Loss:0.8810973167419434\n",
      "Epoch 0[5329/17270] Time:0.238, Train Loss:1.204480767250061\n",
      "Epoch 0[5330/17270] Time:0.234, Train Loss:0.37010470032691956\n",
      "Epoch 0[5331/17270] Time:0.238, Train Loss:0.6124812960624695\n",
      "Epoch 0[5332/17270] Time:0.236, Train Loss:0.4773351550102234\n",
      "Epoch 0[5333/17270] Time:0.236, Train Loss:0.5287613272666931\n",
      "Epoch 0[5334/17270] Time:0.24, Train Loss:0.6072090864181519\n",
      "Epoch 0[5335/17270] Time:0.238, Train Loss:0.45588889718055725\n",
      "Epoch 0[5336/17270] Time:0.232, Train Loss:0.4181102216243744\n",
      "Epoch 0[5337/17270] Time:0.235, Train Loss:1.342063069343567\n",
      "Epoch 0[5338/17270] Time:0.23, Train Loss:0.5900885462760925\n",
      "Epoch 0[5339/17270] Time:0.233, Train Loss:1.5037842988967896\n",
      "Epoch 0[5340/17270] Time:0.23, Train Loss:0.5740227699279785\n",
      "Epoch 0[5341/17270] Time:0.238, Train Loss:0.3359833061695099\n",
      "Epoch 0[5342/17270] Time:0.23, Train Loss:0.4975363314151764\n",
      "Epoch 0[5343/17270] Time:0.229, Train Loss:0.5037223100662231\n",
      "Epoch 0[5344/17270] Time:0.238, Train Loss:0.47203654050827026\n",
      "Epoch 0[5345/17270] Time:0.232, Train Loss:0.3948433995246887\n",
      "Epoch 0[5346/17270] Time:0.229, Train Loss:0.5592172145843506\n",
      "Epoch 0[5347/17270] Time:0.232, Train Loss:1.2498708963394165\n",
      "Epoch 0[5348/17270] Time:0.239, Train Loss:0.5880746841430664\n",
      "Epoch 0[5349/17270] Time:0.232, Train Loss:0.3894985318183899\n",
      "Epoch 0[5350/17270] Time:0.227, Train Loss:0.5562253594398499\n",
      "Epoch 0[5351/17270] Time:0.231, Train Loss:0.4796930253505707\n",
      "Epoch 0[5352/17270] Time:0.24, Train Loss:0.5392852425575256\n",
      "Epoch 0[5353/17270] Time:0.227, Train Loss:0.8657426238059998\n",
      "Epoch 0[5354/17270] Time:0.238, Train Loss:0.6507471799850464\n",
      "Epoch 0[5355/17270] Time:0.234, Train Loss:0.64617520570755\n",
      "Epoch 0[5356/17270] Time:0.229, Train Loss:0.33979880809783936\n",
      "Epoch 0[5357/17270] Time:0.237, Train Loss:0.6424018144607544\n",
      "Epoch 0[5358/17270] Time:0.224, Train Loss:0.4762793481349945\n",
      "Epoch 0[5359/17270] Time:0.234, Train Loss:0.7742932438850403\n",
      "Epoch 0[5360/17270] Time:0.242, Train Loss:0.2918121814727783\n",
      "Epoch 0[5361/17270] Time:0.233, Train Loss:0.6284458041191101\n",
      "Epoch 0[5362/17270] Time:0.239, Train Loss:0.9652714729309082\n",
      "Epoch 0[5363/17270] Time:0.226, Train Loss:0.6678216457366943\n",
      "Epoch 0[5364/17270] Time:0.238, Train Loss:0.7912412285804749\n",
      "Epoch 0[5365/17270] Time:0.235, Train Loss:0.8192005753517151\n",
      "Epoch 0[5366/17270] Time:0.237, Train Loss:0.7149848937988281\n",
      "Epoch 0[5367/17270] Time:0.228, Train Loss:0.4862191379070282\n",
      "Epoch 0[5368/17270] Time:0.23, Train Loss:0.5055973529815674\n",
      "Epoch 0[5369/17270] Time:0.244, Train Loss:0.3084486424922943\n",
      "Epoch 0[5370/17270] Time:0.224, Train Loss:0.43586084246635437\n",
      "Epoch 0[5371/17270] Time:0.225, Train Loss:0.6059069037437439\n",
      "Epoch 0[5372/17270] Time:0.233, Train Loss:0.5346325635910034\n",
      "Epoch 0[5373/17270] Time:0.232, Train Loss:0.8495397567749023\n",
      "Epoch 0[5374/17270] Time:0.231, Train Loss:0.6482774615287781\n",
      "Epoch 0[5375/17270] Time:0.232, Train Loss:0.521335780620575\n",
      "Epoch 0[5376/17270] Time:0.224, Train Loss:0.4514469802379608\n",
      "Epoch 0[5377/17270] Time:0.245, Train Loss:0.5347081422805786\n",
      "Epoch 0[5378/17270] Time:0.236, Train Loss:1.2990086078643799\n",
      "Epoch 0[5379/17270] Time:0.232, Train Loss:0.39737847447395325\n",
      "Epoch 0[5380/17270] Time:0.234, Train Loss:0.538241446018219\n",
      "Epoch 0[5381/17270] Time:0.245, Train Loss:0.3301037847995758\n",
      "Epoch 0[5382/17270] Time:0.226, Train Loss:0.7692627310752869\n",
      "Epoch 0[5383/17270] Time:0.235, Train Loss:0.3643602430820465\n",
      "Epoch 0[5384/17270] Time:0.24, Train Loss:0.3659089207649231\n",
      "Epoch 0[5385/17270] Time:0.222, Train Loss:0.462771475315094\n",
      "Epoch 0[5386/17270] Time:0.239, Train Loss:0.90389484167099\n",
      "Epoch 0[5387/17270] Time:0.234, Train Loss:0.9445967674255371\n",
      "Epoch 0[5388/17270] Time:0.229, Train Loss:0.5806303024291992\n",
      "Epoch 0[5389/17270] Time:0.231, Train Loss:0.33743828535079956\n",
      "Epoch 0[5390/17270] Time:0.228, Train Loss:0.6791238784790039\n",
      "Epoch 0[5391/17270] Time:0.237, Train Loss:0.639289140701294\n",
      "Epoch 0[5392/17270] Time:0.237, Train Loss:0.64401775598526\n",
      "Epoch 0[5393/17270] Time:0.23, Train Loss:0.5079851746559143\n",
      "Epoch 0[5394/17270] Time:0.246, Train Loss:0.8280094861984253\n",
      "Epoch 0[5395/17270] Time:0.241, Train Loss:0.5269070863723755\n",
      "Epoch 0[5396/17270] Time:0.256, Train Loss:0.753346860408783\n",
      "Epoch 0[5397/17270] Time:0.235, Train Loss:0.45707544684410095\n",
      "Epoch 0[5398/17270] Time:0.242, Train Loss:0.6871737837791443\n",
      "Epoch 0[5399/17270] Time:0.225, Train Loss:0.7516288757324219\n",
      "Epoch 0[5400/17270] Time:0.232, Train Loss:0.6100343465805054\n",
      "Epoch 0[5401/17270] Time:0.232, Train Loss:0.500697910785675\n",
      "Epoch 0[5402/17270] Time:0.243, Train Loss:0.578383207321167\n",
      "Epoch 0[5403/17270] Time:0.248, Train Loss:0.4818858206272125\n",
      "Epoch 0[5404/17270] Time:0.232, Train Loss:0.6002901792526245\n",
      "Epoch 0[5405/17270] Time:0.237, Train Loss:0.6857689023017883\n",
      "Epoch 0[5406/17270] Time:0.244, Train Loss:0.7315686345100403\n",
      "Epoch 0[5407/17270] Time:0.225, Train Loss:0.4940474331378937\n",
      "Epoch 0[5408/17270] Time:0.234, Train Loss:0.7064459919929504\n",
      "Epoch 0[5409/17270] Time:0.23, Train Loss:0.8710299730300903\n",
      "Epoch 0[5410/17270] Time:0.237, Train Loss:0.3730957508087158\n",
      "Epoch 0[5411/17270] Time:0.245, Train Loss:0.35987865924835205\n",
      "Epoch 0[5412/17270] Time:0.236, Train Loss:0.5433395504951477\n",
      "Epoch 0[5413/17270] Time:0.242, Train Loss:0.5465946197509766\n",
      "Epoch 0[5414/17270] Time:0.221, Train Loss:0.420792818069458\n",
      "Epoch 0[5415/17270] Time:0.231, Train Loss:0.5141401886940002\n",
      "Epoch 0[5416/17270] Time:0.23, Train Loss:0.6884825229644775\n",
      "Epoch 0[5417/17270] Time:0.229, Train Loss:0.3508099615573883\n",
      "Epoch 0[5418/17270] Time:0.231, Train Loss:0.38925978541374207\n",
      "Epoch 0[5419/17270] Time:0.229, Train Loss:1.4880173206329346\n",
      "Epoch 0[5420/17270] Time:0.229, Train Loss:0.6041212677955627\n",
      "Epoch 0[5421/17270] Time:0.227, Train Loss:0.370266854763031\n",
      "Epoch 0[5422/17270] Time:0.238, Train Loss:0.5324615240097046\n",
      "Epoch 0[5423/17270] Time:0.238, Train Loss:1.009417176246643\n",
      "Epoch 0[5424/17270] Time:0.23, Train Loss:1.3060215711593628\n",
      "Epoch 0[5425/17270] Time:0.239, Train Loss:0.8031300902366638\n",
      "Epoch 0[5426/17270] Time:0.244, Train Loss:0.6397904753684998\n",
      "Epoch 0[5427/17270] Time:0.229, Train Loss:0.44939306378364563\n",
      "Epoch 0[5428/17270] Time:0.24, Train Loss:0.37276220321655273\n",
      "Epoch 0[5429/17270] Time:0.23, Train Loss:1.3573561906814575\n",
      "Epoch 0[5430/17270] Time:0.232, Train Loss:0.5475618839263916\n",
      "Epoch 0[5431/17270] Time:0.232, Train Loss:0.6891486644744873\n",
      "Epoch 0[5432/17270] Time:0.232, Train Loss:0.4397878646850586\n",
      "Epoch 0[5433/17270] Time:0.229, Train Loss:0.6549340486526489\n",
      "Epoch 0[5434/17270] Time:0.237, Train Loss:0.5282999277114868\n",
      "Epoch 0[5435/17270] Time:0.252, Train Loss:0.4059709310531616\n",
      "Epoch 0[5436/17270] Time:0.238, Train Loss:0.616703450679779\n",
      "Epoch 0[5437/17270] Time:0.234, Train Loss:0.5756367444992065\n",
      "Epoch 0[5438/17270] Time:0.236, Train Loss:0.45754510164260864\n",
      "Epoch 0[5439/17270] Time:0.232, Train Loss:0.5988043546676636\n",
      "Epoch 0[5440/17270] Time:0.239, Train Loss:0.7729260325431824\n",
      "Epoch 0[5441/17270] Time:0.232, Train Loss:1.9217008352279663\n",
      "Epoch 0[5442/17270] Time:0.232, Train Loss:0.5691584348678589\n",
      "Epoch 0[5443/17270] Time:0.225, Train Loss:0.7849045395851135\n",
      "Epoch 0[5444/17270] Time:0.238, Train Loss:0.6204995512962341\n",
      "Epoch 0[5445/17270] Time:0.231, Train Loss:0.43911728262901306\n",
      "Epoch 0[5446/17270] Time:0.231, Train Loss:0.4917866885662079\n",
      "Epoch 0[5447/17270] Time:0.232, Train Loss:0.336342453956604\n",
      "Epoch 0[5448/17270] Time:0.229, Train Loss:0.8552529811859131\n",
      "Epoch 0[5449/17270] Time:0.233, Train Loss:0.7355442047119141\n",
      "Epoch 0[5450/17270] Time:0.23, Train Loss:0.5774952173233032\n",
      "Epoch 0[5451/17270] Time:0.233, Train Loss:0.8028634786605835\n",
      "Epoch 0[5452/17270] Time:0.231, Train Loss:0.712231457233429\n",
      "Epoch 0[5453/17270] Time:0.23, Train Loss:0.49447810649871826\n",
      "Epoch 0[5454/17270] Time:0.232, Train Loss:0.9735680222511292\n",
      "Epoch 0[5455/17270] Time:0.24, Train Loss:0.8847663402557373\n",
      "Epoch 0[5456/17270] Time:0.237, Train Loss:0.38984429836273193\n",
      "Epoch 0[5457/17270] Time:0.237, Train Loss:0.6366461515426636\n",
      "Epoch 0[5458/17270] Time:0.238, Train Loss:0.4154116213321686\n",
      "Epoch 0[5459/17270] Time:0.232, Train Loss:0.9415144324302673\n",
      "Epoch 0[5460/17270] Time:0.23, Train Loss:0.3656230568885803\n",
      "Epoch 0[5461/17270] Time:0.227, Train Loss:0.566891074180603\n",
      "Epoch 0[5462/17270] Time:0.226, Train Loss:0.6068670749664307\n",
      "Epoch 0[5463/17270] Time:0.231, Train Loss:0.6996055245399475\n",
      "Epoch 0[5464/17270] Time:0.229, Train Loss:0.4747016429901123\n",
      "Epoch 0[5465/17270] Time:0.229, Train Loss:1.3775304555892944\n",
      "Epoch 0[5466/17270] Time:0.236, Train Loss:0.398853063583374\n",
      "Epoch 0[5467/17270] Time:0.238, Train Loss:0.7131215333938599\n",
      "Epoch 0[5468/17270] Time:0.238, Train Loss:0.5020880699157715\n",
      "Epoch 0[5469/17270] Time:0.229, Train Loss:1.1110031604766846\n",
      "Epoch 0[5470/17270] Time:0.233, Train Loss:0.39117270708084106\n",
      "Epoch 0[5471/17270] Time:0.244, Train Loss:0.6632096171379089\n",
      "Epoch 0[5472/17270] Time:0.236, Train Loss:1.0132849216461182\n",
      "Epoch 0[5473/17270] Time:0.24, Train Loss:0.3667529821395874\n",
      "Epoch 0[5474/17270] Time:0.235, Train Loss:0.5599356889724731\n",
      "Epoch 0[5475/17270] Time:0.232, Train Loss:0.5785772204399109\n",
      "Epoch 0[5476/17270] Time:0.252, Train Loss:0.7040459513664246\n",
      "Epoch 0[5477/17270] Time:0.24, Train Loss:0.6411555409431458\n",
      "Epoch 0[5478/17270] Time:0.224, Train Loss:0.7617620229721069\n",
      "Epoch 0[5479/17270] Time:0.251, Train Loss:0.6476436257362366\n",
      "Epoch 0[5480/17270] Time:0.226, Train Loss:0.4546262323856354\n",
      "Epoch 0[5481/17270] Time:0.234, Train Loss:0.5405629873275757\n",
      "Epoch 0[5482/17270] Time:0.237, Train Loss:0.38739410042762756\n",
      "Epoch 0[5483/17270] Time:0.248, Train Loss:1.0617938041687012\n",
      "Epoch 0[5484/17270] Time:0.252, Train Loss:0.49294376373291016\n",
      "Epoch 0[5485/17270] Time:0.233, Train Loss:0.6982662081718445\n",
      "Epoch 0[5486/17270] Time:0.222, Train Loss:0.3838886618614197\n",
      "Epoch 0[5487/17270] Time:0.25, Train Loss:0.49088528752326965\n",
      "Epoch 0[5488/17270] Time:0.243, Train Loss:0.8907902240753174\n",
      "Epoch 0[5489/17270] Time:0.233, Train Loss:1.2853859663009644\n",
      "Epoch 0[5490/17270] Time:0.249, Train Loss:0.6068016886711121\n",
      "Epoch 0[5491/17270] Time:0.237, Train Loss:0.4137948751449585\n",
      "Epoch 0[5492/17270] Time:0.224, Train Loss:0.6387174725532532\n",
      "Epoch 0[5493/17270] Time:0.242, Train Loss:0.41885003447532654\n",
      "Epoch 0[5494/17270] Time:0.229, Train Loss:0.5606744885444641\n",
      "Epoch 0[5495/17270] Time:0.238, Train Loss:0.8326755166053772\n",
      "Epoch 0[5496/17270] Time:0.222, Train Loss:0.6591082215309143\n",
      "Epoch 0[5497/17270] Time:0.237, Train Loss:0.9214413166046143\n",
      "Epoch 0[5498/17270] Time:0.231, Train Loss:0.4733392298221588\n",
      "Epoch 0[5499/17270] Time:0.242, Train Loss:0.5248578786849976\n",
      "Epoch 0[5500/17270] Time:0.237, Train Loss:0.9444645047187805\n",
      "Epoch 0[5501/17270] Time:0.233, Train Loss:0.5009329319000244\n",
      "Epoch 0[5502/17270] Time:0.244, Train Loss:0.6950949430465698\n",
      "Epoch 0[5503/17270] Time:0.233, Train Loss:0.4883671700954437\n",
      "Epoch 0[5504/17270] Time:0.241, Train Loss:0.6076278686523438\n",
      "Epoch 0[5505/17270] Time:0.24, Train Loss:0.5168041586875916\n",
      "Epoch 0[5506/17270] Time:0.235, Train Loss:0.29442283511161804\n",
      "Epoch 0[5507/17270] Time:0.237, Train Loss:0.5902808904647827\n",
      "Epoch 0[5508/17270] Time:0.238, Train Loss:0.7839987277984619\n",
      "Epoch 0[5509/17270] Time:0.237, Train Loss:0.4352480471134186\n",
      "Epoch 0[5510/17270] Time:0.229, Train Loss:0.4151921272277832\n",
      "Epoch 0[5511/17270] Time:0.233, Train Loss:0.29736360907554626\n",
      "Epoch 0[5512/17270] Time:0.244, Train Loss:0.6281381249427795\n",
      "Epoch 0[5513/17270] Time:0.245, Train Loss:0.4845181405544281\n",
      "Epoch 0[5514/17270] Time:0.224, Train Loss:0.6539761424064636\n",
      "Epoch 0[5515/17270] Time:0.223, Train Loss:0.8473920822143555\n",
      "Epoch 0[5516/17270] Time:0.239, Train Loss:0.6418265700340271\n",
      "Epoch 0[5517/17270] Time:0.246, Train Loss:0.4724760055541992\n",
      "Epoch 0[5518/17270] Time:0.229, Train Loss:0.31141629815101624\n",
      "Epoch 0[5519/17270] Time:0.231, Train Loss:1.2991336584091187\n",
      "Epoch 0[5520/17270] Time:0.242, Train Loss:0.37596797943115234\n",
      "Epoch 0[5521/17270] Time:0.229, Train Loss:0.2778887152671814\n",
      "Epoch 0[5522/17270] Time:0.239, Train Loss:1.0997543334960938\n",
      "Epoch 0[5523/17270] Time:0.24, Train Loss:0.40840455889701843\n",
      "Epoch 0[5524/17270] Time:0.24, Train Loss:0.7777095437049866\n",
      "Epoch 0[5525/17270] Time:0.232, Train Loss:0.5573158860206604\n",
      "Epoch 0[5526/17270] Time:0.236, Train Loss:1.0138059854507446\n",
      "Epoch 0[5527/17270] Time:0.236, Train Loss:0.5222565531730652\n",
      "Epoch 0[5528/17270] Time:0.236, Train Loss:0.546204686164856\n",
      "Epoch 0[5529/17270] Time:0.232, Train Loss:0.45954152941703796\n",
      "Epoch 0[5530/17270] Time:0.239, Train Loss:1.0632599592208862\n",
      "Epoch 0[5531/17270] Time:0.23, Train Loss:0.4649098217487335\n",
      "Epoch 0[5532/17270] Time:0.238, Train Loss:0.3896133601665497\n",
      "Epoch 0[5533/17270] Time:0.231, Train Loss:0.6646056175231934\n",
      "Epoch 0[5534/17270] Time:0.236, Train Loss:1.1046708822250366\n",
      "Epoch 0[5535/17270] Time:0.238, Train Loss:0.6886575222015381\n",
      "Epoch 0[5536/17270] Time:0.229, Train Loss:0.4353111982345581\n",
      "Epoch 0[5537/17270] Time:0.239, Train Loss:0.8266866207122803\n",
      "Epoch 0[5538/17270] Time:0.23, Train Loss:0.7439408302307129\n",
      "Epoch 0[5539/17270] Time:0.23, Train Loss:1.0571330785751343\n",
      "Epoch 0[5540/17270] Time:0.238, Train Loss:0.5809611678123474\n",
      "Epoch 0[5541/17270] Time:0.236, Train Loss:0.3667020797729492\n",
      "Epoch 0[5542/17270] Time:0.23, Train Loss:0.48757314682006836\n",
      "Epoch 0[5543/17270] Time:0.229, Train Loss:0.6214410066604614\n",
      "Epoch 0[5544/17270] Time:0.232, Train Loss:0.6745828986167908\n",
      "Epoch 0[5545/17270] Time:0.23, Train Loss:0.4639554023742676\n",
      "Epoch 0[5546/17270] Time:0.229, Train Loss:0.6919252872467041\n",
      "Epoch 0[5547/17270] Time:0.237, Train Loss:0.6759178638458252\n",
      "Epoch 0[5548/17270] Time:0.239, Train Loss:1.0207232236862183\n",
      "Epoch 0[5549/17270] Time:0.241, Train Loss:0.7464917898178101\n",
      "Epoch 0[5550/17270] Time:0.227, Train Loss:0.4238009452819824\n",
      "Epoch 0[5551/17270] Time:0.238, Train Loss:0.6477547287940979\n",
      "Epoch 0[5552/17270] Time:0.23, Train Loss:0.756240963935852\n",
      "Epoch 0[5553/17270] Time:0.232, Train Loss:0.8217611312866211\n",
      "Epoch 0[5554/17270] Time:0.237, Train Loss:0.28244549036026\n",
      "Epoch 0[5555/17270] Time:0.237, Train Loss:0.6359459161758423\n",
      "Epoch 0[5556/17270] Time:0.231, Train Loss:0.5324551463127136\n",
      "Epoch 0[5557/17270] Time:0.23, Train Loss:0.3730165958404541\n",
      "Epoch 0[5558/17270] Time:0.234, Train Loss:0.5877715945243835\n",
      "Epoch 0[5559/17270] Time:0.236, Train Loss:1.2076472043991089\n",
      "Epoch 0[5560/17270] Time:0.239, Train Loss:0.7077774405479431\n",
      "Epoch 0[5561/17270] Time:0.235, Train Loss:0.6285498738288879\n",
      "Epoch 0[5562/17270] Time:0.228, Train Loss:0.624610960483551\n",
      "Epoch 0[5563/17270] Time:0.23, Train Loss:1.0121419429779053\n",
      "Epoch 0[5564/17270] Time:0.237, Train Loss:0.5366499423980713\n",
      "Epoch 0[5565/17270] Time:0.23, Train Loss:0.709326982498169\n",
      "Epoch 0[5566/17270] Time:0.248, Train Loss:0.9582581520080566\n",
      "Epoch 0[5567/17270] Time:0.225, Train Loss:0.9269010424613953\n",
      "Epoch 0[5568/17270] Time:0.23, Train Loss:0.6533725261688232\n",
      "Epoch 0[5569/17270] Time:0.25, Train Loss:0.7221346497535706\n",
      "Epoch 0[5570/17270] Time:0.25, Train Loss:0.5103297233581543\n",
      "Epoch 0[5571/17270] Time:0.235, Train Loss:0.4650355875492096\n",
      "Epoch 0[5572/17270] Time:0.232, Train Loss:0.5975667238235474\n",
      "Epoch 0[5573/17270] Time:0.237, Train Loss:0.6896464228630066\n",
      "Epoch 0[5574/17270] Time:0.238, Train Loss:0.7846914529800415\n",
      "Epoch 0[5575/17270] Time:0.24, Train Loss:0.46386629343032837\n",
      "Epoch 0[5576/17270] Time:0.233, Train Loss:0.6145351529121399\n",
      "Epoch 0[5577/17270] Time:0.232, Train Loss:0.5859126448631287\n",
      "Epoch 0[5578/17270] Time:0.227, Train Loss:0.889597475528717\n",
      "Epoch 0[5579/17270] Time:0.229, Train Loss:0.4330662488937378\n",
      "Epoch 0[5580/17270] Time:0.23, Train Loss:0.467595636844635\n",
      "Epoch 0[5581/17270] Time:0.229, Train Loss:0.48295682668685913\n",
      "Epoch 0[5582/17270] Time:0.228, Train Loss:0.6362044215202332\n",
      "Epoch 0[5583/17270] Time:0.231, Train Loss:1.0330939292907715\n",
      "Epoch 0[5584/17270] Time:0.231, Train Loss:0.652228593826294\n",
      "Epoch 0[5585/17270] Time:0.237, Train Loss:0.42728498578071594\n",
      "Epoch 0[5586/17270] Time:0.23, Train Loss:0.5877496004104614\n",
      "Epoch 0[5587/17270] Time:0.237, Train Loss:0.7245635390281677\n",
      "Epoch 0[5588/17270] Time:0.233, Train Loss:0.5479640960693359\n",
      "Epoch 0[5589/17270] Time:0.233, Train Loss:0.9843615293502808\n",
      "Epoch 0[5590/17270] Time:0.232, Train Loss:0.5192499160766602\n",
      "Epoch 0[5591/17270] Time:0.232, Train Loss:0.40077605843544006\n",
      "Epoch 0[5592/17270] Time:0.227, Train Loss:0.5686537623405457\n",
      "Epoch 0[5593/17270] Time:0.229, Train Loss:0.5248672962188721\n",
      "Epoch 0[5594/17270] Time:0.237, Train Loss:0.5193079710006714\n",
      "Epoch 0[5595/17270] Time:0.256, Train Loss:0.7993052005767822\n",
      "Epoch 0[5596/17270] Time:0.224, Train Loss:0.6155181527137756\n",
      "Epoch 0[5597/17270] Time:0.223, Train Loss:0.35745933651924133\n",
      "Epoch 0[5598/17270] Time:0.226, Train Loss:0.49301818013191223\n",
      "Epoch 0[5599/17270] Time:0.237, Train Loss:0.609836757183075\n",
      "Epoch 0[5600/17270] Time:0.242, Train Loss:0.6387764811515808\n",
      "Epoch 0[5601/17270] Time:0.236, Train Loss:0.3586541712284088\n",
      "Epoch 0[5602/17270] Time:0.227, Train Loss:0.6381708979606628\n",
      "Epoch 0[5603/17270] Time:0.233, Train Loss:0.5160316824913025\n",
      "Epoch 0[5604/17270] Time:0.238, Train Loss:0.5642473697662354\n",
      "Epoch 0[5605/17270] Time:0.226, Train Loss:0.40123486518859863\n",
      "Epoch 0[5606/17270] Time:0.235, Train Loss:0.7959852814674377\n",
      "Epoch 0[5607/17270] Time:0.238, Train Loss:0.6921065449714661\n",
      "Epoch 0[5608/17270] Time:0.237, Train Loss:0.4856387674808502\n",
      "Epoch 0[5609/17270] Time:0.229, Train Loss:0.6831290125846863\n",
      "Epoch 0[5610/17270] Time:0.229, Train Loss:0.3688666820526123\n",
      "Epoch 0[5611/17270] Time:0.237, Train Loss:0.5542685985565186\n",
      "Epoch 0[5612/17270] Time:0.231, Train Loss:0.40128567814826965\n",
      "Epoch 0[5613/17270] Time:0.232, Train Loss:0.6771974563598633\n",
      "Epoch 0[5614/17270] Time:0.237, Train Loss:0.5117343664169312\n",
      "Epoch 0[5615/17270] Time:0.242, Train Loss:0.8743963837623596\n",
      "Epoch 0[5616/17270] Time:0.233, Train Loss:0.3568447232246399\n",
      "Epoch 0[5617/17270] Time:0.233, Train Loss:1.724587082862854\n",
      "Epoch 0[5618/17270] Time:0.237, Train Loss:0.7232366800308228\n",
      "Epoch 0[5619/17270] Time:0.239, Train Loss:0.5207452774047852\n",
      "Epoch 0[5620/17270] Time:0.231, Train Loss:0.344753235578537\n",
      "Epoch 0[5621/17270] Time:0.228, Train Loss:0.6456232070922852\n",
      "Epoch 0[5622/17270] Time:0.232, Train Loss:0.5968931913375854\n",
      "Epoch 0[5623/17270] Time:0.236, Train Loss:0.47011247277259827\n",
      "Epoch 0[5624/17270] Time:0.239, Train Loss:0.25591790676116943\n",
      "Epoch 0[5625/17270] Time:0.235, Train Loss:0.5722660422325134\n",
      "Epoch 0[5626/17270] Time:0.237, Train Loss:0.7517307996749878\n",
      "Epoch 0[5627/17270] Time:0.236, Train Loss:1.0093117952346802\n",
      "Epoch 0[5628/17270] Time:0.233, Train Loss:0.5695028901100159\n",
      "Epoch 0[5629/17270] Time:0.229, Train Loss:0.9151994585990906\n",
      "Epoch 0[5630/17270] Time:0.235, Train Loss:0.40505826473236084\n",
      "Epoch 0[5631/17270] Time:0.236, Train Loss:0.6812233328819275\n",
      "Epoch 0[5632/17270] Time:0.236, Train Loss:0.9642035365104675\n",
      "Epoch 0[5633/17270] Time:0.24, Train Loss:0.6049578785896301\n",
      "Epoch 0[5634/17270] Time:0.236, Train Loss:0.8164613842964172\n",
      "Epoch 0[5635/17270] Time:0.236, Train Loss:1.2325063943862915\n",
      "Epoch 0[5636/17270] Time:0.236, Train Loss:0.790291965007782\n",
      "Epoch 0[5637/17270] Time:0.23, Train Loss:0.514695942401886\n",
      "Epoch 0[5638/17270] Time:0.238, Train Loss:0.6241368055343628\n",
      "Epoch 0[5639/17270] Time:0.236, Train Loss:0.3289125859737396\n",
      "Epoch 0[5640/17270] Time:0.236, Train Loss:0.6457573175430298\n",
      "Epoch 0[5641/17270] Time:0.238, Train Loss:0.6569674611091614\n",
      "Epoch 0[5642/17270] Time:0.227, Train Loss:0.4471927285194397\n",
      "Epoch 0[5643/17270] Time:0.225, Train Loss:0.532682478427887\n",
      "Epoch 0[5644/17270] Time:0.232, Train Loss:0.5926597118377686\n",
      "Epoch 0[5645/17270] Time:0.234, Train Loss:0.5829164385795593\n",
      "Epoch 0[5646/17270] Time:0.244, Train Loss:0.5262206196784973\n",
      "Epoch 0[5647/17270] Time:0.236, Train Loss:0.42831817269325256\n",
      "Epoch 0[5648/17270] Time:0.249, Train Loss:0.6299881935119629\n",
      "Epoch 0[5649/17270] Time:0.242, Train Loss:0.7883819341659546\n",
      "Epoch 0[5650/17270] Time:0.232, Train Loss:0.6874687075614929\n",
      "Epoch 0[5651/17270] Time:0.231, Train Loss:0.7937232851982117\n",
      "Epoch 0[5652/17270] Time:0.228, Train Loss:1.2108627557754517\n",
      "Epoch 0[5653/17270] Time:0.237, Train Loss:1.1673684120178223\n",
      "Epoch 0[5654/17270] Time:0.224, Train Loss:0.8242284059524536\n",
      "Epoch 0[5655/17270] Time:0.228, Train Loss:0.5369939804077148\n",
      "Epoch 0[5656/17270] Time:0.231, Train Loss:0.46040087938308716\n",
      "Epoch 0[5657/17270] Time:0.234, Train Loss:0.4890540540218353\n",
      "Epoch 0[5658/17270] Time:0.23, Train Loss:0.49126964807510376\n",
      "Epoch 0[5659/17270] Time:0.227, Train Loss:0.4275939464569092\n",
      "Epoch 0[5660/17270] Time:0.255, Train Loss:0.3890383839607239\n",
      "Epoch 0[5661/17270] Time:0.232, Train Loss:0.7223030924797058\n",
      "Epoch 0[5662/17270] Time:0.238, Train Loss:0.8180322647094727\n",
      "Epoch 0[5663/17270] Time:0.236, Train Loss:0.4712522327899933\n",
      "Epoch 0[5664/17270] Time:0.234, Train Loss:0.38363873958587646\n",
      "Epoch 0[5665/17270] Time:0.223, Train Loss:0.7454736828804016\n",
      "Epoch 0[5666/17270] Time:0.238, Train Loss:0.8531071543693542\n",
      "Epoch 0[5667/17270] Time:0.236, Train Loss:0.6456636786460876\n",
      "Epoch 0[5668/17270] Time:0.229, Train Loss:0.5026643872261047\n",
      "Epoch 0[5669/17270] Time:0.228, Train Loss:0.48453718423843384\n",
      "Epoch 0[5670/17270] Time:0.23, Train Loss:0.37324878573417664\n",
      "Epoch 0[5671/17270] Time:0.238, Train Loss:0.959536075592041\n",
      "Epoch 0[5672/17270] Time:0.23, Train Loss:0.4958856701850891\n",
      "Epoch 0[5673/17270] Time:0.229, Train Loss:0.7012931704521179\n",
      "Epoch 0[5674/17270] Time:0.236, Train Loss:0.643930196762085\n",
      "Epoch 0[5675/17270] Time:0.235, Train Loss:0.7073584198951721\n",
      "Epoch 0[5676/17270] Time:0.237, Train Loss:0.6418558955192566\n",
      "Epoch 0[5677/17270] Time:0.238, Train Loss:0.7976905703544617\n",
      "Epoch 0[5678/17270] Time:0.236, Train Loss:0.5585477352142334\n",
      "Epoch 0[5679/17270] Time:0.231, Train Loss:0.8234149217605591\n",
      "Epoch 0[5680/17270] Time:0.231, Train Loss:0.6571540236473083\n",
      "Epoch 0[5681/17270] Time:0.24, Train Loss:0.787732720375061\n",
      "Epoch 0[5682/17270] Time:0.233, Train Loss:0.4652472734451294\n",
      "Epoch 0[5683/17270] Time:0.228, Train Loss:0.4189607501029968\n",
      "Epoch 0[5684/17270] Time:0.227, Train Loss:0.6711602210998535\n",
      "Epoch 0[5685/17270] Time:0.226, Train Loss:0.5441611409187317\n",
      "Epoch 0[5686/17270] Time:0.229, Train Loss:0.6217740774154663\n",
      "Epoch 0[5687/17270] Time:0.235, Train Loss:1.0225296020507812\n",
      "Epoch 0[5688/17270] Time:0.23, Train Loss:0.4622611105442047\n",
      "Epoch 0[5689/17270] Time:0.243, Train Loss:0.42673051357269287\n",
      "Epoch 0[5690/17270] Time:0.238, Train Loss:0.5885992050170898\n",
      "Epoch 0[5691/17270] Time:0.238, Train Loss:0.6366385221481323\n",
      "Epoch 0[5692/17270] Time:0.236, Train Loss:0.42777401208877563\n",
      "Epoch 0[5693/17270] Time:0.236, Train Loss:1.005967378616333\n",
      "Epoch 0[5694/17270] Time:0.232, Train Loss:0.5627136826515198\n",
      "Epoch 0[5695/17270] Time:0.227, Train Loss:0.41180503368377686\n",
      "Epoch 0[5696/17270] Time:0.23, Train Loss:0.4079594016075134\n",
      "Epoch 0[5697/17270] Time:0.234, Train Loss:0.5185065865516663\n",
      "Epoch 0[5698/17270] Time:0.239, Train Loss:0.5738880038261414\n",
      "Epoch 0[5699/17270] Time:0.237, Train Loss:0.8863906264305115\n",
      "Epoch 0[5700/17270] Time:0.231, Train Loss:0.6436476111412048\n",
      "Epoch 0[5701/17270] Time:0.234, Train Loss:0.35891637206077576\n",
      "Epoch 0[5702/17270] Time:0.227, Train Loss:0.4842808246612549\n",
      "Epoch 0[5703/17270] Time:0.236, Train Loss:0.555665910243988\n",
      "Epoch 0[5704/17270] Time:0.25, Train Loss:0.8209080696105957\n",
      "Epoch 0[5705/17270] Time:0.225, Train Loss:0.5978403091430664\n",
      "Epoch 0[5706/17270] Time:0.231, Train Loss:0.4195984899997711\n",
      "Epoch 0[5707/17270] Time:0.248, Train Loss:0.3444722592830658\n",
      "Epoch 0[5708/17270] Time:0.237, Train Loss:0.5071402192115784\n",
      "Epoch 0[5709/17270] Time:0.239, Train Loss:0.8342476487159729\n",
      "Epoch 0[5710/17270] Time:0.223, Train Loss:0.38297149538993835\n",
      "Epoch 0[5711/17270] Time:0.246, Train Loss:0.5430065989494324\n",
      "Epoch 0[5712/17270] Time:0.23, Train Loss:0.734168291091919\n",
      "Epoch 0[5713/17270] Time:0.231, Train Loss:0.7616825699806213\n",
      "Epoch 0[5714/17270] Time:0.236, Train Loss:0.6204034686088562\n",
      "Epoch 0[5715/17270] Time:0.238, Train Loss:0.5101000666618347\n",
      "Epoch 0[5716/17270] Time:0.232, Train Loss:0.47696566581726074\n",
      "Epoch 0[5717/17270] Time:0.23, Train Loss:0.4202600121498108\n",
      "Epoch 0[5718/17270] Time:0.239, Train Loss:1.1282603740692139\n",
      "Epoch 0[5719/17270] Time:0.237, Train Loss:0.33800745010375977\n",
      "Epoch 0[5720/17270] Time:0.239, Train Loss:0.6410583853721619\n",
      "Epoch 0[5721/17270] Time:0.229, Train Loss:0.5105863809585571\n",
      "Epoch 0[5722/17270] Time:0.237, Train Loss:0.8227435946464539\n",
      "Epoch 0[5723/17270] Time:0.235, Train Loss:0.41947582364082336\n",
      "Epoch 0[5724/17270] Time:0.238, Train Loss:0.9211322665214539\n",
      "Epoch 0[5725/17270] Time:0.239, Train Loss:0.45095911622047424\n",
      "Epoch 0[5726/17270] Time:0.241, Train Loss:0.5990607738494873\n",
      "Epoch 0[5727/17270] Time:0.233, Train Loss:0.4911483824253082\n",
      "Epoch 0[5728/17270] Time:0.225, Train Loss:0.5046588182449341\n",
      "Epoch 0[5729/17270] Time:0.235, Train Loss:0.8761710524559021\n",
      "Epoch 0[5730/17270] Time:0.237, Train Loss:1.1548492908477783\n",
      "Epoch 0[5731/17270] Time:0.236, Train Loss:0.45744675397872925\n",
      "Epoch 0[5732/17270] Time:0.232, Train Loss:0.522392749786377\n",
      "Epoch 0[5733/17270] Time:0.231, Train Loss:0.6646161079406738\n",
      "Epoch 0[5734/17270] Time:0.243, Train Loss:0.5160806179046631\n",
      "Epoch 0[5735/17270] Time:0.242, Train Loss:0.8784457445144653\n",
      "Epoch 0[5736/17270] Time:0.239, Train Loss:0.36925777792930603\n",
      "Epoch 0[5737/17270] Time:0.22, Train Loss:0.5348073244094849\n",
      "Epoch 0[5738/17270] Time:0.222, Train Loss:0.5931992530822754\n",
      "Epoch 0[5739/17270] Time:0.239, Train Loss:0.5238955616950989\n",
      "Epoch 0[5740/17270] Time:0.235, Train Loss:0.5468378663063049\n",
      "Epoch 0[5741/17270] Time:0.23, Train Loss:0.46900883316993713\n",
      "Epoch 0[5742/17270] Time:0.23, Train Loss:0.47117653489112854\n",
      "Epoch 0[5743/17270] Time:0.235, Train Loss:0.5338372588157654\n",
      "Epoch 0[5744/17270] Time:0.237, Train Loss:0.48083922266960144\n",
      "Epoch 0[5745/17270] Time:0.237, Train Loss:1.0621778964996338\n",
      "Epoch 0[5746/17270] Time:0.237, Train Loss:0.6030880212783813\n",
      "Epoch 0[5747/17270] Time:0.232, Train Loss:0.42004019021987915\n",
      "Epoch 0[5748/17270] Time:0.23, Train Loss:1.300107479095459\n",
      "Epoch 0[5749/17270] Time:0.237, Train Loss:0.5802592635154724\n",
      "Epoch 0[5750/17270] Time:0.238, Train Loss:0.5573154091835022\n",
      "Epoch 0[5751/17270] Time:0.236, Train Loss:0.48072686791419983\n",
      "Epoch 0[5752/17270] Time:0.231, Train Loss:0.7900521159172058\n",
      "Epoch 0[5753/17270] Time:0.228, Train Loss:0.8212283253669739\n",
      "Epoch 0[5754/17270] Time:0.235, Train Loss:0.7365875840187073\n",
      "Epoch 0[5755/17270] Time:0.246, Train Loss:0.7746474742889404\n",
      "Epoch 0[5756/17270] Time:0.232, Train Loss:0.4801575839519501\n",
      "Epoch 0[5757/17270] Time:0.229, Train Loss:0.7976711392402649\n",
      "Epoch 0[5758/17270] Time:0.236, Train Loss:0.6329501271247864\n",
      "Epoch 0[5759/17270] Time:0.237, Train Loss:0.6079075336456299\n",
      "Epoch 0[5760/17270] Time:0.237, Train Loss:0.4479813873767853\n",
      "Epoch 0[5761/17270] Time:0.232, Train Loss:0.3969564139842987\n",
      "Epoch 0[5762/17270] Time:0.227, Train Loss:0.730293869972229\n",
      "Epoch 0[5763/17270] Time:0.246, Train Loss:0.7096457481384277\n",
      "Epoch 0[5764/17270] Time:0.224, Train Loss:0.7699405550956726\n",
      "Epoch 0[5765/17270] Time:0.23, Train Loss:0.4382407069206238\n",
      "Epoch 0[5766/17270] Time:0.252, Train Loss:1.1968106031417847\n",
      "Epoch 0[5767/17270] Time:0.239, Train Loss:0.5327156782150269\n",
      "Epoch 0[5768/17270] Time:0.237, Train Loss:0.5346067547798157\n",
      "Epoch 0[5769/17270] Time:0.23, Train Loss:0.313296914100647\n",
      "Epoch 0[5770/17270] Time:0.235, Train Loss:0.8935144543647766\n",
      "Epoch 0[5771/17270] Time:0.234, Train Loss:0.5848010182380676\n",
      "Epoch 0[5772/17270] Time:0.224, Train Loss:0.5546473264694214\n",
      "Epoch 0[5773/17270] Time:0.232, Train Loss:0.4181717038154602\n",
      "Epoch 0[5774/17270] Time:0.242, Train Loss:0.31106796860694885\n",
      "Epoch 0[5775/17270] Time:0.238, Train Loss:0.528890073299408\n",
      "Epoch 0[5776/17270] Time:0.236, Train Loss:0.5908688902854919\n",
      "Epoch 0[5777/17270] Time:0.234, Train Loss:1.3011928796768188\n",
      "Epoch 0[5778/17270] Time:0.233, Train Loss:0.7858508229255676\n",
      "Epoch 0[5779/17270] Time:0.238, Train Loss:0.41342347860336304\n",
      "Epoch 0[5780/17270] Time:0.247, Train Loss:0.7262989282608032\n",
      "Epoch 0[5781/17270] Time:0.233, Train Loss:0.384549081325531\n",
      "Epoch 0[5782/17270] Time:0.235, Train Loss:0.6596577167510986\n",
      "Epoch 0[5783/17270] Time:0.234, Train Loss:0.524294376373291\n",
      "Epoch 0[5784/17270] Time:0.23, Train Loss:0.9257889986038208\n",
      "Epoch 0[5785/17270] Time:0.23, Train Loss:0.507175087928772\n",
      "Epoch 0[5786/17270] Time:0.236, Train Loss:1.166500449180603\n",
      "Epoch 0[5787/17270] Time:0.241, Train Loss:0.47140467166900635\n",
      "Epoch 0[5788/17270] Time:0.263, Train Loss:0.6114011406898499\n",
      "Epoch 0[5789/17270] Time:0.241, Train Loss:0.3725210130214691\n",
      "Epoch 0[5790/17270] Time:0.218, Train Loss:0.3553990125656128\n",
      "Epoch 0[5791/17270] Time:0.229, Train Loss:0.9714946150779724\n",
      "Epoch 0[5792/17270] Time:0.232, Train Loss:0.7211902141571045\n",
      "Epoch 0[5793/17270] Time:0.236, Train Loss:0.3340966999530792\n",
      "Epoch 0[5794/17270] Time:0.236, Train Loss:0.6127122044563293\n",
      "Epoch 0[5795/17270] Time:0.236, Train Loss:0.5118641257286072\n",
      "Epoch 0[5796/17270] Time:0.24, Train Loss:0.6065995097160339\n",
      "Epoch 0[5797/17270] Time:0.226, Train Loss:0.7623847723007202\n",
      "Epoch 0[5798/17270] Time:0.224, Train Loss:0.6220817565917969\n",
      "Epoch 0[5799/17270] Time:0.226, Train Loss:0.5692880153656006\n",
      "Epoch 0[5800/17270] Time:0.228, Train Loss:0.5528740286827087\n",
      "Epoch 0[5801/17270] Time:0.233, Train Loss:0.7845888137817383\n",
      "Epoch 0[5802/17270] Time:0.237, Train Loss:0.6096091270446777\n",
      "Epoch 0[5803/17270] Time:0.238, Train Loss:0.6737369894981384\n",
      "Epoch 0[5804/17270] Time:0.231, Train Loss:0.5618851184844971\n",
      "Epoch 0[5805/17270] Time:0.228, Train Loss:0.3022250831127167\n",
      "Epoch 0[5806/17270] Time:0.231, Train Loss:0.44135311245918274\n",
      "Epoch 0[5807/17270] Time:0.228, Train Loss:0.37939727306365967\n",
      "Epoch 0[5808/17270] Time:0.247, Train Loss:0.8125056624412537\n",
      "Epoch 0[5809/17270] Time:0.24, Train Loss:0.3782329857349396\n",
      "Epoch 0[5810/17270] Time:0.231, Train Loss:0.565764307975769\n",
      "Epoch 0[5811/17270] Time:0.24, Train Loss:1.1252955198287964\n",
      "Epoch 0[5812/17270] Time:0.232, Train Loss:1.487228274345398\n",
      "Epoch 0[5813/17270] Time:0.243, Train Loss:0.5096286535263062\n",
      "Epoch 0[5814/17270] Time:0.249, Train Loss:0.6305819153785706\n",
      "Epoch 0[5815/17270] Time:0.237, Train Loss:0.3347713351249695\n",
      "Epoch 0[5816/17270] Time:0.24, Train Loss:0.493430495262146\n",
      "Epoch 0[5817/17270] Time:0.235, Train Loss:0.558876097202301\n",
      "Epoch 0[5818/17270] Time:0.232, Train Loss:0.5461933612823486\n",
      "Epoch 0[5819/17270] Time:0.232, Train Loss:0.6391034126281738\n",
      "Epoch 0[5820/17270] Time:0.239, Train Loss:0.7481650114059448\n",
      "Epoch 0[5821/17270] Time:0.245, Train Loss:0.6358287930488586\n",
      "Epoch 0[5822/17270] Time:0.242, Train Loss:0.5677322745323181\n",
      "Epoch 0[5823/17270] Time:0.232, Train Loss:0.3367883861064911\n",
      "Epoch 0[5824/17270] Time:0.234, Train Loss:0.6161276698112488\n",
      "Epoch 0[5825/17270] Time:0.231, Train Loss:0.5151678919792175\n",
      "Epoch 0[5826/17270] Time:0.227, Train Loss:0.30357030034065247\n",
      "Epoch 0[5827/17270] Time:0.231, Train Loss:0.6054531335830688\n",
      "Epoch 0[5828/17270] Time:0.229, Train Loss:1.337263822555542\n",
      "Epoch 0[5829/17270] Time:0.248, Train Loss:0.4928690195083618\n",
      "Epoch 0[5830/17270] Time:0.221, Train Loss:0.44005513191223145\n",
      "Epoch 0[5831/17270] Time:0.238, Train Loss:0.6884539127349854\n",
      "Epoch 0[5832/17270] Time:0.228, Train Loss:0.6412680745124817\n",
      "Epoch 0[5833/17270] Time:0.228, Train Loss:0.9417184591293335\n",
      "Epoch 0[5834/17270] Time:0.232, Train Loss:0.4363435208797455\n",
      "Epoch 0[5835/17270] Time:0.236, Train Loss:0.5448866486549377\n",
      "Epoch 0[5836/17270] Time:0.238, Train Loss:0.7654667496681213\n",
      "Epoch 0[5837/17270] Time:0.229, Train Loss:0.5339260697364807\n",
      "Epoch 0[5838/17270] Time:0.238, Train Loss:0.44756388664245605\n",
      "Epoch 0[5839/17270] Time:0.232, Train Loss:0.5779315829277039\n",
      "Epoch 0[5840/17270] Time:0.228, Train Loss:0.40846583247184753\n",
      "Epoch 0[5841/17270] Time:0.235, Train Loss:0.35156339406967163\n",
      "Epoch 0[5842/17270] Time:0.228, Train Loss:0.4246068298816681\n",
      "Epoch 0[5843/17270] Time:0.228, Train Loss:0.8022430539131165\n",
      "Epoch 0[5844/17270] Time:0.23, Train Loss:0.7511399388313293\n",
      "Epoch 0[5845/17270] Time:0.238, Train Loss:0.36119890213012695\n",
      "Epoch 0[5846/17270] Time:0.226, Train Loss:0.7422125935554504\n",
      "Epoch 0[5847/17270] Time:0.233, Train Loss:0.4195534586906433\n",
      "Epoch 0[5848/17270] Time:0.224, Train Loss:0.2816694676876068\n",
      "Epoch 0[5849/17270] Time:0.242, Train Loss:0.659458339214325\n",
      "Epoch 0[5850/17270] Time:0.23, Train Loss:0.9506683945655823\n",
      "Epoch 0[5851/17270] Time:0.239, Train Loss:0.516237735748291\n",
      "Epoch 0[5852/17270] Time:0.249, Train Loss:0.3782974183559418\n",
      "Epoch 0[5853/17270] Time:0.231, Train Loss:0.413274347782135\n",
      "Epoch 0[5854/17270] Time:0.222, Train Loss:1.0226349830627441\n",
      "Epoch 0[5855/17270] Time:0.238, Train Loss:1.26200532913208\n",
      "Epoch 0[5856/17270] Time:0.229, Train Loss:0.5306601524353027\n",
      "Epoch 0[5857/17270] Time:0.231, Train Loss:0.42310911417007446\n",
      "Epoch 0[5858/17270] Time:0.237, Train Loss:0.5360282063484192\n",
      "Epoch 0[5859/17270] Time:0.238, Train Loss:0.4075101912021637\n",
      "Epoch 0[5860/17270] Time:0.228, Train Loss:0.5362224578857422\n",
      "Epoch 0[5861/17270] Time:0.237, Train Loss:0.7768367528915405\n",
      "Epoch 0[5862/17270] Time:0.239, Train Loss:0.6837966442108154\n",
      "Epoch 0[5863/17270] Time:0.231, Train Loss:0.4999147057533264\n",
      "Epoch 0[5864/17270] Time:0.233, Train Loss:0.5333902835845947\n",
      "Epoch 0[5865/17270] Time:0.228, Train Loss:0.48696383833885193\n",
      "Epoch 0[5866/17270] Time:0.23, Train Loss:0.6160771250724792\n",
      "Epoch 0[5867/17270] Time:0.235, Train Loss:0.4142161011695862\n",
      "Epoch 0[5868/17270] Time:0.229, Train Loss:1.575014352798462\n",
      "Epoch 0[5869/17270] Time:0.228, Train Loss:0.44695204496383667\n",
      "Epoch 0[5870/17270] Time:0.253, Train Loss:0.8365286588668823\n",
      "Epoch 0[5871/17270] Time:0.233, Train Loss:0.6428207159042358\n",
      "Epoch 0[5872/17270] Time:0.233, Train Loss:0.34279003739356995\n",
      "Epoch 0[5873/17270] Time:0.223, Train Loss:0.4868704676628113\n",
      "Epoch 0[5874/17270] Time:0.235, Train Loss:0.4789978265762329\n",
      "Epoch 0[5875/17270] Time:0.256, Train Loss:0.6201797723770142\n",
      "Epoch 0[5876/17270] Time:0.243, Train Loss:0.43111151456832886\n",
      "Epoch 0[5877/17270] Time:0.231, Train Loss:0.49166175723075867\n",
      "Epoch 0[5878/17270] Time:0.227, Train Loss:0.48883458971977234\n",
      "Epoch 0[5879/17270] Time:0.232, Train Loss:0.7235361933708191\n",
      "Epoch 0[5880/17270] Time:0.232, Train Loss:0.626688539981842\n",
      "Epoch 0[5881/17270] Time:0.226, Train Loss:0.35284850001335144\n",
      "Epoch 0[5882/17270] Time:0.238, Train Loss:0.794748067855835\n",
      "Epoch 0[5883/17270] Time:0.236, Train Loss:1.185036301612854\n",
      "Epoch 0[5884/17270] Time:0.23, Train Loss:0.42706215381622314\n",
      "Epoch 0[5885/17270] Time:0.227, Train Loss:0.4958452880382538\n",
      "Epoch 0[5886/17270] Time:0.225, Train Loss:0.4580228924751282\n",
      "Epoch 0[5887/17270] Time:0.231, Train Loss:0.360567182302475\n",
      "Epoch 0[5888/17270] Time:0.236, Train Loss:0.5009706020355225\n",
      "Epoch 0[5889/17270] Time:0.24, Train Loss:0.4505140483379364\n",
      "Epoch 0[5890/17270] Time:0.236, Train Loss:0.9474059343338013\n",
      "Epoch 0[5891/17270] Time:0.238, Train Loss:0.5552951097488403\n",
      "Epoch 0[5892/17270] Time:0.233, Train Loss:0.5015354156494141\n",
      "Epoch 0[5893/17270] Time:0.234, Train Loss:0.5214655995368958\n",
      "Epoch 0[5894/17270] Time:0.239, Train Loss:1.1742923259735107\n",
      "Epoch 0[5895/17270] Time:0.23, Train Loss:0.5482189655303955\n",
      "Epoch 0[5896/17270] Time:0.239, Train Loss:0.37350067496299744\n",
      "Epoch 0[5897/17270] Time:0.237, Train Loss:0.42697033286094666\n",
      "Epoch 0[5898/17270] Time:0.232, Train Loss:0.6364244818687439\n",
      "Epoch 0[5899/17270] Time:0.231, Train Loss:0.46957939863204956\n",
      "Epoch 0[5900/17270] Time:0.239, Train Loss:0.940352201461792\n",
      "Epoch 0[5901/17270] Time:0.237, Train Loss:0.7637068033218384\n",
      "Epoch 0[5902/17270] Time:0.23, Train Loss:0.7151119709014893\n",
      "Epoch 0[5903/17270] Time:0.236, Train Loss:0.8116936683654785\n",
      "Epoch 0[5904/17270] Time:0.236, Train Loss:0.5217249989509583\n",
      "Epoch 0[5905/17270] Time:0.237, Train Loss:0.5294809937477112\n",
      "Epoch 0[5906/17270] Time:0.23, Train Loss:0.6050388813018799\n",
      "Epoch 0[5907/17270] Time:0.236, Train Loss:0.8954926133155823\n",
      "Epoch 0[5908/17270] Time:0.237, Train Loss:0.6051787734031677\n",
      "Epoch 0[5909/17270] Time:0.232, Train Loss:0.2756337821483612\n",
      "Epoch 0[5910/17270] Time:0.238, Train Loss:0.42056360840797424\n",
      "Epoch 0[5911/17270] Time:0.24, Train Loss:0.7045156955718994\n",
      "Epoch 0[5912/17270] Time:0.236, Train Loss:0.40033596754074097\n",
      "Epoch 0[5913/17270] Time:0.23, Train Loss:0.586523175239563\n",
      "Epoch 0[5914/17270] Time:0.237, Train Loss:0.3107030987739563\n",
      "Epoch 0[5915/17270] Time:0.238, Train Loss:0.43028759956359863\n",
      "Epoch 0[5916/17270] Time:0.231, Train Loss:0.4809648096561432\n",
      "Epoch 0[5917/17270] Time:0.233, Train Loss:0.3458002805709839\n",
      "Epoch 0[5918/17270] Time:0.235, Train Loss:1.3382242918014526\n",
      "Epoch 0[5919/17270] Time:0.235, Train Loss:0.6797398924827576\n",
      "Epoch 0[5920/17270] Time:0.231, Train Loss:1.0614893436431885\n",
      "Epoch 0[5921/17270] Time:0.236, Train Loss:0.33596453070640564\n",
      "Epoch 0[5922/17270] Time:0.236, Train Loss:0.924564003944397\n",
      "Epoch 0[5923/17270] Time:0.231, Train Loss:0.43930384516716003\n",
      "Epoch 0[5924/17270] Time:0.228, Train Loss:1.1269280910491943\n",
      "Epoch 0[5925/17270] Time:0.235, Train Loss:0.9331629872322083\n",
      "Epoch 0[5926/17270] Time:0.237, Train Loss:0.8559204339981079\n",
      "Epoch 0[5927/17270] Time:0.233, Train Loss:0.41451025009155273\n",
      "Epoch 0[5928/17270] Time:0.236, Train Loss:0.9635922908782959\n",
      "Epoch 0[5929/17270] Time:0.235, Train Loss:0.9137576818466187\n",
      "Epoch 0[5930/17270] Time:0.237, Train Loss:0.3895014822483063\n",
      "Epoch 0[5931/17270] Time:0.236, Train Loss:0.29640302062034607\n",
      "Epoch 0[5932/17270] Time:0.229, Train Loss:0.876017153263092\n",
      "Epoch 0[5933/17270] Time:0.229, Train Loss:0.9655576348304749\n",
      "Epoch 0[5934/17270] Time:0.239, Train Loss:0.5564720034599304\n",
      "Epoch 0[5935/17270] Time:0.23, Train Loss:0.5905675888061523\n",
      "Epoch 0[5936/17270] Time:0.224, Train Loss:0.7657299041748047\n",
      "Epoch 0[5937/17270] Time:0.235, Train Loss:1.037318468093872\n",
      "Epoch 0[5938/17270] Time:0.243, Train Loss:0.661765992641449\n",
      "Epoch 0[5939/17270] Time:0.238, Train Loss:0.6982014775276184\n",
      "Epoch 0[5940/17270] Time:0.23, Train Loss:0.8110116124153137\n",
      "Epoch 0[5941/17270] Time:0.232, Train Loss:0.4479724168777466\n",
      "Epoch 0[5942/17270] Time:0.227, Train Loss:0.5975440144538879\n",
      "Epoch 0[5943/17270] Time:0.23, Train Loss:0.784779965877533\n",
      "Epoch 0[5944/17270] Time:0.236, Train Loss:0.5734369158744812\n",
      "Epoch 0[5945/17270] Time:0.238, Train Loss:0.45621877908706665\n",
      "Epoch 0[5946/17270] Time:0.228, Train Loss:0.6219568848609924\n",
      "Epoch 0[5947/17270] Time:0.23, Train Loss:0.6283657550811768\n",
      "Epoch 0[5948/17270] Time:0.236, Train Loss:0.562577486038208\n",
      "Epoch 0[5949/17270] Time:0.247, Train Loss:0.42296549677848816\n",
      "Epoch 0[5950/17270] Time:0.231, Train Loss:0.8631376028060913\n",
      "Epoch 0[5951/17270] Time:0.238, Train Loss:0.5535269379615784\n",
      "Epoch 0[5952/17270] Time:0.239, Train Loss:0.39297720789909363\n",
      "Epoch 0[5953/17270] Time:0.237, Train Loss:0.4635022282600403\n",
      "Epoch 0[5954/17270] Time:0.234, Train Loss:0.5223202705383301\n",
      "Epoch 0[5955/17270] Time:0.243, Train Loss:0.9282599091529846\n",
      "Epoch 0[5956/17270] Time:0.238, Train Loss:0.3156498670578003\n",
      "Epoch 0[5957/17270] Time:0.249, Train Loss:1.2463390827178955\n",
      "Epoch 0[5958/17270] Time:0.234, Train Loss:0.7095130681991577\n",
      "Epoch 0[5959/17270] Time:0.233, Train Loss:0.5613099336624146\n",
      "Epoch 0[5960/17270] Time:0.234, Train Loss:0.39375096559524536\n",
      "Epoch 0[5961/17270] Time:0.23, Train Loss:0.5368149280548096\n",
      "Epoch 0[5962/17270] Time:0.229, Train Loss:0.7822972536087036\n",
      "Epoch 0[5963/17270] Time:0.235, Train Loss:0.4210650324821472\n",
      "Epoch 0[5964/17270] Time:0.229, Train Loss:0.5922677516937256\n",
      "Epoch 0[5965/17270] Time:0.229, Train Loss:1.907028317451477\n",
      "Epoch 0[5966/17270] Time:0.23, Train Loss:0.34545204043388367\n",
      "Epoch 0[5967/17270] Time:0.238, Train Loss:0.7185909152030945\n",
      "Epoch 0[5968/17270] Time:0.236, Train Loss:0.8211306929588318\n",
      "Epoch 0[5969/17270] Time:0.232, Train Loss:0.8411927223205566\n",
      "Epoch 0[5970/17270] Time:0.24, Train Loss:0.6087607741355896\n",
      "Epoch 0[5971/17270] Time:0.234, Train Loss:0.4332728385925293\n",
      "Epoch 0[5972/17270] Time:0.239, Train Loss:0.7801783680915833\n",
      "Epoch 0[5973/17270] Time:0.238, Train Loss:0.4506668150424957\n",
      "Epoch 0[5974/17270] Time:0.236, Train Loss:0.6449986696243286\n",
      "Epoch 0[5975/17270] Time:0.236, Train Loss:0.33503490686416626\n",
      "Epoch 0[5976/17270] Time:0.227, Train Loss:0.5859141945838928\n",
      "Epoch 0[5977/17270] Time:0.237, Train Loss:0.5581251382827759\n",
      "Epoch 0[5978/17270] Time:0.238, Train Loss:1.0962679386138916\n",
      "Epoch 0[5979/17270] Time:0.234, Train Loss:0.5517849922180176\n",
      "Epoch 0[5980/17270] Time:0.222, Train Loss:0.3812546730041504\n",
      "Epoch 0[5981/17270] Time:0.245, Train Loss:0.39613309502601624\n",
      "Epoch 0[5982/17270] Time:0.226, Train Loss:1.1913800239562988\n",
      "Epoch 0[5983/17270] Time:0.241, Train Loss:0.5071601271629333\n",
      "Epoch 0[5984/17270] Time:0.238, Train Loss:0.8209193348884583\n",
      "Epoch 0[5985/17270] Time:0.232, Train Loss:0.8464159369468689\n",
      "Epoch 0[5986/17270] Time:0.233, Train Loss:0.4067581295967102\n",
      "Epoch 0[5987/17270] Time:0.232, Train Loss:0.5684540271759033\n",
      "Epoch 0[5988/17270] Time:0.236, Train Loss:0.3716282248497009\n",
      "Epoch 0[5989/17270] Time:0.228, Train Loss:0.621454656124115\n",
      "Epoch 0[5990/17270] Time:0.248, Train Loss:0.4523506462574005\n",
      "Epoch 0[5991/17270] Time:0.238, Train Loss:0.47513240575790405\n",
      "Epoch 0[5992/17270] Time:0.222, Train Loss:0.5505058765411377\n",
      "Epoch 0[5993/17270] Time:0.24, Train Loss:0.4918816387653351\n",
      "Epoch 0[5994/17270] Time:0.239, Train Loss:0.6054967641830444\n",
      "Epoch 0[5995/17270] Time:0.235, Train Loss:0.5168328285217285\n",
      "Epoch 0[5996/17270] Time:0.237, Train Loss:0.5452703833580017\n",
      "Epoch 0[5997/17270] Time:0.242, Train Loss:0.8252370357513428\n",
      "Epoch 0[5998/17270] Time:0.231, Train Loss:0.3718346357345581\n",
      "Epoch 0[5999/17270] Time:0.237, Train Loss:0.6432110667228699\n",
      "Epoch 0[6000/17270] Time:0.235, Train Loss:0.5541616678237915\n",
      "Epoch 0[6001/17270] Time:0.223, Train Loss:0.39021557569503784\n",
      "Epoch 0[6002/17270] Time:0.235, Train Loss:0.442348837852478\n",
      "Epoch 0[6003/17270] Time:0.237, Train Loss:0.5977439284324646\n",
      "Epoch 0[6004/17270] Time:0.229, Train Loss:0.36973562836647034\n",
      "Epoch 0[6005/17270] Time:0.238, Train Loss:0.5640232563018799\n",
      "Epoch 0[6006/17270] Time:0.236, Train Loss:0.3887171149253845\n",
      "Epoch 0[6007/17270] Time:0.229, Train Loss:0.7046380043029785\n",
      "Epoch 0[6008/17270] Time:0.234, Train Loss:0.5238469839096069\n",
      "Epoch 0[6009/17270] Time:0.229, Train Loss:0.6708884239196777\n",
      "Epoch 0[6010/17270] Time:0.23, Train Loss:0.2752006947994232\n",
      "Epoch 0[6011/17270] Time:0.241, Train Loss:1.1267902851104736\n",
      "Epoch 0[6012/17270] Time:0.229, Train Loss:0.2825320065021515\n",
      "Epoch 0[6013/17270] Time:0.228, Train Loss:0.6790396571159363\n",
      "Epoch 0[6014/17270] Time:0.243, Train Loss:0.4313356876373291\n",
      "Epoch 0[6015/17270] Time:0.225, Train Loss:0.7816823720932007\n",
      "Epoch 0[6016/17270] Time:0.237, Train Loss:0.35840100049972534\n",
      "Epoch 0[6017/17270] Time:0.225, Train Loss:0.5458469390869141\n",
      "Epoch 0[6018/17270] Time:0.236, Train Loss:0.5110659003257751\n",
      "Epoch 0[6019/17270] Time:0.232, Train Loss:0.5291960835456848\n",
      "Epoch 0[6020/17270] Time:0.228, Train Loss:0.6158180236816406\n",
      "Epoch 0[6021/17270] Time:0.229, Train Loss:0.39462143182754517\n",
      "Epoch 0[6022/17270] Time:0.228, Train Loss:0.49959230422973633\n",
      "Epoch 0[6023/17270] Time:0.24, Train Loss:0.5872510075569153\n",
      "Epoch 0[6024/17270] Time:0.231, Train Loss:1.4235492944717407\n",
      "Epoch 0[6025/17270] Time:0.228, Train Loss:0.3579685389995575\n",
      "Epoch 0[6026/17270] Time:0.229, Train Loss:0.3290654122829437\n",
      "Epoch 0[6027/17270] Time:0.235, Train Loss:0.5520390272140503\n",
      "Epoch 0[6028/17270] Time:0.237, Train Loss:0.352093368768692\n",
      "Epoch 0[6029/17270] Time:0.231, Train Loss:0.9059454798698425\n",
      "Epoch 0[6030/17270] Time:0.229, Train Loss:1.5628453493118286\n",
      "Epoch 0[6031/17270] Time:0.228, Train Loss:0.6449640393257141\n",
      "Epoch 0[6032/17270] Time:0.225, Train Loss:0.8916040658950806\n",
      "Epoch 0[6033/17270] Time:0.24, Train Loss:0.4979950785636902\n",
      "Epoch 0[6034/17270] Time:0.231, Train Loss:0.4155723750591278\n",
      "Epoch 0[6035/17270] Time:0.236, Train Loss:0.6150649785995483\n",
      "Epoch 0[6036/17270] Time:0.235, Train Loss:0.49640369415283203\n",
      "Epoch 0[6037/17270] Time:0.23, Train Loss:0.37655606865882874\n",
      "Epoch 0[6038/17270] Time:0.231, Train Loss:1.085760474205017\n",
      "Epoch 0[6039/17270] Time:0.248, Train Loss:0.625809907913208\n",
      "Epoch 0[6040/17270] Time:0.231, Train Loss:0.2702966332435608\n",
      "Epoch 0[6041/17270] Time:0.238, Train Loss:0.4144154191017151\n",
      "Epoch 0[6042/17270] Time:0.221, Train Loss:0.6481338143348694\n",
      "Epoch 0[6043/17270] Time:0.237, Train Loss:1.1740318536758423\n",
      "Epoch 0[6044/17270] Time:0.234, Train Loss:0.36401134729385376\n",
      "Epoch 0[6045/17270] Time:0.233, Train Loss:1.0815150737762451\n",
      "Epoch 0[6046/17270] Time:0.233, Train Loss:1.0566295385360718\n",
      "Epoch 0[6047/17270] Time:0.233, Train Loss:0.5622497797012329\n",
      "Epoch 0[6048/17270] Time:0.228, Train Loss:0.6422421336174011\n",
      "Epoch 0[6049/17270] Time:0.234, Train Loss:1.0950628519058228\n",
      "Epoch 0[6050/17270] Time:0.222, Train Loss:0.8003993630409241\n",
      "Epoch 0[6051/17270] Time:0.246, Train Loss:0.6154230237007141\n",
      "Epoch 0[6052/17270] Time:0.235, Train Loss:1.1899904012680054\n",
      "Epoch 0[6053/17270] Time:0.234, Train Loss:0.7110689878463745\n",
      "Epoch 0[6054/17270] Time:0.231, Train Loss:0.8409402370452881\n",
      "Epoch 0[6055/17270] Time:0.223, Train Loss:0.8830406069755554\n",
      "Epoch 0[6056/17270] Time:0.244, Train Loss:0.6039944887161255\n",
      "Epoch 0[6057/17270] Time:0.232, Train Loss:0.5626640319824219\n",
      "Epoch 0[6058/17270] Time:0.228, Train Loss:0.6729640364646912\n",
      "Epoch 0[6059/17270] Time:0.242, Train Loss:0.519056499004364\n",
      "Epoch 0[6060/17270] Time:0.252, Train Loss:0.6651927828788757\n",
      "Epoch 0[6061/17270] Time:0.233, Train Loss:0.7127645611763\n",
      "Epoch 0[6062/17270] Time:0.232, Train Loss:0.6548131108283997\n",
      "Epoch 0[6063/17270] Time:0.236, Train Loss:0.37530332803726196\n",
      "Epoch 0[6064/17270] Time:0.253, Train Loss:0.7568429708480835\n",
      "Epoch 0[6065/17270] Time:0.243, Train Loss:0.41821935772895813\n",
      "Epoch 0[6066/17270] Time:0.227, Train Loss:0.6296496391296387\n",
      "Epoch 0[6067/17270] Time:0.227, Train Loss:0.4726182222366333\n",
      "Epoch 0[6068/17270] Time:0.237, Train Loss:0.49729180335998535\n",
      "Epoch 0[6069/17270] Time:0.232, Train Loss:0.6512908935546875\n",
      "Epoch 0[6070/17270] Time:0.25, Train Loss:0.7772466540336609\n",
      "Epoch 0[6071/17270] Time:0.231, Train Loss:0.3391425609588623\n",
      "Epoch 0[6072/17270] Time:0.236, Train Loss:0.5541985034942627\n",
      "Epoch 0[6073/17270] Time:0.227, Train Loss:1.3235359191894531\n",
      "Epoch 0[6074/17270] Time:0.233, Train Loss:0.5725265741348267\n",
      "Epoch 0[6075/17270] Time:0.23, Train Loss:0.26716703176498413\n",
      "Epoch 0[6076/17270] Time:0.229, Train Loss:0.8741757273674011\n",
      "Epoch 0[6077/17270] Time:0.228, Train Loss:0.7505186796188354\n",
      "Epoch 0[6078/17270] Time:0.244, Train Loss:0.514391303062439\n",
      "Epoch 0[6079/17270] Time:0.232, Train Loss:0.8816251754760742\n",
      "Epoch 0[6080/17270] Time:0.233, Train Loss:1.0391077995300293\n",
      "Epoch 0[6081/17270] Time:0.231, Train Loss:0.9319404363632202\n",
      "Epoch 0[6082/17270] Time:0.228, Train Loss:0.4772894084453583\n",
      "Epoch 0[6083/17270] Time:0.23, Train Loss:0.5342946648597717\n",
      "Epoch 0[6084/17270] Time:0.231, Train Loss:0.33435341715812683\n",
      "Epoch 0[6085/17270] Time:0.23, Train Loss:0.3683391213417053\n",
      "Epoch 0[6086/17270] Time:0.237, Train Loss:1.0103821754455566\n",
      "Epoch 0[6087/17270] Time:0.231, Train Loss:0.6052031517028809\n",
      "Epoch 0[6088/17270] Time:0.23, Train Loss:0.8312578797340393\n",
      "Epoch 0[6089/17270] Time:0.235, Train Loss:0.4992390275001526\n",
      "Epoch 0[6090/17270] Time:0.25, Train Loss:0.5774337649345398\n",
      "Epoch 0[6091/17270] Time:0.227, Train Loss:0.425514817237854\n",
      "Epoch 0[6092/17270] Time:0.232, Train Loss:0.9151037931442261\n",
      "Epoch 0[6093/17270] Time:0.235, Train Loss:0.5823845863342285\n",
      "Epoch 0[6094/17270] Time:0.245, Train Loss:0.8817976713180542\n",
      "Epoch 0[6095/17270] Time:0.219, Train Loss:0.8575373291969299\n",
      "Epoch 0[6096/17270] Time:0.241, Train Loss:0.6196780204772949\n",
      "Epoch 0[6097/17270] Time:0.245, Train Loss:0.7654321789741516\n",
      "Epoch 0[6098/17270] Time:0.232, Train Loss:0.372720867395401\n",
      "Epoch 0[6099/17270] Time:0.223, Train Loss:0.8805256485939026\n",
      "Epoch 0[6100/17270] Time:0.235, Train Loss:0.5512516498565674\n",
      "Epoch 0[6101/17270] Time:0.231, Train Loss:0.34880518913269043\n",
      "Epoch 0[6102/17270] Time:0.232, Train Loss:0.6029689311981201\n",
      "Epoch 0[6103/17270] Time:0.23, Train Loss:0.8634814023971558\n",
      "Epoch 0[6104/17270] Time:0.23, Train Loss:0.8835546374320984\n",
      "Epoch 0[6105/17270] Time:0.229, Train Loss:0.7003598213195801\n",
      "Epoch 0[6106/17270] Time:0.24, Train Loss:1.0311514139175415\n",
      "Epoch 0[6107/17270] Time:0.236, Train Loss:0.7958859205245972\n",
      "Epoch 0[6108/17270] Time:0.238, Train Loss:0.3946162760257721\n",
      "Epoch 0[6109/17270] Time:0.231, Train Loss:0.7046321034431458\n",
      "Epoch 0[6110/17270] Time:0.23, Train Loss:0.4617716670036316\n",
      "Epoch 0[6111/17270] Time:0.235, Train Loss:0.8057597279548645\n",
      "Epoch 0[6112/17270] Time:0.239, Train Loss:0.5500989556312561\n",
      "Epoch 0[6113/17270] Time:0.245, Train Loss:1.3509589433670044\n",
      "Epoch 0[6114/17270] Time:0.233, Train Loss:0.8560802936553955\n",
      "Epoch 0[6115/17270] Time:0.223, Train Loss:0.595440149307251\n",
      "Epoch 0[6116/17270] Time:0.23, Train Loss:0.5309232473373413\n",
      "Epoch 0[6117/17270] Time:0.229, Train Loss:0.8310169577598572\n",
      "Epoch 0[6118/17270] Time:0.237, Train Loss:0.6505630612373352\n",
      "Epoch 0[6119/17270] Time:0.233, Train Loss:0.6342251896858215\n",
      "Epoch 0[6120/17270] Time:0.233, Train Loss:0.6251214146614075\n",
      "Epoch 0[6121/17270] Time:0.222, Train Loss:0.7997270226478577\n",
      "Epoch 0[6122/17270] Time:0.246, Train Loss:0.6197690367698669\n",
      "Epoch 0[6123/17270] Time:0.241, Train Loss:0.8516083359718323\n",
      "Epoch 0[6124/17270] Time:0.235, Train Loss:0.8072853684425354\n",
      "Epoch 0[6125/17270] Time:0.227, Train Loss:0.7065378427505493\n",
      "Epoch 0[6126/17270] Time:0.234, Train Loss:0.6211907863616943\n",
      "Epoch 0[6127/17270] Time:0.232, Train Loss:0.6281536817550659\n",
      "Epoch 0[6128/17270] Time:0.237, Train Loss:0.4914228618144989\n",
      "Epoch 0[6129/17270] Time:0.238, Train Loss:0.773105263710022\n",
      "Epoch 0[6130/17270] Time:0.228, Train Loss:0.4909001588821411\n",
      "Epoch 0[6131/17270] Time:0.236, Train Loss:0.40577805042266846\n",
      "Epoch 0[6132/17270] Time:0.237, Train Loss:0.46399936079978943\n",
      "Epoch 0[6133/17270] Time:0.239, Train Loss:0.5531582832336426\n",
      "Epoch 0[6134/17270] Time:0.23, Train Loss:0.6503366827964783\n",
      "Epoch 0[6135/17270] Time:0.235, Train Loss:0.4684313237667084\n",
      "Epoch 0[6136/17270] Time:0.248, Train Loss:0.3864521086215973\n",
      "Epoch 0[6137/17270] Time:0.245, Train Loss:0.6702744364738464\n",
      "Epoch 0[6138/17270] Time:0.236, Train Loss:0.5608482956886292\n",
      "Epoch 0[6139/17270] Time:0.237, Train Loss:0.841792106628418\n",
      "Epoch 0[6140/17270] Time:0.24, Train Loss:0.4105546176433563\n",
      "Epoch 0[6141/17270] Time:0.231, Train Loss:0.5041018128395081\n",
      "Epoch 0[6142/17270] Time:0.231, Train Loss:0.8604668378829956\n",
      "Epoch 0[6143/17270] Time:0.234, Train Loss:0.32863086462020874\n",
      "Epoch 0[6144/17270] Time:0.23, Train Loss:0.7317513227462769\n",
      "Epoch 0[6145/17270] Time:0.234, Train Loss:0.568108856678009\n",
      "Epoch 0[6146/17270] Time:0.23, Train Loss:0.5686622858047485\n",
      "Epoch 0[6147/17270] Time:0.232, Train Loss:0.739119291305542\n",
      "Epoch 0[6148/17270] Time:0.229, Train Loss:0.5446270108222961\n",
      "Epoch 0[6149/17270] Time:0.231, Train Loss:0.46373552083969116\n",
      "Epoch 0[6150/17270] Time:0.236, Train Loss:0.9440034031867981\n",
      "Epoch 0[6151/17270] Time:0.248, Train Loss:0.5486510992050171\n",
      "Epoch 0[6152/17270] Time:0.239, Train Loss:0.6729839444160461\n",
      "Epoch 0[6153/17270] Time:0.23, Train Loss:0.33701184391975403\n",
      "Epoch 0[6154/17270] Time:0.227, Train Loss:0.3857683837413788\n",
      "Epoch 0[6155/17270] Time:0.228, Train Loss:0.4277097284793854\n",
      "Epoch 0[6156/17270] Time:0.227, Train Loss:0.4501551687717438\n",
      "Epoch 0[6157/17270] Time:0.237, Train Loss:0.6708917617797852\n",
      "Epoch 0[6158/17270] Time:0.238, Train Loss:0.44314247369766235\n",
      "Epoch 0[6159/17270] Time:0.229, Train Loss:0.5907276272773743\n",
      "Epoch 0[6160/17270] Time:0.234, Train Loss:0.7137880921363831\n",
      "Epoch 0[6161/17270] Time:0.235, Train Loss:0.8250147700309753\n",
      "Epoch 0[6162/17270] Time:0.241, Train Loss:0.45679616928100586\n",
      "Epoch 0[6163/17270] Time:0.237, Train Loss:0.6324419975280762\n",
      "Epoch 0[6164/17270] Time:0.236, Train Loss:0.677308201789856\n",
      "Epoch 0[6165/17270] Time:0.229, Train Loss:0.37091064453125\n",
      "Epoch 0[6166/17270] Time:0.227, Train Loss:0.40001294016838074\n",
      "Epoch 0[6167/17270] Time:0.238, Train Loss:0.39991486072540283\n",
      "Epoch 0[6168/17270] Time:0.238, Train Loss:0.5705735087394714\n",
      "Epoch 0[6169/17270] Time:0.236, Train Loss:0.570470929145813\n",
      "Epoch 0[6170/17270] Time:0.23, Train Loss:1.3920515775680542\n",
      "Epoch 0[6171/17270] Time:0.237, Train Loss:0.5082367062568665\n",
      "Epoch 0[6172/17270] Time:0.229, Train Loss:0.9899671673774719\n",
      "Epoch 0[6173/17270] Time:0.224, Train Loss:0.6602988839149475\n",
      "Epoch 0[6174/17270] Time:0.232, Train Loss:0.44011780619621277\n",
      "Epoch 0[6175/17270] Time:0.246, Train Loss:0.5966908931732178\n",
      "Epoch 0[6176/17270] Time:0.222, Train Loss:1.012760877609253\n",
      "Epoch 0[6177/17270] Time:0.242, Train Loss:0.46201083064079285\n",
      "Epoch 0[6178/17270] Time:0.243, Train Loss:0.5159012675285339\n",
      "Epoch 0[6179/17270] Time:0.235, Train Loss:0.311695396900177\n",
      "Epoch 0[6180/17270] Time:0.231, Train Loss:0.26881077885627747\n",
      "Epoch 0[6181/17270] Time:0.231, Train Loss:0.3494715094566345\n",
      "Epoch 0[6182/17270] Time:0.231, Train Loss:0.5402905941009521\n",
      "Epoch 0[6183/17270] Time:0.23, Train Loss:0.9986587762832642\n",
      "Epoch 0[6184/17270] Time:0.24, Train Loss:0.3597082495689392\n",
      "Epoch 0[6185/17270] Time:0.227, Train Loss:0.5952362418174744\n",
      "Epoch 0[6186/17270] Time:0.231, Train Loss:0.6388911604881287\n",
      "Epoch 0[6187/17270] Time:0.243, Train Loss:0.35840854048728943\n",
      "Epoch 0[6188/17270] Time:0.234, Train Loss:0.4224533140659332\n",
      "Epoch 0[6189/17270] Time:0.232, Train Loss:0.33193090558052063\n",
      "Epoch 0[6190/17270] Time:0.241, Train Loss:0.4664311110973358\n",
      "Epoch 0[6191/17270] Time:0.233, Train Loss:0.43956229090690613\n",
      "Epoch 0[6192/17270] Time:0.235, Train Loss:0.5354147553443909\n",
      "Epoch 0[6193/17270] Time:0.239, Train Loss:0.5631080269813538\n",
      "Epoch 0[6194/17270] Time:0.228, Train Loss:0.719727098941803\n",
      "Epoch 0[6195/17270] Time:0.232, Train Loss:0.5672223567962646\n",
      "Epoch 0[6196/17270] Time:0.229, Train Loss:0.7471943497657776\n",
      "Epoch 0[6197/17270] Time:0.237, Train Loss:0.7245902419090271\n",
      "Epoch 0[6198/17270] Time:0.238, Train Loss:1.2410094738006592\n",
      "Epoch 0[6199/17270] Time:0.231, Train Loss:0.7849590182304382\n",
      "Epoch 0[6200/17270] Time:0.23, Train Loss:0.3533729612827301\n",
      "Epoch 0[6201/17270] Time:0.228, Train Loss:0.23533952236175537\n",
      "Epoch 0[6202/17270] Time:0.23, Train Loss:0.49361535906791687\n",
      "Epoch 0[6203/17270] Time:0.229, Train Loss:0.8655108213424683\n",
      "Epoch 0[6204/17270] Time:0.228, Train Loss:0.5625295639038086\n",
      "Epoch 0[6205/17270] Time:0.23, Train Loss:0.5739235281944275\n",
      "Epoch 0[6206/17270] Time:0.23, Train Loss:0.6746901869773865\n",
      "Epoch 0[6207/17270] Time:0.239, Train Loss:0.6418078541755676\n",
      "Epoch 0[6208/17270] Time:0.232, Train Loss:0.44063645601272583\n",
      "Epoch 0[6209/17270] Time:0.25, Train Loss:0.7444602251052856\n",
      "Epoch 0[6210/17270] Time:0.237, Train Loss:1.1819267272949219\n",
      "Epoch 0[6211/17270] Time:0.24, Train Loss:0.662812352180481\n",
      "Epoch 0[6212/17270] Time:0.238, Train Loss:0.6112699508666992\n",
      "Epoch 0[6213/17270] Time:0.242, Train Loss:0.8566043376922607\n",
      "Epoch 0[6214/17270] Time:0.234, Train Loss:0.7972350120544434\n",
      "Epoch 0[6215/17270] Time:0.227, Train Loss:0.9800602197647095\n",
      "Epoch 0[6216/17270] Time:0.233, Train Loss:0.572063684463501\n",
      "Epoch 0[6217/17270] Time:0.23, Train Loss:0.8085775375366211\n",
      "Epoch 0[6218/17270] Time:0.237, Train Loss:0.4937746226787567\n",
      "Epoch 0[6219/17270] Time:0.236, Train Loss:0.6617097854614258\n",
      "Epoch 0[6220/17270] Time:0.239, Train Loss:0.6511884331703186\n",
      "Epoch 0[6221/17270] Time:0.234, Train Loss:0.646058201789856\n",
      "Epoch 0[6222/17270] Time:0.243, Train Loss:0.9727775454521179\n",
      "Epoch 0[6223/17270] Time:0.231, Train Loss:0.7831270694732666\n",
      "Epoch 0[6224/17270] Time:0.239, Train Loss:1.2760673761367798\n",
      "Epoch 0[6225/17270] Time:0.228, Train Loss:0.5976985096931458\n",
      "Epoch 0[6226/17270] Time:0.23, Train Loss:0.5933569669723511\n",
      "Epoch 0[6227/17270] Time:0.24, Train Loss:1.8866511583328247\n",
      "Epoch 0[6228/17270] Time:0.228, Train Loss:0.7398818135261536\n",
      "Epoch 0[6229/17270] Time:0.23, Train Loss:0.36043018102645874\n",
      "Epoch 0[6230/17270] Time:0.237, Train Loss:0.7814196944236755\n",
      "Epoch 0[6231/17270] Time:0.227, Train Loss:0.5644296407699585\n",
      "Epoch 0[6232/17270] Time:0.23, Train Loss:1.0084666013717651\n",
      "Epoch 0[6233/17270] Time:0.229, Train Loss:0.6677573323249817\n",
      "Epoch 0[6234/17270] Time:0.233, Train Loss:0.5346365571022034\n",
      "Epoch 0[6235/17270] Time:0.238, Train Loss:0.6118908524513245\n",
      "Epoch 0[6236/17270] Time:0.234, Train Loss:0.6696990728378296\n",
      "Epoch 0[6237/17270] Time:0.23, Train Loss:0.6994852423667908\n",
      "Epoch 0[6238/17270] Time:0.237, Train Loss:0.7449620366096497\n",
      "Epoch 0[6239/17270] Time:0.238, Train Loss:0.6011592745780945\n",
      "Epoch 0[6240/17270] Time:0.239, Train Loss:0.7353010773658752\n",
      "Epoch 0[6241/17270] Time:0.229, Train Loss:0.7317054271697998\n",
      "Epoch 0[6242/17270] Time:0.241, Train Loss:0.6753770112991333\n",
      "Epoch 0[6243/17270] Time:0.229, Train Loss:0.6752234697341919\n",
      "Epoch 0[6244/17270] Time:0.234, Train Loss:0.5454373359680176\n",
      "Epoch 0[6245/17270] Time:0.229, Train Loss:0.7472127676010132\n",
      "Epoch 0[6246/17270] Time:0.239, Train Loss:1.0659692287445068\n",
      "Epoch 0[6247/17270] Time:0.235, Train Loss:1.074215292930603\n",
      "Epoch 0[6248/17270] Time:0.235, Train Loss:0.4902028739452362\n",
      "Epoch 0[6249/17270] Time:0.244, Train Loss:0.5374876856803894\n",
      "Epoch 0[6250/17270] Time:0.228, Train Loss:0.5877377390861511\n",
      "Epoch 0[6251/17270] Time:0.234, Train Loss:1.1546730995178223\n",
      "Epoch 0[6252/17270] Time:0.228, Train Loss:0.6708727478981018\n",
      "Epoch 0[6253/17270] Time:0.248, Train Loss:0.5544398427009583\n",
      "Epoch 0[6254/17270] Time:0.244, Train Loss:0.5671244859695435\n",
      "Epoch 0[6255/17270] Time:0.237, Train Loss:0.6707174181938171\n",
      "Epoch 0[6256/17270] Time:0.233, Train Loss:0.4209088981151581\n",
      "Epoch 0[6257/17270] Time:0.235, Train Loss:0.44700008630752563\n",
      "Epoch 0[6258/17270] Time:0.229, Train Loss:0.6008974313735962\n",
      "Epoch 0[6259/17270] Time:0.236, Train Loss:1.0118697881698608\n",
      "Epoch 0[6260/17270] Time:0.224, Train Loss:0.3506951332092285\n",
      "Epoch 0[6261/17270] Time:0.231, Train Loss:0.5862996578216553\n",
      "Epoch 0[6262/17270] Time:0.247, Train Loss:0.9729174375534058\n",
      "Epoch 0[6263/17270] Time:0.236, Train Loss:0.7506011128425598\n",
      "Epoch 0[6264/17270] Time:0.235, Train Loss:0.4920905530452728\n",
      "Epoch 0[6265/17270] Time:0.224, Train Loss:0.46201103925704956\n",
      "Epoch 0[6266/17270] Time:0.233, Train Loss:0.7187648415565491\n",
      "Epoch 0[6267/17270] Time:0.238, Train Loss:0.5511676669120789\n",
      "Epoch 0[6268/17270] Time:0.227, Train Loss:0.6996588706970215\n",
      "Epoch 0[6269/17270] Time:0.235, Train Loss:0.42163512110710144\n",
      "Epoch 0[6270/17270] Time:0.233, Train Loss:0.727096438407898\n",
      "Epoch 0[6271/17270] Time:0.232, Train Loss:0.4539971947669983\n",
      "Epoch 0[6272/17270] Time:0.226, Train Loss:1.2144840955734253\n",
      "Epoch 0[6273/17270] Time:0.234, Train Loss:0.5060900449752808\n",
      "Epoch 0[6274/17270] Time:0.225, Train Loss:0.4917604327201843\n",
      "Epoch 0[6275/17270] Time:0.236, Train Loss:0.45750692486763\n",
      "Epoch 0[6276/17270] Time:0.224, Train Loss:1.2473045587539673\n",
      "Epoch 0[6277/17270] Time:0.229, Train Loss:0.6354748010635376\n",
      "Epoch 0[6278/17270] Time:0.229, Train Loss:0.4449031949043274\n",
      "Epoch 0[6279/17270] Time:0.233, Train Loss:0.4879673421382904\n",
      "Epoch 0[6280/17270] Time:0.232, Train Loss:0.5552824139595032\n",
      "Epoch 0[6281/17270] Time:0.235, Train Loss:0.45825377106666565\n",
      "Epoch 0[6282/17270] Time:0.238, Train Loss:0.5778656601905823\n",
      "Epoch 0[6283/17270] Time:0.257, Train Loss:0.6621431708335876\n",
      "Epoch 0[6284/17270] Time:0.221, Train Loss:0.5514099597930908\n",
      "Epoch 0[6285/17270] Time:0.234, Train Loss:0.4842093884944916\n",
      "Epoch 0[6286/17270] Time:0.225, Train Loss:0.44194626808166504\n",
      "Epoch 0[6287/17270] Time:0.234, Train Loss:0.3732839524745941\n",
      "Epoch 0[6288/17270] Time:0.232, Train Loss:0.2806994616985321\n",
      "Epoch 0[6289/17270] Time:0.234, Train Loss:0.40065690875053406\n",
      "Epoch 0[6290/17270] Time:0.229, Train Loss:0.6723297238349915\n",
      "Epoch 0[6291/17270] Time:0.24, Train Loss:0.7774327993392944\n",
      "Epoch 0[6292/17270] Time:0.234, Train Loss:0.5540977120399475\n",
      "Epoch 0[6293/17270] Time:0.234, Train Loss:0.5577100515365601\n",
      "Epoch 0[6294/17270] Time:0.229, Train Loss:0.5667679905891418\n",
      "Epoch 0[6295/17270] Time:0.237, Train Loss:0.45867639780044556\n",
      "Epoch 0[6296/17270] Time:0.231, Train Loss:0.41633227467536926\n",
      "Epoch 0[6297/17270] Time:0.236, Train Loss:0.45148491859436035\n",
      "Epoch 0[6298/17270] Time:0.235, Train Loss:0.6816920638084412\n",
      "Epoch 0[6299/17270] Time:0.248, Train Loss:0.6887741684913635\n",
      "Epoch 0[6300/17270] Time:0.242, Train Loss:0.5539217591285706\n",
      "Epoch 0[6301/17270] Time:0.233, Train Loss:0.5041943192481995\n",
      "Epoch 0[6302/17270] Time:0.238, Train Loss:0.6967248320579529\n",
      "Epoch 0[6303/17270] Time:0.227, Train Loss:0.7306298613548279\n",
      "Epoch 0[6304/17270] Time:0.233, Train Loss:1.155956506729126\n",
      "Epoch 0[6305/17270] Time:0.233, Train Loss:0.41981789469718933\n",
      "Epoch 0[6306/17270] Time:0.227, Train Loss:0.4602484405040741\n",
      "Epoch 0[6307/17270] Time:0.232, Train Loss:0.8844707608222961\n",
      "Epoch 0[6308/17270] Time:0.229, Train Loss:0.42496415972709656\n",
      "Epoch 0[6309/17270] Time:0.238, Train Loss:0.5280293822288513\n",
      "Epoch 0[6310/17270] Time:0.236, Train Loss:0.40157464146614075\n",
      "Epoch 0[6311/17270] Time:0.237, Train Loss:0.6733426451683044\n",
      "Epoch 0[6312/17270] Time:0.238, Train Loss:0.28150495886802673\n",
      "Epoch 0[6313/17270] Time:0.235, Train Loss:0.7051244974136353\n",
      "Epoch 0[6314/17270] Time:0.228, Train Loss:0.30261126160621643\n",
      "Epoch 0[6315/17270] Time:0.228, Train Loss:0.46857455372810364\n",
      "Epoch 0[6316/17270] Time:0.228, Train Loss:0.3275039494037628\n",
      "Epoch 0[6317/17270] Time:0.239, Train Loss:0.47698521614074707\n",
      "Epoch 0[6318/17270] Time:0.227, Train Loss:1.0343849658966064\n",
      "Epoch 0[6319/17270] Time:0.234, Train Loss:0.42497557401657104\n",
      "Epoch 0[6320/17270] Time:0.222, Train Loss:0.5148901343345642\n",
      "Epoch 0[6321/17270] Time:0.242, Train Loss:0.7069301605224609\n",
      "Epoch 0[6322/17270] Time:0.242, Train Loss:0.39171287417411804\n",
      "Epoch 0[6323/17270] Time:0.233, Train Loss:0.7191409468650818\n",
      "Epoch 0[6324/17270] Time:0.244, Train Loss:0.6108769774436951\n",
      "Epoch 0[6325/17270] Time:0.229, Train Loss:0.476640522480011\n",
      "Epoch 0[6326/17270] Time:0.245, Train Loss:1.3638699054718018\n",
      "Epoch 0[6327/17270] Time:0.239, Train Loss:0.7100828289985657\n",
      "Epoch 0[6328/17270] Time:0.234, Train Loss:0.29475778341293335\n",
      "Epoch 0[6329/17270] Time:0.226, Train Loss:0.48473623394966125\n",
      "Epoch 0[6330/17270] Time:0.233, Train Loss:0.2719767689704895\n",
      "Epoch 0[6331/17270] Time:0.226, Train Loss:0.7611439824104309\n",
      "Epoch 0[6332/17270] Time:0.246, Train Loss:0.7225657105445862\n",
      "Epoch 0[6333/17270] Time:0.236, Train Loss:0.7156510949134827\n",
      "Epoch 0[6334/17270] Time:0.232, Train Loss:0.5703814625740051\n",
      "Epoch 0[6335/17270] Time:0.232, Train Loss:0.4698300361633301\n",
      "Epoch 0[6336/17270] Time:0.238, Train Loss:0.32973068952560425\n",
      "Epoch 0[6337/17270] Time:0.23, Train Loss:0.7956607341766357\n",
      "Epoch 0[6338/17270] Time:0.238, Train Loss:0.4954158365726471\n",
      "Epoch 0[6339/17270] Time:0.238, Train Loss:1.1754827499389648\n",
      "Epoch 0[6340/17270] Time:0.23, Train Loss:0.9686068296432495\n",
      "Epoch 0[6341/17270] Time:0.237, Train Loss:0.4424936771392822\n",
      "Epoch 0[6342/17270] Time:0.238, Train Loss:0.8969155550003052\n",
      "Epoch 0[6343/17270] Time:0.237, Train Loss:1.227340817451477\n",
      "Epoch 0[6344/17270] Time:0.243, Train Loss:0.5303256511688232\n",
      "Epoch 0[6345/17270] Time:0.231, Train Loss:0.40824466943740845\n",
      "Epoch 0[6346/17270] Time:0.224, Train Loss:0.7500171661376953\n",
      "Epoch 0[6347/17270] Time:0.241, Train Loss:0.7854041457176208\n",
      "Epoch 0[6348/17270] Time:0.224, Train Loss:0.8471568822860718\n",
      "Epoch 0[6349/17270] Time:0.237, Train Loss:0.6431142687797546\n",
      "Epoch 0[6350/17270] Time:0.236, Train Loss:1.4141521453857422\n",
      "Epoch 0[6351/17270] Time:0.237, Train Loss:0.7835962772369385\n",
      "Epoch 0[6352/17270] Time:0.237, Train Loss:0.7527260780334473\n",
      "Epoch 0[6353/17270] Time:0.241, Train Loss:0.5630455017089844\n",
      "Epoch 0[6354/17270] Time:0.245, Train Loss:0.7628781199455261\n",
      "Epoch 0[6355/17270] Time:0.23, Train Loss:0.870765209197998\n",
      "Epoch 0[6356/17270] Time:0.236, Train Loss:0.6983230113983154\n",
      "Epoch 0[6357/17270] Time:0.233, Train Loss:0.7015600204467773\n",
      "Epoch 0[6358/17270] Time:0.227, Train Loss:0.9180930256843567\n",
      "Epoch 0[6359/17270] Time:0.237, Train Loss:1.0888856649398804\n",
      "Epoch 0[6360/17270] Time:0.228, Train Loss:0.6955766677856445\n",
      "Epoch 0[6361/17270] Time:0.239, Train Loss:0.7364989519119263\n",
      "Epoch 0[6362/17270] Time:0.236, Train Loss:0.7128871083259583\n",
      "Epoch 0[6363/17270] Time:0.231, Train Loss:0.7816219925880432\n",
      "Epoch 0[6364/17270] Time:0.223, Train Loss:0.6065950989723206\n",
      "Epoch 0[6365/17270] Time:0.228, Train Loss:0.5705805420875549\n",
      "Epoch 0[6366/17270] Time:0.24, Train Loss:0.8504559993743896\n",
      "Epoch 0[6367/17270] Time:0.231, Train Loss:0.9440772533416748\n",
      "Epoch 0[6368/17270] Time:0.23, Train Loss:0.4908204674720764\n",
      "Epoch 0[6369/17270] Time:0.24, Train Loss:0.816760778427124\n",
      "Epoch 0[6370/17270] Time:0.227, Train Loss:0.9117381572723389\n",
      "Epoch 0[6371/17270] Time:0.23, Train Loss:0.829055905342102\n",
      "Epoch 0[6372/17270] Time:0.229, Train Loss:0.8444110751152039\n",
      "Epoch 0[6373/17270] Time:0.227, Train Loss:0.48970285058021545\n",
      "Epoch 0[6374/17270] Time:0.23, Train Loss:0.8840848207473755\n",
      "Epoch 0[6375/17270] Time:0.237, Train Loss:0.6828305125236511\n",
      "Epoch 0[6376/17270] Time:0.229, Train Loss:0.5071225762367249\n",
      "Epoch 0[6377/17270] Time:0.237, Train Loss:0.7564545273780823\n",
      "Epoch 0[6378/17270] Time:0.227, Train Loss:0.5587718486785889\n",
      "Epoch 0[6379/17270] Time:0.23, Train Loss:0.9325149059295654\n",
      "Epoch 0[6380/17270] Time:0.228, Train Loss:0.9847791790962219\n",
      "Epoch 0[6381/17270] Time:0.238, Train Loss:0.625995397567749\n",
      "Epoch 0[6382/17270] Time:0.238, Train Loss:0.8198524117469788\n",
      "Epoch 0[6383/17270] Time:0.231, Train Loss:0.3058888018131256\n",
      "Epoch 0[6384/17270] Time:0.24, Train Loss:0.5974148511886597\n",
      "Epoch 0[6385/17270] Time:0.244, Train Loss:0.5361021161079407\n",
      "Epoch 0[6386/17270] Time:0.224, Train Loss:0.6453292965888977\n",
      "Epoch 0[6387/17270] Time:0.23, Train Loss:0.4995587468147278\n",
      "Epoch 0[6388/17270] Time:0.227, Train Loss:0.400347501039505\n",
      "Epoch 0[6389/17270] Time:0.238, Train Loss:0.4504498243331909\n",
      "Epoch 0[6390/17270] Time:0.236, Train Loss:0.6276693940162659\n",
      "Epoch 0[6391/17270] Time:0.24, Train Loss:0.4894721210002899\n",
      "Epoch 0[6392/17270] Time:0.239, Train Loss:0.5009716749191284\n",
      "Epoch 0[6393/17270] Time:0.229, Train Loss:0.5629342198371887\n",
      "Epoch 0[6394/17270] Time:0.239, Train Loss:0.5886927843093872\n",
      "Epoch 0[6395/17270] Time:0.23, Train Loss:0.694682776927948\n",
      "Epoch 0[6396/17270] Time:0.238, Train Loss:0.5991373658180237\n",
      "Epoch 0[6397/17270] Time:0.24, Train Loss:0.4300421178340912\n",
      "Epoch 0[6398/17270] Time:0.248, Train Loss:1.4484272003173828\n",
      "Epoch 0[6399/17270] Time:0.237, Train Loss:0.610268235206604\n",
      "Epoch 0[6400/17270] Time:0.231, Train Loss:0.6408361792564392\n",
      "Epoch 0[6401/17270] Time:0.235, Train Loss:0.4603053033351898\n",
      "Epoch 0[6402/17270] Time:0.224, Train Loss:0.48156049847602844\n",
      "Epoch 0[6403/17270] Time:0.234, Train Loss:0.7241906523704529\n",
      "Epoch 0[6404/17270] Time:0.229, Train Loss:0.42226535081863403\n",
      "Epoch 0[6405/17270] Time:0.23, Train Loss:0.43738389015197754\n",
      "Epoch 0[6406/17270] Time:0.233, Train Loss:0.4660356044769287\n",
      "Epoch 0[6407/17270] Time:0.231, Train Loss:0.5717056393623352\n",
      "Epoch 0[6408/17270] Time:0.239, Train Loss:0.37346500158309937\n",
      "Epoch 0[6409/17270] Time:0.231, Train Loss:0.5113256573677063\n",
      "Epoch 0[6410/17270] Time:0.228, Train Loss:0.8830189108848572\n",
      "Epoch 0[6411/17270] Time:0.243, Train Loss:0.6382977366447449\n",
      "Epoch 0[6412/17270] Time:0.228, Train Loss:0.6356590986251831\n",
      "Epoch 0[6413/17270] Time:0.247, Train Loss:0.664014995098114\n",
      "Epoch 0[6414/17270] Time:0.237, Train Loss:0.6200679540634155\n",
      "Epoch 0[6415/17270] Time:0.229, Train Loss:0.4632472097873688\n",
      "Epoch 0[6416/17270] Time:0.233, Train Loss:0.6696693897247314\n",
      "Epoch 0[6417/17270] Time:0.236, Train Loss:1.05571711063385\n",
      "Epoch 0[6418/17270] Time:0.23, Train Loss:0.8989081978797913\n",
      "Epoch 0[6419/17270] Time:0.241, Train Loss:0.42280688881874084\n",
      "Epoch 0[6420/17270] Time:0.232, Train Loss:1.069955825805664\n",
      "Epoch 0[6421/17270] Time:0.239, Train Loss:0.2549631595611572\n",
      "Epoch 0[6422/17270] Time:0.228, Train Loss:1.228337287902832\n",
      "Epoch 0[6423/17270] Time:0.237, Train Loss:0.5856813788414001\n",
      "Epoch 0[6424/17270] Time:0.236, Train Loss:0.4174817204475403\n",
      "Epoch 0[6425/17270] Time:0.237, Train Loss:0.41603073477745056\n",
      "Epoch 0[6426/17270] Time:0.243, Train Loss:0.5288500189781189\n",
      "Epoch 0[6427/17270] Time:0.225, Train Loss:0.6020950675010681\n",
      "Epoch 0[6428/17270] Time:0.234, Train Loss:0.651069164276123\n",
      "Epoch 0[6429/17270] Time:0.232, Train Loss:0.6478654742240906\n",
      "Epoch 0[6430/17270] Time:0.231, Train Loss:0.6073225140571594\n",
      "Epoch 0[6431/17270] Time:0.245, Train Loss:0.677315890789032\n",
      "Epoch 0[6432/17270] Time:0.231, Train Loss:0.4107191264629364\n",
      "Epoch 0[6433/17270] Time:0.233, Train Loss:0.26909682154655457\n",
      "Epoch 0[6434/17270] Time:0.24, Train Loss:0.3699416220188141\n",
      "Epoch 0[6435/17270] Time:0.237, Train Loss:0.6560116410255432\n",
      "Epoch 0[6436/17270] Time:0.237, Train Loss:0.3081726133823395\n",
      "Epoch 0[6437/17270] Time:0.231, Train Loss:0.38058584928512573\n",
      "Epoch 0[6438/17270] Time:0.232, Train Loss:1.182438611984253\n",
      "Epoch 0[6439/17270] Time:0.229, Train Loss:0.8030891418457031\n",
      "Epoch 0[6440/17270] Time:0.233, Train Loss:0.5513300895690918\n",
      "Epoch 0[6441/17270] Time:0.238, Train Loss:0.559065580368042\n",
      "Epoch 0[6442/17270] Time:0.24, Train Loss:0.5548332333564758\n",
      "Epoch 0[6443/17270] Time:0.237, Train Loss:0.38234943151474\n",
      "Epoch 0[6444/17270] Time:0.24, Train Loss:0.6706298589706421\n",
      "Epoch 0[6445/17270] Time:0.235, Train Loss:0.7765278220176697\n",
      "Epoch 0[6446/17270] Time:0.238, Train Loss:0.9044561982154846\n",
      "Epoch 0[6447/17270] Time:0.231, Train Loss:1.2004966735839844\n",
      "Epoch 0[6448/17270] Time:0.231, Train Loss:1.6284301280975342\n",
      "Epoch 0[6449/17270] Time:0.233, Train Loss:0.42722374200820923\n",
      "Epoch 0[6450/17270] Time:0.233, Train Loss:0.7626321315765381\n",
      "Epoch 0[6451/17270] Time:0.232, Train Loss:0.6096658706665039\n",
      "Epoch 0[6452/17270] Time:0.234, Train Loss:0.6967130303382874\n",
      "Epoch 0[6453/17270] Time:0.229, Train Loss:0.5291967988014221\n",
      "Epoch 0[6454/17270] Time:0.237, Train Loss:1.7805981636047363\n",
      "Epoch 0[6455/17270] Time:0.249, Train Loss:0.518416702747345\n",
      "Epoch 0[6456/17270] Time:0.233, Train Loss:0.7021698355674744\n",
      "Epoch 0[6457/17270] Time:0.237, Train Loss:0.536312997341156\n",
      "Epoch 0[6458/17270] Time:0.238, Train Loss:0.8024491667747498\n",
      "Epoch 0[6459/17270] Time:0.233, Train Loss:0.7189407348632812\n",
      "Epoch 0[6460/17270] Time:0.238, Train Loss:0.6976999640464783\n",
      "Epoch 0[6461/17270] Time:0.236, Train Loss:0.6321580410003662\n",
      "Epoch 0[6462/17270] Time:0.245, Train Loss:0.4860980212688446\n",
      "Epoch 0[6463/17270] Time:0.239, Train Loss:0.66209876537323\n",
      "Epoch 0[6464/17270] Time:0.24, Train Loss:1.0778700113296509\n",
      "Epoch 0[6465/17270] Time:0.24, Train Loss:0.5502870678901672\n",
      "Epoch 0[6466/17270] Time:0.243, Train Loss:0.48439496755599976\n",
      "Epoch 0[6467/17270] Time:0.255, Train Loss:0.5370908379554749\n",
      "Epoch 0[6468/17270] Time:0.235, Train Loss:0.8575201034545898\n",
      "Epoch 0[6469/17270] Time:0.226, Train Loss:0.6067266464233398\n",
      "Epoch 0[6470/17270] Time:0.234, Train Loss:0.7547212839126587\n",
      "Epoch 0[6471/17270] Time:0.228, Train Loss:1.3353421688079834\n",
      "Epoch 0[6472/17270] Time:0.229, Train Loss:0.5566707253456116\n",
      "Epoch 0[6473/17270] Time:0.229, Train Loss:0.423013836145401\n",
      "Epoch 0[6474/17270] Time:0.229, Train Loss:0.40051543712615967\n",
      "Epoch 0[6475/17270] Time:0.24, Train Loss:0.3868517577648163\n",
      "Epoch 0[6476/17270] Time:0.235, Train Loss:0.9306752681732178\n",
      "Epoch 0[6477/17270] Time:0.238, Train Loss:0.4511463940143585\n",
      "Epoch 0[6478/17270] Time:0.234, Train Loss:0.33013954758644104\n",
      "Epoch 0[6479/17270] Time:0.225, Train Loss:0.517494261264801\n",
      "Epoch 0[6480/17270] Time:0.237, Train Loss:0.4785667359828949\n",
      "Epoch 0[6481/17270] Time:0.239, Train Loss:0.42323315143585205\n",
      "Epoch 0[6482/17270] Time:0.236, Train Loss:0.6602127552032471\n",
      "Epoch 0[6483/17270] Time:0.223, Train Loss:0.887610137462616\n",
      "Epoch 0[6484/17270] Time:0.229, Train Loss:0.8141398429870605\n",
      "Epoch 0[6485/17270] Time:0.237, Train Loss:0.5614603161811829\n",
      "Epoch 0[6486/17270] Time:0.236, Train Loss:0.5436518788337708\n",
      "Epoch 0[6487/17270] Time:0.229, Train Loss:0.5438141822814941\n",
      "Epoch 0[6488/17270] Time:0.23, Train Loss:0.7631007432937622\n",
      "Epoch 0[6489/17270] Time:0.231, Train Loss:0.6372329592704773\n",
      "Epoch 0[6490/17270] Time:0.237, Train Loss:0.7529056072235107\n",
      "Epoch 0[6491/17270] Time:0.238, Train Loss:0.8968215584754944\n",
      "Epoch 0[6492/17270] Time:0.236, Train Loss:0.8741708993911743\n",
      "Epoch 0[6493/17270] Time:0.238, Train Loss:0.6581246256828308\n",
      "Epoch 0[6494/17270] Time:0.23, Train Loss:0.37300682067871094\n",
      "Epoch 0[6495/17270] Time:0.229, Train Loss:0.5983268022537231\n",
      "Epoch 0[6496/17270] Time:0.25, Train Loss:1.003080129623413\n",
      "Epoch 0[6497/17270] Time:0.236, Train Loss:0.5774985551834106\n",
      "Epoch 0[6498/17270] Time:0.223, Train Loss:0.9913018345832825\n",
      "Epoch 0[6499/17270] Time:0.222, Train Loss:0.5776228308677673\n",
      "Epoch 0[6500/17270] Time:0.233, Train Loss:0.7348518967628479\n",
      "Epoch 0[6501/17270] Time:0.235, Train Loss:0.5518195033073425\n",
      "Epoch 0[6502/17270] Time:0.242, Train Loss:1.1795822381973267\n",
      "Epoch 0[6503/17270] Time:0.239, Train Loss:0.48220664262771606\n",
      "Epoch 0[6504/17270] Time:0.231, Train Loss:0.3740931749343872\n",
      "Epoch 0[6505/17270] Time:0.243, Train Loss:0.7197086215019226\n",
      "Epoch 0[6506/17270] Time:0.236, Train Loss:0.558880090713501\n",
      "Epoch 0[6507/17270] Time:0.232, Train Loss:0.8004659414291382\n",
      "Epoch 0[6508/17270] Time:0.235, Train Loss:0.8128481507301331\n",
      "Epoch 0[6509/17270] Time:0.239, Train Loss:0.6533347368240356\n",
      "Epoch 0[6510/17270] Time:0.232, Train Loss:0.6970163583755493\n",
      "Epoch 0[6511/17270] Time:0.23, Train Loss:0.9081649780273438\n",
      "Epoch 0[6512/17270] Time:0.236, Train Loss:0.727676510810852\n",
      "Epoch 0[6513/17270] Time:0.237, Train Loss:0.47273918986320496\n",
      "Epoch 0[6514/17270] Time:0.237, Train Loss:0.5591099262237549\n",
      "Epoch 0[6515/17270] Time:0.236, Train Loss:0.49102264642715454\n",
      "Epoch 0[6516/17270] Time:0.235, Train Loss:0.6170833110809326\n",
      "Epoch 0[6517/17270] Time:0.238, Train Loss:1.2953357696533203\n",
      "Epoch 0[6518/17270] Time:0.228, Train Loss:0.793691098690033\n",
      "Epoch 0[6519/17270] Time:0.227, Train Loss:0.9513577222824097\n",
      "Epoch 0[6520/17270] Time:0.236, Train Loss:0.4657093286514282\n",
      "Epoch 0[6521/17270] Time:0.248, Train Loss:0.6738884449005127\n",
      "Epoch 0[6522/17270] Time:0.25, Train Loss:0.3642065227031708\n",
      "Epoch 0[6523/17270] Time:0.233, Train Loss:0.8681949377059937\n",
      "Epoch 0[6524/17270] Time:0.232, Train Loss:0.7322660684585571\n",
      "Epoch 0[6525/17270] Time:0.238, Train Loss:0.8006002902984619\n",
      "Epoch 0[6526/17270] Time:0.235, Train Loss:0.7937926054000854\n",
      "Epoch 0[6527/17270] Time:0.232, Train Loss:0.8578411340713501\n",
      "Epoch 0[6528/17270] Time:0.239, Train Loss:0.8564490079879761\n",
      "Epoch 0[6529/17270] Time:0.227, Train Loss:0.5640899538993835\n",
      "Epoch 0[6530/17270] Time:0.227, Train Loss:0.4773695766925812\n",
      "Epoch 0[6531/17270] Time:0.227, Train Loss:0.6222166419029236\n",
      "Epoch 0[6532/17270] Time:0.24, Train Loss:0.4893897473812103\n",
      "Epoch 0[6533/17270] Time:0.236, Train Loss:0.431877464056015\n",
      "Epoch 0[6534/17270] Time:0.242, Train Loss:0.4284658133983612\n",
      "Epoch 0[6535/17270] Time:0.238, Train Loss:0.6996697187423706\n",
      "Epoch 0[6536/17270] Time:0.228, Train Loss:0.6886767148971558\n",
      "Epoch 0[6537/17270] Time:0.232, Train Loss:0.6002333164215088\n",
      "Epoch 0[6538/17270] Time:0.233, Train Loss:0.2737307846546173\n",
      "Epoch 0[6539/17270] Time:0.229, Train Loss:0.39682114124298096\n",
      "Epoch 0[6540/17270] Time:0.23, Train Loss:0.6419945955276489\n",
      "Epoch 0[6541/17270] Time:0.238, Train Loss:0.3656793534755707\n",
      "Epoch 0[6542/17270] Time:0.239, Train Loss:0.7410083413124084\n",
      "Epoch 0[6543/17270] Time:0.231, Train Loss:0.3243127167224884\n",
      "Epoch 0[6544/17270] Time:0.229, Train Loss:0.33199238777160645\n",
      "Epoch 0[6545/17270] Time:0.237, Train Loss:0.614920437335968\n",
      "Epoch 0[6546/17270] Time:0.231, Train Loss:0.6492793560028076\n",
      "Epoch 0[6547/17270] Time:0.228, Train Loss:0.6120325922966003\n",
      "Epoch 0[6548/17270] Time:0.229, Train Loss:0.5420822501182556\n",
      "Epoch 0[6549/17270] Time:0.228, Train Loss:0.3185865879058838\n",
      "Epoch 0[6550/17270] Time:0.233, Train Loss:0.4704119563102722\n",
      "Epoch 0[6551/17270] Time:0.24, Train Loss:0.433851033449173\n",
      "Epoch 0[6552/17270] Time:0.228, Train Loss:0.2543499767780304\n",
      "Epoch 0[6553/17270] Time:0.228, Train Loss:0.47101733088493347\n",
      "Epoch 0[6554/17270] Time:0.227, Train Loss:0.5777577757835388\n",
      "Epoch 0[6555/17270] Time:0.225, Train Loss:0.44077232480049133\n",
      "Epoch 0[6556/17270] Time:0.226, Train Loss:0.6467217803001404\n",
      "Epoch 0[6557/17270] Time:0.229, Train Loss:0.4229079484939575\n",
      "Epoch 0[6558/17270] Time:0.237, Train Loss:0.516299307346344\n",
      "Epoch 0[6559/17270] Time:0.241, Train Loss:0.27355751395225525\n",
      "Epoch 0[6560/17270] Time:0.229, Train Loss:0.5432139039039612\n",
      "Epoch 0[6561/17270] Time:0.23, Train Loss:0.648468017578125\n",
      "Epoch 0[6562/17270] Time:0.231, Train Loss:0.3306101858615875\n",
      "Epoch 0[6563/17270] Time:0.227, Train Loss:0.3944653570652008\n",
      "Epoch 0[6564/17270] Time:0.229, Train Loss:0.4720616638660431\n",
      "Epoch 0[6565/17270] Time:0.241, Train Loss:0.5342841148376465\n",
      "Epoch 0[6566/17270] Time:0.24, Train Loss:0.5523704886436462\n",
      "Epoch 0[6567/17270] Time:0.235, Train Loss:0.7471455335617065\n",
      "Epoch 0[6568/17270] Time:0.239, Train Loss:0.5213344097137451\n",
      "Epoch 0[6569/17270] Time:0.238, Train Loss:0.4298701882362366\n",
      "Epoch 0[6570/17270] Time:0.23, Train Loss:0.32939475774765015\n",
      "Epoch 0[6571/17270] Time:0.243, Train Loss:0.43516799807548523\n",
      "Epoch 0[6572/17270] Time:0.248, Train Loss:1.809736967086792\n",
      "Epoch 0[6573/17270] Time:0.238, Train Loss:1.120945692062378\n",
      "Epoch 0[6574/17270] Time:0.23, Train Loss:1.053942084312439\n",
      "Epoch 0[6575/17270] Time:0.234, Train Loss:0.33691808581352234\n",
      "Epoch 0[6576/17270] Time:0.237, Train Loss:0.8770161271095276\n",
      "Epoch 0[6577/17270] Time:0.23, Train Loss:0.3373836576938629\n",
      "Epoch 0[6578/17270] Time:0.24, Train Loss:0.5463153719902039\n",
      "Epoch 0[6579/17270] Time:0.23, Train Loss:1.0634996891021729\n",
      "Epoch 0[6580/17270] Time:0.237, Train Loss:0.6492422819137573\n",
      "Epoch 0[6581/17270] Time:0.233, Train Loss:0.47905483841896057\n",
      "Epoch 0[6582/17270] Time:0.231, Train Loss:0.6990737318992615\n",
      "Epoch 0[6583/17270] Time:0.24, Train Loss:0.2711447775363922\n",
      "Epoch 0[6584/17270] Time:0.246, Train Loss:0.436750590801239\n",
      "Epoch 0[6585/17270] Time:0.233, Train Loss:0.38227733969688416\n",
      "Epoch 0[6586/17270] Time:0.232, Train Loss:0.3942255973815918\n",
      "Epoch 0[6587/17270] Time:0.241, Train Loss:0.6676236391067505\n",
      "Epoch 0[6588/17270] Time:0.243, Train Loss:0.40017250180244446\n",
      "Epoch 0[6589/17270] Time:0.234, Train Loss:1.064385175704956\n",
      "Epoch 0[6590/17270] Time:0.234, Train Loss:0.6126227974891663\n",
      "Epoch 0[6591/17270] Time:0.239, Train Loss:0.7295082211494446\n",
      "Epoch 0[6592/17270] Time:0.238, Train Loss:0.7108580470085144\n",
      "Epoch 0[6593/17270] Time:0.239, Train Loss:1.0673803091049194\n",
      "Epoch 0[6594/17270] Time:0.228, Train Loss:0.6832290291786194\n",
      "Epoch 0[6595/17270] Time:0.238, Train Loss:0.9709078073501587\n",
      "Epoch 0[6596/17270] Time:0.232, Train Loss:1.9941874742507935\n",
      "Epoch 0[6597/17270] Time:0.23, Train Loss:0.7217420339584351\n",
      "Epoch 0[6598/17270] Time:0.232, Train Loss:0.5444495677947998\n",
      "Epoch 0[6599/17270] Time:0.231, Train Loss:0.5145980715751648\n",
      "Epoch 0[6600/17270] Time:0.228, Train Loss:0.8487778902053833\n",
      "Epoch 0[6601/17270] Time:0.229, Train Loss:0.5924643874168396\n",
      "Epoch 0[6602/17270] Time:0.231, Train Loss:0.30271631479263306\n",
      "Epoch 0[6603/17270] Time:0.249, Train Loss:0.7918155193328857\n",
      "Epoch 0[6604/17270] Time:0.251, Train Loss:0.5878000259399414\n",
      "Epoch 0[6605/17270] Time:0.234, Train Loss:0.9115559458732605\n",
      "Epoch 0[6606/17270] Time:0.219, Train Loss:0.7092070579528809\n",
      "Epoch 0[6607/17270] Time:0.228, Train Loss:0.44138455390930176\n",
      "Epoch 0[6608/17270] Time:0.251, Train Loss:0.548125147819519\n",
      "Epoch 0[6609/17270] Time:0.236, Train Loss:1.2444720268249512\n",
      "Epoch 0[6610/17270] Time:0.231, Train Loss:0.4626722037792206\n",
      "Epoch 0[6611/17270] Time:0.226, Train Loss:0.4748387634754181\n",
      "Epoch 0[6612/17270] Time:0.227, Train Loss:0.5639747381210327\n",
      "Epoch 0[6613/17270] Time:0.239, Train Loss:0.5249052047729492\n",
      "Epoch 0[6614/17270] Time:0.231, Train Loss:0.4549765884876251\n",
      "Epoch 0[6615/17270] Time:0.229, Train Loss:0.5144472122192383\n",
      "Epoch 0[6616/17270] Time:0.233, Train Loss:0.537451446056366\n",
      "Epoch 0[6617/17270] Time:0.233, Train Loss:0.5473327040672302\n",
      "Epoch 0[6618/17270] Time:0.225, Train Loss:0.6789839863777161\n",
      "Epoch 0[6619/17270] Time:0.243, Train Loss:0.52385413646698\n",
      "Epoch 0[6620/17270] Time:0.249, Train Loss:0.6780554056167603\n",
      "Epoch 0[6621/17270] Time:0.23, Train Loss:0.6532250046730042\n",
      "Epoch 0[6622/17270] Time:0.23, Train Loss:0.32188165187835693\n",
      "Epoch 0[6623/17270] Time:0.228, Train Loss:1.0419703722000122\n",
      "Epoch 0[6624/17270] Time:0.233, Train Loss:0.6173871755599976\n",
      "Epoch 0[6625/17270] Time:0.228, Train Loss:0.4929858148097992\n",
      "Epoch 0[6626/17270] Time:0.236, Train Loss:1.1814318895339966\n",
      "Epoch 0[6627/17270] Time:0.234, Train Loss:0.48394331336021423\n",
      "Epoch 0[6628/17270] Time:0.225, Train Loss:0.3251765966415405\n",
      "Epoch 0[6629/17270] Time:0.226, Train Loss:0.498710036277771\n",
      "Epoch 0[6630/17270] Time:0.233, Train Loss:1.0490949153900146\n",
      "Epoch 0[6631/17270] Time:0.231, Train Loss:0.5689511299133301\n",
      "Epoch 0[6632/17270] Time:0.243, Train Loss:0.5230062007904053\n",
      "Epoch 0[6633/17270] Time:0.237, Train Loss:0.3691737949848175\n",
      "Epoch 0[6634/17270] Time:0.241, Train Loss:0.4675895571708679\n",
      "Epoch 0[6635/17270] Time:0.236, Train Loss:1.1667611598968506\n",
      "Epoch 0[6636/17270] Time:0.235, Train Loss:0.5900301933288574\n",
      "Epoch 0[6637/17270] Time:0.239, Train Loss:0.8875910043716431\n",
      "Epoch 0[6638/17270] Time:0.247, Train Loss:1.0977041721343994\n",
      "Epoch 0[6639/17270] Time:0.237, Train Loss:0.5029082298278809\n",
      "Epoch 0[6640/17270] Time:0.233, Train Loss:0.49253028631210327\n",
      "Epoch 0[6641/17270] Time:0.238, Train Loss:0.40834978222846985\n",
      "Epoch 0[6642/17270] Time:0.232, Train Loss:0.496976763010025\n",
      "Epoch 0[6643/17270] Time:0.234, Train Loss:0.6166517734527588\n",
      "Epoch 0[6644/17270] Time:0.231, Train Loss:0.4183046817779541\n",
      "Epoch 0[6645/17270] Time:0.231, Train Loss:0.6414124965667725\n",
      "Epoch 0[6646/17270] Time:0.234, Train Loss:0.43065860867500305\n",
      "Epoch 0[6647/17270] Time:0.237, Train Loss:0.41519519686698914\n",
      "Epoch 0[6648/17270] Time:0.234, Train Loss:0.5019606351852417\n",
      "Epoch 0[6649/17270] Time:0.234, Train Loss:1.0103576183319092\n",
      "Epoch 0[6650/17270] Time:0.238, Train Loss:0.9963761568069458\n",
      "Epoch 0[6651/17270] Time:0.231, Train Loss:0.48120051622390747\n",
      "Epoch 0[6652/17270] Time:0.23, Train Loss:0.3486154079437256\n",
      "Epoch 0[6653/17270] Time:0.24, Train Loss:0.6669588685035706\n",
      "Epoch 0[6654/17270] Time:0.231, Train Loss:0.6791141629219055\n",
      "Epoch 0[6655/17270] Time:0.231, Train Loss:0.39400431513786316\n",
      "Epoch 0[6656/17270] Time:0.237, Train Loss:0.47539469599723816\n",
      "Epoch 0[6657/17270] Time:0.239, Train Loss:0.538617730140686\n",
      "Epoch 0[6658/17270] Time:0.231, Train Loss:0.6614890694618225\n",
      "Epoch 0[6659/17270] Time:0.226, Train Loss:0.6654672622680664\n",
      "Epoch 0[6660/17270] Time:0.228, Train Loss:0.8940447568893433\n",
      "Epoch 0[6661/17270] Time:0.229, Train Loss:0.3615666329860687\n",
      "Epoch 0[6662/17270] Time:0.23, Train Loss:0.618280291557312\n",
      "Epoch 0[6663/17270] Time:0.238, Train Loss:0.5963050723075867\n",
      "Epoch 0[6664/17270] Time:0.232, Train Loss:0.46386274695396423\n",
      "Epoch 0[6665/17270] Time:0.23, Train Loss:0.4266260266304016\n",
      "Epoch 0[6666/17270] Time:0.232, Train Loss:0.44523441791534424\n",
      "Epoch 0[6667/17270] Time:0.24, Train Loss:0.45262446999549866\n",
      "Epoch 0[6668/17270] Time:0.236, Train Loss:0.6139019131660461\n",
      "Epoch 0[6669/17270] Time:0.244, Train Loss:0.5777872800827026\n",
      "Epoch 0[6670/17270] Time:0.223, Train Loss:0.6589074730873108\n",
      "Epoch 0[6671/17270] Time:0.234, Train Loss:0.3187553882598877\n",
      "Epoch 0[6672/17270] Time:0.231, Train Loss:0.5990621447563171\n",
      "Epoch 0[6673/17270] Time:0.241, Train Loss:1.1366419792175293\n",
      "Epoch 0[6674/17270] Time:0.235, Train Loss:0.5111061334609985\n",
      "Epoch 0[6675/17270] Time:0.232, Train Loss:0.2475821077823639\n",
      "Epoch 0[6676/17270] Time:0.239, Train Loss:0.7846133708953857\n",
      "Epoch 0[6677/17270] Time:0.238, Train Loss:0.7919403314590454\n",
      "Epoch 0[6678/17270] Time:0.23, Train Loss:0.7104519009590149\n",
      "Epoch 0[6679/17270] Time:0.229, Train Loss:0.5578579902648926\n",
      "Epoch 0[6680/17270] Time:0.237, Train Loss:0.758569598197937\n",
      "Epoch 0[6681/17270] Time:0.23, Train Loss:0.3769369125366211\n",
      "Epoch 0[6682/17270] Time:0.241, Train Loss:0.8467094302177429\n",
      "Epoch 0[6683/17270] Time:0.237, Train Loss:0.4582015573978424\n",
      "Epoch 0[6684/17270] Time:0.232, Train Loss:0.5726941227912903\n",
      "Epoch 0[6685/17270] Time:0.233, Train Loss:0.5081917643547058\n",
      "Epoch 0[6686/17270] Time:0.232, Train Loss:0.4806015193462372\n",
      "Epoch 0[6687/17270] Time:0.236, Train Loss:0.6101559996604919\n",
      "Epoch 0[6688/17270] Time:0.234, Train Loss:0.5208505988121033\n",
      "Epoch 0[6689/17270] Time:0.231, Train Loss:1.1170979738235474\n",
      "Epoch 0[6690/17270] Time:0.231, Train Loss:0.5705000758171082\n",
      "Epoch 0[6691/17270] Time:0.238, Train Loss:0.5959556698799133\n",
      "Epoch 0[6692/17270] Time:0.238, Train Loss:0.5917547941207886\n",
      "Epoch 0[6693/17270] Time:0.231, Train Loss:0.5397914052009583\n",
      "Epoch 0[6694/17270] Time:0.227, Train Loss:0.3810656666755676\n",
      "Epoch 0[6695/17270] Time:0.229, Train Loss:1.5643975734710693\n",
      "Epoch 0[6696/17270] Time:0.229, Train Loss:0.41100645065307617\n",
      "Epoch 0[6697/17270] Time:0.232, Train Loss:0.4630481004714966\n",
      "Epoch 0[6698/17270] Time:0.229, Train Loss:0.41462522745132446\n",
      "Epoch 0[6699/17270] Time:0.236, Train Loss:1.0005608797073364\n",
      "Epoch 0[6700/17270] Time:0.228, Train Loss:0.5023132562637329\n",
      "Epoch 0[6701/17270] Time:0.228, Train Loss:0.5114052891731262\n",
      "Epoch 0[6702/17270] Time:0.228, Train Loss:0.6778058409690857\n",
      "Epoch 0[6703/17270] Time:0.235, Train Loss:0.4532609283924103\n",
      "Epoch 0[6704/17270] Time:0.225, Train Loss:0.6604611873626709\n",
      "Epoch 0[6705/17270] Time:0.229, Train Loss:0.38444095849990845\n",
      "Epoch 0[6706/17270] Time:0.24, Train Loss:0.5243944525718689\n",
      "Epoch 0[6707/17270] Time:0.241, Train Loss:0.5471965670585632\n",
      "Epoch 0[6708/17270] Time:0.226, Train Loss:1.2977705001831055\n",
      "Epoch 0[6709/17270] Time:0.228, Train Loss:0.6030422449111938\n",
      "Epoch 0[6710/17270] Time:0.237, Train Loss:0.5854602456092834\n",
      "Epoch 0[6711/17270] Time:0.23, Train Loss:0.5702705383300781\n",
      "Epoch 0[6712/17270] Time:0.23, Train Loss:0.5629264712333679\n",
      "Epoch 0[6713/17270] Time:0.235, Train Loss:0.5335207581520081\n",
      "Epoch 0[6714/17270] Time:0.23, Train Loss:0.32790184020996094\n",
      "Epoch 0[6715/17270] Time:0.232, Train Loss:0.4092177152633667\n",
      "Epoch 0[6716/17270] Time:0.237, Train Loss:1.3112837076187134\n",
      "Epoch 0[6717/17270] Time:0.229, Train Loss:0.9867410659790039\n",
      "Epoch 0[6718/17270] Time:0.233, Train Loss:0.7273655533790588\n",
      "Epoch 0[6719/17270] Time:0.241, Train Loss:0.3799123764038086\n",
      "Epoch 0[6720/17270] Time:0.232, Train Loss:0.4838929772377014\n",
      "Epoch 0[6721/17270] Time:0.229, Train Loss:0.5629380345344543\n",
      "Epoch 0[6722/17270] Time:0.237, Train Loss:0.5828161239624023\n",
      "Epoch 0[6723/17270] Time:0.239, Train Loss:0.7666382193565369\n",
      "Epoch 0[6724/17270] Time:0.243, Train Loss:0.5783489346504211\n",
      "Epoch 0[6725/17270] Time:0.233, Train Loss:0.4143291115760803\n",
      "Epoch 0[6726/17270] Time:0.229, Train Loss:0.8732811212539673\n",
      "Epoch 0[6727/17270] Time:0.23, Train Loss:0.6282528638839722\n",
      "Epoch 0[6728/17270] Time:0.23, Train Loss:0.5585638880729675\n",
      "Epoch 0[6729/17270] Time:0.235, Train Loss:0.4797935485839844\n",
      "Epoch 0[6730/17270] Time:0.227, Train Loss:0.46470338106155396\n",
      "Epoch 0[6731/17270] Time:0.226, Train Loss:0.8240486979484558\n",
      "Epoch 0[6732/17270] Time:0.236, Train Loss:0.6626874804496765\n",
      "Epoch 0[6733/17270] Time:0.226, Train Loss:0.5000307559967041\n",
      "Epoch 0[6734/17270] Time:0.241, Train Loss:0.5155323147773743\n",
      "Epoch 0[6735/17270] Time:0.234, Train Loss:0.8811357617378235\n",
      "Epoch 0[6736/17270] Time:0.222, Train Loss:1.1763637065887451\n",
      "Epoch 0[6737/17270] Time:0.24, Train Loss:0.412992000579834\n",
      "Epoch 0[6738/17270] Time:0.24, Train Loss:0.5372627973556519\n",
      "Epoch 0[6739/17270] Time:0.237, Train Loss:0.38767901062965393\n",
      "Epoch 0[6740/17270] Time:0.234, Train Loss:0.6758688688278198\n",
      "Epoch 0[6741/17270] Time:0.23, Train Loss:0.4005340337753296\n",
      "Epoch 0[6742/17270] Time:0.232, Train Loss:0.42560523748397827\n",
      "Epoch 0[6743/17270] Time:0.228, Train Loss:0.441810667514801\n",
      "Epoch 0[6744/17270] Time:0.234, Train Loss:0.4180769920349121\n",
      "Epoch 0[6745/17270] Time:0.224, Train Loss:0.38407865166664124\n",
      "Epoch 0[6746/17270] Time:0.243, Train Loss:0.700187623500824\n",
      "Epoch 0[6747/17270] Time:0.238, Train Loss:0.4668116867542267\n",
      "Epoch 0[6748/17270] Time:0.232, Train Loss:0.6869763135910034\n",
      "Epoch 0[6749/17270] Time:0.245, Train Loss:0.5237858891487122\n",
      "Epoch 0[6750/17270] Time:0.24, Train Loss:0.4285101592540741\n",
      "Epoch 0[6751/17270] Time:0.23, Train Loss:0.4109995663166046\n",
      "Epoch 0[6752/17270] Time:0.238, Train Loss:0.6230915784835815\n",
      "Epoch 0[6753/17270] Time:0.236, Train Loss:0.5390913486480713\n",
      "Epoch 0[6754/17270] Time:0.238, Train Loss:0.5455253720283508\n",
      "Epoch 0[6755/17270] Time:0.237, Train Loss:0.5007129311561584\n",
      "Epoch 0[6756/17270] Time:0.232, Train Loss:0.8933246731758118\n",
      "Epoch 0[6757/17270] Time:0.234, Train Loss:0.8672611117362976\n",
      "Epoch 0[6758/17270] Time:0.228, Train Loss:0.7500680685043335\n",
      "Epoch 0[6759/17270] Time:0.231, Train Loss:0.38379526138305664\n",
      "Epoch 0[6760/17270] Time:0.237, Train Loss:0.35034698247909546\n",
      "Epoch 0[6761/17270] Time:0.233, Train Loss:0.4999362826347351\n",
      "Epoch 0[6762/17270] Time:0.234, Train Loss:0.40514081716537476\n",
      "Epoch 0[6763/17270] Time:0.227, Train Loss:0.6505503058433533\n",
      "Epoch 0[6764/17270] Time:0.23, Train Loss:0.8641470670700073\n",
      "Epoch 0[6765/17270] Time:0.238, Train Loss:0.6140025854110718\n",
      "Epoch 0[6766/17270] Time:0.225, Train Loss:0.6927931308746338\n",
      "Epoch 0[6767/17270] Time:0.232, Train Loss:0.8725467324256897\n",
      "Epoch 0[6768/17270] Time:0.227, Train Loss:0.59173983335495\n",
      "Epoch 0[6769/17270] Time:0.237, Train Loss:0.463596910238266\n",
      "Epoch 0[6770/17270] Time:0.227, Train Loss:0.359263151884079\n",
      "Epoch 0[6771/17270] Time:0.24, Train Loss:0.6538916230201721\n",
      "Epoch 0[6772/17270] Time:0.237, Train Loss:0.48458805680274963\n",
      "Epoch 0[6773/17270] Time:0.234, Train Loss:0.5903241038322449\n",
      "Epoch 0[6774/17270] Time:0.238, Train Loss:0.5465510487556458\n",
      "Epoch 0[6775/17270] Time:0.238, Train Loss:0.8114029169082642\n",
      "Epoch 0[6776/17270] Time:0.234, Train Loss:0.2884100079536438\n",
      "Epoch 0[6777/17270] Time:0.227, Train Loss:0.40565699338912964\n",
      "Epoch 0[6778/17270] Time:0.235, Train Loss:0.5349608063697815\n",
      "Epoch 0[6779/17270] Time:0.242, Train Loss:0.42366084456443787\n",
      "Epoch 0[6780/17270] Time:0.23, Train Loss:0.6882718801498413\n",
      "Epoch 0[6781/17270] Time:0.244, Train Loss:0.4133054316043854\n",
      "Epoch 0[6782/17270] Time:0.231, Train Loss:0.5503177046775818\n",
      "Epoch 0[6783/17270] Time:0.233, Train Loss:0.456424355506897\n",
      "Epoch 0[6784/17270] Time:0.236, Train Loss:0.6581106185913086\n",
      "Epoch 0[6785/17270] Time:0.236, Train Loss:0.3094157874584198\n",
      "Epoch 0[6786/17270] Time:0.238, Train Loss:0.39969050884246826\n",
      "Epoch 0[6787/17270] Time:0.231, Train Loss:0.3755219578742981\n",
      "Epoch 0[6788/17270] Time:0.229, Train Loss:0.4689719080924988\n",
      "Epoch 0[6789/17270] Time:0.229, Train Loss:0.29571327567100525\n",
      "Epoch 0[6790/17270] Time:0.233, Train Loss:1.4506964683532715\n",
      "Epoch 0[6791/17270] Time:0.231, Train Loss:0.2730639576911926\n",
      "Epoch 0[6792/17270] Time:0.237, Train Loss:0.2722499668598175\n",
      "Epoch 0[6793/17270] Time:0.236, Train Loss:0.3134517967700958\n",
      "Epoch 0[6794/17270] Time:0.253, Train Loss:0.8257461786270142\n",
      "Epoch 0[6795/17270] Time:0.231, Train Loss:0.541175365447998\n",
      "Epoch 0[6796/17270] Time:0.236, Train Loss:0.5512479543685913\n",
      "Epoch 0[6797/17270] Time:0.225, Train Loss:0.546680212020874\n",
      "Epoch 0[6798/17270] Time:0.239, Train Loss:0.6276569962501526\n",
      "Epoch 0[6799/17270] Time:0.228, Train Loss:0.9891970753669739\n",
      "Epoch 0[6800/17270] Time:0.225, Train Loss:0.9235801100730896\n",
      "Epoch 0[6801/17270] Time:0.239, Train Loss:1.2001464366912842\n",
      "Epoch 0[6802/17270] Time:0.241, Train Loss:0.64618319272995\n",
      "Epoch 0[6803/17270] Time:0.235, Train Loss:1.508466362953186\n",
      "Epoch 0[6804/17270] Time:0.229, Train Loss:0.5344834923744202\n",
      "Epoch 0[6805/17270] Time:0.229, Train Loss:0.5662480592727661\n",
      "Epoch 0[6806/17270] Time:0.238, Train Loss:0.4227868318557739\n",
      "Epoch 0[6807/17270] Time:0.237, Train Loss:0.3637692630290985\n",
      "Epoch 0[6808/17270] Time:0.235, Train Loss:0.5875540375709534\n",
      "Epoch 0[6809/17270] Time:0.23, Train Loss:0.4178577959537506\n",
      "Epoch 0[6810/17270] Time:0.238, Train Loss:0.2624702751636505\n",
      "Epoch 0[6811/17270] Time:0.231, Train Loss:0.49037864804267883\n",
      "Epoch 0[6812/17270] Time:0.239, Train Loss:0.5905798077583313\n",
      "Epoch 0[6813/17270] Time:0.23, Train Loss:0.484513521194458\n",
      "Epoch 0[6814/17270] Time:0.236, Train Loss:0.4102191627025604\n",
      "Epoch 0[6815/17270] Time:0.238, Train Loss:0.3573225438594818\n",
      "Epoch 0[6816/17270] Time:0.238, Train Loss:0.7530127763748169\n",
      "Epoch 0[6817/17270] Time:0.231, Train Loss:0.6434689164161682\n",
      "Epoch 0[6818/17270] Time:0.237, Train Loss:0.5343562364578247\n",
      "Epoch 0[6819/17270] Time:0.238, Train Loss:0.439843088388443\n",
      "Epoch 0[6820/17270] Time:0.235, Train Loss:0.7097105979919434\n",
      "Epoch 0[6821/17270] Time:0.225, Train Loss:0.3878065049648285\n",
      "Epoch 0[6822/17270] Time:0.231, Train Loss:0.7301465272903442\n",
      "Epoch 0[6823/17270] Time:0.239, Train Loss:0.4806966483592987\n",
      "Epoch 0[6824/17270] Time:0.23, Train Loss:0.22333833575248718\n",
      "Epoch 0[6825/17270] Time:0.232, Train Loss:1.5874332189559937\n",
      "Epoch 0[6826/17270] Time:0.239, Train Loss:0.5829644799232483\n",
      "Epoch 0[6827/17270] Time:0.229, Train Loss:1.1592680215835571\n",
      "Epoch 0[6828/17270] Time:0.23, Train Loss:0.4579854905605316\n",
      "Epoch 0[6829/17270] Time:0.239, Train Loss:0.5373510122299194\n",
      "Epoch 0[6830/17270] Time:0.232, Train Loss:0.6786579489707947\n",
      "Epoch 0[6831/17270] Time:0.235, Train Loss:0.5099233984947205\n",
      "Epoch 0[6832/17270] Time:0.234, Train Loss:0.9046128392219543\n",
      "Epoch 0[6833/17270] Time:0.239, Train Loss:0.48302626609802246\n",
      "Epoch 0[6834/17270] Time:0.235, Train Loss:0.35548853874206543\n",
      "Epoch 0[6835/17270] Time:0.238, Train Loss:0.6635821461677551\n",
      "Epoch 0[6836/17270] Time:0.231, Train Loss:0.5781367421150208\n",
      "Epoch 0[6837/17270] Time:0.232, Train Loss:0.9740535020828247\n",
      "Epoch 0[6838/17270] Time:0.229, Train Loss:0.8788580298423767\n",
      "Epoch 0[6839/17270] Time:0.252, Train Loss:0.6708316802978516\n",
      "Epoch 0[6840/17270] Time:0.241, Train Loss:0.5755322575569153\n",
      "Epoch 0[6841/17270] Time:0.231, Train Loss:0.3549138307571411\n",
      "Epoch 0[6842/17270] Time:0.235, Train Loss:0.8392450213432312\n",
      "Epoch 0[6843/17270] Time:0.236, Train Loss:0.7058637738227844\n",
      "Epoch 0[6844/17270] Time:0.235, Train Loss:0.7841619253158569\n",
      "Epoch 0[6845/17270] Time:0.238, Train Loss:0.5139022469520569\n",
      "Epoch 0[6846/17270] Time:0.236, Train Loss:0.4227743446826935\n",
      "Epoch 0[6847/17270] Time:0.228, Train Loss:0.5927066206932068\n",
      "Epoch 0[6848/17270] Time:0.228, Train Loss:0.4865013659000397\n",
      "Epoch 0[6849/17270] Time:0.231, Train Loss:1.2230511903762817\n",
      "Epoch 0[6850/17270] Time:0.239, Train Loss:0.6953591704368591\n",
      "Epoch 0[6851/17270] Time:0.236, Train Loss:0.7744602560997009\n",
      "Epoch 0[6852/17270] Time:0.237, Train Loss:0.4975612759590149\n",
      "Epoch 0[6853/17270] Time:0.232, Train Loss:0.4191729426383972\n",
      "Epoch 0[6854/17270] Time:0.235, Train Loss:0.708666205406189\n",
      "Epoch 0[6855/17270] Time:0.239, Train Loss:0.4454040825366974\n",
      "Epoch 0[6856/17270] Time:0.23, Train Loss:0.39941611886024475\n",
      "Epoch 0[6857/17270] Time:0.232, Train Loss:0.39661428332328796\n",
      "Epoch 0[6858/17270] Time:0.23, Train Loss:0.7554425597190857\n",
      "Epoch 0[6859/17270] Time:0.231, Train Loss:1.4600862264633179\n",
      "Epoch 0[6860/17270] Time:0.239, Train Loss:0.686899721622467\n",
      "Epoch 0[6861/17270] Time:0.231, Train Loss:0.415709912776947\n",
      "Epoch 0[6862/17270] Time:0.238, Train Loss:0.5796048641204834\n",
      "Epoch 0[6863/17270] Time:0.238, Train Loss:0.5049250721931458\n",
      "Epoch 0[6864/17270] Time:0.231, Train Loss:0.5435594916343689\n",
      "Epoch 0[6865/17270] Time:0.246, Train Loss:0.6793089509010315\n",
      "Epoch 0[6866/17270] Time:0.226, Train Loss:0.6565226912498474\n",
      "Epoch 0[6867/17270] Time:0.242, Train Loss:0.9141588807106018\n",
      "Epoch 0[6868/17270] Time:0.24, Train Loss:0.3801881968975067\n",
      "Epoch 0[6869/17270] Time:0.226, Train Loss:0.4299008250236511\n",
      "Epoch 0[6870/17270] Time:0.24, Train Loss:0.43637192249298096\n",
      "Epoch 0[6871/17270] Time:0.238, Train Loss:0.5257576107978821\n",
      "Epoch 0[6872/17270] Time:0.233, Train Loss:0.7053240537643433\n",
      "Epoch 0[6873/17270] Time:0.229, Train Loss:0.635005533695221\n",
      "Epoch 0[6874/17270] Time:0.225, Train Loss:0.5577195286750793\n",
      "Epoch 0[6875/17270] Time:0.237, Train Loss:0.5480374097824097\n",
      "Epoch 0[6876/17270] Time:0.238, Train Loss:1.105957269668579\n",
      "Epoch 0[6877/17270] Time:0.227, Train Loss:0.41346850991249084\n",
      "Epoch 0[6878/17270] Time:0.227, Train Loss:0.5908264517784119\n",
      "Epoch 0[6879/17270] Time:0.228, Train Loss:0.8286106586456299\n",
      "Epoch 0[6880/17270] Time:0.239, Train Loss:0.40638184547424316\n",
      "Epoch 0[6881/17270] Time:0.235, Train Loss:0.397908478975296\n",
      "Epoch 0[6882/17270] Time:0.237, Train Loss:0.6965562105178833\n",
      "Epoch 0[6883/17270] Time:0.243, Train Loss:0.34790560603141785\n",
      "Epoch 0[6884/17270] Time:0.236, Train Loss:0.7120749950408936\n",
      "Epoch 0[6885/17270] Time:0.241, Train Loss:0.5494698286056519\n",
      "Epoch 0[6886/17270] Time:0.218, Train Loss:0.5154993534088135\n",
      "Epoch 0[6887/17270] Time:0.237, Train Loss:0.7342180609703064\n",
      "Epoch 0[6888/17270] Time:0.238, Train Loss:0.3336917459964752\n",
      "Epoch 0[6889/17270] Time:0.228, Train Loss:0.6394756436347961\n",
      "Epoch 0[6890/17270] Time:0.238, Train Loss:0.660051167011261\n",
      "Epoch 0[6891/17270] Time:0.238, Train Loss:0.44159138202667236\n",
      "Epoch 0[6892/17270] Time:0.231, Train Loss:0.5105771422386169\n",
      "Epoch 0[6893/17270] Time:0.23, Train Loss:0.5721790790557861\n",
      "Epoch 0[6894/17270] Time:0.237, Train Loss:0.5373799800872803\n",
      "Epoch 0[6895/17270] Time:0.238, Train Loss:0.3862810730934143\n",
      "Epoch 0[6896/17270] Time:0.229, Train Loss:1.1133967638015747\n",
      "Epoch 0[6897/17270] Time:0.237, Train Loss:0.6227309107780457\n",
      "Epoch 0[6898/17270] Time:0.236, Train Loss:0.3608560860157013\n",
      "Epoch 0[6899/17270] Time:0.235, Train Loss:1.0508695840835571\n",
      "Epoch 0[6900/17270] Time:0.239, Train Loss:0.623625636100769\n",
      "Epoch 0[6901/17270] Time:0.238, Train Loss:0.45751139521598816\n",
      "Epoch 0[6902/17270] Time:0.232, Train Loss:0.35323959589004517\n",
      "Epoch 0[6903/17270] Time:0.239, Train Loss:0.8665075898170471\n",
      "Epoch 0[6904/17270] Time:0.234, Train Loss:0.6505559682846069\n",
      "Epoch 0[6905/17270] Time:0.228, Train Loss:0.4702596068382263\n",
      "Epoch 0[6906/17270] Time:0.231, Train Loss:0.38983583450317383\n",
      "Epoch 0[6907/17270] Time:0.23, Train Loss:1.1056773662567139\n",
      "Epoch 0[6908/17270] Time:0.238, Train Loss:0.537304699420929\n",
      "Epoch 0[6909/17270] Time:0.237, Train Loss:0.4501822590827942\n",
      "Epoch 0[6910/17270] Time:0.229, Train Loss:0.9106361269950867\n",
      "Epoch 0[6911/17270] Time:0.237, Train Loss:0.8163235187530518\n",
      "Epoch 0[6912/17270] Time:0.241, Train Loss:0.6696403622627258\n",
      "Epoch 0[6913/17270] Time:0.238, Train Loss:0.411139577627182\n",
      "Epoch 0[6914/17270] Time:0.233, Train Loss:0.5353696942329407\n",
      "Epoch 0[6915/17270] Time:0.229, Train Loss:0.41750818490982056\n",
      "Epoch 0[6916/17270] Time:0.231, Train Loss:0.754498302936554\n",
      "Epoch 0[6917/17270] Time:0.256, Train Loss:0.44535741209983826\n",
      "Epoch 0[6918/17270] Time:0.234, Train Loss:0.5443538427352905\n",
      "Epoch 0[6919/17270] Time:0.237, Train Loss:0.6467490792274475\n",
      "Epoch 0[6920/17270] Time:0.238, Train Loss:0.7440195679664612\n",
      "Epoch 0[6921/17270] Time:0.231, Train Loss:0.5663948655128479\n",
      "Epoch 0[6922/17270] Time:0.238, Train Loss:0.6192527413368225\n",
      "Epoch 0[6923/17270] Time:0.22, Train Loss:0.3657384514808655\n",
      "Epoch 0[6924/17270] Time:0.231, Train Loss:0.5017440319061279\n",
      "Epoch 0[6925/17270] Time:0.23, Train Loss:0.2408917099237442\n",
      "Epoch 0[6926/17270] Time:0.232, Train Loss:0.36202818155288696\n",
      "Epoch 0[6927/17270] Time:0.243, Train Loss:0.713205099105835\n",
      "Epoch 0[6928/17270] Time:0.239, Train Loss:0.35973063111305237\n",
      "Epoch 0[6929/17270] Time:0.231, Train Loss:0.29467788338661194\n",
      "Epoch 0[6930/17270] Time:0.236, Train Loss:1.1838572025299072\n",
      "Epoch 0[6931/17270] Time:0.227, Train Loss:0.44608527421951294\n",
      "Epoch 0[6932/17270] Time:0.239, Train Loss:0.31992843747138977\n",
      "Epoch 0[6933/17270] Time:0.232, Train Loss:0.42006415128707886\n",
      "Epoch 0[6934/17270] Time:0.231, Train Loss:0.5933599472045898\n",
      "Epoch 0[6935/17270] Time:0.23, Train Loss:0.7846711277961731\n",
      "Epoch 0[6936/17270] Time:0.229, Train Loss:0.6715397834777832\n",
      "Epoch 0[6937/17270] Time:0.239, Train Loss:0.38962647318840027\n",
      "Epoch 0[6938/17270] Time:0.231, Train Loss:0.7108681797981262\n",
      "Epoch 0[6939/17270] Time:0.228, Train Loss:0.5501730442047119\n",
      "Epoch 0[6940/17270] Time:0.238, Train Loss:0.6485269665718079\n",
      "Epoch 0[6941/17270] Time:0.237, Train Loss:0.4862734377384186\n",
      "Epoch 0[6942/17270] Time:0.238, Train Loss:0.24001576006412506\n",
      "Epoch 0[6943/17270] Time:0.225, Train Loss:0.31024882197380066\n",
      "Epoch 0[6944/17270] Time:0.237, Train Loss:0.5026114583015442\n",
      "Epoch 0[6945/17270] Time:0.238, Train Loss:0.49968865513801575\n",
      "Epoch 0[6946/17270] Time:0.23, Train Loss:0.37869319319725037\n",
      "Epoch 0[6947/17270] Time:0.236, Train Loss:0.5666247606277466\n",
      "Epoch 0[6948/17270] Time:0.23, Train Loss:0.7824693918228149\n",
      "Epoch 0[6949/17270] Time:0.239, Train Loss:0.372725248336792\n",
      "Epoch 0[6950/17270] Time:0.252, Train Loss:0.28643110394477844\n",
      "Epoch 0[6951/17270] Time:0.23, Train Loss:0.36088091135025024\n",
      "Epoch 0[6952/17270] Time:0.234, Train Loss:0.7182002663612366\n",
      "Epoch 0[6953/17270] Time:0.23, Train Loss:0.7743678092956543\n",
      "Epoch 0[6954/17270] Time:0.248, Train Loss:0.8207272887229919\n",
      "Epoch 0[6955/17270] Time:0.226, Train Loss:0.44966039061546326\n",
      "Epoch 0[6956/17270] Time:0.233, Train Loss:0.6487107276916504\n",
      "Epoch 0[6957/17270] Time:0.24, Train Loss:0.7228080034255981\n",
      "Epoch 0[6958/17270] Time:0.253, Train Loss:0.46147865056991577\n",
      "Epoch 0[6959/17270] Time:0.241, Train Loss:0.4681125581264496\n",
      "Epoch 0[6960/17270] Time:0.238, Train Loss:2.1768102645874023\n",
      "Epoch 0[6961/17270] Time:0.23, Train Loss:0.5890281796455383\n",
      "Epoch 0[6962/17270] Time:0.243, Train Loss:0.9582722783088684\n",
      "Epoch 0[6963/17270] Time:0.238, Train Loss:0.6153044104576111\n",
      "Epoch 0[6964/17270] Time:0.227, Train Loss:0.6523293256759644\n",
      "Epoch 0[6965/17270] Time:0.223, Train Loss:0.7953761219978333\n",
      "Epoch 0[6966/17270] Time:0.227, Train Loss:0.6179659962654114\n",
      "Epoch 0[6967/17270] Time:0.239, Train Loss:0.6173084378242493\n",
      "Epoch 0[6968/17270] Time:0.23, Train Loss:0.5278167724609375\n",
      "Epoch 0[6969/17270] Time:0.235, Train Loss:0.6881749629974365\n",
      "Epoch 0[6970/17270] Time:0.239, Train Loss:0.49287790060043335\n",
      "Epoch 0[6971/17270] Time:0.238, Train Loss:0.7758795619010925\n",
      "Epoch 0[6972/17270] Time:0.229, Train Loss:0.8962666988372803\n",
      "Epoch 0[6973/17270] Time:0.227, Train Loss:0.510627269744873\n",
      "Epoch 0[6974/17270] Time:0.227, Train Loss:0.4946095943450928\n",
      "Epoch 0[6975/17270] Time:0.228, Train Loss:0.6317364573478699\n",
      "Epoch 0[6976/17270] Time:0.23, Train Loss:0.6563310027122498\n",
      "Epoch 0[6977/17270] Time:0.228, Train Loss:0.47613829374313354\n",
      "Epoch 0[6978/17270] Time:0.235, Train Loss:0.8496232628822327\n",
      "Epoch 0[6979/17270] Time:0.233, Train Loss:0.8988395929336548\n",
      "Epoch 0[6980/17270] Time:0.233, Train Loss:0.911005973815918\n",
      "Epoch 0[6981/17270] Time:0.223, Train Loss:0.3853684961795807\n",
      "Epoch 0[6982/17270] Time:0.228, Train Loss:0.5067685842514038\n",
      "Epoch 0[6983/17270] Time:0.236, Train Loss:0.5360804200172424\n",
      "Epoch 0[6984/17270] Time:0.234, Train Loss:0.3850814402103424\n",
      "Epoch 0[6985/17270] Time:0.245, Train Loss:1.1691741943359375\n",
      "Epoch 0[6986/17270] Time:0.236, Train Loss:0.8923851847648621\n",
      "Epoch 0[6987/17270] Time:0.231, Train Loss:0.5647025108337402\n",
      "Epoch 0[6988/17270] Time:0.232, Train Loss:1.0624098777770996\n",
      "Epoch 0[6989/17270] Time:0.235, Train Loss:1.7796003818511963\n",
      "Epoch 0[6990/17270] Time:0.241, Train Loss:0.4455977976322174\n",
      "Epoch 0[6991/17270] Time:0.233, Train Loss:0.4217047095298767\n",
      "Epoch 0[6992/17270] Time:0.235, Train Loss:0.7596301436424255\n",
      "Epoch 0[6993/17270] Time:0.23, Train Loss:0.6865193247795105\n",
      "Epoch 0[6994/17270] Time:0.229, Train Loss:0.7735844254493713\n",
      "Epoch 0[6995/17270] Time:0.228, Train Loss:0.7113929986953735\n",
      "Epoch 0[6996/17270] Time:0.237, Train Loss:0.4549603760242462\n",
      "Epoch 0[6997/17270] Time:0.222, Train Loss:0.8096778392791748\n",
      "Epoch 0[6998/17270] Time:0.232, Train Loss:0.373009592294693\n",
      "Epoch 0[6999/17270] Time:0.23, Train Loss:0.6638336181640625\n",
      "Epoch 0[7000/17270] Time:0.229, Train Loss:0.6630792617797852\n",
      "Epoch 0[7001/17270] Time:0.238, Train Loss:0.7945958971977234\n",
      "Epoch 0[7002/17270] Time:0.229, Train Loss:0.5088917016983032\n",
      "Epoch 0[7003/17270] Time:0.245, Train Loss:0.6214554309844971\n",
      "Epoch 0[7004/17270] Time:0.244, Train Loss:1.0569658279418945\n",
      "Epoch 0[7005/17270] Time:0.246, Train Loss:0.27178114652633667\n",
      "Epoch 0[7006/17270] Time:0.236, Train Loss:0.3624066710472107\n",
      "Epoch 0[7007/17270] Time:0.234, Train Loss:0.47913116216659546\n",
      "Epoch 0[7008/17270] Time:0.227, Train Loss:0.6584805250167847\n",
      "Epoch 0[7009/17270] Time:0.232, Train Loss:0.4217371940612793\n",
      "Epoch 0[7010/17270] Time:0.231, Train Loss:0.4343038499355316\n",
      "Epoch 0[7011/17270] Time:0.23, Train Loss:0.36859893798828125\n",
      "Epoch 0[7012/17270] Time:0.23, Train Loss:0.638071596622467\n",
      "Epoch 0[7013/17270] Time:0.236, Train Loss:0.9784551858901978\n",
      "Epoch 0[7014/17270] Time:0.23, Train Loss:0.4145277440547943\n",
      "Epoch 0[7015/17270] Time:0.236, Train Loss:0.4121452569961548\n",
      "Epoch 0[7016/17270] Time:0.234, Train Loss:0.9280007481575012\n",
      "Epoch 0[7017/17270] Time:0.238, Train Loss:0.3442222476005554\n",
      "Epoch 0[7018/17270] Time:0.23, Train Loss:0.5371367335319519\n",
      "Epoch 0[7019/17270] Time:0.231, Train Loss:0.33417949080467224\n",
      "Epoch 0[7020/17270] Time:0.238, Train Loss:1.0003291368484497\n",
      "Epoch 0[7021/17270] Time:0.236, Train Loss:0.982703447341919\n",
      "Epoch 0[7022/17270] Time:0.244, Train Loss:0.6543317437171936\n",
      "Epoch 0[7023/17270] Time:0.228, Train Loss:0.36438170075416565\n",
      "Epoch 0[7024/17270] Time:0.244, Train Loss:0.5065141916275024\n",
      "Epoch 0[7025/17270] Time:0.216, Train Loss:1.213515043258667\n",
      "Epoch 0[7026/17270] Time:0.24, Train Loss:0.5378604531288147\n",
      "Epoch 0[7027/17270] Time:0.232, Train Loss:0.7735101580619812\n",
      "Epoch 0[7028/17270] Time:0.226, Train Loss:0.6618785858154297\n",
      "Epoch 0[7029/17270] Time:0.228, Train Loss:0.3614584803581238\n",
      "Epoch 0[7030/17270] Time:0.233, Train Loss:0.49799928069114685\n",
      "Epoch 0[7031/17270] Time:0.228, Train Loss:0.6110028028488159\n",
      "Epoch 0[7032/17270] Time:0.238, Train Loss:0.38311561942100525\n",
      "Epoch 0[7033/17270] Time:0.236, Train Loss:1.2736183404922485\n",
      "Epoch 0[7034/17270] Time:0.235, Train Loss:0.5564029216766357\n",
      "Epoch 0[7035/17270] Time:0.237, Train Loss:0.5867384076118469\n",
      "Epoch 0[7036/17270] Time:0.232, Train Loss:0.9796662926673889\n",
      "Epoch 0[7037/17270] Time:0.228, Train Loss:0.4087948799133301\n",
      "Epoch 0[7038/17270] Time:0.235, Train Loss:0.76875239610672\n",
      "Epoch 0[7039/17270] Time:0.238, Train Loss:0.4022071361541748\n",
      "Epoch 0[7040/17270] Time:0.23, Train Loss:0.9488224387168884\n",
      "Epoch 0[7041/17270] Time:0.24, Train Loss:0.4801699221134186\n",
      "Epoch 0[7042/17270] Time:0.236, Train Loss:0.6923838257789612\n",
      "Epoch 0[7043/17270] Time:0.228, Train Loss:0.4791080057621002\n",
      "Epoch 0[7044/17270] Time:0.239, Train Loss:0.38875147700309753\n",
      "Epoch 0[7045/17270] Time:0.231, Train Loss:0.964478075504303\n",
      "Epoch 0[7046/17270] Time:0.242, Train Loss:0.9354323148727417\n",
      "Epoch 0[7047/17270] Time:0.24, Train Loss:0.5115513801574707\n",
      "Epoch 0[7048/17270] Time:0.234, Train Loss:0.5879440903663635\n",
      "Epoch 0[7049/17270] Time:0.226, Train Loss:0.40746474266052246\n",
      "Epoch 0[7050/17270] Time:0.237, Train Loss:0.6204521059989929\n",
      "Epoch 0[7051/17270] Time:0.231, Train Loss:0.4623763859272003\n",
      "Epoch 0[7052/17270] Time:0.237, Train Loss:0.5116496086120605\n",
      "Epoch 0[7053/17270] Time:0.246, Train Loss:0.7335017919540405\n",
      "Epoch 0[7054/17270] Time:0.236, Train Loss:0.866724967956543\n",
      "Epoch 0[7055/17270] Time:0.233, Train Loss:0.6089339256286621\n",
      "Epoch 0[7056/17270] Time:0.234, Train Loss:0.4265349209308624\n",
      "Epoch 0[7057/17270] Time:0.245, Train Loss:0.4458133578300476\n",
      "Epoch 0[7058/17270] Time:0.22, Train Loss:0.7419347763061523\n",
      "Epoch 0[7059/17270] Time:0.241, Train Loss:0.5085902214050293\n",
      "Epoch 0[7060/17270] Time:0.228, Train Loss:0.482982337474823\n",
      "Epoch 0[7061/17270] Time:0.229, Train Loss:0.5231252908706665\n",
      "Epoch 0[7062/17270] Time:0.236, Train Loss:0.7168182134628296\n",
      "Epoch 0[7063/17270] Time:0.231, Train Loss:0.4779861569404602\n",
      "Epoch 0[7064/17270] Time:0.24, Train Loss:0.5006158947944641\n",
      "Epoch 0[7065/17270] Time:0.23, Train Loss:0.492329478263855\n",
      "Epoch 0[7066/17270] Time:0.233, Train Loss:0.49873948097229004\n",
      "Epoch 0[7067/17270] Time:0.235, Train Loss:0.9238828420639038\n",
      "Epoch 0[7068/17270] Time:0.236, Train Loss:0.8553322553634644\n",
      "Epoch 0[7069/17270] Time:0.238, Train Loss:0.8282055258750916\n",
      "Epoch 0[7070/17270] Time:0.23, Train Loss:1.1459208726882935\n",
      "Epoch 0[7071/17270] Time:0.23, Train Loss:0.36037880182266235\n",
      "Epoch 0[7072/17270] Time:0.231, Train Loss:0.47732964158058167\n",
      "Epoch 0[7073/17270] Time:0.225, Train Loss:0.5564245581626892\n",
      "Epoch 0[7074/17270] Time:0.238, Train Loss:0.42719823122024536\n",
      "Epoch 0[7075/17270] Time:0.237, Train Loss:0.5120466947555542\n",
      "Epoch 0[7076/17270] Time:0.239, Train Loss:0.44739440083503723\n",
      "Epoch 0[7077/17270] Time:0.23, Train Loss:0.7941864728927612\n",
      "Epoch 0[7078/17270] Time:0.229, Train Loss:0.8382694125175476\n",
      "Epoch 0[7079/17270] Time:0.229, Train Loss:0.6854140162467957\n",
      "Epoch 0[7080/17270] Time:0.239, Train Loss:0.7093716859817505\n",
      "Epoch 0[7081/17270] Time:0.237, Train Loss:1.0178470611572266\n",
      "Epoch 0[7082/17270] Time:0.235, Train Loss:0.610146701335907\n",
      "Epoch 0[7083/17270] Time:0.235, Train Loss:0.5037418007850647\n",
      "Epoch 0[7084/17270] Time:0.237, Train Loss:0.3559982478618622\n",
      "Epoch 0[7085/17270] Time:0.227, Train Loss:0.535904049873352\n",
      "Epoch 0[7086/17270] Time:0.239, Train Loss:0.8802085518836975\n",
      "Epoch 0[7087/17270] Time:0.247, Train Loss:0.4409550130367279\n",
      "Epoch 0[7088/17270] Time:0.229, Train Loss:0.7614834904670715\n",
      "Epoch 0[7089/17270] Time:0.244, Train Loss:0.6713152527809143\n",
      "Epoch 0[7090/17270] Time:0.239, Train Loss:0.7386579513549805\n",
      "Epoch 0[7091/17270] Time:0.244, Train Loss:0.6067856550216675\n",
      "Epoch 0[7092/17270] Time:0.25, Train Loss:0.43357065320014954\n",
      "Epoch 0[7093/17270] Time:0.228, Train Loss:0.4436563551425934\n",
      "Epoch 0[7094/17270] Time:0.241, Train Loss:0.5169395208358765\n",
      "Epoch 0[7095/17270] Time:0.24, Train Loss:0.30485033988952637\n",
      "Epoch 0[7096/17270] Time:0.239, Train Loss:0.45964398980140686\n",
      "Epoch 0[7097/17270] Time:0.232, Train Loss:0.3559284210205078\n",
      "Epoch 0[7098/17270] Time:0.227, Train Loss:0.5475199222564697\n",
      "Epoch 0[7099/17270] Time:0.232, Train Loss:0.5227128863334656\n",
      "Epoch 0[7100/17270] Time:0.239, Train Loss:0.9820593595504761\n",
      "Epoch 0[7101/17270] Time:0.231, Train Loss:0.5703716278076172\n",
      "Epoch 0[7102/17270] Time:0.235, Train Loss:0.7623957991600037\n",
      "Epoch 0[7103/17270] Time:0.236, Train Loss:0.4447975754737854\n",
      "Epoch 0[7104/17270] Time:0.237, Train Loss:0.4998651444911957\n",
      "Epoch 0[7105/17270] Time:0.236, Train Loss:0.37451040744781494\n",
      "Epoch 0[7106/17270] Time:0.233, Train Loss:0.6601967215538025\n",
      "Epoch 0[7107/17270] Time:0.228, Train Loss:0.3613566756248474\n",
      "Epoch 0[7108/17270] Time:0.239, Train Loss:0.3097916841506958\n",
      "Epoch 0[7109/17270] Time:0.233, Train Loss:1.0166949033737183\n",
      "Epoch 0[7110/17270] Time:0.227, Train Loss:0.30159759521484375\n",
      "Epoch 0[7111/17270] Time:0.23, Train Loss:1.042270302772522\n",
      "Epoch 0[7112/17270] Time:0.225, Train Loss:0.316715270280838\n",
      "Epoch 0[7113/17270] Time:0.231, Train Loss:0.5087060332298279\n",
      "Epoch 0[7114/17270] Time:0.237, Train Loss:0.5862569212913513\n",
      "Epoch 0[7115/17270] Time:0.236, Train Loss:0.2534945011138916\n",
      "Epoch 0[7116/17270] Time:0.236, Train Loss:0.3463399410247803\n",
      "Epoch 0[7117/17270] Time:0.236, Train Loss:0.7921743392944336\n",
      "Epoch 0[7118/17270] Time:0.232, Train Loss:1.1078559160232544\n",
      "Epoch 0[7119/17270] Time:0.236, Train Loss:0.633351743221283\n",
      "Epoch 0[7120/17270] Time:0.237, Train Loss:0.5653136968612671\n",
      "Epoch 0[7121/17270] Time:0.238, Train Loss:0.5055385231971741\n",
      "Epoch 0[7122/17270] Time:0.232, Train Loss:0.5149096250534058\n",
      "Epoch 0[7123/17270] Time:0.229, Train Loss:0.6679487228393555\n",
      "Epoch 0[7124/17270] Time:0.247, Train Loss:0.5780348777770996\n",
      "Epoch 0[7125/17270] Time:0.241, Train Loss:0.7197378277778625\n",
      "Epoch 0[7126/17270] Time:0.244, Train Loss:0.8008646368980408\n",
      "Epoch 0[7127/17270] Time:0.23, Train Loss:1.0783193111419678\n",
      "Epoch 0[7128/17270] Time:0.236, Train Loss:0.626609742641449\n",
      "Epoch 0[7129/17270] Time:0.234, Train Loss:0.5304564833641052\n",
      "Epoch 0[7130/17270] Time:0.233, Train Loss:0.8838813304901123\n",
      "Epoch 0[7131/17270] Time:0.236, Train Loss:0.549170732498169\n",
      "Epoch 0[7132/17270] Time:0.237, Train Loss:0.4678904414176941\n",
      "Epoch 0[7133/17270] Time:0.236, Train Loss:0.5602700114250183\n",
      "Epoch 0[7134/17270] Time:0.225, Train Loss:0.4154900312423706\n",
      "Epoch 0[7135/17270] Time:0.245, Train Loss:0.6737776398658752\n",
      "Epoch 0[7136/17270] Time:0.232, Train Loss:0.6082262396812439\n",
      "Epoch 0[7137/17270] Time:0.239, Train Loss:0.6805274486541748\n",
      "Epoch 0[7138/17270] Time:0.233, Train Loss:0.6293367147445679\n",
      "Epoch 0[7139/17270] Time:0.238, Train Loss:0.64093017578125\n",
      "Epoch 0[7140/17270] Time:0.233, Train Loss:0.6678658723831177\n",
      "Epoch 0[7141/17270] Time:0.234, Train Loss:0.47698086500167847\n",
      "Epoch 0[7142/17270] Time:0.234, Train Loss:0.6166718602180481\n",
      "Epoch 0[7143/17270] Time:0.23, Train Loss:0.601699948310852\n",
      "Epoch 0[7144/17270] Time:0.234, Train Loss:0.5933932662010193\n",
      "Epoch 0[7145/17270] Time:0.234, Train Loss:0.6267970204353333\n",
      "Epoch 0[7146/17270] Time:0.237, Train Loss:0.4621911346912384\n",
      "Epoch 0[7147/17270] Time:0.224, Train Loss:0.6421340703964233\n",
      "Epoch 0[7148/17270] Time:0.237, Train Loss:1.1313350200653076\n",
      "Epoch 0[7149/17270] Time:0.235, Train Loss:0.6468850374221802\n",
      "Epoch 0[7150/17270] Time:0.239, Train Loss:1.1956489086151123\n",
      "Epoch 0[7151/17270] Time:0.23, Train Loss:0.6380406022071838\n",
      "Epoch 0[7152/17270] Time:0.229, Train Loss:1.3061195611953735\n",
      "Epoch 0[7153/17270] Time:0.238, Train Loss:0.4440006613731384\n",
      "Epoch 0[7154/17270] Time:0.241, Train Loss:0.532411515712738\n",
      "Epoch 0[7155/17270] Time:0.235, Train Loss:0.7927746772766113\n",
      "Epoch 0[7156/17270] Time:0.23, Train Loss:0.64813631772995\n",
      "Epoch 0[7157/17270] Time:0.24, Train Loss:0.6577292084693909\n",
      "Epoch 0[7158/17270] Time:0.23, Train Loss:0.6449923515319824\n",
      "Epoch 0[7159/17270] Time:0.232, Train Loss:0.49948805570602417\n",
      "Epoch 0[7160/17270] Time:0.239, Train Loss:0.8455545902252197\n",
      "Epoch 0[7161/17270] Time:0.228, Train Loss:0.7651856541633606\n",
      "Epoch 0[7162/17270] Time:0.238, Train Loss:0.30242741107940674\n",
      "Epoch 0[7163/17270] Time:0.237, Train Loss:0.4758358299732208\n",
      "Epoch 0[7164/17270] Time:0.231, Train Loss:0.5204738974571228\n",
      "Epoch 0[7165/17270] Time:0.23, Train Loss:0.559478759765625\n",
      "Epoch 0[7166/17270] Time:0.239, Train Loss:1.3544113636016846\n",
      "Epoch 0[7167/17270] Time:0.236, Train Loss:1.0872091054916382\n",
      "Epoch 0[7168/17270] Time:0.243, Train Loss:0.7874943614006042\n",
      "Epoch 0[7169/17270] Time:0.231, Train Loss:0.726538360118866\n",
      "Epoch 0[7170/17270] Time:0.243, Train Loss:0.40284693241119385\n",
      "Epoch 0[7171/17270] Time:0.234, Train Loss:0.8507255911827087\n",
      "Epoch 0[7172/17270] Time:0.238, Train Loss:0.5782649517059326\n",
      "Epoch 0[7173/17270] Time:0.235, Train Loss:0.46590158343315125\n",
      "Epoch 0[7174/17270] Time:0.228, Train Loss:1.2710320949554443\n",
      "Epoch 0[7175/17270] Time:0.227, Train Loss:1.065406084060669\n",
      "Epoch 0[7176/17270] Time:0.228, Train Loss:0.572607159614563\n",
      "Epoch 0[7177/17270] Time:0.239, Train Loss:0.5175873041152954\n",
      "Epoch 0[7178/17270] Time:0.244, Train Loss:0.8162873387336731\n",
      "Epoch 0[7179/17270] Time:0.229, Train Loss:1.0038402080535889\n",
      "Epoch 0[7180/17270] Time:0.24, Train Loss:0.3906801640987396\n",
      "Epoch 0[7181/17270] Time:0.232, Train Loss:0.9342103600502014\n",
      "Epoch 0[7182/17270] Time:0.236, Train Loss:0.46121445298194885\n",
      "Epoch 0[7183/17270] Time:0.237, Train Loss:0.8378329873085022\n",
      "Epoch 0[7184/17270] Time:0.229, Train Loss:0.4953967034816742\n",
      "Epoch 0[7185/17270] Time:0.232, Train Loss:0.4222191572189331\n",
      "Epoch 0[7186/17270] Time:0.228, Train Loss:0.5317730903625488\n",
      "Epoch 0[7187/17270] Time:0.24, Train Loss:0.9203689098358154\n",
      "Epoch 0[7188/17270] Time:0.238, Train Loss:0.4942953586578369\n",
      "Epoch 0[7189/17270] Time:0.231, Train Loss:0.979950487613678\n",
      "Epoch 0[7190/17270] Time:0.238, Train Loss:0.8322872519493103\n",
      "Epoch 0[7191/17270] Time:0.232, Train Loss:0.4663461744785309\n",
      "Epoch 0[7192/17270] Time:0.231, Train Loss:0.7211664915084839\n",
      "Epoch 0[7193/17270] Time:0.239, Train Loss:0.6163814663887024\n",
      "Epoch 0[7194/17270] Time:0.242, Train Loss:0.5421445965766907\n",
      "Epoch 0[7195/17270] Time:0.232, Train Loss:0.6098181009292603\n",
      "Epoch 0[7196/17270] Time:0.226, Train Loss:0.45893529057502747\n",
      "Epoch 0[7197/17270] Time:0.236, Train Loss:0.4403139352798462\n",
      "Epoch 0[7198/17270] Time:0.238, Train Loss:0.5546841025352478\n",
      "Epoch 0[7199/17270] Time:0.237, Train Loss:0.757002592086792\n",
      "Epoch 0[7200/17270] Time:0.243, Train Loss:0.7281728386878967\n",
      "Epoch 0[7201/17270] Time:0.236, Train Loss:0.36840999126434326\n",
      "Epoch 0[7202/17270] Time:0.236, Train Loss:0.6006984114646912\n",
      "Epoch 0[7203/17270] Time:0.234, Train Loss:0.8145599961280823\n",
      "Epoch 0[7204/17270] Time:0.229, Train Loss:0.5233744382858276\n",
      "Epoch 0[7205/17270] Time:0.235, Train Loss:0.774106502532959\n",
      "Epoch 0[7206/17270] Time:0.237, Train Loss:0.3666525185108185\n",
      "Epoch 0[7207/17270] Time:0.243, Train Loss:0.6828183531761169\n",
      "Epoch 0[7208/17270] Time:0.225, Train Loss:0.5756003856658936\n",
      "Epoch 0[7209/17270] Time:0.226, Train Loss:0.48868584632873535\n",
      "Epoch 0[7210/17270] Time:0.228, Train Loss:0.4331653118133545\n",
      "Epoch 0[7211/17270] Time:0.239, Train Loss:0.5300801992416382\n",
      "Epoch 0[7212/17270] Time:0.238, Train Loss:0.44515708088874817\n",
      "Epoch 0[7213/17270] Time:0.235, Train Loss:0.6577758193016052\n",
      "Epoch 0[7214/17270] Time:0.242, Train Loss:0.40672537684440613\n",
      "Epoch 0[7215/17270] Time:0.221, Train Loss:1.015474796295166\n",
      "Epoch 0[7216/17270] Time:0.228, Train Loss:0.6215680241584778\n",
      "Epoch 0[7217/17270] Time:0.23, Train Loss:0.5897854566574097\n",
      "Epoch 0[7218/17270] Time:0.229, Train Loss:0.7940753698348999\n",
      "Epoch 0[7219/17270] Time:0.233, Train Loss:0.9933263659477234\n",
      "Epoch 0[7220/17270] Time:0.231, Train Loss:0.5782400369644165\n",
      "Epoch 0[7221/17270] Time:0.23, Train Loss:0.5182271003723145\n",
      "Epoch 0[7222/17270] Time:0.23, Train Loss:1.301398754119873\n",
      "Epoch 0[7223/17270] Time:0.228, Train Loss:0.4150293171405792\n",
      "Epoch 0[7224/17270] Time:0.232, Train Loss:0.5540706515312195\n",
      "Epoch 0[7225/17270] Time:0.228, Train Loss:0.3977961838245392\n",
      "Epoch 0[7226/17270] Time:0.236, Train Loss:0.813983678817749\n",
      "Epoch 0[7227/17270] Time:0.241, Train Loss:1.1649503707885742\n",
      "Epoch 0[7228/17270] Time:0.231, Train Loss:0.41227254271507263\n",
      "Epoch 0[7229/17270] Time:0.231, Train Loss:0.5337275266647339\n",
      "Epoch 0[7230/17270] Time:0.236, Train Loss:0.5357868075370789\n",
      "Epoch 0[7231/17270] Time:0.239, Train Loss:0.4372345209121704\n",
      "Epoch 0[7232/17270] Time:0.231, Train Loss:0.5165714025497437\n",
      "Epoch 0[7233/17270] Time:0.23, Train Loss:0.4828304052352905\n",
      "Epoch 0[7234/17270] Time:0.239, Train Loss:0.5320239067077637\n",
      "Epoch 0[7235/17270] Time:0.239, Train Loss:0.6429657936096191\n",
      "Epoch 0[7236/17270] Time:0.235, Train Loss:0.5318188667297363\n",
      "Epoch 0[7237/17270] Time:0.234, Train Loss:0.41416165232658386\n",
      "Epoch 0[7238/17270] Time:0.239, Train Loss:0.6370846629142761\n",
      "Epoch 0[7239/17270] Time:0.231, Train Loss:0.47551101446151733\n",
      "Epoch 0[7240/17270] Time:0.238, Train Loss:0.3795848786830902\n",
      "Epoch 0[7241/17270] Time:0.23, Train Loss:0.3330674171447754\n",
      "Epoch 0[7242/17270] Time:0.238, Train Loss:0.6495351791381836\n",
      "Epoch 0[7243/17270] Time:0.238, Train Loss:0.35111331939697266\n",
      "Epoch 0[7244/17270] Time:0.23, Train Loss:0.5285586714744568\n",
      "Epoch 0[7245/17270] Time:0.238, Train Loss:0.38396158814430237\n",
      "Epoch 0[7246/17270] Time:0.229, Train Loss:0.3893635869026184\n",
      "Epoch 0[7247/17270] Time:0.237, Train Loss:0.37802621722221375\n",
      "Epoch 0[7248/17270] Time:0.232, Train Loss:0.3139864504337311\n",
      "Epoch 0[7249/17270] Time:0.228, Train Loss:0.43881651759147644\n",
      "Epoch 0[7250/17270] Time:0.23, Train Loss:0.5092238783836365\n",
      "Epoch 0[7251/17270] Time:0.239, Train Loss:1.5460714101791382\n",
      "Epoch 0[7252/17270] Time:0.233, Train Loss:0.5020057559013367\n",
      "Epoch 0[7253/17270] Time:0.232, Train Loss:0.5699987411499023\n",
      "Epoch 0[7254/17270] Time:0.231, Train Loss:0.44487395882606506\n",
      "Epoch 0[7255/17270] Time:0.258, Train Loss:0.6236476898193359\n",
      "Epoch 0[7256/17270] Time:0.231, Train Loss:0.9779079556465149\n",
      "Epoch 0[7257/17270] Time:0.236, Train Loss:0.6743898987770081\n",
      "Epoch 0[7258/17270] Time:0.236, Train Loss:0.7153591513633728\n",
      "Epoch 0[7259/17270] Time:0.231, Train Loss:0.7776000499725342\n",
      "Epoch 0[7260/17270] Time:0.231, Train Loss:0.3498248755931854\n",
      "Epoch 0[7261/17270] Time:0.234, Train Loss:0.547457218170166\n",
      "Epoch 0[7262/17270] Time:0.224, Train Loss:0.8004611730575562\n",
      "Epoch 0[7263/17270] Time:0.245, Train Loss:0.7061089873313904\n",
      "Epoch 0[7264/17270] Time:0.232, Train Loss:0.5815333127975464\n",
      "Epoch 0[7265/17270] Time:0.231, Train Loss:0.8540235161781311\n",
      "Epoch 0[7266/17270] Time:0.232, Train Loss:0.6277472972869873\n",
      "Epoch 0[7267/17270] Time:0.236, Train Loss:0.5452419519424438\n",
      "Epoch 0[7268/17270] Time:0.237, Train Loss:0.5761325359344482\n",
      "Epoch 0[7269/17270] Time:0.229, Train Loss:0.5024740695953369\n",
      "Epoch 0[7270/17270] Time:0.23, Train Loss:0.6232334971427917\n",
      "Epoch 0[7271/17270] Time:0.238, Train Loss:0.4010588824748993\n",
      "Epoch 0[7272/17270] Time:0.228, Train Loss:0.7432163953781128\n",
      "Epoch 0[7273/17270] Time:0.226, Train Loss:1.099356770515442\n",
      "Epoch 0[7274/17270] Time:0.239, Train Loss:0.43803641200065613\n",
      "Epoch 0[7275/17270] Time:0.231, Train Loss:0.7353007793426514\n",
      "Epoch 0[7276/17270] Time:0.238, Train Loss:0.8312898278236389\n",
      "Epoch 0[7277/17270] Time:0.233, Train Loss:0.4905462861061096\n",
      "Epoch 0[7278/17270] Time:0.237, Train Loss:0.5564863085746765\n",
      "Epoch 0[7279/17270] Time:0.237, Train Loss:0.6382236480712891\n",
      "Epoch 0[7280/17270] Time:0.252, Train Loss:0.5442507266998291\n",
      "Epoch 0[7281/17270] Time:0.229, Train Loss:0.5028627514839172\n",
      "Epoch 0[7282/17270] Time:0.238, Train Loss:0.3808308243751526\n",
      "Epoch 0[7283/17270] Time:0.229, Train Loss:0.5200669765472412\n",
      "Epoch 0[7284/17270] Time:0.235, Train Loss:0.5651633143424988\n",
      "Epoch 0[7285/17270] Time:0.239, Train Loss:0.49512621760368347\n",
      "Epoch 0[7286/17270] Time:0.229, Train Loss:0.6639233827590942\n",
      "Epoch 0[7287/17270] Time:0.24, Train Loss:0.8687921762466431\n",
      "Epoch 0[7288/17270] Time:0.239, Train Loss:0.5994003415107727\n",
      "Epoch 0[7289/17270] Time:0.236, Train Loss:0.47785842418670654\n",
      "Epoch 0[7290/17270] Time:0.237, Train Loss:0.4802042543888092\n",
      "Epoch 0[7291/17270] Time:0.24, Train Loss:0.5535721182823181\n",
      "Epoch 0[7292/17270] Time:0.233, Train Loss:0.49746352434158325\n",
      "Epoch 0[7293/17270] Time:0.241, Train Loss:0.5125221014022827\n",
      "Epoch 0[7294/17270] Time:0.236, Train Loss:0.7792947888374329\n",
      "Epoch 0[7295/17270] Time:0.235, Train Loss:0.656160831451416\n",
      "Epoch 0[7296/17270] Time:0.233, Train Loss:0.3649362325668335\n",
      "Epoch 0[7297/17270] Time:0.237, Train Loss:0.7834460735321045\n",
      "Epoch 0[7298/17270] Time:0.237, Train Loss:0.9002987742424011\n",
      "Epoch 0[7299/17270] Time:0.222, Train Loss:0.8629133105278015\n",
      "Epoch 0[7300/17270] Time:0.241, Train Loss:0.48265740275382996\n",
      "Epoch 0[7301/17270] Time:0.235, Train Loss:0.45585983991622925\n",
      "Epoch 0[7302/17270] Time:0.23, Train Loss:0.9891669750213623\n",
      "Epoch 0[7303/17270] Time:0.227, Train Loss:0.33972790837287903\n",
      "Epoch 0[7304/17270] Time:0.239, Train Loss:0.5740182995796204\n",
      "Epoch 0[7305/17270] Time:0.229, Train Loss:0.5533021688461304\n",
      "Epoch 0[7306/17270] Time:0.239, Train Loss:1.1739559173583984\n",
      "Epoch 0[7307/17270] Time:0.23, Train Loss:0.5411985516548157\n",
      "Epoch 0[7308/17270] Time:0.23, Train Loss:0.44750821590423584\n",
      "Epoch 0[7309/17270] Time:0.231, Train Loss:0.8491259217262268\n",
      "Epoch 0[7310/17270] Time:0.23, Train Loss:0.4182976484298706\n",
      "Epoch 0[7311/17270] Time:0.229, Train Loss:0.29908233880996704\n",
      "Epoch 0[7312/17270] Time:0.23, Train Loss:0.6876704692840576\n",
      "Epoch 0[7313/17270] Time:0.245, Train Loss:0.5850149989128113\n",
      "Epoch 0[7314/17270] Time:0.233, Train Loss:0.707252025604248\n",
      "Epoch 0[7315/17270] Time:0.224, Train Loss:0.8200825452804565\n",
      "Epoch 0[7316/17270] Time:0.23, Train Loss:0.3674941658973694\n",
      "Epoch 0[7317/17270] Time:0.228, Train Loss:0.7933328151702881\n",
      "Epoch 0[7318/17270] Time:0.227, Train Loss:0.5742114782333374\n",
      "Epoch 0[7319/17270] Time:0.231, Train Loss:0.5903671979904175\n",
      "Epoch 0[7320/17270] Time:0.235, Train Loss:0.9196193814277649\n",
      "Epoch 0[7321/17270] Time:0.237, Train Loss:0.3275397717952728\n",
      "Epoch 0[7322/17270] Time:0.239, Train Loss:0.7321699857711792\n",
      "Epoch 0[7323/17270] Time:0.229, Train Loss:0.6387277841567993\n",
      "Epoch 0[7324/17270] Time:0.238, Train Loss:0.38696274161338806\n",
      "Epoch 0[7325/17270] Time:0.237, Train Loss:0.5663824081420898\n",
      "Epoch 0[7326/17270] Time:0.236, Train Loss:0.5885933041572571\n",
      "Epoch 0[7327/17270] Time:0.239, Train Loss:1.0632046461105347\n",
      "Epoch 0[7328/17270] Time:0.228, Train Loss:0.4325355589389801\n",
      "Epoch 0[7329/17270] Time:0.239, Train Loss:0.7388787269592285\n",
      "Epoch 0[7330/17270] Time:0.241, Train Loss:0.5983971953392029\n",
      "Epoch 0[7331/17270] Time:0.241, Train Loss:0.44927340745925903\n",
      "Epoch 0[7332/17270] Time:0.241, Train Loss:0.41181543469429016\n",
      "Epoch 0[7333/17270] Time:0.238, Train Loss:0.9557427167892456\n",
      "Epoch 0[7334/17270] Time:0.24, Train Loss:0.43127503991127014\n",
      "Epoch 0[7335/17270] Time:0.247, Train Loss:0.3826223611831665\n",
      "Epoch 0[7336/17270] Time:0.234, Train Loss:0.4820728003978729\n",
      "Epoch 0[7337/17270] Time:0.227, Train Loss:0.6594330072402954\n",
      "Epoch 0[7338/17270] Time:0.236, Train Loss:0.6103665828704834\n",
      "Epoch 0[7339/17270] Time:0.232, Train Loss:1.0640114545822144\n",
      "Epoch 0[7340/17270] Time:0.232, Train Loss:0.7140936851501465\n",
      "Epoch 0[7341/17270] Time:0.249, Train Loss:0.7242626547813416\n",
      "Epoch 0[7342/17270] Time:0.24, Train Loss:0.468517929315567\n",
      "Epoch 0[7343/17270] Time:0.238, Train Loss:0.601929247379303\n",
      "Epoch 0[7344/17270] Time:0.238, Train Loss:0.6399316191673279\n",
      "Epoch 0[7345/17270] Time:0.222, Train Loss:0.42423611879348755\n",
      "Epoch 0[7346/17270] Time:0.251, Train Loss:0.47316035628318787\n",
      "Epoch 0[7347/17270] Time:0.228, Train Loss:0.929763913154602\n",
      "Epoch 0[7348/17270] Time:0.224, Train Loss:0.474200040102005\n",
      "Epoch 0[7349/17270] Time:0.237, Train Loss:0.5823187232017517\n",
      "Epoch 0[7350/17270] Time:0.237, Train Loss:0.5251180529594421\n",
      "Epoch 0[7351/17270] Time:0.233, Train Loss:0.3969120383262634\n",
      "Epoch 0[7352/17270] Time:0.228, Train Loss:0.7965260148048401\n",
      "Epoch 0[7353/17270] Time:0.231, Train Loss:0.4248470067977905\n",
      "Epoch 0[7354/17270] Time:0.238, Train Loss:0.3943514823913574\n",
      "Epoch 0[7355/17270] Time:0.23, Train Loss:0.6903052926063538\n",
      "Epoch 0[7356/17270] Time:0.231, Train Loss:1.1156359910964966\n",
      "Epoch 0[7357/17270] Time:0.236, Train Loss:0.36247533559799194\n",
      "Epoch 0[7358/17270] Time:0.239, Train Loss:0.5463860630989075\n",
      "Epoch 0[7359/17270] Time:0.238, Train Loss:0.4042275547981262\n",
      "Epoch 0[7360/17270] Time:0.23, Train Loss:0.6058239936828613\n",
      "Epoch 0[7361/17270] Time:0.237, Train Loss:0.39289921522140503\n",
      "Epoch 0[7362/17270] Time:0.237, Train Loss:0.2758452296257019\n",
      "Epoch 0[7363/17270] Time:0.23, Train Loss:0.5751248002052307\n",
      "Epoch 0[7364/17270] Time:0.238, Train Loss:0.6581960320472717\n",
      "Epoch 0[7365/17270] Time:0.232, Train Loss:0.3075488805770874\n",
      "Epoch 0[7366/17270] Time:0.231, Train Loss:0.3977563679218292\n",
      "Epoch 0[7367/17270] Time:0.229, Train Loss:0.5399608612060547\n",
      "Epoch 0[7368/17270] Time:0.229, Train Loss:0.6448708176612854\n",
      "Epoch 0[7369/17270] Time:0.236, Train Loss:0.6413920521736145\n",
      "Epoch 0[7370/17270] Time:0.233, Train Loss:0.5794937014579773\n",
      "Epoch 0[7371/17270] Time:0.229, Train Loss:0.4815221130847931\n",
      "Epoch 0[7372/17270] Time:0.235, Train Loss:0.27550989389419556\n",
      "Epoch 0[7373/17270] Time:0.249, Train Loss:0.7424092292785645\n",
      "Epoch 0[7374/17270] Time:0.239, Train Loss:1.0610566139221191\n",
      "Epoch 0[7375/17270] Time:0.243, Train Loss:0.36092814803123474\n",
      "Epoch 0[7376/17270] Time:0.232, Train Loss:0.3736785054206848\n",
      "Epoch 0[7377/17270] Time:0.227, Train Loss:0.8711448311805725\n",
      "Epoch 0[7378/17270] Time:0.224, Train Loss:0.3765101134777069\n",
      "Epoch 0[7379/17270] Time:0.239, Train Loss:0.6776441931724548\n",
      "Epoch 0[7380/17270] Time:0.239, Train Loss:0.379655659198761\n",
      "Epoch 0[7381/17270] Time:0.23, Train Loss:1.5806885957717896\n",
      "Epoch 0[7382/17270] Time:0.236, Train Loss:0.45866671204566956\n",
      "Epoch 0[7383/17270] Time:0.231, Train Loss:0.6477463841438293\n",
      "Epoch 0[7384/17270] Time:0.237, Train Loss:0.5150282382965088\n",
      "Epoch 0[7385/17270] Time:0.229, Train Loss:0.41499146819114685\n",
      "Epoch 0[7386/17270] Time:0.228, Train Loss:0.6640773415565491\n",
      "Epoch 0[7387/17270] Time:0.231, Train Loss:0.3107261061668396\n",
      "Epoch 0[7388/17270] Time:0.236, Train Loss:1.122187852859497\n",
      "Epoch 0[7389/17270] Time:0.241, Train Loss:0.34388670325279236\n",
      "Epoch 0[7390/17270] Time:0.229, Train Loss:0.3794170022010803\n",
      "Epoch 0[7391/17270] Time:0.237, Train Loss:0.6816194653511047\n",
      "Epoch 0[7392/17270] Time:0.239, Train Loss:1.2194398641586304\n",
      "Epoch 0[7393/17270] Time:0.229, Train Loss:0.629905104637146\n",
      "Epoch 0[7394/17270] Time:0.23, Train Loss:0.6015024185180664\n",
      "Epoch 0[7395/17270] Time:0.231, Train Loss:0.6323842406272888\n",
      "Epoch 0[7396/17270] Time:0.247, Train Loss:0.4083710014820099\n",
      "Epoch 0[7397/17270] Time:0.223, Train Loss:0.3511955738067627\n",
      "Epoch 0[7398/17270] Time:0.225, Train Loss:0.787583589553833\n",
      "Epoch 0[7399/17270] Time:0.24, Train Loss:0.2792086601257324\n",
      "Epoch 0[7400/17270] Time:0.249, Train Loss:0.3759322166442871\n",
      "Epoch 0[7401/17270] Time:0.231, Train Loss:0.7341970205307007\n",
      "Epoch 0[7402/17270] Time:0.226, Train Loss:0.545420229434967\n",
      "Epoch 0[7403/17270] Time:0.24, Train Loss:0.4030117988586426\n",
      "Epoch 0[7404/17270] Time:0.232, Train Loss:0.9115627408027649\n",
      "Epoch 0[7405/17270] Time:0.229, Train Loss:0.5565500259399414\n",
      "Epoch 0[7406/17270] Time:0.239, Train Loss:0.48711180686950684\n",
      "Epoch 0[7407/17270] Time:0.231, Train Loss:0.8161670565605164\n",
      "Epoch 0[7408/17270] Time:0.233, Train Loss:0.6914123892784119\n",
      "Epoch 0[7409/17270] Time:0.238, Train Loss:0.6843703985214233\n",
      "Epoch 0[7410/17270] Time:0.238, Train Loss:0.634687602519989\n",
      "Epoch 0[7411/17270] Time:0.229, Train Loss:0.5949377417564392\n",
      "Epoch 0[7412/17270] Time:0.23, Train Loss:0.5817071199417114\n",
      "Epoch 0[7413/17270] Time:0.23, Train Loss:0.5258704423904419\n",
      "Epoch 0[7414/17270] Time:0.228, Train Loss:0.43956825137138367\n",
      "Epoch 0[7415/17270] Time:0.231, Train Loss:0.5982011556625366\n",
      "Epoch 0[7416/17270] Time:0.241, Train Loss:0.4301106929779053\n",
      "Epoch 0[7417/17270] Time:0.227, Train Loss:0.717076301574707\n",
      "Epoch 0[7418/17270] Time:0.238, Train Loss:0.3552503287792206\n",
      "Epoch 0[7419/17270] Time:0.236, Train Loss:0.5793530941009521\n",
      "Epoch 0[7420/17270] Time:0.238, Train Loss:1.011841893196106\n",
      "Epoch 0[7421/17270] Time:0.239, Train Loss:0.5053730010986328\n",
      "Epoch 0[7422/17270] Time:0.235, Train Loss:0.3798195719718933\n",
      "Epoch 0[7423/17270] Time:0.238, Train Loss:0.5558008551597595\n",
      "Epoch 0[7424/17270] Time:0.238, Train Loss:1.0268830060958862\n",
      "Epoch 0[7425/17270] Time:0.237, Train Loss:0.6018320918083191\n",
      "Epoch 0[7426/17270] Time:0.238, Train Loss:0.46994102001190186\n",
      "Epoch 0[7427/17270] Time:0.228, Train Loss:0.4562322199344635\n",
      "Epoch 0[7428/17270] Time:0.238, Train Loss:0.39197617769241333\n",
      "Epoch 0[7429/17270] Time:0.25, Train Loss:0.5309098958969116\n",
      "Epoch 0[7430/17270] Time:0.232, Train Loss:0.677288293838501\n",
      "Epoch 0[7431/17270] Time:0.254, Train Loss:0.4487808048725128\n",
      "Epoch 0[7432/17270] Time:0.235, Train Loss:0.4685607850551605\n",
      "Epoch 0[7433/17270] Time:0.247, Train Loss:0.6615532040596008\n",
      "Epoch 0[7434/17270] Time:0.235, Train Loss:0.4645248353481293\n",
      "Epoch 0[7435/17270] Time:0.244, Train Loss:0.4428071677684784\n",
      "Epoch 0[7436/17270] Time:0.236, Train Loss:0.6443590521812439\n",
      "Epoch 0[7437/17270] Time:0.224, Train Loss:1.6894636154174805\n",
      "Epoch 0[7438/17270] Time:0.242, Train Loss:0.3532198965549469\n",
      "Epoch 0[7439/17270] Time:0.244, Train Loss:0.5809578895568848\n",
      "Epoch 0[7440/17270] Time:0.226, Train Loss:0.5779619812965393\n",
      "Epoch 0[7441/17270] Time:0.238, Train Loss:0.4784700572490692\n",
      "Epoch 0[7442/17270] Time:0.231, Train Loss:0.6802356243133545\n",
      "Epoch 0[7443/17270] Time:0.232, Train Loss:0.3216540813446045\n",
      "Epoch 0[7444/17270] Time:0.238, Train Loss:0.42492949962615967\n",
      "Epoch 0[7445/17270] Time:0.228, Train Loss:0.4098527729511261\n",
      "Epoch 0[7446/17270] Time:0.237, Train Loss:1.7311898469924927\n",
      "Epoch 0[7447/17270] Time:0.238, Train Loss:0.428047776222229\n",
      "Epoch 0[7448/17270] Time:0.23, Train Loss:1.1254279613494873\n",
      "Epoch 0[7449/17270] Time:0.238, Train Loss:0.6782271265983582\n",
      "Epoch 0[7450/17270] Time:0.231, Train Loss:1.000874638557434\n",
      "Epoch 0[7451/17270] Time:0.231, Train Loss:0.3807484209537506\n",
      "Epoch 0[7452/17270] Time:0.234, Train Loss:1.143968105316162\n",
      "Epoch 0[7453/17270] Time:0.23, Train Loss:0.39401909708976746\n",
      "Epoch 0[7454/17270] Time:0.237, Train Loss:0.5150914788246155\n",
      "Epoch 0[7455/17270] Time:0.233, Train Loss:0.8944152593612671\n",
      "Epoch 0[7456/17270] Time:0.229, Train Loss:0.5250798463821411\n",
      "Epoch 0[7457/17270] Time:0.232, Train Loss:0.4104621112346649\n",
      "Epoch 0[7458/17270] Time:0.23, Train Loss:0.44951683282852173\n",
      "Epoch 0[7459/17270] Time:0.238, Train Loss:0.33376461267471313\n",
      "Epoch 0[7460/17270] Time:0.228, Train Loss:0.4252336323261261\n",
      "Epoch 0[7461/17270] Time:0.237, Train Loss:0.40993157029151917\n",
      "Epoch 0[7462/17270] Time:0.231, Train Loss:0.6264195442199707\n",
      "Epoch 0[7463/17270] Time:0.23, Train Loss:0.5693199038505554\n",
      "Epoch 0[7464/17270] Time:0.236, Train Loss:0.41219937801361084\n",
      "Epoch 0[7465/17270] Time:0.252, Train Loss:0.5365123152732849\n",
      "Epoch 0[7466/17270] Time:0.225, Train Loss:0.43111521005630493\n",
      "Epoch 0[7467/17270] Time:0.222, Train Loss:0.3671548664569855\n",
      "Epoch 0[7468/17270] Time:0.233, Train Loss:0.5669218301773071\n",
      "Epoch 0[7469/17270] Time:0.225, Train Loss:0.6313980221748352\n",
      "Epoch 0[7470/17270] Time:0.233, Train Loss:0.492597371339798\n",
      "Epoch 0[7471/17270] Time:0.234, Train Loss:0.8355953097343445\n",
      "Epoch 0[7472/17270] Time:0.237, Train Loss:0.5169032216072083\n",
      "Epoch 0[7473/17270] Time:0.23, Train Loss:0.6299600601196289\n",
      "Epoch 0[7474/17270] Time:0.23, Train Loss:0.4734199047088623\n",
      "Epoch 0[7475/17270] Time:0.229, Train Loss:0.5141292810440063\n",
      "Epoch 0[7476/17270] Time:0.231, Train Loss:0.5056530237197876\n",
      "Epoch 0[7477/17270] Time:0.23, Train Loss:0.580416738986969\n",
      "Epoch 0[7478/17270] Time:0.236, Train Loss:0.5522840619087219\n",
      "Epoch 0[7479/17270] Time:0.231, Train Loss:0.756729245185852\n",
      "Epoch 0[7480/17270] Time:0.229, Train Loss:0.8802034854888916\n",
      "Epoch 0[7481/17270] Time:0.238, Train Loss:0.3096238076686859\n",
      "Epoch 0[7482/17270] Time:0.23, Train Loss:0.6493898630142212\n",
      "Epoch 0[7483/17270] Time:0.237, Train Loss:1.26566481590271\n",
      "Epoch 0[7484/17270] Time:0.238, Train Loss:0.5029167532920837\n",
      "Epoch 0[7485/17270] Time:0.233, Train Loss:0.5530422925949097\n",
      "Epoch 0[7486/17270] Time:0.24, Train Loss:0.5702787637710571\n",
      "Epoch 0[7487/17270] Time:0.229, Train Loss:0.8468621373176575\n",
      "Epoch 0[7488/17270] Time:0.231, Train Loss:0.38752976059913635\n",
      "Epoch 0[7489/17270] Time:0.234, Train Loss:0.5396203994750977\n",
      "Epoch 0[7490/17270] Time:0.239, Train Loss:0.8036959171295166\n",
      "Epoch 0[7491/17270] Time:0.23, Train Loss:0.6101379990577698\n",
      "Epoch 0[7492/17270] Time:0.229, Train Loss:0.6365900635719299\n",
      "Epoch 0[7493/17270] Time:0.238, Train Loss:0.7540286779403687\n",
      "Epoch 0[7494/17270] Time:0.237, Train Loss:0.5912363529205322\n",
      "Epoch 0[7495/17270] Time:0.238, Train Loss:0.7627610564231873\n",
      "Epoch 0[7496/17270] Time:0.238, Train Loss:0.4072810113430023\n",
      "Epoch 0[7497/17270] Time:0.23, Train Loss:0.5513551235198975\n",
      "Epoch 0[7498/17270] Time:0.236, Train Loss:0.7194706201553345\n",
      "Epoch 0[7499/17270] Time:0.235, Train Loss:0.607623279094696\n",
      "Epoch 0[7500/17270] Time:0.237, Train Loss:0.9339582920074463\n",
      "Epoch 0[7501/17270] Time:0.25, Train Loss:0.6715819835662842\n",
      "Epoch 0[7502/17270] Time:0.235, Train Loss:0.6376180648803711\n",
      "Epoch 0[7503/17270] Time:0.24, Train Loss:0.5102717876434326\n",
      "Epoch 0[7504/17270] Time:0.239, Train Loss:0.4738425016403198\n",
      "Epoch 0[7505/17270] Time:0.238, Train Loss:0.42157530784606934\n",
      "Epoch 0[7506/17270] Time:0.236, Train Loss:0.3430551588535309\n",
      "Epoch 0[7507/17270] Time:0.238, Train Loss:1.2644304037094116\n",
      "Epoch 0[7508/17270] Time:0.24, Train Loss:0.713472306728363\n",
      "Epoch 0[7509/17270] Time:0.229, Train Loss:0.30703461170196533\n",
      "Epoch 0[7510/17270] Time:0.235, Train Loss:0.32342955470085144\n",
      "Epoch 0[7511/17270] Time:0.232, Train Loss:0.5206054449081421\n",
      "Epoch 0[7512/17270] Time:0.227, Train Loss:1.147457242012024\n",
      "Epoch 0[7513/17270] Time:0.236, Train Loss:0.5883229374885559\n",
      "Epoch 0[7514/17270] Time:0.229, Train Loss:0.5088169574737549\n",
      "Epoch 0[7515/17270] Time:0.23, Train Loss:0.6542591452598572\n",
      "Epoch 0[7516/17270] Time:0.227, Train Loss:0.6707403659820557\n",
      "Epoch 0[7517/17270] Time:0.228, Train Loss:0.6506620645523071\n",
      "Epoch 0[7518/17270] Time:0.24, Train Loss:0.5307601094245911\n",
      "Epoch 0[7519/17270] Time:0.227, Train Loss:0.5055561661720276\n",
      "Epoch 0[7520/17270] Time:0.233, Train Loss:0.459289014339447\n",
      "Epoch 0[7521/17270] Time:0.231, Train Loss:0.3987279534339905\n",
      "Epoch 0[7522/17270] Time:0.227, Train Loss:0.5733566880226135\n",
      "Epoch 0[7523/17270] Time:0.23, Train Loss:0.9764551520347595\n",
      "Epoch 0[7524/17270] Time:0.232, Train Loss:0.4819508492946625\n",
      "Epoch 0[7525/17270] Time:0.23, Train Loss:0.469789981842041\n",
      "Epoch 0[7526/17270] Time:0.236, Train Loss:0.5321511030197144\n",
      "Epoch 0[7527/17270] Time:0.23, Train Loss:0.48789626359939575\n",
      "Epoch 0[7528/17270] Time:0.231, Train Loss:0.4994153082370758\n",
      "Epoch 0[7529/17270] Time:0.232, Train Loss:0.6037885546684265\n",
      "Epoch 0[7530/17270] Time:0.236, Train Loss:0.6044682264328003\n",
      "Epoch 0[7531/17270] Time:0.23, Train Loss:0.6472510099411011\n",
      "Epoch 0[7532/17270] Time:0.231, Train Loss:0.5627290606498718\n",
      "Epoch 0[7533/17270] Time:0.23, Train Loss:0.34729909896850586\n",
      "Epoch 0[7534/17270] Time:0.229, Train Loss:0.43937015533447266\n",
      "Epoch 0[7535/17270] Time:0.226, Train Loss:0.5082771182060242\n",
      "Epoch 0[7536/17270] Time:0.236, Train Loss:0.7470197081565857\n",
      "Epoch 0[7537/17270] Time:0.239, Train Loss:0.741827130317688\n",
      "Epoch 0[7538/17270] Time:0.239, Train Loss:0.3159419298171997\n",
      "Epoch 0[7539/17270] Time:0.232, Train Loss:0.5402960777282715\n",
      "Epoch 0[7540/17270] Time:0.226, Train Loss:0.525820791721344\n",
      "Epoch 0[7541/17270] Time:0.245, Train Loss:0.35805830359458923\n",
      "Epoch 0[7542/17270] Time:0.232, Train Loss:0.45040425658226013\n",
      "Epoch 0[7543/17270] Time:0.238, Train Loss:0.6292272210121155\n",
      "Epoch 0[7544/17270] Time:0.238, Train Loss:2.244313955307007\n",
      "Epoch 0[7545/17270] Time:0.237, Train Loss:1.635359764099121\n",
      "Epoch 0[7546/17270] Time:0.224, Train Loss:0.3882475197315216\n",
      "Epoch 0[7547/17270] Time:0.23, Train Loss:0.3348306119441986\n",
      "Epoch 0[7548/17270] Time:0.251, Train Loss:0.611243724822998\n",
      "Epoch 0[7549/17270] Time:0.237, Train Loss:0.3721073865890503\n",
      "Epoch 0[7550/17270] Time:0.245, Train Loss:0.5821278691291809\n",
      "Epoch 0[7551/17270] Time:0.235, Train Loss:0.509566605091095\n",
      "Epoch 0[7552/17270] Time:0.233, Train Loss:0.695635974407196\n",
      "Epoch 0[7553/17270] Time:0.252, Train Loss:0.8213408589363098\n",
      "Epoch 0[7554/17270] Time:0.246, Train Loss:1.104026198387146\n",
      "Epoch 0[7555/17270] Time:0.238, Train Loss:1.347329020500183\n",
      "Epoch 0[7556/17270] Time:0.24, Train Loss:0.6266602873802185\n",
      "Epoch 0[7557/17270] Time:0.241, Train Loss:0.6367599368095398\n",
      "Epoch 0[7558/17270] Time:0.23, Train Loss:0.5713290572166443\n",
      "Epoch 0[7559/17270] Time:0.242, Train Loss:1.203331470489502\n",
      "Epoch 0[7560/17270] Time:0.244, Train Loss:0.854483962059021\n",
      "Epoch 0[7561/17270] Time:0.239, Train Loss:0.5659636855125427\n",
      "Epoch 0[7562/17270] Time:0.235, Train Loss:0.6307470202445984\n",
      "Epoch 0[7563/17270] Time:0.244, Train Loss:0.7140016555786133\n",
      "Epoch 0[7564/17270] Time:0.235, Train Loss:0.3627544641494751\n",
      "Epoch 0[7565/17270] Time:0.234, Train Loss:0.6652654409408569\n",
      "Epoch 0[7566/17270] Time:0.239, Train Loss:0.8969500660896301\n",
      "Epoch 0[7567/17270] Time:0.239, Train Loss:1.0516257286071777\n",
      "Epoch 0[7568/17270] Time:0.244, Train Loss:0.7492220997810364\n",
      "Epoch 0[7569/17270] Time:0.225, Train Loss:0.6679887175559998\n",
      "Epoch 0[7570/17270] Time:0.235, Train Loss:0.5928831696510315\n",
      "Epoch 0[7571/17270] Time:0.231, Train Loss:0.45317432284355164\n",
      "Epoch 0[7572/17270] Time:0.239, Train Loss:0.34006813168525696\n",
      "Epoch 0[7573/17270] Time:0.227, Train Loss:0.558370053768158\n",
      "Epoch 0[7574/17270] Time:0.238, Train Loss:0.5351754426956177\n",
      "Epoch 0[7575/17270] Time:0.233, Train Loss:0.3838724195957184\n",
      "Epoch 0[7576/17270] Time:0.228, Train Loss:0.4557613134384155\n",
      "Epoch 0[7577/17270] Time:0.239, Train Loss:0.8942724466323853\n",
      "Epoch 0[7578/17270] Time:0.225, Train Loss:0.3377721607685089\n",
      "Epoch 0[7579/17270] Time:0.232, Train Loss:0.5595868229866028\n",
      "Epoch 0[7580/17270] Time:0.25, Train Loss:0.7822230458259583\n",
      "Epoch 0[7581/17270] Time:0.254, Train Loss:0.39245548844337463\n",
      "Epoch 0[7582/17270] Time:0.238, Train Loss:0.6276542544364929\n",
      "Epoch 0[7583/17270] Time:0.227, Train Loss:0.6978875994682312\n",
      "Epoch 0[7584/17270] Time:0.231, Train Loss:0.6959746479988098\n",
      "Epoch 0[7585/17270] Time:0.238, Train Loss:0.40451863408088684\n",
      "Epoch 0[7586/17270] Time:0.239, Train Loss:0.9004297852516174\n",
      "Epoch 0[7587/17270] Time:0.23, Train Loss:1.0121879577636719\n",
      "Epoch 0[7588/17270] Time:0.225, Train Loss:0.6403558254241943\n",
      "Epoch 0[7589/17270] Time:0.248, Train Loss:0.8592804074287415\n",
      "Epoch 0[7590/17270] Time:0.236, Train Loss:0.7729865908622742\n",
      "Epoch 0[7591/17270] Time:0.222, Train Loss:0.46614161133766174\n",
      "Epoch 0[7592/17270] Time:0.237, Train Loss:0.3184671700000763\n",
      "Epoch 0[7593/17270] Time:0.239, Train Loss:0.35243773460388184\n",
      "Epoch 0[7594/17270] Time:0.236, Train Loss:0.68709397315979\n",
      "Epoch 0[7595/17270] Time:0.23, Train Loss:0.5240299105644226\n",
      "Epoch 0[7596/17270] Time:0.235, Train Loss:0.6907399296760559\n",
      "Epoch 0[7597/17270] Time:0.228, Train Loss:1.094030499458313\n",
      "Epoch 0[7598/17270] Time:0.229, Train Loss:0.7332754135131836\n",
      "Epoch 0[7599/17270] Time:0.237, Train Loss:0.8085842132568359\n",
      "Epoch 0[7600/17270] Time:0.239, Train Loss:0.6367008686065674\n",
      "Epoch 0[7601/17270] Time:0.231, Train Loss:0.7578451633453369\n",
      "Epoch 0[7602/17270] Time:0.239, Train Loss:0.5009376406669617\n",
      "Epoch 0[7603/17270] Time:0.229, Train Loss:0.3461984097957611\n",
      "Epoch 0[7604/17270] Time:0.258, Train Loss:0.7533947229385376\n",
      "Epoch 0[7605/17270] Time:0.235, Train Loss:0.706030011177063\n",
      "Epoch 0[7606/17270] Time:0.239, Train Loss:0.7034465670585632\n",
      "Epoch 0[7607/17270] Time:0.23, Train Loss:0.5009418725967407\n",
      "Epoch 0[7608/17270] Time:0.233, Train Loss:0.40806350111961365\n",
      "Epoch 0[7609/17270] Time:0.245, Train Loss:0.9757500886917114\n",
      "Epoch 0[7610/17270] Time:0.228, Train Loss:1.327288031578064\n",
      "Epoch 0[7611/17270] Time:0.248, Train Loss:0.6303889155387878\n",
      "Epoch 0[7612/17270] Time:0.235, Train Loss:0.5997461676597595\n",
      "Epoch 0[7613/17270] Time:0.23, Train Loss:0.536597490310669\n",
      "Epoch 0[7614/17270] Time:0.231, Train Loss:0.5000429749488831\n",
      "Epoch 0[7615/17270] Time:0.231, Train Loss:0.6525610089302063\n",
      "Epoch 0[7616/17270] Time:0.237, Train Loss:0.5764950513839722\n",
      "Epoch 0[7617/17270] Time:0.232, Train Loss:0.5384889841079712\n",
      "Epoch 0[7618/17270] Time:0.238, Train Loss:1.492750883102417\n",
      "Epoch 0[7619/17270] Time:0.236, Train Loss:0.8513519167900085\n",
      "Epoch 0[7620/17270] Time:0.238, Train Loss:0.5111421942710876\n",
      "Epoch 0[7621/17270] Time:0.243, Train Loss:0.406948983669281\n",
      "Epoch 0[7622/17270] Time:0.238, Train Loss:0.6748586893081665\n",
      "Epoch 0[7623/17270] Time:0.24, Train Loss:0.5929409265518188\n",
      "Epoch 0[7624/17270] Time:0.221, Train Loss:0.4143894612789154\n",
      "Epoch 0[7625/17270] Time:0.237, Train Loss:0.48150748014450073\n",
      "Epoch 0[7626/17270] Time:0.24, Train Loss:0.5688477158546448\n",
      "Epoch 0[7627/17270] Time:0.23, Train Loss:1.347212791442871\n",
      "Epoch 0[7628/17270] Time:0.236, Train Loss:1.2754932641983032\n",
      "Epoch 0[7629/17270] Time:0.236, Train Loss:0.6319712996482849\n",
      "Epoch 0[7630/17270] Time:0.236, Train Loss:0.7180488705635071\n",
      "Epoch 0[7631/17270] Time:0.239, Train Loss:0.5372602343559265\n",
      "Epoch 0[7632/17270] Time:0.231, Train Loss:0.6936537027359009\n",
      "Epoch 0[7633/17270] Time:0.231, Train Loss:0.6425991654396057\n",
      "Epoch 0[7634/17270] Time:0.232, Train Loss:0.6356170773506165\n",
      "Epoch 0[7635/17270] Time:0.237, Train Loss:0.5452273488044739\n",
      "Epoch 0[7636/17270] Time:0.236, Train Loss:0.6426241993904114\n",
      "Epoch 0[7637/17270] Time:0.239, Train Loss:0.609512448310852\n",
      "Epoch 0[7638/17270] Time:0.239, Train Loss:0.4605534076690674\n",
      "Epoch 0[7639/17270] Time:0.233, Train Loss:0.9594938158988953\n",
      "Epoch 0[7640/17270] Time:0.237, Train Loss:0.5921180844306946\n",
      "Epoch 0[7641/17270] Time:0.227, Train Loss:0.9769411683082581\n",
      "Epoch 0[7642/17270] Time:0.231, Train Loss:0.751940906047821\n",
      "Epoch 0[7643/17270] Time:0.228, Train Loss:0.49275070428848267\n",
      "Epoch 0[7644/17270] Time:0.229, Train Loss:0.5346781015396118\n",
      "Epoch 0[7645/17270] Time:0.231, Train Loss:0.5575417280197144\n",
      "Epoch 0[7646/17270] Time:0.227, Train Loss:0.3847905099391937\n",
      "Epoch 0[7647/17270] Time:0.232, Train Loss:0.510279655456543\n",
      "Epoch 0[7648/17270] Time:0.237, Train Loss:0.5084968209266663\n",
      "Epoch 0[7649/17270] Time:0.227, Train Loss:0.5057864785194397\n",
      "Epoch 0[7650/17270] Time:0.229, Train Loss:0.3712710738182068\n",
      "Epoch 0[7651/17270] Time:0.228, Train Loss:0.47024765610694885\n",
      "Epoch 0[7652/17270] Time:0.232, Train Loss:0.9159577488899231\n",
      "Epoch 0[7653/17270] Time:0.226, Train Loss:0.8264845013618469\n",
      "Epoch 0[7654/17270] Time:0.228, Train Loss:0.5605046153068542\n",
      "Epoch 0[7655/17270] Time:0.25, Train Loss:0.6291443705558777\n",
      "Epoch 0[7656/17270] Time:0.237, Train Loss:0.5511332154273987\n",
      "Epoch 0[7657/17270] Time:0.237, Train Loss:0.6413203477859497\n",
      "Epoch 0[7658/17270] Time:0.227, Train Loss:0.7173877358436584\n",
      "Epoch 0[7659/17270] Time:0.248, Train Loss:1.1010534763336182\n",
      "Epoch 0[7660/17270] Time:0.227, Train Loss:0.9842748045921326\n",
      "Epoch 0[7661/17270] Time:0.247, Train Loss:0.37996459007263184\n",
      "Epoch 0[7662/17270] Time:0.242, Train Loss:1.3335703611373901\n",
      "Epoch 0[7663/17270] Time:0.238, Train Loss:0.7457539439201355\n",
      "Epoch 0[7664/17270] Time:0.23, Train Loss:0.5965450406074524\n",
      "Epoch 0[7665/17270] Time:0.224, Train Loss:0.6370201110839844\n",
      "Epoch 0[7666/17270] Time:0.23, Train Loss:1.230476975440979\n",
      "Epoch 0[7667/17270] Time:0.23, Train Loss:0.6072690486907959\n",
      "Epoch 0[7668/17270] Time:0.24, Train Loss:0.7135699987411499\n",
      "Epoch 0[7669/17270] Time:0.231, Train Loss:0.7156018018722534\n",
      "Epoch 0[7670/17270] Time:0.233, Train Loss:0.3740861415863037\n",
      "Epoch 0[7671/17270] Time:0.23, Train Loss:0.6084698438644409\n",
      "Epoch 0[7672/17270] Time:0.233, Train Loss:0.531583309173584\n",
      "Epoch 0[7673/17270] Time:0.229, Train Loss:0.561358630657196\n",
      "Epoch 0[7674/17270] Time:0.233, Train Loss:0.8931754231452942\n",
      "Epoch 0[7675/17270] Time:0.233, Train Loss:0.6667429208755493\n",
      "Epoch 0[7676/17270] Time:0.229, Train Loss:0.6172893047332764\n",
      "Epoch 0[7677/17270] Time:0.229, Train Loss:0.6041332483291626\n",
      "Epoch 0[7678/17270] Time:0.236, Train Loss:0.4969591498374939\n",
      "Epoch 0[7679/17270] Time:0.237, Train Loss:0.5793696045875549\n",
      "Epoch 0[7680/17270] Time:0.229, Train Loss:0.5960162281990051\n",
      "Epoch 0[7681/17270] Time:0.237, Train Loss:0.38686540722846985\n",
      "Epoch 0[7682/17270] Time:0.241, Train Loss:0.6445866823196411\n",
      "Epoch 0[7683/17270] Time:0.234, Train Loss:0.737584114074707\n",
      "Epoch 0[7684/17270] Time:0.228, Train Loss:0.7168385982513428\n",
      "Epoch 0[7685/17270] Time:0.235, Train Loss:0.772326648235321\n",
      "Epoch 0[7686/17270] Time:0.239, Train Loss:0.7434855103492737\n",
      "Epoch 0[7687/17270] Time:0.231, Train Loss:0.381570965051651\n",
      "Epoch 0[7688/17270] Time:0.244, Train Loss:0.37489035725593567\n",
      "Epoch 0[7689/17270] Time:0.24, Train Loss:0.7508972883224487\n",
      "Epoch 0[7690/17270] Time:0.231, Train Loss:0.8614609837532043\n",
      "Epoch 0[7691/17270] Time:0.228, Train Loss:0.5891546010971069\n",
      "Epoch 0[7692/17270] Time:0.23, Train Loss:0.5068529844284058\n",
      "Epoch 0[7693/17270] Time:0.238, Train Loss:0.5770528316497803\n",
      "Epoch 0[7694/17270] Time:0.233, Train Loss:0.6082064509391785\n",
      "Epoch 0[7695/17270] Time:0.226, Train Loss:0.44507482647895813\n",
      "Epoch 0[7696/17270] Time:0.235, Train Loss:0.7077391743659973\n",
      "Epoch 0[7697/17270] Time:0.235, Train Loss:0.6467853784561157\n",
      "Epoch 0[7698/17270] Time:0.239, Train Loss:0.5165075063705444\n",
      "Epoch 0[7699/17270] Time:0.232, Train Loss:0.6602264642715454\n",
      "Epoch 0[7700/17270] Time:0.226, Train Loss:1.128243088722229\n",
      "Epoch 0[7701/17270] Time:0.229, Train Loss:0.6961345076560974\n",
      "Epoch 0[7702/17270] Time:0.241, Train Loss:0.6711857318878174\n",
      "Epoch 0[7703/17270] Time:0.227, Train Loss:0.4748944640159607\n",
      "Epoch 0[7704/17270] Time:0.24, Train Loss:0.4921983778476715\n",
      "Epoch 0[7705/17270] Time:0.222, Train Loss:0.5865700840950012\n",
      "Epoch 0[7706/17270] Time:0.243, Train Loss:0.4787541925907135\n",
      "Epoch 0[7707/17270] Time:0.237, Train Loss:0.4035252332687378\n",
      "Epoch 0[7708/17270] Time:0.242, Train Loss:1.028665542602539\n",
      "Epoch 0[7709/17270] Time:0.241, Train Loss:0.5206325054168701\n",
      "Epoch 0[7710/17270] Time:0.237, Train Loss:0.40207406878471375\n",
      "Epoch 0[7711/17270] Time:0.225, Train Loss:0.5186129808425903\n",
      "Epoch 0[7712/17270] Time:0.248, Train Loss:0.6009665131568909\n",
      "Epoch 0[7713/17270] Time:0.23, Train Loss:0.549063503742218\n",
      "Epoch 0[7714/17270] Time:0.237, Train Loss:0.5586621165275574\n",
      "Epoch 0[7715/17270] Time:0.233, Train Loss:0.7684006690979004\n",
      "Epoch 0[7716/17270] Time:0.224, Train Loss:0.325197696685791\n",
      "Epoch 0[7717/17270] Time:0.237, Train Loss:0.503108024597168\n",
      "Epoch 0[7718/17270] Time:0.236, Train Loss:0.5919067859649658\n",
      "Epoch 0[7719/17270] Time:0.231, Train Loss:0.7172139286994934\n",
      "Epoch 0[7720/17270] Time:0.235, Train Loss:0.7289473414421082\n",
      "Epoch 0[7721/17270] Time:0.23, Train Loss:0.37181729078292847\n",
      "Epoch 0[7722/17270] Time:0.237, Train Loss:1.172621488571167\n",
      "Epoch 0[7723/17270] Time:0.238, Train Loss:0.35146424174308777\n",
      "Epoch 0[7724/17270] Time:0.254, Train Loss:1.004341959953308\n",
      "Epoch 0[7725/17270] Time:0.238, Train Loss:0.5026876926422119\n",
      "Epoch 0[7726/17270] Time:0.233, Train Loss:0.6158087253570557\n",
      "Epoch 0[7727/17270] Time:0.25, Train Loss:0.47795894742012024\n",
      "Epoch 0[7728/17270] Time:0.232, Train Loss:0.36519119143486023\n",
      "Epoch 0[7729/17270] Time:0.249, Train Loss:0.7969486117362976\n",
      "Epoch 0[7730/17270] Time:0.237, Train Loss:0.8128206133842468\n",
      "Epoch 0[7731/17270] Time:0.238, Train Loss:0.9452531337738037\n",
      "Epoch 0[7732/17270] Time:0.237, Train Loss:0.6586353778839111\n",
      "Epoch 0[7733/17270] Time:0.235, Train Loss:0.8257436752319336\n",
      "Epoch 0[7734/17270] Time:0.226, Train Loss:0.4194304645061493\n",
      "Epoch 0[7735/17270] Time:0.23, Train Loss:1.1115599870681763\n",
      "Epoch 0[7736/17270] Time:0.243, Train Loss:0.6010429263114929\n",
      "Epoch 0[7737/17270] Time:0.232, Train Loss:0.5396045446395874\n",
      "Epoch 0[7738/17270] Time:0.249, Train Loss:0.4624719023704529\n",
      "Epoch 0[7739/17270] Time:0.235, Train Loss:0.46179020404815674\n",
      "Epoch 0[7740/17270] Time:0.217, Train Loss:0.7125253677368164\n",
      "Epoch 0[7741/17270] Time:0.231, Train Loss:0.4127384424209595\n",
      "Epoch 0[7742/17270] Time:0.237, Train Loss:0.39062854647636414\n",
      "Epoch 0[7743/17270] Time:0.237, Train Loss:0.4446576237678528\n",
      "Epoch 0[7744/17270] Time:0.238, Train Loss:0.44626909494400024\n",
      "Epoch 0[7745/17270] Time:0.237, Train Loss:0.7221962809562683\n",
      "Epoch 0[7746/17270] Time:0.243, Train Loss:0.40455323457717896\n",
      "Epoch 0[7747/17270] Time:0.246, Train Loss:0.813714861869812\n",
      "Epoch 0[7748/17270] Time:0.238, Train Loss:0.4494260251522064\n",
      "Epoch 0[7749/17270] Time:0.238, Train Loss:0.4496559798717499\n",
      "Epoch 0[7750/17270] Time:0.242, Train Loss:0.5739902853965759\n",
      "Epoch 0[7751/17270] Time:0.229, Train Loss:0.31174415349960327\n",
      "Epoch 0[7752/17270] Time:0.243, Train Loss:0.5742106437683105\n",
      "Epoch 0[7753/17270] Time:0.252, Train Loss:0.4727943241596222\n",
      "Epoch 0[7754/17270] Time:0.237, Train Loss:0.7284874320030212\n",
      "Epoch 0[7755/17270] Time:0.247, Train Loss:0.4571274220943451\n",
      "Epoch 0[7756/17270] Time:0.236, Train Loss:0.3881600499153137\n",
      "Epoch 0[7757/17270] Time:0.232, Train Loss:0.8277533054351807\n",
      "Epoch 0[7758/17270] Time:0.231, Train Loss:0.6256129145622253\n",
      "Epoch 0[7759/17270] Time:0.238, Train Loss:0.5817130208015442\n",
      "Epoch 0[7760/17270] Time:0.247, Train Loss:0.7716917991638184\n",
      "Epoch 0[7761/17270] Time:0.251, Train Loss:0.6873590350151062\n",
      "Epoch 0[7762/17270] Time:0.242, Train Loss:0.555461049079895\n",
      "Epoch 0[7763/17270] Time:0.229, Train Loss:0.5280601382255554\n",
      "Epoch 0[7764/17270] Time:0.229, Train Loss:0.6406335830688477\n",
      "Epoch 0[7765/17270] Time:0.252, Train Loss:0.7235538959503174\n",
      "Epoch 0[7766/17270] Time:0.231, Train Loss:0.890324592590332\n",
      "Epoch 0[7767/17270] Time:0.237, Train Loss:0.6304592490196228\n",
      "Epoch 0[7768/17270] Time:0.235, Train Loss:0.37291988730430603\n",
      "Epoch 0[7769/17270] Time:0.227, Train Loss:0.7254080772399902\n",
      "Epoch 0[7770/17270] Time:0.228, Train Loss:0.3933003544807434\n",
      "Epoch 0[7771/17270] Time:0.224, Train Loss:1.1620333194732666\n",
      "Epoch 0[7772/17270] Time:0.238, Train Loss:0.36729568243026733\n",
      "Epoch 0[7773/17270] Time:0.235, Train Loss:0.6061263680458069\n",
      "Epoch 0[7774/17270] Time:0.23, Train Loss:0.3774828612804413\n",
      "Epoch 0[7775/17270] Time:0.231, Train Loss:0.5443986654281616\n",
      "Epoch 0[7776/17270] Time:0.228, Train Loss:0.46309274435043335\n",
      "Epoch 0[7777/17270] Time:0.227, Train Loss:0.9980591535568237\n",
      "Epoch 0[7778/17270] Time:0.238, Train Loss:0.3904542028903961\n",
      "Epoch 0[7779/17270] Time:0.233, Train Loss:0.3680950701236725\n",
      "Epoch 0[7780/17270] Time:0.23, Train Loss:0.7811450958251953\n",
      "Epoch 0[7781/17270] Time:0.227, Train Loss:0.46816083788871765\n",
      "Epoch 0[7782/17270] Time:0.228, Train Loss:0.5687413215637207\n",
      "Epoch 0[7783/17270] Time:0.229, Train Loss:0.4921667277812958\n",
      "Epoch 0[7784/17270] Time:0.229, Train Loss:0.5647811889648438\n",
      "Epoch 0[7785/17270] Time:0.23, Train Loss:0.5638676881790161\n",
      "Epoch 0[7786/17270] Time:0.237, Train Loss:0.6267490983009338\n",
      "Epoch 0[7787/17270] Time:0.242, Train Loss:0.5988180041313171\n",
      "Epoch 0[7788/17270] Time:0.238, Train Loss:0.32784679532051086\n",
      "Epoch 0[7789/17270] Time:0.237, Train Loss:1.5443052053451538\n",
      "Epoch 0[7790/17270] Time:0.228, Train Loss:0.7311710119247437\n",
      "Epoch 0[7791/17270] Time:0.228, Train Loss:1.2565022706985474\n",
      "Epoch 0[7792/17270] Time:0.23, Train Loss:0.6516119241714478\n",
      "Epoch 0[7793/17270] Time:0.229, Train Loss:0.3521762490272522\n",
      "Epoch 0[7794/17270] Time:0.232, Train Loss:0.8898200392723083\n",
      "Epoch 0[7795/17270] Time:0.238, Train Loss:0.9044036865234375\n",
      "Epoch 0[7796/17270] Time:0.238, Train Loss:1.2015597820281982\n",
      "Epoch 0[7797/17270] Time:0.236, Train Loss:0.6384193897247314\n",
      "Epoch 0[7798/17270] Time:0.23, Train Loss:0.6976768970489502\n",
      "Epoch 0[7799/17270] Time:0.228, Train Loss:0.3344038128852844\n",
      "Epoch 0[7800/17270] Time:0.229, Train Loss:0.43397411704063416\n",
      "Epoch 0[7801/17270] Time:0.24, Train Loss:1.3715996742248535\n",
      "Epoch 0[7802/17270] Time:0.231, Train Loss:0.4457865059375763\n",
      "Epoch 0[7803/17270] Time:0.235, Train Loss:0.9929280877113342\n",
      "Epoch 0[7804/17270] Time:0.235, Train Loss:0.3319739103317261\n",
      "Epoch 0[7805/17270] Time:0.232, Train Loss:0.43921351432800293\n",
      "Epoch 0[7806/17270] Time:0.239, Train Loss:0.6056527495384216\n",
      "Epoch 0[7807/17270] Time:0.231, Train Loss:0.3533313274383545\n",
      "Epoch 0[7808/17270] Time:0.233, Train Loss:0.5420904755592346\n",
      "Epoch 0[7809/17270] Time:0.227, Train Loss:0.6072387099266052\n",
      "Epoch 0[7810/17270] Time:0.237, Train Loss:0.5036631226539612\n",
      "Epoch 0[7811/17270] Time:0.229, Train Loss:0.7767696380615234\n",
      "Epoch 0[7812/17270] Time:0.231, Train Loss:0.756965160369873\n",
      "Epoch 0[7813/17270] Time:0.228, Train Loss:0.6563078761100769\n",
      "Epoch 0[7814/17270] Time:0.228, Train Loss:0.8633494973182678\n",
      "Epoch 0[7815/17270] Time:0.227, Train Loss:0.441913366317749\n",
      "Epoch 0[7816/17270] Time:0.229, Train Loss:0.6456764340400696\n",
      "Epoch 0[7817/17270] Time:0.23, Train Loss:0.8959026336669922\n",
      "Epoch 0[7818/17270] Time:0.227, Train Loss:0.5494145750999451\n",
      "Epoch 0[7819/17270] Time:0.227, Train Loss:0.7327085137367249\n",
      "Epoch 0[7820/17270] Time:0.241, Train Loss:0.5845043063163757\n",
      "Epoch 0[7821/17270] Time:0.227, Train Loss:0.34097710251808167\n",
      "Epoch 0[7822/17270] Time:0.237, Train Loss:0.36075887084007263\n",
      "Epoch 0[7823/17270] Time:0.236, Train Loss:0.5096599459648132\n",
      "Epoch 0[7824/17270] Time:0.246, Train Loss:0.6263217926025391\n",
      "Epoch 0[7825/17270] Time:0.237, Train Loss:0.5506978631019592\n",
      "Epoch 0[7826/17270] Time:0.222, Train Loss:0.7362956404685974\n",
      "Epoch 0[7827/17270] Time:0.241, Train Loss:0.43964749574661255\n",
      "Epoch 0[7828/17270] Time:0.235, Train Loss:0.6002630591392517\n",
      "Epoch 0[7829/17270] Time:0.225, Train Loss:0.39520499110221863\n",
      "Epoch 0[7830/17270] Time:0.237, Train Loss:0.4944923520088196\n",
      "Epoch 0[7831/17270] Time:0.238, Train Loss:0.5415883660316467\n",
      "Epoch 0[7832/17270] Time:0.238, Train Loss:0.28537866473197937\n",
      "Epoch 0[7833/17270] Time:0.24, Train Loss:0.48926353454589844\n",
      "Epoch 0[7834/17270] Time:0.231, Train Loss:0.5031075477600098\n",
      "Epoch 0[7835/17270] Time:0.24, Train Loss:0.6969725489616394\n",
      "Epoch 0[7836/17270] Time:0.237, Train Loss:1.0686739683151245\n",
      "Epoch 0[7837/17270] Time:0.236, Train Loss:0.6285068392753601\n",
      "Epoch 0[7838/17270] Time:0.218, Train Loss:0.6160644292831421\n",
      "Epoch 0[7839/17270] Time:0.227, Train Loss:0.7056354880332947\n",
      "Epoch 0[7840/17270] Time:0.238, Train Loss:0.6070235967636108\n",
      "Epoch 0[7841/17270] Time:0.228, Train Loss:1.1088896989822388\n",
      "Epoch 0[7842/17270] Time:0.237, Train Loss:0.2860639989376068\n",
      "Epoch 0[7843/17270] Time:0.229, Train Loss:0.5685190558433533\n",
      "Epoch 0[7844/17270] Time:0.229, Train Loss:1.3519905805587769\n",
      "Epoch 0[7845/17270] Time:0.237, Train Loss:0.5277556777000427\n",
      "Epoch 0[7846/17270] Time:0.229, Train Loss:0.5473634600639343\n",
      "Epoch 0[7847/17270] Time:0.231, Train Loss:0.5627887845039368\n",
      "Epoch 0[7848/17270] Time:0.236, Train Loss:0.9818969368934631\n",
      "Epoch 0[7849/17270] Time:0.23, Train Loss:1.2993468046188354\n",
      "Epoch 0[7850/17270] Time:0.243, Train Loss:0.3692235052585602\n",
      "Epoch 0[7851/17270] Time:0.23, Train Loss:1.010352373123169\n",
      "Epoch 0[7852/17270] Time:0.23, Train Loss:0.5836066007614136\n",
      "Epoch 0[7853/17270] Time:0.231, Train Loss:0.49486425518989563\n",
      "Epoch 0[7854/17270] Time:0.237, Train Loss:0.5259048342704773\n",
      "Epoch 0[7855/17270] Time:0.238, Train Loss:0.5647252798080444\n",
      "Epoch 0[7856/17270] Time:0.23, Train Loss:0.6254706382751465\n",
      "Epoch 0[7857/17270] Time:0.231, Train Loss:0.44694992899894714\n",
      "Epoch 0[7858/17270] Time:0.231, Train Loss:1.2859413623809814\n",
      "Epoch 0[7859/17270] Time:0.232, Train Loss:0.6309949159622192\n",
      "Epoch 0[7860/17270] Time:0.232, Train Loss:1.0055266618728638\n",
      "Epoch 0[7861/17270] Time:0.237, Train Loss:0.5938495397567749\n",
      "Epoch 0[7862/17270] Time:0.231, Train Loss:0.5799616575241089\n",
      "Epoch 0[7863/17270] Time:0.233, Train Loss:0.568348228931427\n",
      "Epoch 0[7864/17270] Time:0.234, Train Loss:0.5760664939880371\n",
      "Epoch 0[7865/17270] Time:0.237, Train Loss:0.6255698204040527\n",
      "Epoch 0[7866/17270] Time:0.231, Train Loss:0.5041986107826233\n",
      "Epoch 0[7867/17270] Time:0.239, Train Loss:0.5451750755310059\n",
      "Epoch 0[7868/17270] Time:0.228, Train Loss:0.6659543514251709\n",
      "Epoch 0[7869/17270] Time:0.239, Train Loss:0.6445974707603455\n",
      "Epoch 0[7870/17270] Time:0.236, Train Loss:0.8070767521858215\n",
      "Epoch 0[7871/17270] Time:0.249, Train Loss:1.0453202724456787\n",
      "Epoch 0[7872/17270] Time:0.237, Train Loss:0.6059364080429077\n",
      "Epoch 0[7873/17270] Time:0.261, Train Loss:0.9029773473739624\n",
      "Epoch 0[7874/17270] Time:0.233, Train Loss:0.9322453737258911\n",
      "Epoch 0[7875/17270] Time:0.24, Train Loss:0.6057147979736328\n",
      "Epoch 0[7876/17270] Time:0.237, Train Loss:0.3949160575866699\n",
      "Epoch 0[7877/17270] Time:0.232, Train Loss:0.65541011095047\n",
      "Epoch 0[7878/17270] Time:0.231, Train Loss:0.5550246238708496\n",
      "Epoch 0[7879/17270] Time:0.237, Train Loss:0.6762651205062866\n",
      "Epoch 0[7880/17270] Time:0.225, Train Loss:0.7230986952781677\n",
      "Epoch 0[7881/17270] Time:0.223, Train Loss:0.5438077449798584\n",
      "Epoch 0[7882/17270] Time:0.239, Train Loss:0.4081799387931824\n",
      "Epoch 0[7883/17270] Time:0.228, Train Loss:0.5954511165618896\n",
      "Epoch 0[7884/17270] Time:0.23, Train Loss:0.4376378655433655\n",
      "Epoch 0[7885/17270] Time:0.239, Train Loss:0.4885481297969818\n",
      "Epoch 0[7886/17270] Time:0.237, Train Loss:0.7433778047561646\n",
      "Epoch 0[7887/17270] Time:0.228, Train Loss:0.5817204117774963\n",
      "Epoch 0[7888/17270] Time:0.228, Train Loss:1.0192596912384033\n",
      "Epoch 0[7889/17270] Time:0.235, Train Loss:0.4522643983364105\n",
      "Epoch 0[7890/17270] Time:0.24, Train Loss:0.5330117344856262\n",
      "Epoch 0[7891/17270] Time:0.228, Train Loss:0.3999587297439575\n",
      "Epoch 0[7892/17270] Time:0.24, Train Loss:0.5315528512001038\n",
      "Epoch 0[7893/17270] Time:0.231, Train Loss:0.7340861558914185\n",
      "Epoch 0[7894/17270] Time:0.229, Train Loss:0.6158947348594666\n",
      "Epoch 0[7895/17270] Time:0.239, Train Loss:0.48667973279953003\n",
      "Epoch 0[7896/17270] Time:0.236, Train Loss:0.3807462155818939\n",
      "Epoch 0[7897/17270] Time:0.235, Train Loss:0.4750237762928009\n",
      "Epoch 0[7898/17270] Time:0.236, Train Loss:0.6588767170906067\n",
      "Epoch 0[7899/17270] Time:0.233, Train Loss:0.4091844856739044\n",
      "Epoch 0[7900/17270] Time:0.24, Train Loss:0.40921080112457275\n",
      "Epoch 0[7901/17270] Time:0.229, Train Loss:0.46153008937835693\n",
      "Epoch 0[7902/17270] Time:0.238, Train Loss:0.6503345966339111\n",
      "Epoch 0[7903/17270] Time:0.229, Train Loss:0.5851312875747681\n",
      "Epoch 0[7904/17270] Time:0.248, Train Loss:0.34568578004837036\n",
      "Epoch 0[7905/17270] Time:0.232, Train Loss:0.4414779543876648\n",
      "Epoch 0[7906/17270] Time:0.236, Train Loss:0.3159950077533722\n",
      "Epoch 0[7907/17270] Time:0.237, Train Loss:0.6147328615188599\n",
      "Epoch 0[7908/17270] Time:0.233, Train Loss:0.9436885118484497\n",
      "Epoch 0[7909/17270] Time:0.227, Train Loss:0.5517733097076416\n",
      "Epoch 0[7910/17270] Time:0.237, Train Loss:0.5836374759674072\n",
      "Epoch 0[7911/17270] Time:0.236, Train Loss:0.47601318359375\n",
      "Epoch 0[7912/17270] Time:0.232, Train Loss:0.5204228758811951\n",
      "Epoch 0[7913/17270] Time:0.235, Train Loss:0.5172609090805054\n",
      "Epoch 0[7914/17270] Time:0.238, Train Loss:0.44212087988853455\n",
      "Epoch 0[7915/17270] Time:0.229, Train Loss:0.7983055114746094\n",
      "Epoch 0[7916/17270] Time:0.238, Train Loss:0.4974515438079834\n",
      "Epoch 0[7917/17270] Time:0.237, Train Loss:0.3568480908870697\n",
      "Epoch 0[7918/17270] Time:0.23, Train Loss:0.7070019245147705\n",
      "Epoch 0[7919/17270] Time:0.227, Train Loss:0.468492716550827\n",
      "Epoch 0[7920/17270] Time:0.239, Train Loss:0.4597897231578827\n",
      "Epoch 0[7921/17270] Time:0.233, Train Loss:0.31435737013816833\n",
      "Epoch 0[7922/17270] Time:0.253, Train Loss:0.9581063389778137\n",
      "Epoch 0[7923/17270] Time:0.229, Train Loss:0.4229997992515564\n",
      "Epoch 0[7924/17270] Time:0.233, Train Loss:0.5336981415748596\n",
      "Epoch 0[7925/17270] Time:0.236, Train Loss:0.44456812739372253\n",
      "Epoch 0[7926/17270] Time:0.235, Train Loss:0.27410537004470825\n",
      "Epoch 0[7927/17270] Time:0.243, Train Loss:0.35357794165611267\n",
      "Epoch 0[7928/17270] Time:0.233, Train Loss:1.3176923990249634\n",
      "Epoch 0[7929/17270] Time:0.227, Train Loss:0.5450336337089539\n",
      "Epoch 0[7930/17270] Time:0.235, Train Loss:1.04659104347229\n",
      "Epoch 0[7931/17270] Time:0.23, Train Loss:0.8038187026977539\n",
      "Epoch 0[7932/17270] Time:0.239, Train Loss:0.46524056792259216\n",
      "Epoch 0[7933/17270] Time:0.231, Train Loss:0.45685046911239624\n",
      "Epoch 0[7934/17270] Time:0.231, Train Loss:0.7348231673240662\n",
      "Epoch 0[7935/17270] Time:0.237, Train Loss:0.5816094279289246\n",
      "Epoch 0[7936/17270] Time:0.235, Train Loss:0.4848894476890564\n",
      "Epoch 0[7937/17270] Time:0.224, Train Loss:0.9369125962257385\n",
      "Epoch 0[7938/17270] Time:0.231, Train Loss:0.313982218503952\n",
      "Epoch 0[7939/17270] Time:0.232, Train Loss:0.9987991452217102\n",
      "Epoch 0[7940/17270] Time:0.232, Train Loss:0.6077076196670532\n",
      "Epoch 0[7941/17270] Time:0.236, Train Loss:0.6076433062553406\n",
      "Epoch 0[7942/17270] Time:0.237, Train Loss:0.5488607883453369\n",
      "Epoch 0[7943/17270] Time:0.234, Train Loss:0.44958069920539856\n",
      "Epoch 0[7944/17270] Time:0.235, Train Loss:0.4735799729824066\n",
      "Epoch 0[7945/17270] Time:0.237, Train Loss:0.4460011422634125\n",
      "Epoch 0[7946/17270] Time:0.227, Train Loss:0.6524608731269836\n",
      "Epoch 0[7947/17270] Time:0.241, Train Loss:0.5097087025642395\n",
      "Epoch 0[7948/17270] Time:0.227, Train Loss:0.7957894206047058\n",
      "Epoch 0[7949/17270] Time:0.241, Train Loss:0.43332380056381226\n",
      "Epoch 0[7950/17270] Time:0.233, Train Loss:0.4553738534450531\n",
      "Epoch 0[7951/17270] Time:0.236, Train Loss:0.44410625100135803\n",
      "Epoch 0[7952/17270] Time:0.243, Train Loss:0.5812700390815735\n",
      "Epoch 0[7953/17270] Time:0.234, Train Loss:0.5444539785385132\n",
      "Epoch 0[7954/17270] Time:0.231, Train Loss:1.7404353618621826\n",
      "Epoch 0[7955/17270] Time:0.237, Train Loss:0.6182500720024109\n",
      "Epoch 0[7956/17270] Time:0.24, Train Loss:0.35397863388061523\n",
      "Epoch 0[7957/17270] Time:0.24, Train Loss:0.4112730920314789\n",
      "Epoch 0[7958/17270] Time:0.238, Train Loss:0.741074800491333\n",
      "Epoch 0[7959/17270] Time:0.234, Train Loss:0.8976885676383972\n",
      "Epoch 0[7960/17270] Time:0.23, Train Loss:1.2890689373016357\n",
      "Epoch 0[7961/17270] Time:0.236, Train Loss:0.5173778533935547\n",
      "Epoch 0[7962/17270] Time:0.239, Train Loss:1.104376196861267\n",
      "Epoch 0[7963/17270] Time:0.23, Train Loss:0.5445078611373901\n",
      "Epoch 0[7964/17270] Time:0.237, Train Loss:0.6707044243812561\n",
      "Epoch 0[7965/17270] Time:0.237, Train Loss:0.3520037829875946\n",
      "Epoch 0[7966/17270] Time:0.238, Train Loss:0.6121398210525513\n",
      "Epoch 0[7967/17270] Time:0.237, Train Loss:0.44957488775253296\n",
      "Epoch 0[7968/17270] Time:0.235, Train Loss:1.0435024499893188\n",
      "Epoch 0[7969/17270] Time:0.231, Train Loss:0.7707433104515076\n",
      "Epoch 0[7970/17270] Time:0.24, Train Loss:0.5685307383537292\n",
      "Epoch 0[7971/17270] Time:0.235, Train Loss:0.377534955739975\n",
      "Epoch 0[7972/17270] Time:0.229, Train Loss:0.5147045254707336\n",
      "Epoch 0[7973/17270] Time:0.228, Train Loss:0.39754876494407654\n",
      "Epoch 0[7974/17270] Time:0.233, Train Loss:0.8056653141975403\n",
      "Epoch 0[7975/17270] Time:0.238, Train Loss:0.36252033710479736\n",
      "Epoch 0[7976/17270] Time:0.237, Train Loss:0.246019184589386\n",
      "Epoch 0[7977/17270] Time:0.235, Train Loss:0.45419538021087646\n",
      "Epoch 0[7978/17270] Time:0.23, Train Loss:0.8826329112052917\n",
      "Epoch 0[7979/17270] Time:0.236, Train Loss:0.3521115481853485\n",
      "Epoch 0[7980/17270] Time:0.24, Train Loss:0.33680304884910583\n",
      "Epoch 0[7981/17270] Time:0.231, Train Loss:0.446527898311615\n",
      "Epoch 0[7982/17270] Time:0.231, Train Loss:0.6983243227005005\n",
      "Epoch 0[7983/17270] Time:0.229, Train Loss:1.1477535963058472\n",
      "Epoch 0[7984/17270] Time:0.23, Train Loss:0.4493101239204407\n",
      "Epoch 0[7985/17270] Time:0.231, Train Loss:0.3791637718677521\n",
      "Epoch 0[7986/17270] Time:0.23, Train Loss:0.4507521688938141\n",
      "Epoch 0[7987/17270] Time:0.238, Train Loss:0.6819366812705994\n",
      "Epoch 0[7988/17270] Time:0.251, Train Loss:1.2984819412231445\n",
      "Epoch 0[7989/17270] Time:0.23, Train Loss:0.5274823904037476\n",
      "Epoch 0[7990/17270] Time:0.234, Train Loss:0.6633004546165466\n",
      "Epoch 0[7991/17270] Time:0.238, Train Loss:0.7163975834846497\n",
      "Epoch 0[7992/17270] Time:0.223, Train Loss:1.0170193910598755\n",
      "Epoch 0[7993/17270] Time:0.249, Train Loss:0.5942139029502869\n",
      "Epoch 0[7994/17270] Time:0.236, Train Loss:0.5358659029006958\n",
      "Epoch 0[7995/17270] Time:0.233, Train Loss:0.5129815340042114\n",
      "Epoch 0[7996/17270] Time:0.223, Train Loss:0.4756096303462982\n",
      "Epoch 0[7997/17270] Time:0.24, Train Loss:0.5940943360328674\n",
      "Epoch 0[7998/17270] Time:0.241, Train Loss:0.47120383381843567\n",
      "Epoch 0[7999/17270] Time:0.24, Train Loss:0.42673417925834656\n",
      "Epoch 0[8000/17270] Time:0.222, Train Loss:0.8193209171295166\n",
      "Epoch 0[8001/17270] Time:0.251, Train Loss:0.5604110360145569\n",
      "Epoch 0[8002/17270] Time:0.231, Train Loss:0.5466992855072021\n",
      "Epoch 0[8003/17270] Time:0.236, Train Loss:0.5234940052032471\n",
      "Epoch 0[8004/17270] Time:0.243, Train Loss:0.39570361375808716\n",
      "Epoch 0[8005/17270] Time:0.237, Train Loss:0.4894787073135376\n",
      "Epoch 0[8006/17270] Time:0.23, Train Loss:0.5830047130584717\n",
      "Epoch 0[8007/17270] Time:0.244, Train Loss:0.5422648191452026\n",
      "Epoch 0[8008/17270] Time:0.233, Train Loss:0.37579724192619324\n",
      "Epoch 0[8009/17270] Time:0.243, Train Loss:0.7050754427909851\n",
      "Epoch 0[8010/17270] Time:0.236, Train Loss:0.512706995010376\n",
      "Epoch 0[8011/17270] Time:0.232, Train Loss:0.9243977069854736\n",
      "Epoch 0[8012/17270] Time:0.245, Train Loss:0.5682239532470703\n",
      "Epoch 0[8013/17270] Time:0.234, Train Loss:0.4089418053627014\n",
      "Epoch 0[8014/17270] Time:0.241, Train Loss:0.39388349652290344\n",
      "Epoch 0[8015/17270] Time:0.237, Train Loss:0.8904396891593933\n",
      "Epoch 0[8016/17270] Time:0.235, Train Loss:0.955467700958252\n",
      "Epoch 0[8017/17270] Time:0.232, Train Loss:0.9420311450958252\n",
      "Epoch 0[8018/17270] Time:0.251, Train Loss:0.7916445136070251\n",
      "Epoch 0[8019/17270] Time:0.226, Train Loss:0.4166199564933777\n",
      "Epoch 0[8020/17270] Time:0.239, Train Loss:0.7921854257583618\n",
      "Epoch 0[8021/17270] Time:0.235, Train Loss:0.33724167943000793\n",
      "Epoch 0[8022/17270] Time:0.25, Train Loss:0.7770559787750244\n",
      "Epoch 0[8023/17270] Time:0.226, Train Loss:1.0025503635406494\n",
      "Epoch 0[8024/17270] Time:0.228, Train Loss:0.4245941638946533\n",
      "Epoch 0[8025/17270] Time:0.237, Train Loss:0.6415625810623169\n",
      "Epoch 0[8026/17270] Time:0.234, Train Loss:0.4121912717819214\n",
      "Epoch 0[8027/17270] Time:0.242, Train Loss:0.5350033044815063\n",
      "Epoch 0[8028/17270] Time:0.231, Train Loss:0.759882926940918\n",
      "Epoch 0[8029/17270] Time:0.24, Train Loss:0.5766452550888062\n",
      "Epoch 0[8030/17270] Time:0.228, Train Loss:0.47685369849205017\n",
      "Epoch 0[8031/17270] Time:0.241, Train Loss:1.1575806140899658\n",
      "Epoch 0[8032/17270] Time:0.234, Train Loss:0.8673646450042725\n",
      "Epoch 0[8033/17270] Time:0.229, Train Loss:0.5844702124595642\n",
      "Epoch 0[8034/17270] Time:0.239, Train Loss:0.628108024597168\n",
      "Epoch 0[8035/17270] Time:0.228, Train Loss:0.8993372321128845\n",
      "Epoch 0[8036/17270] Time:0.227, Train Loss:0.6825066208839417\n",
      "Epoch 0[8037/17270] Time:0.229, Train Loss:0.5618777871131897\n",
      "Epoch 0[8038/17270] Time:0.23, Train Loss:0.5543511509895325\n",
      "Epoch 0[8039/17270] Time:0.238, Train Loss:0.5110997557640076\n",
      "Epoch 0[8040/17270] Time:0.23, Train Loss:1.4868712425231934\n",
      "Epoch 0[8041/17270] Time:0.228, Train Loss:0.5835519433021545\n",
      "Epoch 0[8042/17270] Time:0.24, Train Loss:0.5960227251052856\n",
      "Epoch 0[8043/17270] Time:0.233, Train Loss:0.5058778524398804\n",
      "Epoch 0[8044/17270] Time:0.227, Train Loss:0.8586483001708984\n",
      "Epoch 0[8045/17270] Time:0.24, Train Loss:0.3396800756454468\n",
      "Epoch 0[8046/17270] Time:0.233, Train Loss:0.5168333649635315\n",
      "Epoch 0[8047/17270] Time:0.236, Train Loss:0.46214914321899414\n",
      "Epoch 0[8048/17270] Time:0.235, Train Loss:0.6784531474113464\n",
      "Epoch 0[8049/17270] Time:0.231, Train Loss:0.3260604739189148\n",
      "Epoch 0[8050/17270] Time:0.238, Train Loss:0.7933613657951355\n",
      "Epoch 0[8051/17270] Time:0.236, Train Loss:0.7044445276260376\n",
      "Epoch 0[8052/17270] Time:0.235, Train Loss:0.7642045617103577\n",
      "Epoch 0[8053/17270] Time:0.235, Train Loss:0.5081701278686523\n",
      "Epoch 0[8054/17270] Time:0.24, Train Loss:0.46890366077423096\n",
      "Epoch 0[8055/17270] Time:0.227, Train Loss:0.8747685551643372\n",
      "Epoch 0[8056/17270] Time:0.236, Train Loss:0.9604790806770325\n",
      "Epoch 0[8057/17270] Time:0.238, Train Loss:0.6041861176490784\n",
      "Epoch 0[8058/17270] Time:0.229, Train Loss:0.6345193386077881\n",
      "Epoch 0[8059/17270] Time:0.237, Train Loss:0.6546677350997925\n",
      "Epoch 0[8060/17270] Time:0.237, Train Loss:0.9597283601760864\n",
      "Epoch 0[8061/17270] Time:0.232, Train Loss:0.44231700897216797\n",
      "Epoch 0[8062/17270] Time:0.238, Train Loss:0.6662672758102417\n",
      "Epoch 0[8063/17270] Time:0.225, Train Loss:0.5103808045387268\n",
      "Epoch 0[8064/17270] Time:0.231, Train Loss:0.47972163558006287\n",
      "Epoch 0[8065/17270] Time:0.237, Train Loss:0.6857205033302307\n",
      "Epoch 0[8066/17270] Time:0.238, Train Loss:0.5064234137535095\n",
      "Epoch 0[8067/17270] Time:0.236, Train Loss:0.9255134463310242\n",
      "Epoch 0[8068/17270] Time:0.23, Train Loss:0.43454664945602417\n",
      "Epoch 0[8069/17270] Time:0.239, Train Loss:0.45234131813049316\n",
      "Epoch 0[8070/17270] Time:0.23, Train Loss:0.44784390926361084\n",
      "Epoch 0[8071/17270] Time:0.238, Train Loss:0.7508265376091003\n",
      "Epoch 0[8072/17270] Time:0.238, Train Loss:0.3191048204898834\n",
      "Epoch 0[8073/17270] Time:0.231, Train Loss:0.6723448038101196\n",
      "Epoch 0[8074/17270] Time:0.235, Train Loss:0.5645201206207275\n",
      "Epoch 0[8075/17270] Time:0.231, Train Loss:0.4120359718799591\n",
      "Epoch 0[8076/17270] Time:0.234, Train Loss:1.0851726531982422\n",
      "Epoch 0[8077/17270] Time:0.237, Train Loss:0.41848114132881165\n",
      "Epoch 0[8078/17270] Time:0.237, Train Loss:0.40345948934555054\n",
      "Epoch 0[8079/17270] Time:0.238, Train Loss:1.092984914779663\n",
      "Epoch 0[8080/17270] Time:0.228, Train Loss:0.6553503274917603\n",
      "Epoch 0[8081/17270] Time:0.237, Train Loss:0.522997260093689\n",
      "Epoch 0[8082/17270] Time:0.241, Train Loss:0.5685433149337769\n",
      "Epoch 0[8083/17270] Time:0.252, Train Loss:0.5420321822166443\n",
      "Epoch 0[8084/17270] Time:0.248, Train Loss:0.5091135501861572\n",
      "Epoch 0[8085/17270] Time:0.238, Train Loss:0.5342277884483337\n",
      "Epoch 0[8086/17270] Time:0.232, Train Loss:0.45146486163139343\n",
      "Epoch 0[8087/17270] Time:0.237, Train Loss:1.0480726957321167\n",
      "Epoch 0[8088/17270] Time:0.231, Train Loss:0.727072536945343\n",
      "Epoch 0[8089/17270] Time:0.231, Train Loss:0.5032290816307068\n",
      "Epoch 0[8090/17270] Time:0.231, Train Loss:0.7965102195739746\n",
      "Epoch 0[8091/17270] Time:0.239, Train Loss:0.684407651424408\n",
      "Epoch 0[8092/17270] Time:0.231, Train Loss:0.47235724329948425\n",
      "Epoch 0[8093/17270] Time:0.237, Train Loss:0.7494383454322815\n",
      "Epoch 0[8094/17270] Time:0.236, Train Loss:0.35282668471336365\n",
      "Epoch 0[8095/17270] Time:0.235, Train Loss:1.0439091920852661\n",
      "Epoch 0[8096/17270] Time:0.232, Train Loss:0.4914162755012512\n",
      "Epoch 0[8097/17270] Time:0.235, Train Loss:0.41417738795280457\n",
      "Epoch 0[8098/17270] Time:0.228, Train Loss:0.5115293264389038\n",
      "Epoch 0[8099/17270] Time:0.239, Train Loss:0.6568882465362549\n",
      "Epoch 0[8100/17270] Time:0.237, Train Loss:0.640129029750824\n",
      "Epoch 0[8101/17270] Time:0.229, Train Loss:0.8788238763809204\n",
      "Epoch 0[8102/17270] Time:0.24, Train Loss:0.6408689022064209\n",
      "Epoch 0[8103/17270] Time:0.236, Train Loss:0.8307111263275146\n",
      "Epoch 0[8104/17270] Time:0.239, Train Loss:0.6940454840660095\n",
      "Epoch 0[8105/17270] Time:0.231, Train Loss:0.6293874979019165\n",
      "Epoch 0[8106/17270] Time:0.236, Train Loss:0.5971828699111938\n",
      "Epoch 0[8107/17270] Time:0.238, Train Loss:0.3772510290145874\n",
      "Epoch 0[8108/17270] Time:0.235, Train Loss:0.760351836681366\n",
      "Epoch 0[8109/17270] Time:0.239, Train Loss:0.5631815195083618\n",
      "Epoch 0[8110/17270] Time:0.231, Train Loss:0.6485363245010376\n",
      "Epoch 0[8111/17270] Time:0.236, Train Loss:0.8383134007453918\n",
      "Epoch 0[8112/17270] Time:0.238, Train Loss:0.7174102663993835\n",
      "Epoch 0[8113/17270] Time:0.23, Train Loss:0.782268226146698\n",
      "Epoch 0[8114/17270] Time:0.239, Train Loss:0.29186493158340454\n",
      "Epoch 0[8115/17270] Time:0.232, Train Loss:0.8111706376075745\n",
      "Epoch 0[8116/17270] Time:0.237, Train Loss:0.3383956551551819\n",
      "Epoch 0[8117/17270] Time:0.241, Train Loss:0.5513912439346313\n",
      "Epoch 0[8118/17270] Time:0.23, Train Loss:0.4722861051559448\n",
      "Epoch 0[8119/17270] Time:0.229, Train Loss:0.5922868251800537\n",
      "Epoch 0[8120/17270] Time:0.244, Train Loss:0.39389747381210327\n",
      "Epoch 0[8121/17270] Time:0.236, Train Loss:0.34502485394477844\n",
      "Epoch 0[8122/17270] Time:0.235, Train Loss:1.3960832357406616\n",
      "Epoch 0[8123/17270] Time:0.237, Train Loss:0.706536054611206\n",
      "Epoch 0[8124/17270] Time:0.229, Train Loss:0.39120250940322876\n",
      "Epoch 0[8125/17270] Time:0.232, Train Loss:0.7027677893638611\n",
      "Epoch 0[8126/17270] Time:0.239, Train Loss:0.5042514801025391\n",
      "Epoch 0[8127/17270] Time:0.239, Train Loss:0.4655359387397766\n",
      "Epoch 0[8128/17270] Time:0.228, Train Loss:0.6933153867721558\n",
      "Epoch 0[8129/17270] Time:0.228, Train Loss:0.36415207386016846\n",
      "Epoch 0[8130/17270] Time:0.236, Train Loss:0.6558265089988708\n",
      "Epoch 0[8131/17270] Time:0.238, Train Loss:0.8068382143974304\n",
      "Epoch 0[8132/17270] Time:0.237, Train Loss:0.648155152797699\n",
      "Epoch 0[8133/17270] Time:0.229, Train Loss:0.46666085720062256\n",
      "Epoch 0[8134/17270] Time:0.249, Train Loss:0.7293460369110107\n",
      "Epoch 0[8135/17270] Time:0.236, Train Loss:0.5236668586730957\n",
      "Epoch 0[8136/17270] Time:0.237, Train Loss:0.39004218578338623\n",
      "Epoch 0[8137/17270] Time:0.239, Train Loss:0.9000127911567688\n",
      "Epoch 0[8138/17270] Time:0.241, Train Loss:0.9813660979270935\n",
      "Epoch 0[8139/17270] Time:0.238, Train Loss:0.3872913122177124\n",
      "Epoch 0[8140/17270] Time:0.23, Train Loss:0.9021236300468445\n",
      "Epoch 0[8141/17270] Time:0.236, Train Loss:0.45913341641426086\n",
      "Epoch 0[8142/17270] Time:0.235, Train Loss:0.6856394410133362\n",
      "Epoch 0[8143/17270] Time:0.236, Train Loss:0.8642172813415527\n",
      "Epoch 0[8144/17270] Time:0.239, Train Loss:0.828646719455719\n",
      "Epoch 0[8145/17270] Time:0.244, Train Loss:0.9707111716270447\n",
      "Epoch 0[8146/17270] Time:0.233, Train Loss:0.8146973252296448\n",
      "Epoch 0[8147/17270] Time:0.221, Train Loss:0.48721548914909363\n",
      "Epoch 0[8148/17270] Time:0.228, Train Loss:0.7175875902175903\n",
      "Epoch 0[8149/17270] Time:0.236, Train Loss:0.45477494597435\n",
      "Epoch 0[8150/17270] Time:0.24, Train Loss:1.0512206554412842\n",
      "Epoch 0[8151/17270] Time:0.238, Train Loss:0.31004101037979126\n",
      "Epoch 0[8152/17270] Time:0.238, Train Loss:1.0256551504135132\n",
      "Epoch 0[8153/17270] Time:0.239, Train Loss:0.7191553115844727\n",
      "Epoch 0[8154/17270] Time:0.236, Train Loss:0.9117857813835144\n",
      "Epoch 0[8155/17270] Time:0.243, Train Loss:0.506973385810852\n",
      "Epoch 0[8156/17270] Time:0.241, Train Loss:0.67262864112854\n",
      "Epoch 0[8157/17270] Time:0.248, Train Loss:0.4674026072025299\n",
      "Epoch 0[8158/17270] Time:0.221, Train Loss:0.6976884603500366\n",
      "Epoch 0[8159/17270] Time:0.244, Train Loss:0.6324831247329712\n",
      "Epoch 0[8160/17270] Time:0.243, Train Loss:0.7733697891235352\n",
      "Epoch 0[8161/17270] Time:0.243, Train Loss:0.8240609765052795\n",
      "Epoch 0[8162/17270] Time:0.235, Train Loss:0.4634411633014679\n",
      "Epoch 0[8163/17270] Time:0.236, Train Loss:0.7914229035377502\n",
      "Epoch 0[8164/17270] Time:0.25, Train Loss:0.41805315017700195\n",
      "Epoch 0[8165/17270] Time:0.242, Train Loss:0.640242338180542\n",
      "Epoch 0[8166/17270] Time:0.234, Train Loss:0.650684654712677\n",
      "Epoch 0[8167/17270] Time:0.239, Train Loss:0.8077329993247986\n",
      "Epoch 0[8168/17270] Time:0.224, Train Loss:0.6329660415649414\n",
      "Epoch 0[8169/17270] Time:0.236, Train Loss:0.6028022766113281\n",
      "Epoch 0[8170/17270] Time:0.226, Train Loss:0.6077603697776794\n",
      "Epoch 0[8171/17270] Time:0.235, Train Loss:0.44815877079963684\n",
      "Epoch 0[8172/17270] Time:0.229, Train Loss:0.3578839898109436\n",
      "Epoch 0[8173/17270] Time:0.235, Train Loss:0.8418753743171692\n",
      "Epoch 0[8174/17270] Time:0.236, Train Loss:1.0350216627120972\n",
      "Epoch 0[8175/17270] Time:0.236, Train Loss:0.4381200075149536\n",
      "Epoch 0[8176/17270] Time:0.238, Train Loss:0.7059300541877747\n",
      "Epoch 0[8177/17270] Time:0.244, Train Loss:0.5293573141098022\n",
      "Epoch 0[8178/17270] Time:0.229, Train Loss:0.34643346071243286\n",
      "Epoch 0[8179/17270] Time:0.229, Train Loss:0.7015483975410461\n",
      "Epoch 0[8180/17270] Time:0.233, Train Loss:0.986198365688324\n",
      "Epoch 0[8181/17270] Time:0.231, Train Loss:0.7271093726158142\n",
      "Epoch 0[8182/17270] Time:0.228, Train Loss:0.3952876031398773\n",
      "Epoch 0[8183/17270] Time:0.236, Train Loss:0.5451255440711975\n",
      "Epoch 0[8184/17270] Time:0.238, Train Loss:0.7212691903114319\n",
      "Epoch 0[8185/17270] Time:0.238, Train Loss:0.5807375907897949\n",
      "Epoch 0[8186/17270] Time:0.233, Train Loss:0.375474214553833\n",
      "Epoch 0[8187/17270] Time:0.226, Train Loss:0.6870822906494141\n",
      "Epoch 0[8188/17270] Time:0.231, Train Loss:0.6046780943870544\n",
      "Epoch 0[8189/17270] Time:0.235, Train Loss:0.6742850542068481\n",
      "Epoch 0[8190/17270] Time:0.239, Train Loss:0.46048787236213684\n",
      "Epoch 0[8191/17270] Time:0.236, Train Loss:0.6089077591896057\n",
      "Epoch 0[8192/17270] Time:0.231, Train Loss:0.48002058267593384\n",
      "Epoch 0[8193/17270] Time:0.236, Train Loss:0.5984672904014587\n",
      "Epoch 0[8194/17270] Time:0.239, Train Loss:0.9036422371864319\n",
      "Epoch 0[8195/17270] Time:0.231, Train Loss:0.31648680567741394\n",
      "Epoch 0[8196/17270] Time:0.238, Train Loss:0.8903336524963379\n",
      "Epoch 0[8197/17270] Time:0.242, Train Loss:0.9059235453605652\n",
      "Epoch 0[8198/17270] Time:0.241, Train Loss:0.3386768698692322\n",
      "Epoch 0[8199/17270] Time:0.228, Train Loss:0.7300506234169006\n",
      "Epoch 0[8200/17270] Time:0.235, Train Loss:0.3782530725002289\n",
      "Epoch 0[8201/17270] Time:0.23, Train Loss:0.33046987652778625\n",
      "Epoch 0[8202/17270] Time:0.229, Train Loss:0.38744333386421204\n",
      "Epoch 0[8203/17270] Time:0.234, Train Loss:0.3781611919403076\n",
      "Epoch 0[8204/17270] Time:0.236, Train Loss:0.5055007338523865\n",
      "Epoch 0[8205/17270] Time:0.231, Train Loss:0.33458003401756287\n",
      "Epoch 0[8206/17270] Time:0.237, Train Loss:0.4611299932003021\n",
      "Epoch 0[8207/17270] Time:0.237, Train Loss:0.49913501739501953\n",
      "Epoch 0[8208/17270] Time:0.236, Train Loss:0.8350901007652283\n",
      "Epoch 0[8209/17270] Time:0.242, Train Loss:0.5252342820167542\n",
      "Epoch 0[8210/17270] Time:0.231, Train Loss:0.5521030426025391\n",
      "Epoch 0[8211/17270] Time:0.228, Train Loss:0.7250220775604248\n",
      "Epoch 0[8212/17270] Time:0.234, Train Loss:0.6239191293716431\n",
      "Epoch 0[8213/17270] Time:0.218, Train Loss:0.5885899662971497\n",
      "Epoch 0[8214/17270] Time:0.238, Train Loss:0.4471077024936676\n",
      "Epoch 0[8215/17270] Time:0.236, Train Loss:0.5387172698974609\n",
      "Epoch 0[8216/17270] Time:0.242, Train Loss:0.44917500019073486\n",
      "Epoch 0[8217/17270] Time:0.229, Train Loss:0.441739946603775\n",
      "Epoch 0[8218/17270] Time:0.235, Train Loss:0.40253880620002747\n",
      "Epoch 0[8219/17270] Time:0.233, Train Loss:0.5490540862083435\n",
      "Epoch 0[8220/17270] Time:0.235, Train Loss:0.47672170400619507\n",
      "Epoch 0[8221/17270] Time:0.222, Train Loss:0.2727464437484741\n",
      "Epoch 0[8222/17270] Time:0.227, Train Loss:0.2612517476081848\n",
      "Epoch 0[8223/17270] Time:0.228, Train Loss:0.41588038206100464\n",
      "Epoch 0[8224/17270] Time:0.238, Train Loss:0.3038547933101654\n",
      "Epoch 0[8225/17270] Time:0.232, Train Loss:0.8730901479721069\n",
      "Epoch 0[8226/17270] Time:0.23, Train Loss:0.6043182015419006\n",
      "Epoch 0[8227/17270] Time:0.24, Train Loss:0.6584476828575134\n",
      "Epoch 0[8228/17270] Time:0.238, Train Loss:0.29888561367988586\n",
      "Epoch 0[8229/17270] Time:0.238, Train Loss:0.44202920794487\n",
      "Epoch 0[8230/17270] Time:0.231, Train Loss:0.4328206479549408\n",
      "Epoch 0[8231/17270] Time:0.234, Train Loss:0.30358946323394775\n",
      "Epoch 0[8232/17270] Time:0.232, Train Loss:0.3572666645050049\n",
      "Epoch 0[8233/17270] Time:0.235, Train Loss:0.41737380623817444\n",
      "Epoch 0[8234/17270] Time:0.224, Train Loss:1.2057408094406128\n",
      "Epoch 0[8235/17270] Time:0.255, Train Loss:0.7454890608787537\n",
      "Epoch 0[8236/17270] Time:0.235, Train Loss:0.3270494043827057\n",
      "Epoch 0[8237/17270] Time:0.232, Train Loss:0.5306097269058228\n",
      "Epoch 0[8238/17270] Time:0.238, Train Loss:0.5569902658462524\n",
      "Epoch 0[8239/17270] Time:0.227, Train Loss:0.6581557393074036\n",
      "Epoch 0[8240/17270] Time:0.243, Train Loss:0.7573419213294983\n",
      "Epoch 0[8241/17270] Time:0.257, Train Loss:1.0110167264938354\n",
      "Epoch 0[8242/17270] Time:0.232, Train Loss:0.4648236036300659\n",
      "Epoch 0[8243/17270] Time:0.248, Train Loss:0.41582971811294556\n",
      "Epoch 0[8244/17270] Time:0.239, Train Loss:0.6005346179008484\n",
      "Epoch 0[8245/17270] Time:0.241, Train Loss:2.1010632514953613\n",
      "Epoch 0[8246/17270] Time:0.243, Train Loss:0.4761796295642853\n",
      "Epoch 0[8247/17270] Time:0.239, Train Loss:0.48136088252067566\n",
      "Epoch 0[8248/17270] Time:0.235, Train Loss:0.6421643495559692\n",
      "Epoch 0[8249/17270] Time:0.243, Train Loss:0.5279884934425354\n",
      "Epoch 0[8250/17270] Time:0.236, Train Loss:0.38746482133865356\n",
      "Epoch 0[8251/17270] Time:0.243, Train Loss:1.7037339210510254\n",
      "Epoch 0[8252/17270] Time:0.232, Train Loss:0.6552814245223999\n",
      "Epoch 0[8253/17270] Time:0.24, Train Loss:0.8138020038604736\n",
      "Epoch 0[8254/17270] Time:0.23, Train Loss:0.33922797441482544\n",
      "Epoch 0[8255/17270] Time:0.233, Train Loss:0.699661135673523\n",
      "Epoch 0[8256/17270] Time:0.237, Train Loss:0.5874205231666565\n",
      "Epoch 0[8257/17270] Time:0.237, Train Loss:0.5229695439338684\n",
      "Epoch 0[8258/17270] Time:0.233, Train Loss:0.44705578684806824\n",
      "Epoch 0[8259/17270] Time:0.235, Train Loss:0.5509565472602844\n",
      "Epoch 0[8260/17270] Time:0.236, Train Loss:0.46657028794288635\n",
      "Epoch 0[8261/17270] Time:0.231, Train Loss:0.6500083804130554\n",
      "Epoch 0[8262/17270] Time:0.23, Train Loss:0.47175976634025574\n",
      "Epoch 0[8263/17270] Time:0.248, Train Loss:1.108691692352295\n",
      "Epoch 0[8264/17270] Time:0.23, Train Loss:0.7424507737159729\n",
      "Epoch 0[8265/17270] Time:0.238, Train Loss:0.8433675169944763\n",
      "Epoch 0[8266/17270] Time:0.231, Train Loss:0.46067824959754944\n",
      "Epoch 0[8267/17270] Time:0.239, Train Loss:0.7230860590934753\n",
      "Epoch 0[8268/17270] Time:0.235, Train Loss:0.3381913900375366\n",
      "Epoch 0[8269/17270] Time:0.224, Train Loss:0.7297959327697754\n",
      "Epoch 0[8270/17270] Time:0.238, Train Loss:0.4939226508140564\n",
      "Epoch 0[8271/17270] Time:0.227, Train Loss:0.720973789691925\n",
      "Epoch 0[8272/17270] Time:0.233, Train Loss:0.9933021068572998\n",
      "Epoch 0[8273/17270] Time:0.236, Train Loss:0.45922377705574036\n",
      "Epoch 0[8274/17270] Time:0.231, Train Loss:0.3520490527153015\n",
      "Epoch 0[8275/17270] Time:0.232, Train Loss:0.6327177882194519\n",
      "Epoch 0[8276/17270] Time:0.237, Train Loss:0.5114447474479675\n",
      "Epoch 0[8277/17270] Time:0.239, Train Loss:0.31909075379371643\n",
      "Epoch 0[8278/17270] Time:0.238, Train Loss:0.8337448835372925\n",
      "Epoch 0[8279/17270] Time:0.226, Train Loss:0.3623557686805725\n",
      "Epoch 0[8280/17270] Time:0.248, Train Loss:0.470622181892395\n",
      "Epoch 0[8281/17270] Time:0.238, Train Loss:0.2135341316461563\n",
      "Epoch 0[8282/17270] Time:0.229, Train Loss:0.6586699485778809\n",
      "Epoch 0[8283/17270] Time:0.226, Train Loss:0.7078840732574463\n",
      "Epoch 0[8284/17270] Time:0.229, Train Loss:0.7137907147407532\n",
      "Epoch 0[8285/17270] Time:0.238, Train Loss:0.4939677119255066\n",
      "Epoch 0[8286/17270] Time:0.231, Train Loss:0.3892481327056885\n",
      "Epoch 0[8287/17270] Time:0.23, Train Loss:0.41670411825180054\n",
      "Epoch 0[8288/17270] Time:0.227, Train Loss:0.6741140484809875\n",
      "Epoch 0[8289/17270] Time:0.227, Train Loss:0.6384080648422241\n",
      "Epoch 0[8290/17270] Time:0.232, Train Loss:0.9107686877250671\n",
      "Epoch 0[8291/17270] Time:0.228, Train Loss:0.44058138132095337\n",
      "Epoch 0[8292/17270] Time:0.237, Train Loss:0.4101201593875885\n",
      "Epoch 0[8293/17270] Time:0.236, Train Loss:0.7528854012489319\n",
      "Epoch 0[8294/17270] Time:0.238, Train Loss:0.5145776867866516\n",
      "Epoch 0[8295/17270] Time:0.233, Train Loss:0.9831047654151917\n",
      "Epoch 0[8296/17270] Time:0.254, Train Loss:0.8585211038589478\n",
      "Epoch 0[8297/17270] Time:0.236, Train Loss:0.3058859705924988\n",
      "Epoch 0[8298/17270] Time:0.226, Train Loss:0.3982371389865875\n",
      "Epoch 0[8299/17270] Time:0.232, Train Loss:0.47119641304016113\n",
      "Epoch 0[8300/17270] Time:0.246, Train Loss:0.5359100103378296\n",
      "Epoch 0[8301/17270] Time:0.24, Train Loss:1.1714006662368774\n",
      "Epoch 0[8302/17270] Time:0.237, Train Loss:0.615868330001831\n",
      "Epoch 0[8303/17270] Time:0.237, Train Loss:0.39407551288604736\n",
      "Epoch 0[8304/17270] Time:0.232, Train Loss:0.5691438317298889\n",
      "Epoch 0[8305/17270] Time:0.244, Train Loss:0.6905222535133362\n",
      "Epoch 0[8306/17270] Time:0.231, Train Loss:0.33808115124702454\n",
      "Epoch 0[8307/17270] Time:0.24, Train Loss:1.0912340879440308\n",
      "Epoch 0[8308/17270] Time:0.237, Train Loss:0.5514705181121826\n",
      "Epoch 0[8309/17270] Time:0.226, Train Loss:1.2141313552856445\n",
      "Epoch 0[8310/17270] Time:0.233, Train Loss:0.413192480802536\n",
      "Epoch 0[8311/17270] Time:0.23, Train Loss:0.7304794192314148\n",
      "Epoch 0[8312/17270] Time:0.236, Train Loss:0.5458249449729919\n",
      "Epoch 0[8313/17270] Time:0.232, Train Loss:0.7411417961120605\n",
      "Epoch 0[8314/17270] Time:0.23, Train Loss:0.49080055952072144\n",
      "Epoch 0[8315/17270] Time:0.237, Train Loss:0.4549206495285034\n",
      "Epoch 0[8316/17270] Time:0.232, Train Loss:0.4376846253871918\n",
      "Epoch 0[8317/17270] Time:0.243, Train Loss:0.34887129068374634\n",
      "Epoch 0[8318/17270] Time:0.227, Train Loss:1.2670629024505615\n",
      "Epoch 0[8319/17270] Time:0.224, Train Loss:0.5001314878463745\n",
      "Epoch 0[8320/17270] Time:0.229, Train Loss:0.4232621490955353\n",
      "Epoch 0[8321/17270] Time:0.239, Train Loss:0.5048019289970398\n",
      "Epoch 0[8322/17270] Time:0.241, Train Loss:0.41495561599731445\n",
      "Epoch 0[8323/17270] Time:0.227, Train Loss:0.5647498965263367\n",
      "Epoch 0[8324/17270] Time:0.236, Train Loss:0.6218488216400146\n",
      "Epoch 0[8325/17270] Time:0.241, Train Loss:0.4569858908653259\n",
      "Epoch 0[8326/17270] Time:0.232, Train Loss:0.4015240967273712\n",
      "Epoch 0[8327/17270] Time:0.246, Train Loss:0.4587548077106476\n",
      "Epoch 0[8328/17270] Time:0.232, Train Loss:0.6960737109184265\n",
      "Epoch 0[8329/17270] Time:0.238, Train Loss:0.6133978962898254\n",
      "Epoch 0[8330/17270] Time:0.248, Train Loss:0.52569180727005\n",
      "Epoch 0[8331/17270] Time:0.245, Train Loss:0.6046191453933716\n",
      "Epoch 0[8332/17270] Time:0.235, Train Loss:0.36693036556243896\n",
      "Epoch 0[8333/17270] Time:0.238, Train Loss:0.2636532783508301\n",
      "Epoch 0[8334/17270] Time:0.234, Train Loss:0.9272440075874329\n",
      "Epoch 0[8335/17270] Time:0.243, Train Loss:0.9293866157531738\n",
      "Epoch 0[8336/17270] Time:0.222, Train Loss:0.3540538251399994\n",
      "Epoch 0[8337/17270] Time:0.244, Train Loss:0.24714362621307373\n",
      "Epoch 0[8338/17270] Time:0.228, Train Loss:0.5656877160072327\n",
      "Epoch 0[8339/17270] Time:0.239, Train Loss:0.381299763917923\n",
      "Epoch 0[8340/17270] Time:0.229, Train Loss:0.5638744831085205\n",
      "Epoch 0[8341/17270] Time:0.24, Train Loss:0.6103211045265198\n",
      "Epoch 0[8342/17270] Time:0.238, Train Loss:0.640896201133728\n",
      "Epoch 0[8343/17270] Time:0.24, Train Loss:0.6886051893234253\n",
      "Epoch 0[8344/17270] Time:0.236, Train Loss:0.46272149682044983\n",
      "Epoch 0[8345/17270] Time:0.226, Train Loss:0.6881420016288757\n",
      "Epoch 0[8346/17270] Time:0.237, Train Loss:0.6812484264373779\n",
      "Epoch 0[8347/17270] Time:0.234, Train Loss:0.7529172897338867\n",
      "Epoch 0[8348/17270] Time:0.24, Train Loss:0.5809201598167419\n",
      "Epoch 0[8349/17270] Time:0.246, Train Loss:1.0603760480880737\n",
      "Epoch 0[8350/17270] Time:0.237, Train Loss:0.414221853017807\n",
      "Epoch 0[8351/17270] Time:0.223, Train Loss:0.5398280024528503\n",
      "Epoch 0[8352/17270] Time:0.237, Train Loss:0.660791277885437\n",
      "Epoch 0[8353/17270] Time:0.236, Train Loss:0.3910687267780304\n",
      "Epoch 0[8354/17270] Time:0.237, Train Loss:1.1235361099243164\n",
      "Epoch 0[8355/17270] Time:0.228, Train Loss:0.6755859851837158\n",
      "Epoch 0[8356/17270] Time:0.239, Train Loss:0.5048615336418152\n",
      "Epoch 0[8357/17270] Time:0.231, Train Loss:0.5701017379760742\n",
      "Epoch 0[8358/17270] Time:0.229, Train Loss:0.92256760597229\n",
      "Epoch 0[8359/17270] Time:0.233, Train Loss:0.4995482265949249\n",
      "Epoch 0[8360/17270] Time:0.238, Train Loss:0.30997908115386963\n",
      "Epoch 0[8361/17270] Time:0.23, Train Loss:1.1648550033569336\n",
      "Epoch 0[8362/17270] Time:0.234, Train Loss:0.8444721102714539\n",
      "Epoch 0[8363/17270] Time:0.233, Train Loss:0.6695820093154907\n",
      "Epoch 0[8364/17270] Time:0.231, Train Loss:1.4061212539672852\n",
      "Epoch 0[8365/17270] Time:0.227, Train Loss:0.7563848495483398\n",
      "Epoch 0[8366/17270] Time:0.244, Train Loss:0.3651139438152313\n",
      "Epoch 0[8367/17270] Time:0.224, Train Loss:0.5521214604377747\n",
      "Epoch 0[8368/17270] Time:0.23, Train Loss:0.7274584174156189\n",
      "Epoch 0[8369/17270] Time:0.243, Train Loss:0.5006296038627625\n",
      "Epoch 0[8370/17270] Time:0.231, Train Loss:0.5885078310966492\n",
      "Epoch 0[8371/17270] Time:0.245, Train Loss:0.5365942716598511\n",
      "Epoch 0[8372/17270] Time:0.234, Train Loss:0.4062657952308655\n",
      "Epoch 0[8373/17270] Time:0.235, Train Loss:0.6015171408653259\n",
      "Epoch 0[8374/17270] Time:0.235, Train Loss:0.5377389192581177\n",
      "Epoch 0[8375/17270] Time:0.236, Train Loss:0.6481382846832275\n",
      "Epoch 0[8376/17270] Time:0.233, Train Loss:0.5313825011253357\n",
      "Epoch 0[8377/17270] Time:0.226, Train Loss:0.5921885967254639\n",
      "Epoch 0[8378/17270] Time:0.228, Train Loss:0.8802039623260498\n",
      "Epoch 0[8379/17270] Time:0.242, Train Loss:0.784651517868042\n",
      "Epoch 0[8380/17270] Time:0.233, Train Loss:0.3741193115711212\n",
      "Epoch 0[8381/17270] Time:0.238, Train Loss:0.9014893174171448\n",
      "Epoch 0[8382/17270] Time:0.237, Train Loss:0.7274762988090515\n",
      "Epoch 0[8383/17270] Time:0.231, Train Loss:0.4682302474975586\n",
      "Epoch 0[8384/17270] Time:0.234, Train Loss:0.9259139895439148\n",
      "Epoch 0[8385/17270] Time:0.23, Train Loss:0.5218822956085205\n",
      "Epoch 0[8386/17270] Time:0.24, Train Loss:0.5559753775596619\n",
      "Epoch 0[8387/17270] Time:0.231, Train Loss:0.49133527278900146\n",
      "Epoch 0[8388/17270] Time:0.226, Train Loss:0.4296054244041443\n",
      "Epoch 0[8389/17270] Time:0.23, Train Loss:0.6481346487998962\n",
      "Epoch 0[8390/17270] Time:0.226, Train Loss:0.5318114161491394\n",
      "Epoch 0[8391/17270] Time:0.241, Train Loss:0.4204463064670563\n",
      "Epoch 0[8392/17270] Time:0.243, Train Loss:0.5351784229278564\n",
      "Epoch 0[8393/17270] Time:0.23, Train Loss:0.3434036076068878\n",
      "Epoch 0[8394/17270] Time:0.229, Train Loss:0.4196438789367676\n",
      "Epoch 0[8395/17270] Time:0.243, Train Loss:0.41884732246398926\n",
      "Epoch 0[8396/17270] Time:0.234, Train Loss:0.5120137929916382\n",
      "Epoch 0[8397/17270] Time:0.231, Train Loss:1.3388769626617432\n",
      "Epoch 0[8398/17270] Time:0.223, Train Loss:0.5660247802734375\n",
      "Epoch 0[8399/17270] Time:0.233, Train Loss:0.4603865444660187\n",
      "Epoch 0[8400/17270] Time:0.247, Train Loss:0.49216872453689575\n",
      "Epoch 0[8401/17270] Time:0.237, Train Loss:0.30163446068763733\n",
      "Epoch 0[8402/17270] Time:0.234, Train Loss:0.33016693592071533\n",
      "Epoch 0[8403/17270] Time:0.225, Train Loss:0.7811133861541748\n",
      "Epoch 0[8404/17270] Time:0.24, Train Loss:0.4109809696674347\n",
      "Epoch 0[8405/17270] Time:0.232, Train Loss:0.5070834159851074\n",
      "Epoch 0[8406/17270] Time:0.231, Train Loss:0.4732402563095093\n",
      "Epoch 0[8407/17270] Time:0.232, Train Loss:0.5005238652229309\n",
      "Epoch 0[8408/17270] Time:0.248, Train Loss:0.4706612825393677\n",
      "Epoch 0[8409/17270] Time:0.232, Train Loss:0.6308939456939697\n",
      "Epoch 0[8410/17270] Time:0.222, Train Loss:0.35486117005348206\n",
      "Epoch 0[8411/17270] Time:0.24, Train Loss:0.6947908997535706\n",
      "Epoch 0[8412/17270] Time:0.232, Train Loss:0.5252654552459717\n",
      "Epoch 0[8413/17270] Time:0.235, Train Loss:0.39884573221206665\n",
      "Epoch 0[8414/17270] Time:0.239, Train Loss:1.2743842601776123\n",
      "Epoch 0[8415/17270] Time:0.228, Train Loss:0.7199986577033997\n",
      "Epoch 0[8416/17270] Time:0.243, Train Loss:0.796903133392334\n",
      "Epoch 0[8417/17270] Time:0.225, Train Loss:0.45926764607429504\n",
      "Epoch 0[8418/17270] Time:0.229, Train Loss:0.497952401638031\n",
      "Epoch 0[8419/17270] Time:0.227, Train Loss:0.7383480072021484\n",
      "Epoch 0[8420/17270] Time:0.226, Train Loss:0.36139094829559326\n",
      "Epoch 0[8421/17270] Time:0.233, Train Loss:0.3813899755477905\n",
      "Epoch 0[8422/17270] Time:0.236, Train Loss:0.7150226831436157\n",
      "Epoch 0[8423/17270] Time:0.237, Train Loss:0.2848605215549469\n",
      "Epoch 0[8424/17270] Time:0.234, Train Loss:0.5762253403663635\n",
      "Epoch 0[8425/17270] Time:0.224, Train Loss:0.4613916277885437\n",
      "Epoch 0[8426/17270] Time:0.237, Train Loss:0.5373051166534424\n",
      "Epoch 0[8427/17270] Time:0.229, Train Loss:0.5544226765632629\n",
      "Epoch 0[8428/17270] Time:0.235, Train Loss:0.5018495917320251\n",
      "Epoch 0[8429/17270] Time:0.232, Train Loss:0.3359788954257965\n",
      "Epoch 0[8430/17270] Time:0.234, Train Loss:0.4867437779903412\n",
      "Epoch 0[8431/17270] Time:0.235, Train Loss:0.8941112160682678\n",
      "Epoch 0[8432/17270] Time:0.235, Train Loss:0.5440957546234131\n",
      "Epoch 0[8433/17270] Time:0.233, Train Loss:0.5495133399963379\n",
      "Epoch 0[8434/17270] Time:0.236, Train Loss:0.2721554934978485\n",
      "Epoch 0[8435/17270] Time:0.233, Train Loss:0.6496753692626953\n",
      "Epoch 0[8436/17270] Time:0.234, Train Loss:0.9337447285652161\n",
      "Epoch 0[8437/17270] Time:0.238, Train Loss:0.48699313402175903\n",
      "Epoch 0[8438/17270] Time:0.238, Train Loss:1.1157258749008179\n",
      "Epoch 0[8439/17270] Time:0.235, Train Loss:0.6582842469215393\n",
      "Epoch 0[8440/17270] Time:0.238, Train Loss:0.476624459028244\n",
      "Epoch 0[8441/17270] Time:0.235, Train Loss:0.38295474648475647\n",
      "Epoch 0[8442/17270] Time:0.234, Train Loss:0.35571685433387756\n",
      "Epoch 0[8443/17270] Time:0.229, Train Loss:0.7024978995323181\n",
      "Epoch 0[8444/17270] Time:0.243, Train Loss:0.5117277503013611\n",
      "Epoch 0[8445/17270] Time:0.226, Train Loss:1.5719728469848633\n",
      "Epoch 0[8446/17270] Time:0.229, Train Loss:0.3561180531978607\n",
      "Epoch 0[8447/17270] Time:0.23, Train Loss:0.45234671235084534\n",
      "Epoch 0[8448/17270] Time:0.233, Train Loss:0.6094977259635925\n",
      "Epoch 0[8449/17270] Time:0.237, Train Loss:0.7169772982597351\n",
      "Epoch 0[8450/17270] Time:0.232, Train Loss:0.4449635446071625\n",
      "Epoch 0[8451/17270] Time:0.235, Train Loss:0.7008880972862244\n",
      "Epoch 0[8452/17270] Time:0.223, Train Loss:0.5524635314941406\n",
      "Epoch 0[8453/17270] Time:0.246, Train Loss:0.6357395052909851\n",
      "Epoch 0[8454/17270] Time:0.243, Train Loss:0.4456711709499359\n",
      "Epoch 0[8455/17270] Time:0.235, Train Loss:0.7898012399673462\n",
      "Epoch 0[8456/17270] Time:0.233, Train Loss:0.5363343358039856\n",
      "Epoch 0[8457/17270] Time:0.237, Train Loss:0.38944217562675476\n",
      "Epoch 0[8458/17270] Time:0.234, Train Loss:0.5835384130477905\n",
      "Epoch 0[8459/17270] Time:0.222, Train Loss:0.6040189266204834\n",
      "Epoch 0[8460/17270] Time:0.237, Train Loss:0.3534286916255951\n",
      "Epoch 0[8461/17270] Time:0.238, Train Loss:0.36462631821632385\n",
      "Epoch 0[8462/17270] Time:0.235, Train Loss:0.6675600409507751\n",
      "Epoch 0[8463/17270] Time:0.224, Train Loss:0.3674178719520569\n",
      "Epoch 0[8464/17270] Time:0.243, Train Loss:0.488483190536499\n",
      "Epoch 0[8465/17270] Time:0.242, Train Loss:0.4802524447441101\n",
      "Epoch 0[8466/17270] Time:0.232, Train Loss:0.7151049971580505\n",
      "Epoch 0[8467/17270] Time:0.226, Train Loss:0.579872727394104\n",
      "Epoch 0[8468/17270] Time:0.236, Train Loss:0.5773347616195679\n",
      "Epoch 0[8469/17270] Time:0.235, Train Loss:0.488106369972229\n",
      "Epoch 0[8470/17270] Time:0.225, Train Loss:0.5562722682952881\n",
      "Epoch 0[8471/17270] Time:0.238, Train Loss:1.0177959203720093\n",
      "Epoch 0[8472/17270] Time:0.239, Train Loss:0.421416699886322\n",
      "Epoch 0[8473/17270] Time:0.226, Train Loss:0.5107194781303406\n",
      "Epoch 0[8474/17270] Time:0.235, Train Loss:0.8559682965278625\n",
      "Epoch 0[8475/17270] Time:0.238, Train Loss:0.5709749460220337\n",
      "Epoch 0[8476/17270] Time:0.231, Train Loss:0.36826106905937195\n",
      "Epoch 0[8477/17270] Time:0.234, Train Loss:0.3693893849849701\n",
      "Epoch 0[8478/17270] Time:0.236, Train Loss:0.7192479372024536\n",
      "Epoch 0[8479/17270] Time:0.247, Train Loss:0.5305581092834473\n",
      "Epoch 0[8480/17270] Time:0.216, Train Loss:0.25582703948020935\n",
      "Epoch 0[8481/17270] Time:0.248, Train Loss:0.6133930683135986\n",
      "Epoch 0[8482/17270] Time:0.244, Train Loss:0.45523807406425476\n",
      "Epoch 0[8483/17270] Time:0.233, Train Loss:0.29996457695961\n",
      "Epoch 0[8484/17270] Time:0.225, Train Loss:0.3630194067955017\n",
      "Epoch 0[8485/17270] Time:0.243, Train Loss:0.9442456960678101\n",
      "Epoch 0[8486/17270] Time:0.231, Train Loss:0.4397532641887665\n",
      "Epoch 0[8487/17270] Time:0.246, Train Loss:0.5197833180427551\n",
      "Epoch 0[8488/17270] Time:0.238, Train Loss:1.2094782590866089\n",
      "Epoch 0[8489/17270] Time:0.241, Train Loss:0.2685905694961548\n",
      "Epoch 0[8490/17270] Time:0.234, Train Loss:0.7286965250968933\n",
      "Epoch 0[8491/17270] Time:0.247, Train Loss:1.209112286567688\n",
      "Epoch 0[8492/17270] Time:0.235, Train Loss:0.8409262299537659\n",
      "Epoch 0[8493/17270] Time:0.24, Train Loss:0.5480444431304932\n",
      "Epoch 0[8494/17270] Time:0.232, Train Loss:0.8441002368927002\n",
      "Epoch 0[8495/17270] Time:0.247, Train Loss:0.5787253379821777\n",
      "Epoch 0[8496/17270] Time:0.245, Train Loss:0.45074930787086487\n",
      "Epoch 0[8497/17270] Time:0.24, Train Loss:0.5526766777038574\n",
      "Epoch 0[8498/17270] Time:0.228, Train Loss:0.6648991703987122\n",
      "Epoch 0[8499/17270] Time:0.23, Train Loss:0.49444684386253357\n",
      "Epoch 0[8500/17270] Time:0.233, Train Loss:0.27319759130477905\n",
      "Epoch 0[8501/17270] Time:0.231, Train Loss:0.7606183290481567\n",
      "Epoch 0[8502/17270] Time:0.229, Train Loss:0.3371557295322418\n",
      "Epoch 0[8503/17270] Time:0.236, Train Loss:0.9617022275924683\n",
      "Epoch 0[8504/17270] Time:0.241, Train Loss:0.5401092767715454\n",
      "Epoch 0[8505/17270] Time:0.233, Train Loss:0.3366755247116089\n",
      "Epoch 0[8506/17270] Time:0.228, Train Loss:0.41890206933021545\n",
      "Epoch 0[8507/17270] Time:0.228, Train Loss:0.6196087598800659\n",
      "Epoch 0[8508/17270] Time:0.238, Train Loss:0.4005924463272095\n",
      "Epoch 0[8509/17270] Time:0.237, Train Loss:0.5855921506881714\n",
      "Epoch 0[8510/17270] Time:0.237, Train Loss:0.3258717358112335\n",
      "Epoch 0[8511/17270] Time:0.237, Train Loss:0.6110724806785583\n",
      "Epoch 0[8512/17270] Time:0.23, Train Loss:0.689020574092865\n",
      "Epoch 0[8513/17270] Time:0.233, Train Loss:0.6555982828140259\n",
      "Epoch 0[8514/17270] Time:0.235, Train Loss:0.6590043306350708\n",
      "Epoch 0[8515/17270] Time:0.233, Train Loss:0.4127899408340454\n",
      "Epoch 0[8516/17270] Time:0.231, Train Loss:0.6685088276863098\n",
      "Epoch 0[8517/17270] Time:0.231, Train Loss:1.0777153968811035\n",
      "Epoch 0[8518/17270] Time:0.246, Train Loss:0.7962252497673035\n",
      "Epoch 0[8519/17270] Time:0.231, Train Loss:0.49213311076164246\n",
      "Epoch 0[8520/17270] Time:0.234, Train Loss:0.5762668251991272\n",
      "Epoch 0[8521/17270] Time:0.234, Train Loss:0.42835116386413574\n",
      "Epoch 0[8522/17270] Time:0.225, Train Loss:0.5570380687713623\n",
      "Epoch 0[8523/17270] Time:0.239, Train Loss:0.4638656675815582\n",
      "Epoch 0[8524/17270] Time:0.236, Train Loss:0.9366520047187805\n",
      "Epoch 0[8525/17270] Time:0.224, Train Loss:0.394290566444397\n",
      "Epoch 0[8526/17270] Time:0.24, Train Loss:0.3569028973579407\n",
      "Epoch 0[8527/17270] Time:0.225, Train Loss:0.2339443415403366\n",
      "Epoch 0[8528/17270] Time:0.234, Train Loss:0.51690673828125\n",
      "Epoch 0[8529/17270] Time:0.239, Train Loss:0.46083715558052063\n",
      "Epoch 0[8530/17270] Time:0.24, Train Loss:0.3296077847480774\n",
      "Epoch 0[8531/17270] Time:0.227, Train Loss:1.0236477851867676\n",
      "Epoch 0[8532/17270] Time:0.233, Train Loss:0.4232322573661804\n",
      "Epoch 0[8533/17270] Time:0.238, Train Loss:0.5264236927032471\n",
      "Epoch 0[8534/17270] Time:0.238, Train Loss:0.752875804901123\n",
      "Epoch 0[8535/17270] Time:0.233, Train Loss:1.6078907251358032\n",
      "Epoch 0[8536/17270] Time:0.225, Train Loss:0.8540983200073242\n",
      "Epoch 0[8537/17270] Time:0.229, Train Loss:0.5138442516326904\n",
      "Epoch 0[8538/17270] Time:0.228, Train Loss:0.7305824160575867\n",
      "Epoch 0[8539/17270] Time:0.238, Train Loss:0.4397105574607849\n",
      "Epoch 0[8540/17270] Time:0.239, Train Loss:0.40124985575675964\n",
      "Epoch 0[8541/17270] Time:0.231, Train Loss:0.6724209785461426\n",
      "Epoch 0[8542/17270] Time:0.235, Train Loss:0.393555223941803\n",
      "Epoch 0[8543/17270] Time:0.225, Train Loss:0.568450927734375\n",
      "Epoch 0[8544/17270] Time:0.228, Train Loss:1.1114362478256226\n",
      "Epoch 0[8545/17270] Time:0.236, Train Loss:0.7763517498970032\n",
      "Epoch 0[8546/17270] Time:0.232, Train Loss:0.7900838255882263\n",
      "Epoch 0[8547/17270] Time:0.244, Train Loss:0.34380078315734863\n",
      "Epoch 0[8548/17270] Time:0.228, Train Loss:0.36741799116134644\n",
      "Epoch 0[8549/17270] Time:0.233, Train Loss:0.5775238871574402\n",
      "Epoch 0[8550/17270] Time:0.238, Train Loss:0.7492554187774658\n",
      "Epoch 0[8551/17270] Time:0.238, Train Loss:1.2481712102890015\n",
      "Epoch 0[8552/17270] Time:0.231, Train Loss:0.6856482625007629\n",
      "Epoch 0[8553/17270] Time:0.233, Train Loss:0.43405744433403015\n",
      "Epoch 0[8554/17270] Time:0.233, Train Loss:0.6493135094642639\n",
      "Epoch 0[8555/17270] Time:0.233, Train Loss:0.42077240347862244\n",
      "Epoch 0[8556/17270] Time:0.234, Train Loss:0.5147532820701599\n",
      "Epoch 0[8557/17270] Time:0.238, Train Loss:0.41712674498558044\n",
      "Epoch 0[8558/17270] Time:0.236, Train Loss:0.8174558281898499\n",
      "Epoch 0[8559/17270] Time:0.238, Train Loss:0.5619983673095703\n",
      "Epoch 0[8560/17270] Time:0.238, Train Loss:0.6441025733947754\n",
      "Epoch 0[8561/17270] Time:0.229, Train Loss:0.6200724244117737\n",
      "Epoch 0[8562/17270] Time:0.23, Train Loss:0.38560500741004944\n",
      "Epoch 0[8563/17270] Time:0.228, Train Loss:0.8362576365470886\n",
      "Epoch 0[8564/17270] Time:0.231, Train Loss:0.45088255405426025\n",
      "Epoch 0[8565/17270] Time:0.239, Train Loss:0.9328665137290955\n",
      "Epoch 0[8566/17270] Time:0.236, Train Loss:0.48856231570243835\n",
      "Epoch 0[8567/17270] Time:0.23, Train Loss:0.960348904132843\n",
      "Epoch 0[8568/17270] Time:0.242, Train Loss:0.5954245924949646\n",
      "Epoch 0[8569/17270] Time:0.234, Train Loss:0.5142441987991333\n",
      "Epoch 0[8570/17270] Time:0.247, Train Loss:0.6068935394287109\n",
      "Epoch 0[8571/17270] Time:0.239, Train Loss:0.6239005327224731\n",
      "Epoch 0[8572/17270] Time:0.231, Train Loss:0.9129022359848022\n",
      "Epoch 0[8573/17270] Time:0.243, Train Loss:0.5713015794754028\n",
      "Epoch 0[8574/17270] Time:0.24, Train Loss:0.8444449305534363\n",
      "Epoch 0[8575/17270] Time:0.247, Train Loss:0.46290266513824463\n",
      "Epoch 0[8576/17270] Time:0.24, Train Loss:0.9016952514648438\n",
      "Epoch 0[8577/17270] Time:0.25, Train Loss:0.5335646867752075\n",
      "Epoch 0[8578/17270] Time:0.23, Train Loss:0.37392905354499817\n",
      "Epoch 0[8579/17270] Time:0.234, Train Loss:0.6279406547546387\n",
      "Epoch 0[8580/17270] Time:0.226, Train Loss:0.6011067628860474\n",
      "Epoch 0[8581/17270] Time:0.235, Train Loss:0.5730559825897217\n",
      "Epoch 0[8582/17270] Time:0.236, Train Loss:0.5079430341720581\n",
      "Epoch 0[8583/17270] Time:0.22, Train Loss:0.5908488631248474\n",
      "Epoch 0[8584/17270] Time:0.232, Train Loss:1.1851985454559326\n",
      "Epoch 0[8585/17270] Time:0.23, Train Loss:0.5773336887359619\n",
      "Epoch 0[8586/17270] Time:0.23, Train Loss:0.5425105094909668\n",
      "Epoch 0[8587/17270] Time:0.24, Train Loss:0.6954153776168823\n",
      "Epoch 0[8588/17270] Time:0.249, Train Loss:0.5703322291374207\n",
      "Epoch 0[8589/17270] Time:0.236, Train Loss:0.6386995315551758\n",
      "Epoch 0[8590/17270] Time:0.237, Train Loss:0.47804564237594604\n",
      "Epoch 0[8591/17270] Time:0.233, Train Loss:0.29666340351104736\n",
      "Epoch 0[8592/17270] Time:0.232, Train Loss:0.6079211831092834\n",
      "Epoch 0[8593/17270] Time:0.234, Train Loss:0.5728000998497009\n",
      "Epoch 0[8594/17270] Time:0.225, Train Loss:0.6355494260787964\n",
      "Epoch 0[8595/17270] Time:0.245, Train Loss:2.2996838092803955\n",
      "Epoch 0[8596/17270] Time:0.234, Train Loss:0.4540357291698456\n",
      "Epoch 0[8597/17270] Time:0.225, Train Loss:0.748093843460083\n",
      "Epoch 0[8598/17270] Time:0.245, Train Loss:0.9972918629646301\n",
      "Epoch 0[8599/17270] Time:0.238, Train Loss:0.3709579408168793\n",
      "Epoch 0[8600/17270] Time:0.233, Train Loss:0.7459375858306885\n",
      "Epoch 0[8601/17270] Time:0.232, Train Loss:0.45328715443611145\n",
      "Epoch 0[8602/17270] Time:0.235, Train Loss:0.4782679080963135\n",
      "Epoch 0[8603/17270] Time:0.236, Train Loss:0.5544938445091248\n",
      "Epoch 0[8604/17270] Time:0.236, Train Loss:0.5037115812301636\n",
      "Epoch 0[8605/17270] Time:0.239, Train Loss:0.45526987314224243\n",
      "Epoch 0[8606/17270] Time:0.242, Train Loss:0.6170985102653503\n",
      "Epoch 0[8607/17270] Time:0.224, Train Loss:0.7656644582748413\n",
      "Epoch 0[8608/17270] Time:0.232, Train Loss:0.5398276448249817\n",
      "Epoch 0[8609/17270] Time:0.237, Train Loss:0.6417891383171082\n",
      "Epoch 0[8610/17270] Time:0.245, Train Loss:0.691238284111023\n",
      "Epoch 0[8611/17270] Time:0.241, Train Loss:0.7099153399467468\n",
      "Epoch 0[8612/17270] Time:0.227, Train Loss:0.8978634476661682\n",
      "Epoch 0[8613/17270] Time:0.247, Train Loss:0.404617577791214\n",
      "Epoch 0[8614/17270] Time:0.235, Train Loss:0.40905117988586426\n",
      "Epoch 0[8615/17270] Time:0.231, Train Loss:0.5107912421226501\n",
      "Epoch 0[8616/17270] Time:0.235, Train Loss:0.6053789854049683\n",
      "Epoch 0[8617/17270] Time:0.229, Train Loss:0.550285816192627\n",
      "Epoch 0[8618/17270] Time:0.23, Train Loss:1.0065137147903442\n",
      "Epoch 0[8619/17270] Time:0.235, Train Loss:0.472079336643219\n",
      "Epoch 0[8620/17270] Time:0.238, Train Loss:0.5301360487937927\n",
      "Epoch 0[8621/17270] Time:0.236, Train Loss:0.7587977647781372\n",
      "Epoch 0[8622/17270] Time:0.232, Train Loss:0.5693840384483337\n",
      "Epoch 0[8623/17270] Time:0.228, Train Loss:1.0552363395690918\n",
      "Epoch 0[8624/17270] Time:0.227, Train Loss:0.5016835331916809\n",
      "Epoch 0[8625/17270] Time:0.237, Train Loss:0.5596917867660522\n",
      "Epoch 0[8626/17270] Time:0.236, Train Loss:0.6578516960144043\n",
      "Epoch 0[8627/17270] Time:0.231, Train Loss:0.3889961242675781\n",
      "Epoch 0[8628/17270] Time:0.24, Train Loss:0.4145004153251648\n",
      "Epoch 0[8629/17270] Time:0.255, Train Loss:0.45451804995536804\n",
      "Epoch 0[8630/17270] Time:0.237, Train Loss:0.29031527042388916\n",
      "Epoch 0[8631/17270] Time:0.234, Train Loss:0.43474507331848145\n",
      "Epoch 0[8632/17270] Time:0.227, Train Loss:0.543366014957428\n",
      "Epoch 0[8633/17270] Time:0.243, Train Loss:0.522253155708313\n",
      "Epoch 0[8634/17270] Time:0.225, Train Loss:0.7002736926078796\n",
      "Epoch 0[8635/17270] Time:0.245, Train Loss:0.4761578440666199\n",
      "Epoch 0[8636/17270] Time:0.234, Train Loss:0.4901798963546753\n",
      "Epoch 0[8637/17270] Time:0.233, Train Loss:0.38528868556022644\n",
      "Epoch 0[8638/17270] Time:0.23, Train Loss:0.37983450293540955\n",
      "Epoch 0[8639/17270] Time:0.222, Train Loss:0.3826744556427002\n",
      "Epoch 0[8640/17270] Time:0.227, Train Loss:0.351779967546463\n",
      "Epoch 0[8641/17270] Time:0.249, Train Loss:0.5004770755767822\n",
      "Epoch 0[8642/17270] Time:0.233, Train Loss:0.3296074867248535\n",
      "Epoch 0[8643/17270] Time:0.252, Train Loss:0.731833279132843\n",
      "Epoch 0[8644/17270] Time:0.234, Train Loss:0.8766956329345703\n",
      "Epoch 0[8645/17270] Time:0.237, Train Loss:0.5808546543121338\n",
      "Epoch 0[8646/17270] Time:0.237, Train Loss:0.6365228891372681\n",
      "Epoch 0[8647/17270] Time:0.238, Train Loss:1.2681207656860352\n",
      "Epoch 0[8648/17270] Time:0.236, Train Loss:0.8973662257194519\n",
      "Epoch 0[8649/17270] Time:0.234, Train Loss:0.6327884793281555\n",
      "Epoch 0[8650/17270] Time:0.226, Train Loss:0.8245837688446045\n",
      "Epoch 0[8651/17270] Time:0.244, Train Loss:0.5868105292320251\n",
      "Epoch 0[8652/17270] Time:0.224, Train Loss:0.8762067556381226\n",
      "Epoch 0[8653/17270] Time:0.236, Train Loss:0.4925481975078583\n",
      "Epoch 0[8654/17270] Time:0.239, Train Loss:0.374839723110199\n",
      "Epoch 0[8655/17270] Time:0.23, Train Loss:0.474190890789032\n",
      "Epoch 0[8656/17270] Time:0.238, Train Loss:0.43112778663635254\n",
      "Epoch 0[8657/17270] Time:0.236, Train Loss:0.4972350597381592\n",
      "Epoch 0[8658/17270] Time:0.238, Train Loss:0.3189478814601898\n",
      "Epoch 0[8659/17270] Time:0.236, Train Loss:0.32172027230262756\n",
      "Epoch 0[8660/17270] Time:0.228, Train Loss:0.44227612018585205\n",
      "Epoch 0[8661/17270] Time:0.24, Train Loss:0.6995012760162354\n",
      "Epoch 0[8662/17270] Time:0.237, Train Loss:0.6846048831939697\n",
      "Epoch 0[8663/17270] Time:0.235, Train Loss:0.3777691125869751\n",
      "Epoch 0[8664/17270] Time:0.229, Train Loss:0.39183446764945984\n",
      "Epoch 0[8665/17270] Time:0.226, Train Loss:0.2132076770067215\n",
      "Epoch 0[8666/17270] Time:0.233, Train Loss:0.6029295921325684\n",
      "Epoch 0[8667/17270] Time:0.241, Train Loss:0.8800839185714722\n",
      "Epoch 0[8668/17270] Time:0.229, Train Loss:0.5940858721733093\n",
      "Epoch 0[8669/17270] Time:0.238, Train Loss:0.5339201092720032\n",
      "Epoch 0[8670/17270] Time:0.239, Train Loss:0.6286417245864868\n",
      "Epoch 0[8671/17270] Time:0.242, Train Loss:0.5000506043434143\n",
      "Epoch 0[8672/17270] Time:0.237, Train Loss:0.6957765817642212\n",
      "Epoch 0[8673/17270] Time:0.231, Train Loss:0.7350736260414124\n",
      "Epoch 0[8674/17270] Time:0.262, Train Loss:0.5701939463615417\n",
      "Epoch 0[8675/17270] Time:0.229, Train Loss:0.23153850436210632\n",
      "Epoch 0[8676/17270] Time:0.228, Train Loss:0.7378631830215454\n",
      "Epoch 0[8677/17270] Time:0.232, Train Loss:0.36830654740333557\n",
      "Epoch 0[8678/17270] Time:0.243, Train Loss:0.31314903497695923\n",
      "Epoch 0[8679/17270] Time:0.231, Train Loss:0.6202032566070557\n",
      "Epoch 0[8680/17270] Time:0.232, Train Loss:0.6267930269241333\n",
      "Epoch 0[8681/17270] Time:0.237, Train Loss:0.44047215580940247\n",
      "Epoch 0[8682/17270] Time:0.238, Train Loss:0.4788075387477875\n",
      "Epoch 0[8683/17270] Time:0.232, Train Loss:0.48901262879371643\n",
      "Epoch 0[8684/17270] Time:0.241, Train Loss:0.47315654158592224\n",
      "Epoch 0[8685/17270] Time:0.229, Train Loss:0.3351907432079315\n",
      "Epoch 0[8686/17270] Time:0.234, Train Loss:1.3018442392349243\n",
      "Epoch 0[8687/17270] Time:0.239, Train Loss:0.7549228072166443\n",
      "Epoch 0[8688/17270] Time:0.229, Train Loss:0.3925962448120117\n",
      "Epoch 0[8689/17270] Time:0.232, Train Loss:0.842899739742279\n",
      "Epoch 0[8690/17270] Time:0.227, Train Loss:0.38378405570983887\n",
      "Epoch 0[8691/17270] Time:0.234, Train Loss:0.31822583079338074\n",
      "Epoch 0[8692/17270] Time:0.238, Train Loss:0.6224119663238525\n",
      "Epoch 0[8693/17270] Time:0.231, Train Loss:0.9434394240379333\n",
      "Epoch 0[8694/17270] Time:0.234, Train Loss:0.5168092846870422\n",
      "Epoch 0[8695/17270] Time:0.233, Train Loss:0.684315025806427\n",
      "Epoch 0[8696/17270] Time:0.236, Train Loss:0.4743001461029053\n",
      "Epoch 0[8697/17270] Time:0.229, Train Loss:0.5575660467147827\n",
      "Epoch 0[8698/17270] Time:0.23, Train Loss:0.5019406080245972\n",
      "Epoch 0[8699/17270] Time:0.243, Train Loss:0.4729470908641815\n",
      "Epoch 0[8700/17270] Time:0.232, Train Loss:0.405602365732193\n",
      "Epoch 0[8701/17270] Time:0.232, Train Loss:0.8434321284294128\n",
      "Epoch 0[8702/17270] Time:0.229, Train Loss:0.5149810910224915\n",
      "Epoch 0[8703/17270] Time:0.227, Train Loss:1.1875640153884888\n",
      "Epoch 0[8704/17270] Time:0.235, Train Loss:1.191927194595337\n",
      "Epoch 0[8705/17270] Time:0.227, Train Loss:0.6038497090339661\n",
      "Epoch 0[8706/17270] Time:0.23, Train Loss:1.0539335012435913\n",
      "Epoch 0[8707/17270] Time:0.233, Train Loss:0.5520686507225037\n",
      "Epoch 0[8708/17270] Time:0.219, Train Loss:0.35174453258514404\n",
      "Epoch 0[8709/17270] Time:0.233, Train Loss:0.5829989314079285\n",
      "Epoch 0[8710/17270] Time:0.229, Train Loss:0.2480938583612442\n",
      "Epoch 0[8711/17270] Time:0.239, Train Loss:0.6778017282485962\n",
      "Epoch 0[8712/17270] Time:0.241, Train Loss:0.5169287323951721\n",
      "Epoch 0[8713/17270] Time:0.234, Train Loss:0.44098928570747375\n",
      "Epoch 0[8714/17270] Time:0.22, Train Loss:0.5307767987251282\n",
      "Epoch 0[8715/17270] Time:0.232, Train Loss:0.43682435154914856\n",
      "Epoch 0[8716/17270] Time:0.24, Train Loss:0.44497308135032654\n",
      "Epoch 0[8717/17270] Time:0.248, Train Loss:0.6448102593421936\n",
      "Epoch 0[8718/17270] Time:0.233, Train Loss:0.4233973026275635\n",
      "Epoch 0[8719/17270] Time:0.234, Train Loss:0.753406822681427\n",
      "Epoch 0[8720/17270] Time:0.232, Train Loss:0.6141447424888611\n",
      "Epoch 0[8721/17270] Time:0.237, Train Loss:0.5595080256462097\n",
      "Epoch 0[8722/17270] Time:0.237, Train Loss:0.6844061613082886\n",
      "Epoch 0[8723/17270] Time:0.231, Train Loss:0.2897167503833771\n",
      "Epoch 0[8724/17270] Time:0.245, Train Loss:0.5011800527572632\n",
      "Epoch 0[8725/17270] Time:0.23, Train Loss:0.5117268562316895\n",
      "Epoch 0[8726/17270] Time:0.239, Train Loss:0.3902859091758728\n",
      "Epoch 0[8727/17270] Time:0.222, Train Loss:0.4298554062843323\n",
      "Epoch 0[8728/17270] Time:0.25, Train Loss:0.6022682189941406\n",
      "Epoch 0[8729/17270] Time:0.224, Train Loss:0.5798091292381287\n",
      "Epoch 0[8730/17270] Time:0.246, Train Loss:0.43053901195526123\n",
      "Epoch 0[8731/17270] Time:0.223, Train Loss:0.5205886363983154\n",
      "Epoch 0[8732/17270] Time:0.237, Train Loss:0.5206007361412048\n",
      "Epoch 0[8733/17270] Time:0.233, Train Loss:0.5762938261032104\n",
      "Epoch 0[8734/17270] Time:0.231, Train Loss:0.2817167341709137\n",
      "Epoch 0[8735/17270] Time:0.236, Train Loss:1.45614755153656\n",
      "Epoch 0[8736/17270] Time:0.254, Train Loss:1.0055376291275024\n",
      "Epoch 0[8737/17270] Time:0.23, Train Loss:0.9402188658714294\n",
      "Epoch 0[8738/17270] Time:0.233, Train Loss:0.3435308337211609\n",
      "Epoch 0[8739/17270] Time:0.224, Train Loss:0.7835118770599365\n",
      "Epoch 0[8740/17270] Time:0.245, Train Loss:0.3270207643508911\n",
      "Epoch 0[8741/17270] Time:0.247, Train Loss:0.755871593952179\n",
      "Epoch 0[8742/17270] Time:0.225, Train Loss:0.42832455039024353\n",
      "Epoch 0[8743/17270] Time:0.234, Train Loss:0.5457780361175537\n",
      "Epoch 0[8744/17270] Time:0.238, Train Loss:0.8528385162353516\n",
      "Epoch 0[8745/17270] Time:0.237, Train Loss:0.6612502932548523\n",
      "Epoch 0[8746/17270] Time:0.24, Train Loss:0.72908616065979\n",
      "Epoch 0[8747/17270] Time:0.237, Train Loss:0.4078385531902313\n",
      "Epoch 0[8748/17270] Time:0.231, Train Loss:0.4214685559272766\n",
      "Epoch 0[8749/17270] Time:0.231, Train Loss:0.4514350891113281\n",
      "Epoch 0[8750/17270] Time:0.23, Train Loss:0.6628640294075012\n",
      "Epoch 0[8751/17270] Time:0.236, Train Loss:0.7454662322998047\n",
      "Epoch 0[8752/17270] Time:0.239, Train Loss:0.4642913043498993\n",
      "Epoch 0[8753/17270] Time:0.23, Train Loss:0.8727393746376038\n",
      "Epoch 0[8754/17270] Time:0.227, Train Loss:0.36300745606422424\n",
      "Epoch 0[8755/17270] Time:0.227, Train Loss:0.3652101457118988\n",
      "Epoch 0[8756/17270] Time:0.228, Train Loss:0.5587053298950195\n",
      "Epoch 0[8757/17270] Time:0.231, Train Loss:0.6061953902244568\n",
      "Epoch 0[8758/17270] Time:0.236, Train Loss:0.878207266330719\n",
      "Epoch 0[8759/17270] Time:0.239, Train Loss:0.6544845104217529\n",
      "Epoch 0[8760/17270] Time:0.237, Train Loss:0.5887988209724426\n",
      "Epoch 0[8761/17270] Time:0.23, Train Loss:0.5410490036010742\n",
      "Epoch 0[8762/17270] Time:0.228, Train Loss:1.170981526374817\n",
      "Epoch 0[8763/17270] Time:0.229, Train Loss:1.0151828527450562\n",
      "Epoch 0[8764/17270] Time:0.232, Train Loss:0.42973580956459045\n",
      "Epoch 0[8765/17270] Time:0.23, Train Loss:0.3231047987937927\n",
      "Epoch 0[8766/17270] Time:0.237, Train Loss:0.6142185926437378\n",
      "Epoch 0[8767/17270] Time:0.231, Train Loss:1.053611159324646\n",
      "Epoch 0[8768/17270] Time:0.233, Train Loss:1.5107883214950562\n",
      "Epoch 0[8769/17270] Time:0.237, Train Loss:1.0677225589752197\n",
      "Epoch 0[8770/17270] Time:0.236, Train Loss:0.5343031883239746\n",
      "Epoch 0[8771/17270] Time:0.234, Train Loss:0.5078014731407166\n",
      "Epoch 0[8772/17270] Time:0.239, Train Loss:0.6080132126808167\n",
      "Epoch 0[8773/17270] Time:0.239, Train Loss:0.8370484709739685\n",
      "Epoch 0[8774/17270] Time:0.236, Train Loss:0.7012112736701965\n",
      "Epoch 0[8775/17270] Time:0.241, Train Loss:0.6733070015907288\n",
      "Epoch 0[8776/17270] Time:0.227, Train Loss:0.37134140729904175\n",
      "Epoch 0[8777/17270] Time:0.233, Train Loss:0.6018825173377991\n",
      "Epoch 0[8778/17270] Time:0.25, Train Loss:0.6122241020202637\n",
      "Epoch 0[8779/17270] Time:0.23, Train Loss:0.6895737648010254\n",
      "Epoch 0[8780/17270] Time:0.232, Train Loss:0.49534597992897034\n",
      "Epoch 0[8781/17270] Time:0.233, Train Loss:0.743557870388031\n",
      "Epoch 0[8782/17270] Time:0.233, Train Loss:0.5680116415023804\n",
      "Epoch 0[8783/17270] Time:0.245, Train Loss:0.4987078011035919\n",
      "Epoch 0[8784/17270] Time:0.232, Train Loss:0.5579010844230652\n",
      "Epoch 0[8785/17270] Time:0.23, Train Loss:0.38986659049987793\n",
      "Epoch 0[8786/17270] Time:0.237, Train Loss:0.5886057615280151\n",
      "Epoch 0[8787/17270] Time:0.235, Train Loss:0.5243884325027466\n",
      "Epoch 0[8788/17270] Time:0.239, Train Loss:0.6495867371559143\n",
      "Epoch 0[8789/17270] Time:0.23, Train Loss:0.7820568084716797\n",
      "Epoch 0[8790/17270] Time:0.226, Train Loss:0.40198197960853577\n",
      "Epoch 0[8791/17270] Time:0.228, Train Loss:0.6222636103630066\n",
      "Epoch 0[8792/17270] Time:0.237, Train Loss:0.6785683035850525\n",
      "Epoch 0[8793/17270] Time:0.229, Train Loss:0.7480660080909729\n",
      "Epoch 0[8794/17270] Time:0.233, Train Loss:0.8168632984161377\n",
      "Epoch 0[8795/17270] Time:0.255, Train Loss:0.3487251102924347\n",
      "Epoch 0[8796/17270] Time:0.247, Train Loss:0.3201639950275421\n",
      "Epoch 0[8797/17270] Time:0.225, Train Loss:0.6257990598678589\n",
      "Epoch 0[8798/17270] Time:0.251, Train Loss:0.5589984655380249\n",
      "Epoch 0[8799/17270] Time:0.229, Train Loss:0.5347596406936646\n",
      "Epoch 0[8800/17270] Time:0.233, Train Loss:0.4064016342163086\n",
      "Epoch 0[8801/17270] Time:0.246, Train Loss:0.3161565065383911\n",
      "Epoch 0[8802/17270] Time:0.237, Train Loss:0.4316076636314392\n",
      "Epoch 0[8803/17270] Time:0.247, Train Loss:0.3566235601902008\n",
      "Epoch 0[8804/17270] Time:0.238, Train Loss:0.3815975487232208\n",
      "Epoch 0[8805/17270] Time:0.245, Train Loss:0.36725980043411255\n",
      "Epoch 0[8806/17270] Time:0.24, Train Loss:0.5749660730361938\n",
      "Epoch 0[8807/17270] Time:0.224, Train Loss:0.4277719557285309\n",
      "Epoch 0[8808/17270] Time:0.246, Train Loss:0.7731560468673706\n",
      "Epoch 0[8809/17270] Time:0.245, Train Loss:0.8835725784301758\n",
      "Epoch 0[8810/17270] Time:0.233, Train Loss:0.2849384546279907\n",
      "Epoch 0[8811/17270] Time:0.229, Train Loss:0.7122393846511841\n",
      "Epoch 0[8812/17270] Time:0.241, Train Loss:0.7294022440910339\n",
      "Epoch 0[8813/17270] Time:0.238, Train Loss:0.7775529623031616\n",
      "Epoch 0[8814/17270] Time:0.236, Train Loss:0.21567271649837494\n",
      "Epoch 0[8815/17270] Time:0.239, Train Loss:0.2318866103887558\n",
      "Epoch 0[8816/17270] Time:0.241, Train Loss:0.43614593148231506\n",
      "Epoch 0[8817/17270] Time:0.238, Train Loss:0.6603811383247375\n",
      "Epoch 0[8818/17270] Time:0.243, Train Loss:0.2852720022201538\n",
      "Epoch 0[8819/17270] Time:0.229, Train Loss:0.4852375090122223\n",
      "Epoch 0[8820/17270] Time:0.237, Train Loss:0.42219483852386475\n",
      "Epoch 0[8821/17270] Time:0.232, Train Loss:0.27706363797187805\n",
      "Epoch 0[8822/17270] Time:0.233, Train Loss:0.9753803610801697\n",
      "Epoch 0[8823/17270] Time:0.242, Train Loss:0.53937166929245\n",
      "Epoch 0[8824/17270] Time:0.246, Train Loss:0.5708263516426086\n",
      "Epoch 0[8825/17270] Time:0.241, Train Loss:0.40576431155204773\n",
      "Epoch 0[8826/17270] Time:0.236, Train Loss:0.4322735667228699\n",
      "Epoch 0[8827/17270] Time:0.231, Train Loss:0.5567916035652161\n",
      "Epoch 0[8828/17270] Time:0.233, Train Loss:0.846716046333313\n",
      "Epoch 0[8829/17270] Time:0.229, Train Loss:1.013550877571106\n",
      "Epoch 0[8830/17270] Time:0.25, Train Loss:0.650915265083313\n",
      "Epoch 0[8831/17270] Time:0.231, Train Loss:0.703011691570282\n",
      "Epoch 0[8832/17270] Time:0.233, Train Loss:0.547145664691925\n",
      "Epoch 0[8833/17270] Time:0.249, Train Loss:0.7010819911956787\n",
      "Epoch 0[8834/17270] Time:0.225, Train Loss:1.8204506635665894\n",
      "Epoch 0[8835/17270] Time:0.243, Train Loss:0.6591891050338745\n",
      "Epoch 0[8836/17270] Time:0.239, Train Loss:0.5288860201835632\n",
      "Epoch 0[8837/17270] Time:0.233, Train Loss:0.3050430715084076\n",
      "Epoch 0[8838/17270] Time:0.233, Train Loss:1.2185031175613403\n",
      "Epoch 0[8839/17270] Time:0.237, Train Loss:0.6390544176101685\n",
      "Epoch 0[8840/17270] Time:0.23, Train Loss:0.576183021068573\n",
      "Epoch 0[8841/17270] Time:0.234, Train Loss:0.6769633889198303\n",
      "Epoch 0[8842/17270] Time:0.232, Train Loss:1.1256823539733887\n",
      "Epoch 0[8843/17270] Time:0.235, Train Loss:0.7853876948356628\n",
      "Epoch 0[8844/17270] Time:0.237, Train Loss:0.8329760432243347\n",
      "Epoch 0[8845/17270] Time:0.238, Train Loss:0.3231979310512543\n",
      "Epoch 0[8846/17270] Time:0.222, Train Loss:0.5182231068611145\n",
      "Epoch 0[8847/17270] Time:0.249, Train Loss:0.48621314764022827\n",
      "Epoch 0[8848/17270] Time:0.226, Train Loss:0.5789076685905457\n",
      "Epoch 0[8849/17270] Time:0.225, Train Loss:0.9644110798835754\n",
      "Epoch 0[8850/17270] Time:0.238, Train Loss:0.564663290977478\n",
      "Epoch 0[8851/17270] Time:0.229, Train Loss:0.558464527130127\n",
      "Epoch 0[8852/17270] Time:0.233, Train Loss:0.5880662202835083\n",
      "Epoch 0[8853/17270] Time:0.24, Train Loss:0.39863908290863037\n",
      "Epoch 0[8854/17270] Time:0.239, Train Loss:0.7653749585151672\n",
      "Epoch 0[8855/17270] Time:0.245, Train Loss:0.44027179479599\n",
      "Epoch 0[8856/17270] Time:0.234, Train Loss:0.6668314337730408\n",
      "Epoch 0[8857/17270] Time:0.232, Train Loss:1.1142888069152832\n",
      "Epoch 0[8858/17270] Time:0.237, Train Loss:0.42512038350105286\n",
      "Epoch 0[8859/17270] Time:0.24, Train Loss:0.948824942111969\n",
      "Epoch 0[8860/17270] Time:0.233, Train Loss:0.6687880158424377\n",
      "Epoch 0[8861/17270] Time:0.237, Train Loss:0.6156144738197327\n",
      "Epoch 0[8862/17270] Time:0.237, Train Loss:0.5240746736526489\n",
      "Epoch 0[8863/17270] Time:0.239, Train Loss:0.5737683176994324\n",
      "Epoch 0[8864/17270] Time:0.229, Train Loss:0.5335158705711365\n",
      "Epoch 0[8865/17270] Time:0.235, Train Loss:1.1599200963974\n",
      "Epoch 0[8866/17270] Time:0.232, Train Loss:1.5476799011230469\n",
      "Epoch 0[8867/17270] Time:0.225, Train Loss:0.34927454590797424\n",
      "Epoch 0[8868/17270] Time:0.246, Train Loss:0.481254518032074\n",
      "Epoch 0[8869/17270] Time:0.239, Train Loss:1.1830602884292603\n",
      "Epoch 0[8870/17270] Time:0.238, Train Loss:0.5156068205833435\n",
      "Epoch 0[8871/17270] Time:0.234, Train Loss:0.8518706560134888\n",
      "Epoch 0[8872/17270] Time:0.234, Train Loss:0.6709584593772888\n",
      "Epoch 0[8873/17270] Time:0.224, Train Loss:0.8588311672210693\n",
      "Epoch 0[8874/17270] Time:0.237, Train Loss:0.5627021193504333\n",
      "Epoch 0[8875/17270] Time:0.231, Train Loss:0.6189172863960266\n",
      "Epoch 0[8876/17270] Time:0.224, Train Loss:0.6320023536682129\n",
      "Epoch 0[8877/17270] Time:0.245, Train Loss:0.4563181400299072\n",
      "Epoch 0[8878/17270] Time:0.231, Train Loss:0.6174261569976807\n",
      "Epoch 0[8879/17270] Time:0.231, Train Loss:0.46890974044799805\n",
      "Epoch 0[8880/17270] Time:0.237, Train Loss:0.5966472029685974\n",
      "Epoch 0[8881/17270] Time:0.23, Train Loss:0.6480215191841125\n",
      "Epoch 0[8882/17270] Time:0.23, Train Loss:0.5571083426475525\n",
      "Epoch 0[8883/17270] Time:0.231, Train Loss:0.8028733730316162\n",
      "Epoch 0[8884/17270] Time:0.23, Train Loss:0.5312268733978271\n",
      "Epoch 0[8885/17270] Time:0.236, Train Loss:0.6956871151924133\n",
      "Epoch 0[8886/17270] Time:0.231, Train Loss:0.7789161205291748\n",
      "Epoch 0[8887/17270] Time:0.229, Train Loss:0.6100701093673706\n",
      "Epoch 0[8888/17270] Time:0.232, Train Loss:0.5143207311630249\n",
      "Epoch 0[8889/17270] Time:0.23, Train Loss:0.7713863849639893\n",
      "Epoch 0[8890/17270] Time:0.237, Train Loss:0.5255354642868042\n",
      "Epoch 0[8891/17270] Time:0.231, Train Loss:0.7308753728866577\n",
      "Epoch 0[8892/17270] Time:0.23, Train Loss:0.37912890315055847\n",
      "Epoch 0[8893/17270] Time:0.233, Train Loss:0.47632336616516113\n",
      "Epoch 0[8894/17270] Time:0.227, Train Loss:0.6817052364349365\n",
      "Epoch 0[8895/17270] Time:0.234, Train Loss:0.39162859320640564\n",
      "Epoch 0[8896/17270] Time:0.237, Train Loss:0.7727242708206177\n",
      "Epoch 0[8897/17270] Time:0.238, Train Loss:0.2854450047016144\n",
      "Epoch 0[8898/17270] Time:0.226, Train Loss:0.547711968421936\n",
      "Epoch 0[8899/17270] Time:0.229, Train Loss:0.9606989622116089\n",
      "Epoch 0[8900/17270] Time:0.237, Train Loss:0.5911871790885925\n",
      "Epoch 0[8901/17270] Time:0.25, Train Loss:1.3821476697921753\n",
      "Epoch 0[8902/17270] Time:0.221, Train Loss:0.37372395396232605\n",
      "Epoch 0[8903/17270] Time:0.24, Train Loss:0.35157275199890137\n",
      "Epoch 0[8904/17270] Time:0.232, Train Loss:0.46392130851745605\n",
      "Epoch 0[8905/17270] Time:0.234, Train Loss:0.46171140670776367\n",
      "Epoch 0[8906/17270] Time:0.224, Train Loss:0.31430700421333313\n",
      "Epoch 0[8907/17270] Time:0.244, Train Loss:0.8057680726051331\n",
      "Epoch 0[8908/17270] Time:0.241, Train Loss:0.6251542568206787\n",
      "Epoch 0[8909/17270] Time:0.227, Train Loss:0.4309900104999542\n",
      "Epoch 0[8910/17270] Time:0.237, Train Loss:0.2967309355735779\n",
      "Epoch 0[8911/17270] Time:0.233, Train Loss:0.40887710452079773\n",
      "Epoch 0[8912/17270] Time:0.237, Train Loss:0.7219812870025635\n",
      "Epoch 0[8913/17270] Time:0.219, Train Loss:0.47286123037338257\n",
      "Epoch 0[8914/17270] Time:0.23, Train Loss:0.33855047821998596\n",
      "Epoch 0[8915/17270] Time:0.234, Train Loss:0.5168055295944214\n",
      "Epoch 0[8916/17270] Time:0.229, Train Loss:0.48663944005966187\n",
      "Epoch 0[8917/17270] Time:0.239, Train Loss:0.5686584115028381\n",
      "Epoch 0[8918/17270] Time:0.238, Train Loss:1.121369481086731\n",
      "Epoch 0[8919/17270] Time:0.235, Train Loss:0.4564906060695648\n",
      "Epoch 0[8920/17270] Time:0.237, Train Loss:0.5617278218269348\n",
      "Epoch 0[8921/17270] Time:0.236, Train Loss:0.5251842141151428\n",
      "Epoch 0[8922/17270] Time:0.233, Train Loss:0.6942471265792847\n",
      "Epoch 0[8923/17270] Time:0.229, Train Loss:0.39771807193756104\n",
      "Epoch 0[8924/17270] Time:0.233, Train Loss:0.5072179436683655\n",
      "Epoch 0[8925/17270] Time:0.223, Train Loss:1.0460848808288574\n",
      "Epoch 0[8926/17270] Time:0.243, Train Loss:0.5307356715202332\n",
      "Epoch 0[8927/17270] Time:0.239, Train Loss:0.7574072480201721\n",
      "Epoch 0[8928/17270] Time:0.221, Train Loss:0.3590121269226074\n",
      "Epoch 0[8929/17270] Time:0.239, Train Loss:0.36526161432266235\n",
      "Epoch 0[8930/17270] Time:0.249, Train Loss:0.5347424149513245\n",
      "Epoch 0[8931/17270] Time:0.235, Train Loss:0.5889109969139099\n",
      "Epoch 0[8932/17270] Time:0.222, Train Loss:0.5281606912612915\n",
      "Epoch 0[8933/17270] Time:0.226, Train Loss:0.7880762219429016\n",
      "Epoch 0[8934/17270] Time:0.23, Train Loss:1.0117779970169067\n",
      "Epoch 0[8935/17270] Time:0.23, Train Loss:0.5575231313705444\n",
      "Epoch 0[8936/17270] Time:0.23, Train Loss:0.35719406604766846\n",
      "Epoch 0[8937/17270] Time:0.244, Train Loss:0.48983582854270935\n",
      "Epoch 0[8938/17270] Time:0.226, Train Loss:0.8280717134475708\n",
      "Epoch 0[8939/17270] Time:0.251, Train Loss:0.45351213216781616\n",
      "Epoch 0[8940/17270] Time:0.238, Train Loss:0.6380554437637329\n",
      "Epoch 0[8941/17270] Time:0.245, Train Loss:0.5281685590744019\n",
      "Epoch 0[8942/17270] Time:0.244, Train Loss:0.5341899394989014\n",
      "Epoch 0[8943/17270] Time:0.246, Train Loss:0.6275625228881836\n",
      "Epoch 0[8944/17270] Time:0.237, Train Loss:0.7632520198822021\n",
      "Epoch 0[8945/17270] Time:0.238, Train Loss:0.41252991557121277\n",
      "Epoch 0[8946/17270] Time:0.224, Train Loss:0.6692399382591248\n",
      "Epoch 0[8947/17270] Time:0.247, Train Loss:0.559283435344696\n",
      "Epoch 0[8948/17270] Time:0.236, Train Loss:0.5159158110618591\n",
      "Epoch 0[8949/17270] Time:0.241, Train Loss:0.910680890083313\n",
      "Epoch 0[8950/17270] Time:0.237, Train Loss:0.5152271389961243\n",
      "Epoch 0[8951/17270] Time:0.228, Train Loss:0.8877871632575989\n",
      "Epoch 0[8952/17270] Time:0.235, Train Loss:0.6923460960388184\n",
      "Epoch 0[8953/17270] Time:0.23, Train Loss:0.3593754768371582\n",
      "Epoch 0[8954/17270] Time:0.238, Train Loss:0.5307396650314331\n",
      "Epoch 0[8955/17270] Time:0.241, Train Loss:0.29166555404663086\n",
      "Epoch 0[8956/17270] Time:0.235, Train Loss:0.5757901072502136\n",
      "Epoch 0[8957/17270] Time:0.223, Train Loss:0.38274067640304565\n",
      "Epoch 0[8958/17270] Time:0.24, Train Loss:0.49128738045692444\n",
      "Epoch 0[8959/17270] Time:0.238, Train Loss:0.6911147236824036\n",
      "Epoch 0[8960/17270] Time:0.224, Train Loss:0.7999457120895386\n",
      "Epoch 0[8961/17270] Time:0.234, Train Loss:0.5024120807647705\n",
      "Epoch 0[8962/17270] Time:0.232, Train Loss:0.26415929198265076\n",
      "Epoch 0[8963/17270] Time:0.233, Train Loss:0.5482836365699768\n",
      "Epoch 0[8964/17270] Time:0.229, Train Loss:0.8412322402000427\n",
      "Epoch 0[8965/17270] Time:0.231, Train Loss:1.050953984260559\n",
      "Epoch 0[8966/17270] Time:0.234, Train Loss:0.38935422897338867\n",
      "Epoch 0[8967/17270] Time:0.244, Train Loss:0.5157610774040222\n",
      "Epoch 0[8968/17270] Time:0.22, Train Loss:0.4514631927013397\n",
      "Epoch 0[8969/17270] Time:0.235, Train Loss:0.35976067185401917\n",
      "Epoch 0[8970/17270] Time:0.232, Train Loss:0.5406680703163147\n",
      "Epoch 0[8971/17270] Time:0.244, Train Loss:0.7577264308929443\n",
      "Epoch 0[8972/17270] Time:0.243, Train Loss:0.9659170508384705\n",
      "Epoch 0[8973/17270] Time:0.226, Train Loss:0.5157027244567871\n",
      "Epoch 0[8974/17270] Time:0.242, Train Loss:0.41825631260871887\n",
      "Epoch 0[8975/17270] Time:0.235, Train Loss:0.370967298746109\n",
      "Epoch 0[8976/17270] Time:0.244, Train Loss:0.3967463970184326\n",
      "Epoch 0[8977/17270] Time:0.223, Train Loss:0.809548020362854\n",
      "Epoch 0[8978/17270] Time:0.243, Train Loss:0.5994639992713928\n",
      "Epoch 0[8979/17270] Time:0.225, Train Loss:0.5065140128135681\n",
      "Epoch 0[8980/17270] Time:0.238, Train Loss:0.5739956498146057\n",
      "Epoch 0[8981/17270] Time:0.239, Train Loss:0.7779176831245422\n",
      "Epoch 0[8982/17270] Time:0.248, Train Loss:0.3664667308330536\n",
      "Epoch 0[8983/17270] Time:0.237, Train Loss:0.7012900710105896\n",
      "Epoch 0[8984/17270] Time:0.236, Train Loss:0.47022444009780884\n",
      "Epoch 0[8985/17270] Time:0.228, Train Loss:0.4583781957626343\n",
      "Epoch 0[8986/17270] Time:0.226, Train Loss:0.6806130409240723\n",
      "Epoch 0[8987/17270] Time:0.237, Train Loss:0.5437447428703308\n",
      "Epoch 0[8988/17270] Time:0.234, Train Loss:0.37963131070137024\n",
      "Epoch 0[8989/17270] Time:0.232, Train Loss:0.5553620457649231\n",
      "Epoch 0[8990/17270] Time:0.238, Train Loss:0.481949120759964\n",
      "Epoch 0[8991/17270] Time:0.231, Train Loss:0.4157790541648865\n",
      "Epoch 0[8992/17270] Time:0.23, Train Loss:0.5587723255157471\n",
      "Epoch 0[8993/17270] Time:0.247, Train Loss:0.4451260566711426\n",
      "Epoch 0[8994/17270] Time:0.238, Train Loss:0.46485984325408936\n",
      "Epoch 0[8995/17270] Time:0.236, Train Loss:0.8483248949050903\n",
      "Epoch 0[8996/17270] Time:0.237, Train Loss:0.30279740691185\n",
      "Epoch 0[8997/17270] Time:0.238, Train Loss:0.4067983329296112\n",
      "Epoch 0[8998/17270] Time:0.236, Train Loss:0.3653895854949951\n",
      "Epoch 0[8999/17270] Time:0.231, Train Loss:0.6598272323608398\n",
      "Epoch 0[9000/17270] Time:0.233, Train Loss:0.6973815560340881\n",
      "Epoch 0[9001/17270] Time:0.235, Train Loss:0.5340335369110107\n",
      "Epoch 0[9002/17270] Time:0.237, Train Loss:0.47895944118499756\n",
      "Epoch 0[9003/17270] Time:0.228, Train Loss:0.6991981267929077\n",
      "Epoch 0[9004/17270] Time:0.239, Train Loss:0.6720482110977173\n",
      "Epoch 0[9005/17270] Time:0.232, Train Loss:0.6909804940223694\n",
      "Epoch 0[9006/17270] Time:0.236, Train Loss:0.3543458580970764\n",
      "Epoch 0[9007/17270] Time:0.251, Train Loss:0.4711245000362396\n",
      "Epoch 0[9008/17270] Time:0.235, Train Loss:0.8532752990722656\n",
      "Epoch 0[9009/17270] Time:0.218, Train Loss:0.7052736282348633\n",
      "Epoch 0[9010/17270] Time:0.227, Train Loss:0.7970113754272461\n",
      "Epoch 0[9011/17270] Time:0.233, Train Loss:0.5673964619636536\n",
      "Epoch 0[9012/17270] Time:0.231, Train Loss:0.5717774033546448\n",
      "Epoch 0[9013/17270] Time:0.234, Train Loss:0.5589364767074585\n",
      "Epoch 0[9014/17270] Time:0.239, Train Loss:0.6876934170722961\n",
      "Epoch 0[9015/17270] Time:0.228, Train Loss:0.596973717212677\n",
      "Epoch 0[9016/17270] Time:0.229, Train Loss:0.47522515058517456\n",
      "Epoch 0[9017/17270] Time:0.24, Train Loss:0.5221503973007202\n",
      "Epoch 0[9018/17270] Time:0.226, Train Loss:0.5269886255264282\n",
      "Epoch 0[9019/17270] Time:0.24, Train Loss:0.36332449316978455\n",
      "Epoch 0[9020/17270] Time:0.229, Train Loss:0.6776252388954163\n",
      "Epoch 0[9021/17270] Time:0.237, Train Loss:0.3900149464607239\n",
      "Epoch 0[9022/17270] Time:0.231, Train Loss:0.6525075435638428\n",
      "Epoch 0[9023/17270] Time:0.229, Train Loss:0.3389691710472107\n",
      "Epoch 0[9024/17270] Time:0.229, Train Loss:0.6299886703491211\n",
      "Epoch 0[9025/17270] Time:0.23, Train Loss:0.5139318704605103\n",
      "Epoch 0[9026/17270] Time:0.239, Train Loss:0.6143033504486084\n",
      "Epoch 0[9027/17270] Time:0.228, Train Loss:0.5756544470787048\n",
      "Epoch 0[9028/17270] Time:0.233, Train Loss:1.2446119785308838\n",
      "Epoch 0[9029/17270] Time:0.231, Train Loss:0.8098697662353516\n",
      "Epoch 0[9030/17270] Time:0.235, Train Loss:0.4604858160018921\n",
      "Epoch 0[9031/17270] Time:0.239, Train Loss:0.563938558101654\n",
      "Epoch 0[9032/17270] Time:0.224, Train Loss:0.886845052242279\n",
      "Epoch 0[9033/17270] Time:0.235, Train Loss:0.474694162607193\n",
      "Epoch 0[9034/17270] Time:0.243, Train Loss:0.4775625169277191\n",
      "Epoch 0[9035/17270] Time:0.233, Train Loss:0.557024359703064\n",
      "Epoch 0[9036/17270] Time:0.223, Train Loss:0.500646710395813\n",
      "Epoch 0[9037/17270] Time:0.245, Train Loss:0.3938301205635071\n",
      "Epoch 0[9038/17270] Time:0.237, Train Loss:0.604960560798645\n",
      "Epoch 0[9039/17270] Time:0.226, Train Loss:0.981600821018219\n",
      "Epoch 0[9040/17270] Time:0.234, Train Loss:0.3528329133987427\n",
      "Epoch 0[9041/17270] Time:0.243, Train Loss:0.35431182384490967\n",
      "Epoch 0[9042/17270] Time:0.228, Train Loss:0.3698715567588806\n",
      "Epoch 0[9043/17270] Time:0.237, Train Loss:1.4303079843521118\n",
      "Epoch 0[9044/17270] Time:0.238, Train Loss:0.7493129372596741\n",
      "Epoch 0[9045/17270] Time:0.23, Train Loss:0.5864283442497253\n",
      "Epoch 0[9046/17270] Time:0.237, Train Loss:0.724626898765564\n",
      "Epoch 0[9047/17270] Time:0.237, Train Loss:0.7216207385063171\n",
      "Epoch 0[9048/17270] Time:0.237, Train Loss:0.4487605690956116\n",
      "Epoch 0[9049/17270] Time:0.231, Train Loss:0.6379541754722595\n",
      "Epoch 0[9050/17270] Time:0.235, Train Loss:0.4635857045650482\n",
      "Epoch 0[9051/17270] Time:0.238, Train Loss:0.6253972053527832\n",
      "Epoch 0[9052/17270] Time:0.235, Train Loss:0.45855361223220825\n",
      "Epoch 0[9053/17270] Time:0.239, Train Loss:0.4476509392261505\n",
      "Epoch 0[9054/17270] Time:0.237, Train Loss:0.39430028200149536\n",
      "Epoch 0[9055/17270] Time:0.233, Train Loss:0.5136343240737915\n",
      "Epoch 0[9056/17270] Time:0.239, Train Loss:0.6110048294067383\n",
      "Epoch 0[9057/17270] Time:0.237, Train Loss:0.3636404275894165\n",
      "Epoch 0[9058/17270] Time:0.237, Train Loss:0.2853928208351135\n",
      "Epoch 0[9059/17270] Time:0.239, Train Loss:0.4718870222568512\n",
      "Epoch 0[9060/17270] Time:0.234, Train Loss:0.4398458003997803\n",
      "Epoch 0[9061/17270] Time:0.238, Train Loss:0.3864821493625641\n",
      "Epoch 0[9062/17270] Time:0.253, Train Loss:0.6235430836677551\n",
      "Epoch 0[9063/17270] Time:0.235, Train Loss:0.683142900466919\n",
      "Epoch 0[9064/17270] Time:0.253, Train Loss:1.2105321884155273\n",
      "Epoch 0[9065/17270] Time:0.228, Train Loss:0.5101781487464905\n",
      "Epoch 0[9066/17270] Time:0.23, Train Loss:0.7706485986709595\n",
      "Epoch 0[9067/17270] Time:0.241, Train Loss:0.5464546084403992\n",
      "Epoch 0[9068/17270] Time:0.246, Train Loss:0.33019399642944336\n",
      "Epoch 0[9069/17270] Time:0.233, Train Loss:0.41135793924331665\n",
      "Epoch 0[9070/17270] Time:0.241, Train Loss:0.5718036890029907\n",
      "Epoch 0[9071/17270] Time:0.246, Train Loss:0.5743655562400818\n",
      "Epoch 0[9072/17270] Time:0.24, Train Loss:0.7748680114746094\n",
      "Epoch 0[9073/17270] Time:0.241, Train Loss:0.32572099566459656\n",
      "Epoch 0[9074/17270] Time:0.243, Train Loss:0.44581007957458496\n",
      "Epoch 0[9075/17270] Time:0.233, Train Loss:0.9401929378509521\n",
      "Epoch 0[9076/17270] Time:0.238, Train Loss:0.5963453054428101\n",
      "Epoch 0[9077/17270] Time:0.239, Train Loss:0.26292288303375244\n",
      "Epoch 0[9078/17270] Time:0.231, Train Loss:0.7944460511207581\n",
      "Epoch 0[9079/17270] Time:0.229, Train Loss:0.6106643080711365\n",
      "Epoch 0[9080/17270] Time:0.232, Train Loss:0.5061653852462769\n",
      "Epoch 0[9081/17270] Time:0.231, Train Loss:0.33130717277526855\n",
      "Epoch 0[9082/17270] Time:0.231, Train Loss:0.5957291126251221\n",
      "Epoch 0[9083/17270] Time:0.245, Train Loss:0.4441154897212982\n",
      "Epoch 0[9084/17270] Time:0.239, Train Loss:0.48147666454315186\n",
      "Epoch 0[9085/17270] Time:0.225, Train Loss:0.5393845438957214\n",
      "Epoch 0[9086/17270] Time:0.247, Train Loss:0.7061260342597961\n",
      "Epoch 0[9087/17270] Time:0.244, Train Loss:0.6354007720947266\n",
      "Epoch 0[9088/17270] Time:0.224, Train Loss:0.24951669573783875\n",
      "Epoch 0[9089/17270] Time:0.244, Train Loss:0.9332708120346069\n",
      "Epoch 0[9090/17270] Time:0.238, Train Loss:1.3166767358779907\n",
      "Epoch 0[9091/17270] Time:0.235, Train Loss:0.8490673899650574\n",
      "Epoch 0[9092/17270] Time:0.247, Train Loss:0.7447909712791443\n",
      "Epoch 0[9093/17270] Time:0.234, Train Loss:0.3398565649986267\n",
      "Epoch 0[9094/17270] Time:0.249, Train Loss:0.3184758424758911\n",
      "Epoch 0[9095/17270] Time:0.227, Train Loss:0.3840298652648926\n",
      "Epoch 0[9096/17270] Time:0.229, Train Loss:0.9537590146064758\n",
      "Epoch 0[9097/17270] Time:0.244, Train Loss:0.4768286347389221\n",
      "Epoch 0[9098/17270] Time:0.232, Train Loss:0.6048757433891296\n",
      "Epoch 0[9099/17270] Time:0.232, Train Loss:0.5000059604644775\n",
      "Epoch 0[9100/17270] Time:0.226, Train Loss:0.6850658059120178\n",
      "Epoch 0[9101/17270] Time:0.237, Train Loss:0.6686341762542725\n",
      "Epoch 0[9102/17270] Time:0.231, Train Loss:0.3150504529476166\n",
      "Epoch 0[9103/17270] Time:0.239, Train Loss:0.5009358525276184\n",
      "Epoch 0[9104/17270] Time:0.232, Train Loss:0.6460296511650085\n",
      "Epoch 0[9105/17270] Time:0.234, Train Loss:0.570942223072052\n",
      "Epoch 0[9106/17270] Time:0.237, Train Loss:0.2765142321586609\n",
      "Epoch 0[9107/17270] Time:0.229, Train Loss:0.5184965133666992\n",
      "Epoch 0[9108/17270] Time:0.234, Train Loss:0.35098952054977417\n",
      "Epoch 0[9109/17270] Time:0.233, Train Loss:0.45385488867759705\n",
      "Epoch 0[9110/17270] Time:0.228, Train Loss:0.4353320002555847\n",
      "Epoch 0[9111/17270] Time:0.233, Train Loss:0.6953164935112\n",
      "Epoch 0[9112/17270] Time:0.226, Train Loss:0.40874040126800537\n",
      "Epoch 0[9113/17270] Time:0.23, Train Loss:0.3744243085384369\n",
      "Epoch 0[9114/17270] Time:0.225, Train Loss:0.4928256571292877\n",
      "Epoch 0[9115/17270] Time:0.237, Train Loss:0.3033079504966736\n",
      "Epoch 0[9116/17270] Time:0.238, Train Loss:0.6371667981147766\n",
      "Epoch 0[9117/17270] Time:0.237, Train Loss:0.9941275119781494\n",
      "Epoch 0[9118/17270] Time:0.242, Train Loss:0.4511924684047699\n",
      "Epoch 0[9119/17270] Time:0.232, Train Loss:0.43890589475631714\n",
      "Epoch 0[9120/17270] Time:0.233, Train Loss:0.33696165680885315\n",
      "Epoch 0[9121/17270] Time:0.23, Train Loss:0.7589262127876282\n",
      "Epoch 0[9122/17270] Time:0.234, Train Loss:0.7278599739074707\n",
      "Epoch 0[9123/17270] Time:0.228, Train Loss:0.5766128301620483\n",
      "Epoch 0[9124/17270] Time:0.234, Train Loss:0.7072293162345886\n",
      "Epoch 0[9125/17270] Time:0.246, Train Loss:0.49456965923309326\n",
      "Epoch 0[9126/17270] Time:0.244, Train Loss:0.6950843930244446\n",
      "Epoch 0[9127/17270] Time:0.216, Train Loss:0.45071858167648315\n",
      "Epoch 0[9128/17270] Time:0.244, Train Loss:0.5023856163024902\n",
      "Epoch 0[9129/17270] Time:0.247, Train Loss:0.5160866379737854\n",
      "Epoch 0[9130/17270] Time:0.224, Train Loss:0.5906437039375305\n",
      "Epoch 0[9131/17270] Time:0.244, Train Loss:0.4269798994064331\n",
      "Epoch 0[9132/17270] Time:0.242, Train Loss:0.6802277565002441\n",
      "Epoch 0[9133/17270] Time:0.23, Train Loss:0.7163713574409485\n",
      "Epoch 0[9134/17270] Time:0.235, Train Loss:0.633438229560852\n",
      "Epoch 0[9135/17270] Time:0.244, Train Loss:1.0757660865783691\n",
      "Epoch 0[9136/17270] Time:0.236, Train Loss:0.7222915291786194\n",
      "Epoch 0[9137/17270] Time:0.242, Train Loss:0.8636513352394104\n",
      "Epoch 0[9138/17270] Time:0.227, Train Loss:0.47266656160354614\n",
      "Epoch 0[9139/17270] Time:0.234, Train Loss:0.8067992329597473\n",
      "Epoch 0[9140/17270] Time:0.244, Train Loss:0.48116186261177063\n",
      "Epoch 0[9141/17270] Time:0.235, Train Loss:0.3354143500328064\n",
      "Epoch 0[9142/17270] Time:0.24, Train Loss:0.8243978023529053\n",
      "Epoch 0[9143/17270] Time:0.231, Train Loss:0.7060942649841309\n",
      "Epoch 0[9144/17270] Time:0.236, Train Loss:0.41742318868637085\n",
      "Epoch 0[9145/17270] Time:0.233, Train Loss:0.6040767431259155\n",
      "Epoch 0[9146/17270] Time:0.238, Train Loss:0.7754666209220886\n",
      "Epoch 0[9147/17270] Time:0.235, Train Loss:0.35950276255607605\n",
      "Epoch 0[9148/17270] Time:0.232, Train Loss:0.4148215353488922\n",
      "Epoch 0[9149/17270] Time:0.237, Train Loss:0.6898760795593262\n",
      "Epoch 0[9150/17270] Time:0.235, Train Loss:0.49284300208091736\n",
      "Epoch 0[9151/17270] Time:0.232, Train Loss:0.4496895670890808\n",
      "Epoch 0[9152/17270] Time:0.233, Train Loss:0.4610131084918976\n",
      "Epoch 0[9153/17270] Time:0.232, Train Loss:0.6768613457679749\n",
      "Epoch 0[9154/17270] Time:0.224, Train Loss:0.3434373438358307\n",
      "Epoch 0[9155/17270] Time:0.242, Train Loss:0.6132891774177551\n",
      "Epoch 0[9156/17270] Time:0.243, Train Loss:1.0102087259292603\n",
      "Epoch 0[9157/17270] Time:0.235, Train Loss:0.7994644641876221\n",
      "Epoch 0[9158/17270] Time:0.245, Train Loss:0.6519657373428345\n",
      "Epoch 0[9159/17270] Time:0.248, Train Loss:0.5815941691398621\n",
      "Epoch 0[9160/17270] Time:0.235, Train Loss:0.30715227127075195\n",
      "Epoch 0[9161/17270] Time:0.223, Train Loss:0.3254092037677765\n",
      "Epoch 0[9162/17270] Time:0.241, Train Loss:0.9627631902694702\n",
      "Epoch 0[9163/17270] Time:0.238, Train Loss:0.4871062636375427\n",
      "Epoch 0[9164/17270] Time:0.236, Train Loss:0.5052697062492371\n",
      "Epoch 0[9165/17270] Time:0.234, Train Loss:0.6161186695098877\n",
      "Epoch 0[9166/17270] Time:0.223, Train Loss:0.5773267149925232\n",
      "Epoch 0[9167/17270] Time:0.23, Train Loss:0.7414416670799255\n",
      "Epoch 0[9168/17270] Time:0.225, Train Loss:0.4624893367290497\n",
      "Epoch 0[9169/17270] Time:0.24, Train Loss:1.0119353532791138\n",
      "Epoch 0[9170/17270] Time:0.241, Train Loss:0.6098845601081848\n",
      "Epoch 0[9171/17270] Time:0.239, Train Loss:0.5666350722312927\n",
      "Epoch 0[9172/17270] Time:0.224, Train Loss:0.7088220715522766\n",
      "Epoch 0[9173/17270] Time:0.247, Train Loss:0.34029725193977356\n",
      "Epoch 0[9174/17270] Time:0.23, Train Loss:1.339613676071167\n",
      "Epoch 0[9175/17270] Time:0.225, Train Loss:0.5311681032180786\n",
      "Epoch 0[9176/17270] Time:0.229, Train Loss:0.41223475337028503\n",
      "Epoch 0[9177/17270] Time:0.243, Train Loss:0.7188260555267334\n",
      "Epoch 0[9178/17270] Time:0.224, Train Loss:0.567945122718811\n",
      "Epoch 0[9179/17270] Time:0.242, Train Loss:0.4079609513282776\n",
      "Epoch 0[9180/17270] Time:0.228, Train Loss:0.45925432443618774\n",
      "Epoch 0[9181/17270] Time:0.239, Train Loss:0.6012958288192749\n",
      "Epoch 0[9182/17270] Time:0.232, Train Loss:0.4271077513694763\n",
      "Epoch 0[9183/17270] Time:0.237, Train Loss:0.9218564033508301\n",
      "Epoch 0[9184/17270] Time:0.239, Train Loss:0.4817255139350891\n",
      "Epoch 0[9185/17270] Time:0.222, Train Loss:0.5997833609580994\n",
      "Epoch 0[9186/17270] Time:0.231, Train Loss:0.8698169589042664\n",
      "Epoch 0[9187/17270] Time:0.229, Train Loss:0.3979722559452057\n",
      "Epoch 0[9188/17270] Time:0.233, Train Loss:0.44484850764274597\n",
      "Epoch 0[9189/17270] Time:0.228, Train Loss:0.5942369103431702\n",
      "Epoch 0[9190/17270] Time:0.229, Train Loss:0.4277264475822449\n",
      "Epoch 0[9191/17270] Time:0.231, Train Loss:0.3472716808319092\n",
      "Epoch 0[9192/17270] Time:0.238, Train Loss:0.7380048632621765\n",
      "Epoch 0[9193/17270] Time:0.239, Train Loss:0.44060277938842773\n",
      "Epoch 0[9194/17270] Time:0.231, Train Loss:0.4349193274974823\n",
      "Epoch 0[9195/17270] Time:0.23, Train Loss:0.508173942565918\n",
      "Epoch 0[9196/17270] Time:0.232, Train Loss:0.7535168528556824\n",
      "Epoch 0[9197/17270] Time:0.237, Train Loss:0.46616941690444946\n",
      "Epoch 0[9198/17270] Time:0.228, Train Loss:0.3997271955013275\n",
      "Epoch 0[9199/17270] Time:0.229, Train Loss:0.6226087808609009\n",
      "Epoch 0[9200/17270] Time:0.233, Train Loss:0.8231499195098877\n",
      "Epoch 0[9201/17270] Time:0.236, Train Loss:0.5654471516609192\n",
      "Epoch 0[9202/17270] Time:0.239, Train Loss:0.6243943572044373\n",
      "Epoch 0[9203/17270] Time:0.236, Train Loss:0.5692074298858643\n",
      "Epoch 0[9204/17270] Time:0.231, Train Loss:1.1016755104064941\n",
      "Epoch 0[9205/17270] Time:0.243, Train Loss:0.9784995913505554\n",
      "Epoch 0[9206/17270] Time:0.233, Train Loss:0.5517311692237854\n",
      "Epoch 0[9207/17270] Time:0.222, Train Loss:0.40207037329673767\n",
      "Epoch 0[9208/17270] Time:0.251, Train Loss:0.4520140290260315\n",
      "Epoch 0[9209/17270] Time:0.24, Train Loss:0.40013179183006287\n",
      "Epoch 0[9210/17270] Time:0.236, Train Loss:0.3876344561576843\n",
      "Epoch 0[9211/17270] Time:0.235, Train Loss:0.4280981421470642\n",
      "Epoch 0[9212/17270] Time:0.251, Train Loss:0.5884304642677307\n",
      "Epoch 0[9213/17270] Time:0.228, Train Loss:0.2926582396030426\n",
      "Epoch 0[9214/17270] Time:0.247, Train Loss:0.5231318473815918\n",
      "Epoch 0[9215/17270] Time:0.241, Train Loss:0.6129254102706909\n",
      "Epoch 0[9216/17270] Time:0.231, Train Loss:1.800956130027771\n",
      "Epoch 0[9217/17270] Time:0.226, Train Loss:0.3589088022708893\n",
      "Epoch 0[9218/17270] Time:0.236, Train Loss:0.4222383201122284\n",
      "Epoch 0[9219/17270] Time:0.229, Train Loss:0.6231998801231384\n",
      "Epoch 0[9220/17270] Time:0.248, Train Loss:0.6837323307991028\n",
      "Epoch 0[9221/17270] Time:0.237, Train Loss:0.3785637617111206\n",
      "Epoch 0[9222/17270] Time:0.233, Train Loss:0.6846428513526917\n",
      "Epoch 0[9223/17270] Time:0.238, Train Loss:0.43513739109039307\n",
      "Epoch 0[9224/17270] Time:0.231, Train Loss:0.7910346984863281\n",
      "Epoch 0[9225/17270] Time:0.254, Train Loss:0.5944965481758118\n",
      "Epoch 0[9226/17270] Time:0.233, Train Loss:0.7363792657852173\n",
      "Epoch 0[9227/17270] Time:0.226, Train Loss:0.7096860408782959\n",
      "Epoch 0[9228/17270] Time:0.233, Train Loss:0.4164041578769684\n",
      "Epoch 0[9229/17270] Time:0.229, Train Loss:0.5928671956062317\n",
      "Epoch 0[9230/17270] Time:0.233, Train Loss:0.4454788565635681\n",
      "Epoch 0[9231/17270] Time:0.237, Train Loss:0.5301555395126343\n",
      "Epoch 0[9232/17270] Time:0.238, Train Loss:1.40184485912323\n",
      "Epoch 0[9233/17270] Time:0.233, Train Loss:0.4842667579650879\n",
      "Epoch 0[9234/17270] Time:0.239, Train Loss:0.5151888728141785\n",
      "Epoch 0[9235/17270] Time:0.232, Train Loss:1.0018254518508911\n",
      "Epoch 0[9236/17270] Time:0.238, Train Loss:0.7702241539955139\n",
      "Epoch 0[9237/17270] Time:0.231, Train Loss:0.5051441192626953\n",
      "Epoch 0[9238/17270] Time:0.241, Train Loss:0.5505300164222717\n",
      "Epoch 0[9239/17270] Time:0.243, Train Loss:0.5836916565895081\n",
      "Epoch 0[9240/17270] Time:0.236, Train Loss:0.6943962574005127\n",
      "Epoch 0[9241/17270] Time:0.237, Train Loss:0.4487714171409607\n",
      "Epoch 0[9242/17270] Time:0.238, Train Loss:0.6108698844909668\n",
      "Epoch 0[9243/17270] Time:0.235, Train Loss:0.4973471164703369\n",
      "Epoch 0[9244/17270] Time:0.237, Train Loss:0.21835212409496307\n",
      "Epoch 0[9245/17270] Time:0.235, Train Loss:0.8117087483406067\n",
      "Epoch 0[9246/17270] Time:0.237, Train Loss:0.3217066526412964\n",
      "Epoch 0[9247/17270] Time:0.238, Train Loss:0.44341766834259033\n",
      "Epoch 0[9248/17270] Time:0.242, Train Loss:0.40514910221099854\n",
      "Epoch 0[9249/17270] Time:0.239, Train Loss:0.5794259309768677\n",
      "Epoch 0[9250/17270] Time:0.24, Train Loss:0.785103440284729\n",
      "Epoch 0[9251/17270] Time:0.232, Train Loss:0.6496753096580505\n",
      "Epoch 0[9252/17270] Time:0.236, Train Loss:0.5813166499137878\n",
      "Epoch 0[9253/17270] Time:0.219, Train Loss:0.2981831133365631\n",
      "Epoch 0[9254/17270] Time:0.228, Train Loss:0.7752290964126587\n",
      "Epoch 0[9255/17270] Time:0.229, Train Loss:0.5684114098548889\n",
      "Epoch 0[9256/17270] Time:0.238, Train Loss:0.49031591415405273\n",
      "Epoch 0[9257/17270] Time:0.232, Train Loss:0.5558823347091675\n",
      "Epoch 0[9258/17270] Time:0.231, Train Loss:0.5514419078826904\n",
      "Epoch 0[9259/17270] Time:0.232, Train Loss:0.3351583778858185\n",
      "Epoch 0[9260/17270] Time:0.236, Train Loss:0.8408063650131226\n",
      "Epoch 0[9261/17270] Time:0.236, Train Loss:0.72806715965271\n",
      "Epoch 0[9262/17270] Time:0.238, Train Loss:1.2583191394805908\n",
      "Epoch 0[9263/17270] Time:0.233, Train Loss:1.381140112876892\n",
      "Epoch 0[9264/17270] Time:0.234, Train Loss:0.6562049984931946\n",
      "Epoch 0[9265/17270] Time:0.239, Train Loss:0.5167537331581116\n",
      "Epoch 0[9266/17270] Time:0.241, Train Loss:0.7285000085830688\n",
      "Epoch 0[9267/17270] Time:0.238, Train Loss:0.5699722170829773\n",
      "Epoch 0[9268/17270] Time:0.23, Train Loss:0.561651885509491\n",
      "Epoch 0[9269/17270] Time:0.233, Train Loss:0.8282163739204407\n",
      "Epoch 0[9270/17270] Time:0.248, Train Loss:0.660784900188446\n",
      "Epoch 0[9271/17270] Time:0.232, Train Loss:0.4164871871471405\n",
      "Epoch 0[9272/17270] Time:0.226, Train Loss:0.6277197003364563\n",
      "Epoch 0[9273/17270] Time:0.241, Train Loss:0.5365062952041626\n",
      "Epoch 0[9274/17270] Time:0.244, Train Loss:0.4423501789569855\n",
      "Epoch 0[9275/17270] Time:0.227, Train Loss:0.7040040493011475\n",
      "Epoch 0[9276/17270] Time:0.238, Train Loss:0.7395376563072205\n",
      "Epoch 0[9277/17270] Time:0.234, Train Loss:0.5441178679466248\n",
      "Epoch 0[9278/17270] Time:0.222, Train Loss:0.5877479314804077\n",
      "Epoch 0[9279/17270] Time:0.246, Train Loss:0.5545604825019836\n",
      "Epoch 0[9280/17270] Time:0.233, Train Loss:0.42168158292770386\n",
      "Epoch 0[9281/17270] Time:0.228, Train Loss:0.37384700775146484\n",
      "Epoch 0[9282/17270] Time:0.225, Train Loss:0.8663856983184814\n",
      "Epoch 0[9283/17270] Time:0.243, Train Loss:1.3285462856292725\n",
      "Epoch 0[9284/17270] Time:0.233, Train Loss:0.9108057618141174\n",
      "Epoch 0[9285/17270] Time:0.232, Train Loss:0.4598197937011719\n",
      "Epoch 0[9286/17270] Time:0.236, Train Loss:0.5075953006744385\n",
      "Epoch 0[9287/17270] Time:0.233, Train Loss:1.0745646953582764\n",
      "Epoch 0[9288/17270] Time:0.232, Train Loss:0.5053649544715881\n",
      "Epoch 0[9289/17270] Time:0.234, Train Loss:0.5051511526107788\n",
      "Epoch 0[9290/17270] Time:0.238, Train Loss:0.5026657581329346\n",
      "Epoch 0[9291/17270] Time:0.258, Train Loss:0.41874468326568604\n",
      "Epoch 0[9292/17270] Time:0.227, Train Loss:0.40500155091285706\n",
      "Epoch 0[9293/17270] Time:0.229, Train Loss:0.5353652834892273\n",
      "Epoch 0[9294/17270] Time:0.237, Train Loss:0.48442429304122925\n",
      "Epoch 0[9295/17270] Time:0.256, Train Loss:0.5302904844284058\n",
      "Epoch 0[9296/17270] Time:0.233, Train Loss:0.7450959086418152\n",
      "Epoch 0[9297/17270] Time:0.227, Train Loss:0.811530351638794\n",
      "Epoch 0[9298/17270] Time:0.222, Train Loss:0.6864744424819946\n",
      "Epoch 0[9299/17270] Time:0.229, Train Loss:0.6113602519035339\n",
      "Epoch 0[9300/17270] Time:0.23, Train Loss:0.6473953127861023\n",
      "Epoch 0[9301/17270] Time:0.232, Train Loss:0.5677270889282227\n",
      "Epoch 0[9302/17270] Time:0.225, Train Loss:0.4605134427547455\n",
      "Epoch 0[9303/17270] Time:0.23, Train Loss:0.8198453187942505\n",
      "Epoch 0[9304/17270] Time:0.227, Train Loss:0.4182610511779785\n",
      "Epoch 0[9305/17270] Time:0.229, Train Loss:0.6393872499465942\n",
      "Epoch 0[9306/17270] Time:0.239, Train Loss:0.7329267263412476\n",
      "Epoch 0[9307/17270] Time:0.26, Train Loss:0.39595022797584534\n",
      "Epoch 0[9308/17270] Time:0.234, Train Loss:0.37190717458724976\n",
      "Epoch 0[9309/17270] Time:0.237, Train Loss:0.6317825317382812\n",
      "Epoch 0[9310/17270] Time:0.232, Train Loss:0.389102041721344\n",
      "Epoch 0[9311/17270] Time:0.237, Train Loss:0.41008272767066956\n",
      "Epoch 0[9312/17270] Time:0.236, Train Loss:0.46468833088874817\n",
      "Epoch 0[9313/17270] Time:0.236, Train Loss:0.44103938341140747\n",
      "Epoch 0[9314/17270] Time:0.233, Train Loss:0.3648909628391266\n",
      "Epoch 0[9315/17270] Time:0.235, Train Loss:0.6728087067604065\n",
      "Epoch 0[9316/17270] Time:0.236, Train Loss:0.4974769651889801\n",
      "Epoch 0[9317/17270] Time:0.228, Train Loss:0.46010854840278625\n",
      "Epoch 0[9318/17270] Time:0.232, Train Loss:0.4412417411804199\n",
      "Epoch 0[9319/17270] Time:0.226, Train Loss:0.6180153489112854\n",
      "Epoch 0[9320/17270] Time:0.231, Train Loss:0.6742882132530212\n",
      "Epoch 0[9321/17270] Time:0.223, Train Loss:0.7182833552360535\n",
      "Epoch 0[9322/17270] Time:0.237, Train Loss:1.6945068836212158\n",
      "Epoch 0[9323/17270] Time:0.238, Train Loss:0.7136115431785583\n",
      "Epoch 0[9324/17270] Time:0.236, Train Loss:0.41594821214675903\n",
      "Epoch 0[9325/17270] Time:0.236, Train Loss:0.5264554619789124\n",
      "Epoch 0[9326/17270] Time:0.25, Train Loss:0.6735774278640747\n",
      "Epoch 0[9327/17270] Time:0.225, Train Loss:0.5680118799209595\n",
      "Epoch 0[9328/17270] Time:0.232, Train Loss:0.6942789554595947\n",
      "Epoch 0[9329/17270] Time:0.228, Train Loss:0.6509945392608643\n",
      "Epoch 0[9330/17270] Time:0.239, Train Loss:0.42468729615211487\n",
      "Epoch 0[9331/17270] Time:0.239, Train Loss:0.47118422389030457\n",
      "Epoch 0[9332/17270] Time:0.235, Train Loss:0.4154059588909149\n",
      "Epoch 0[9333/17270] Time:0.239, Train Loss:0.8662897348403931\n",
      "Epoch 0[9334/17270] Time:0.231, Train Loss:0.906889021396637\n",
      "Epoch 0[9335/17270] Time:0.231, Train Loss:0.5240816473960876\n",
      "Epoch 0[9336/17270] Time:0.23, Train Loss:0.41794222593307495\n",
      "Epoch 0[9337/17270] Time:0.232, Train Loss:0.31341153383255005\n",
      "Epoch 0[9338/17270] Time:0.234, Train Loss:0.6132592558860779\n",
      "Epoch 0[9339/17270] Time:0.234, Train Loss:0.7710925340652466\n",
      "Epoch 0[9340/17270] Time:0.252, Train Loss:0.48808327317237854\n",
      "Epoch 0[9341/17270] Time:0.233, Train Loss:0.4611615836620331\n",
      "Epoch 0[9342/17270] Time:0.228, Train Loss:0.9729127883911133\n",
      "Epoch 0[9343/17270] Time:0.238, Train Loss:0.7072414755821228\n",
      "Epoch 0[9344/17270] Time:0.235, Train Loss:0.6295106410980225\n",
      "Epoch 0[9345/17270] Time:0.228, Train Loss:1.0039044618606567\n",
      "Epoch 0[9346/17270] Time:0.233, Train Loss:0.5640618205070496\n",
      "Epoch 0[9347/17270] Time:0.24, Train Loss:0.5624535083770752\n",
      "Epoch 0[9348/17270] Time:0.239, Train Loss:0.8962849974632263\n",
      "Epoch 0[9349/17270] Time:0.237, Train Loss:0.3846149444580078\n",
      "Epoch 0[9350/17270] Time:0.226, Train Loss:0.27732235193252563\n",
      "Epoch 0[9351/17270] Time:0.238, Train Loss:0.5081560611724854\n",
      "Epoch 0[9352/17270] Time:0.23, Train Loss:0.4991222321987152\n",
      "Epoch 0[9353/17270] Time:0.238, Train Loss:0.6050931215286255\n",
      "Epoch 0[9354/17270] Time:0.228, Train Loss:1.0915427207946777\n",
      "Epoch 0[9355/17270] Time:0.237, Train Loss:0.426403671503067\n",
      "Epoch 0[9356/17270] Time:0.239, Train Loss:0.4723959267139435\n",
      "Epoch 0[9357/17270] Time:0.235, Train Loss:0.4362848103046417\n",
      "Epoch 0[9358/17270] Time:0.238, Train Loss:0.49483174085617065\n",
      "Epoch 0[9359/17270] Time:0.237, Train Loss:0.5652973651885986\n",
      "Epoch 0[9360/17270] Time:0.236, Train Loss:0.4562373161315918\n",
      "Epoch 0[9361/17270] Time:0.234, Train Loss:0.4218926429748535\n",
      "Epoch 0[9362/17270] Time:0.223, Train Loss:0.4796255826950073\n",
      "Epoch 0[9363/17270] Time:0.237, Train Loss:0.5490636825561523\n",
      "Epoch 0[9364/17270] Time:0.23, Train Loss:0.5907166004180908\n",
      "Epoch 0[9365/17270] Time:0.244, Train Loss:0.6189257502555847\n",
      "Epoch 0[9366/17270] Time:0.234, Train Loss:0.7603318691253662\n",
      "Epoch 0[9367/17270] Time:0.231, Train Loss:0.3746250867843628\n",
      "Epoch 0[9368/17270] Time:0.233, Train Loss:0.7093342542648315\n",
      "Epoch 0[9369/17270] Time:0.231, Train Loss:0.6284627318382263\n",
      "Epoch 0[9370/17270] Time:0.237, Train Loss:0.37800970673561096\n",
      "Epoch 0[9371/17270] Time:0.232, Train Loss:1.016535997390747\n",
      "Epoch 0[9372/17270] Time:0.229, Train Loss:0.24617217481136322\n",
      "Epoch 0[9373/17270] Time:0.227, Train Loss:0.4957720935344696\n",
      "Epoch 0[9374/17270] Time:0.232, Train Loss:0.617801308631897\n",
      "Epoch 0[9375/17270] Time:0.236, Train Loss:0.4882858991622925\n",
      "Epoch 0[9376/17270] Time:0.239, Train Loss:0.5860249996185303\n",
      "Epoch 0[9377/17270] Time:0.237, Train Loss:0.425751268863678\n",
      "Epoch 0[9378/17270] Time:0.234, Train Loss:0.6795496940612793\n",
      "Epoch 0[9379/17270] Time:0.23, Train Loss:1.1716570854187012\n",
      "Epoch 0[9380/17270] Time:0.236, Train Loss:0.8621875047683716\n",
      "Epoch 0[9381/17270] Time:0.233, Train Loss:0.4497276842594147\n",
      "Epoch 0[9382/17270] Time:0.222, Train Loss:1.0010788440704346\n",
      "Epoch 0[9383/17270] Time:0.238, Train Loss:0.9211786985397339\n",
      "Epoch 0[9384/17270] Time:0.238, Train Loss:0.41621285676956177\n",
      "Epoch 0[9385/17270] Time:0.236, Train Loss:0.5547979474067688\n",
      "Epoch 0[9386/17270] Time:0.223, Train Loss:0.7589450478553772\n",
      "Epoch 0[9387/17270] Time:0.246, Train Loss:0.5093114972114563\n",
      "Epoch 0[9388/17270] Time:0.248, Train Loss:0.4947596788406372\n",
      "Epoch 0[9389/17270] Time:0.219, Train Loss:0.7348998188972473\n",
      "Epoch 0[9390/17270] Time:0.232, Train Loss:1.10853910446167\n",
      "Epoch 0[9391/17270] Time:0.232, Train Loss:0.6853553652763367\n",
      "Epoch 0[9392/17270] Time:0.231, Train Loss:0.5863071084022522\n",
      "Epoch 0[9393/17270] Time:0.233, Train Loss:0.40007540583610535\n",
      "Epoch 0[9394/17270] Time:0.235, Train Loss:0.48774516582489014\n",
      "Epoch 0[9395/17270] Time:0.231, Train Loss:0.4792745113372803\n",
      "Epoch 0[9396/17270] Time:0.236, Train Loss:0.6104936003684998\n",
      "Epoch 0[9397/17270] Time:0.237, Train Loss:0.4262699484825134\n",
      "Epoch 0[9398/17270] Time:0.232, Train Loss:0.6443046927452087\n",
      "Epoch 0[9399/17270] Time:0.228, Train Loss:0.6471800208091736\n",
      "Epoch 0[9400/17270] Time:0.243, Train Loss:0.47881364822387695\n",
      "Epoch 0[9401/17270] Time:0.228, Train Loss:0.5285069942474365\n",
      "Epoch 0[9402/17270] Time:0.23, Train Loss:0.41647639870643616\n",
      "Epoch 0[9403/17270] Time:0.251, Train Loss:0.802246630191803\n",
      "Epoch 0[9404/17270] Time:0.225, Train Loss:0.3337244987487793\n",
      "Epoch 0[9405/17270] Time:0.231, Train Loss:0.732383131980896\n",
      "Epoch 0[9406/17270] Time:0.247, Train Loss:0.5482832789421082\n",
      "Epoch 0[9407/17270] Time:0.23, Train Loss:0.4049452841281891\n",
      "Epoch 0[9408/17270] Time:0.23, Train Loss:0.4801902770996094\n",
      "Epoch 0[9409/17270] Time:0.231, Train Loss:0.7598361372947693\n",
      "Epoch 0[9410/17270] Time:0.235, Train Loss:0.7567662000656128\n",
      "Epoch 0[9411/17270] Time:0.236, Train Loss:0.5512304306030273\n",
      "Epoch 0[9412/17270] Time:0.228, Train Loss:0.6202432513237\n",
      "Epoch 0[9413/17270] Time:0.233, Train Loss:0.5818440318107605\n",
      "Epoch 0[9414/17270] Time:0.237, Train Loss:0.5184492468833923\n",
      "Epoch 0[9415/17270] Time:0.234, Train Loss:0.32454389333724976\n",
      "Epoch 0[9416/17270] Time:0.233, Train Loss:0.42164096236228943\n",
      "Epoch 0[9417/17270] Time:0.234, Train Loss:0.36464738845825195\n",
      "Epoch 0[9418/17270] Time:0.237, Train Loss:0.3157181143760681\n",
      "Epoch 0[9419/17270] Time:0.227, Train Loss:0.35535871982574463\n",
      "Epoch 0[9420/17270] Time:0.239, Train Loss:0.920365571975708\n",
      "Epoch 0[9421/17270] Time:0.236, Train Loss:0.31025344133377075\n",
      "Epoch 0[9422/17270] Time:0.237, Train Loss:0.3762019872665405\n",
      "Epoch 0[9423/17270] Time:0.235, Train Loss:0.6117202043533325\n",
      "Epoch 0[9424/17270] Time:0.235, Train Loss:1.322036623954773\n",
      "Epoch 0[9425/17270] Time:0.23, Train Loss:1.820997953414917\n",
      "Epoch 0[9426/17270] Time:0.239, Train Loss:0.3458613455295563\n",
      "Epoch 0[9427/17270] Time:0.238, Train Loss:1.1154484748840332\n",
      "Epoch 0[9428/17270] Time:0.228, Train Loss:0.35108351707458496\n",
      "Epoch 0[9429/17270] Time:0.232, Train Loss:0.76890629529953\n",
      "Epoch 0[9430/17270] Time:0.238, Train Loss:0.589276134967804\n",
      "Epoch 0[9431/17270] Time:0.229, Train Loss:0.7714908719062805\n",
      "Epoch 0[9432/17270] Time:0.237, Train Loss:0.5480809807777405\n",
      "Epoch 0[9433/17270] Time:0.23, Train Loss:0.5040282607078552\n",
      "Epoch 0[9434/17270] Time:0.242, Train Loss:0.40160316228866577\n",
      "Epoch 0[9435/17270] Time:0.223, Train Loss:0.3998676836490631\n",
      "Epoch 0[9436/17270] Time:0.249, Train Loss:0.5149856209754944\n",
      "Epoch 0[9437/17270] Time:0.241, Train Loss:0.7133299708366394\n",
      "Epoch 0[9438/17270] Time:0.236, Train Loss:0.6587535738945007\n",
      "Epoch 0[9439/17270] Time:0.235, Train Loss:0.4806692898273468\n",
      "Epoch 0[9440/17270] Time:0.236, Train Loss:0.6339462995529175\n",
      "Epoch 0[9441/17270] Time:0.242, Train Loss:0.7530809640884399\n",
      "Epoch 0[9442/17270] Time:0.239, Train Loss:0.5935145616531372\n",
      "Epoch 0[9443/17270] Time:0.245, Train Loss:0.4499557316303253\n",
      "Epoch 0[9444/17270] Time:0.236, Train Loss:0.5241972804069519\n",
      "Epoch 0[9445/17270] Time:0.233, Train Loss:0.5403260588645935\n",
      "Epoch 0[9446/17270] Time:0.234, Train Loss:0.29014238715171814\n",
      "Epoch 0[9447/17270] Time:0.237, Train Loss:0.382625550031662\n",
      "Epoch 0[9448/17270] Time:0.228, Train Loss:0.5892912149429321\n",
      "Epoch 0[9449/17270] Time:0.227, Train Loss:0.5392501950263977\n",
      "Epoch 0[9450/17270] Time:0.231, Train Loss:0.39308488368988037\n",
      "Epoch 0[9451/17270] Time:0.231, Train Loss:0.31264492869377136\n",
      "Epoch 0[9452/17270] Time:0.228, Train Loss:0.6108076572418213\n",
      "Epoch 0[9453/17270] Time:0.227, Train Loss:0.7307818531990051\n",
      "Epoch 0[9454/17270] Time:0.229, Train Loss:0.6231361031532288\n",
      "Epoch 0[9455/17270] Time:0.23, Train Loss:0.5419268012046814\n",
      "Epoch 0[9456/17270] Time:0.227, Train Loss:0.5559784173965454\n",
      "Epoch 0[9457/17270] Time:0.237, Train Loss:0.9220390319824219\n",
      "Epoch 0[9458/17270] Time:0.238, Train Loss:0.43570712208747864\n",
      "Epoch 0[9459/17270] Time:0.231, Train Loss:0.6357972025871277\n",
      "Epoch 0[9460/17270] Time:0.237, Train Loss:0.7902379035949707\n",
      "Epoch 0[9461/17270] Time:0.237, Train Loss:0.4830378293991089\n",
      "Epoch 0[9462/17270] Time:0.228, Train Loss:0.5133082270622253\n",
      "Epoch 0[9463/17270] Time:0.227, Train Loss:0.5022141933441162\n",
      "Epoch 0[9464/17270] Time:0.23, Train Loss:0.6272534728050232\n",
      "Epoch 0[9465/17270] Time:0.23, Train Loss:0.7455665469169617\n",
      "Epoch 0[9466/17270] Time:0.232, Train Loss:0.5403164625167847\n",
      "Epoch 0[9467/17270] Time:0.238, Train Loss:0.4718783497810364\n",
      "Epoch 0[9468/17270] Time:0.23, Train Loss:0.4343271255493164\n",
      "Epoch 0[9469/17270] Time:0.236, Train Loss:0.6099647879600525\n",
      "Epoch 0[9470/17270] Time:0.237, Train Loss:0.31310537457466125\n",
      "Epoch 0[9471/17270] Time:0.238, Train Loss:0.7560991048812866\n",
      "Epoch 0[9472/17270] Time:0.241, Train Loss:0.34763213992118835\n",
      "Epoch 0[9473/17270] Time:0.225, Train Loss:0.3043057322502136\n",
      "Epoch 0[9474/17270] Time:0.232, Train Loss:0.3880317509174347\n",
      "Epoch 0[9475/17270] Time:0.238, Train Loss:0.5864390134811401\n",
      "Epoch 0[9476/17270] Time:0.236, Train Loss:1.824022650718689\n",
      "Epoch 0[9477/17270] Time:0.23, Train Loss:0.4909122586250305\n",
      "Epoch 0[9478/17270] Time:0.23, Train Loss:0.44633448123931885\n",
      "Epoch 0[9479/17270] Time:0.241, Train Loss:0.5308653712272644\n",
      "Epoch 0[9480/17270] Time:0.228, Train Loss:0.3326572775840759\n",
      "Epoch 0[9481/17270] Time:0.228, Train Loss:0.4912840723991394\n",
      "Epoch 0[9482/17270] Time:0.238, Train Loss:0.6624352931976318\n",
      "Epoch 0[9483/17270] Time:0.236, Train Loss:0.9191564321517944\n",
      "Epoch 0[9484/17270] Time:0.237, Train Loss:1.2325587272644043\n",
      "Epoch 0[9485/17270] Time:0.228, Train Loss:0.8696348071098328\n",
      "Epoch 0[9486/17270] Time:0.237, Train Loss:0.35518500208854675\n",
      "Epoch 0[9487/17270] Time:0.245, Train Loss:0.6123123168945312\n",
      "Epoch 0[9488/17270] Time:0.236, Train Loss:1.1287243366241455\n",
      "Epoch 0[9489/17270] Time:0.242, Train Loss:0.6356180906295776\n",
      "Epoch 0[9490/17270] Time:0.233, Train Loss:0.43344646692276\n",
      "Epoch 0[9491/17270] Time:0.247, Train Loss:0.4766926169395447\n",
      "Epoch 0[9492/17270] Time:0.227, Train Loss:1.4372854232788086\n",
      "Epoch 0[9493/17270] Time:0.233, Train Loss:0.5412372946739197\n",
      "Epoch 0[9494/17270] Time:0.235, Train Loss:0.46922722458839417\n",
      "Epoch 0[9495/17270] Time:0.239, Train Loss:0.6723968982696533\n",
      "Epoch 0[9496/17270] Time:0.238, Train Loss:0.8025153279304504\n",
      "Epoch 0[9497/17270] Time:0.228, Train Loss:0.406306654214859\n",
      "Epoch 0[9498/17270] Time:0.224, Train Loss:0.4753100574016571\n",
      "Epoch 0[9499/17270] Time:0.25, Train Loss:0.7821574211120605\n",
      "Epoch 0[9500/17270] Time:0.234, Train Loss:0.34018176794052124\n",
      "Epoch 0[9501/17270] Time:0.242, Train Loss:0.5855883359909058\n",
      "Epoch 0[9502/17270] Time:0.226, Train Loss:0.30597928166389465\n",
      "Epoch 0[9503/17270] Time:0.228, Train Loss:0.364401638507843\n",
      "Epoch 0[9504/17270] Time:0.246, Train Loss:0.8995408415794373\n",
      "Epoch 0[9505/17270] Time:0.239, Train Loss:0.3140726089477539\n",
      "Epoch 0[9506/17270] Time:0.231, Train Loss:0.35193920135498047\n",
      "Epoch 0[9507/17270] Time:0.233, Train Loss:0.6450722813606262\n",
      "Epoch 0[9508/17270] Time:0.234, Train Loss:0.6198511123657227\n",
      "Epoch 0[9509/17270] Time:0.247, Train Loss:0.6015873551368713\n",
      "Epoch 0[9510/17270] Time:0.237, Train Loss:0.6028052568435669\n",
      "Epoch 0[9511/17270] Time:0.226, Train Loss:0.45801812410354614\n",
      "Epoch 0[9512/17270] Time:0.245, Train Loss:0.7746152877807617\n",
      "Epoch 0[9513/17270] Time:0.255, Train Loss:0.768925666809082\n",
      "Epoch 0[9514/17270] Time:0.232, Train Loss:0.43497434258461\n",
      "Epoch 0[9515/17270] Time:0.224, Train Loss:0.5938960909843445\n",
      "Epoch 0[9516/17270] Time:0.231, Train Loss:0.5625290870666504\n",
      "Epoch 0[9517/17270] Time:0.242, Train Loss:0.3464083671569824\n",
      "Epoch 0[9518/17270] Time:0.238, Train Loss:0.459621399641037\n",
      "Epoch 0[9519/17270] Time:0.223, Train Loss:0.9111455678939819\n",
      "Epoch 0[9520/17270] Time:0.223, Train Loss:0.738625168800354\n",
      "Epoch 0[9521/17270] Time:0.23, Train Loss:0.412026584148407\n",
      "Epoch 0[9522/17270] Time:0.237, Train Loss:0.5727384090423584\n",
      "Epoch 0[9523/17270] Time:0.227, Train Loss:0.43230965733528137\n",
      "Epoch 0[9524/17270] Time:0.239, Train Loss:0.5870676636695862\n",
      "Epoch 0[9525/17270] Time:0.231, Train Loss:0.43125224113464355\n",
      "Epoch 0[9526/17270] Time:0.236, Train Loss:0.2368762195110321\n",
      "Epoch 0[9527/17270] Time:0.232, Train Loss:0.4642132818698883\n",
      "Epoch 0[9528/17270] Time:0.228, Train Loss:1.143113374710083\n",
      "Epoch 0[9529/17270] Time:0.229, Train Loss:0.5574402213096619\n",
      "Epoch 0[9530/17270] Time:0.236, Train Loss:0.6117252707481384\n",
      "Epoch 0[9531/17270] Time:0.231, Train Loss:0.5805330276489258\n",
      "Epoch 0[9532/17270] Time:0.23, Train Loss:1.0861045122146606\n",
      "Epoch 0[9533/17270] Time:0.237, Train Loss:0.4885879456996918\n",
      "Epoch 0[9534/17270] Time:0.231, Train Loss:0.45586860179901123\n",
      "Epoch 0[9535/17270] Time:0.232, Train Loss:0.6738395690917969\n",
      "Epoch 0[9536/17270] Time:0.238, Train Loss:0.6198161840438843\n",
      "Epoch 0[9537/17270] Time:0.23, Train Loss:0.5381956696510315\n",
      "Epoch 0[9538/17270] Time:0.233, Train Loss:0.497781366109848\n",
      "Epoch 0[9539/17270] Time:0.235, Train Loss:0.49946129322052\n",
      "Epoch 0[9540/17270] Time:0.24, Train Loss:0.8522308468818665\n",
      "Epoch 0[9541/17270] Time:0.253, Train Loss:0.4423762261867523\n",
      "Epoch 0[9542/17270] Time:0.231, Train Loss:0.5704994797706604\n",
      "Epoch 0[9543/17270] Time:0.228, Train Loss:0.3269857168197632\n",
      "Epoch 0[9544/17270] Time:0.227, Train Loss:0.5529007315635681\n",
      "Epoch 0[9545/17270] Time:0.238, Train Loss:0.8107750415802002\n",
      "Epoch 0[9546/17270] Time:0.247, Train Loss:0.2736695110797882\n",
      "Epoch 0[9547/17270] Time:0.235, Train Loss:0.2865186929702759\n",
      "Epoch 0[9548/17270] Time:0.223, Train Loss:0.362464040517807\n",
      "Epoch 0[9549/17270] Time:0.225, Train Loss:0.6951982975006104\n",
      "Epoch 0[9550/17270] Time:0.228, Train Loss:0.29502442479133606\n",
      "Epoch 0[9551/17270] Time:0.239, Train Loss:0.48082298040390015\n",
      "Epoch 0[9552/17270] Time:0.229, Train Loss:0.4357297420501709\n",
      "Epoch 0[9553/17270] Time:0.241, Train Loss:0.7227730751037598\n",
      "Epoch 0[9554/17270] Time:0.241, Train Loss:0.567960798740387\n",
      "Epoch 0[9555/17270] Time:0.224, Train Loss:0.45725905895233154\n",
      "Epoch 0[9556/17270] Time:0.238, Train Loss:0.5869807600975037\n",
      "Epoch 0[9557/17270] Time:0.234, Train Loss:0.5629658699035645\n",
      "Epoch 0[9558/17270] Time:0.246, Train Loss:0.563492476940155\n",
      "Epoch 0[9559/17270] Time:0.226, Train Loss:0.6452459096908569\n",
      "Epoch 0[9560/17270] Time:0.243, Train Loss:0.7082920074462891\n",
      "Epoch 0[9561/17270] Time:0.242, Train Loss:0.5887293219566345\n",
      "Epoch 0[9562/17270] Time:0.239, Train Loss:0.4056738615036011\n",
      "Epoch 0[9563/17270] Time:0.226, Train Loss:1.8856818675994873\n",
      "Epoch 0[9564/17270] Time:0.23, Train Loss:0.5054337382316589\n",
      "Epoch 0[9565/17270] Time:0.245, Train Loss:0.8771040439605713\n",
      "Epoch 0[9566/17270] Time:0.234, Train Loss:0.5210279822349548\n",
      "Epoch 0[9567/17270] Time:0.233, Train Loss:0.41533708572387695\n",
      "Epoch 0[9568/17270] Time:0.234, Train Loss:0.3734530210494995\n",
      "Epoch 0[9569/17270] Time:0.235, Train Loss:0.6325504183769226\n",
      "Epoch 0[9570/17270] Time:0.233, Train Loss:0.5068489909172058\n",
      "Epoch 0[9571/17270] Time:0.23, Train Loss:0.9253160357475281\n",
      "Epoch 0[9572/17270] Time:0.233, Train Loss:0.9182222485542297\n",
      "Epoch 0[9573/17270] Time:0.229, Train Loss:0.37385886907577515\n",
      "Epoch 0[9574/17270] Time:0.226, Train Loss:0.6480785608291626\n",
      "Epoch 0[9575/17270] Time:0.229, Train Loss:0.4378877580165863\n",
      "Epoch 0[9576/17270] Time:0.235, Train Loss:0.42895567417144775\n",
      "Epoch 0[9577/17270] Time:0.237, Train Loss:0.7805725932121277\n",
      "Epoch 0[9578/17270] Time:0.238, Train Loss:0.7093162536621094\n",
      "Epoch 0[9579/17270] Time:0.23, Train Loss:0.3626977205276489\n",
      "Epoch 0[9580/17270] Time:0.237, Train Loss:0.43048641085624695\n",
      "Epoch 0[9581/17270] Time:0.24, Train Loss:0.532566249370575\n",
      "Epoch 0[9582/17270] Time:0.233, Train Loss:0.5751263499259949\n",
      "Epoch 0[9583/17270] Time:0.242, Train Loss:0.7492847442626953\n",
      "Epoch 0[9584/17270] Time:0.242, Train Loss:0.35952135920524597\n",
      "Epoch 0[9585/17270] Time:0.239, Train Loss:0.5730456113815308\n",
      "Epoch 0[9586/17270] Time:0.233, Train Loss:0.8738711476325989\n",
      "Epoch 0[9587/17270] Time:0.226, Train Loss:1.0694435834884644\n",
      "Epoch 0[9588/17270] Time:0.237, Train Loss:0.42928212881088257\n",
      "Epoch 0[9589/17270] Time:0.245, Train Loss:0.5872864127159119\n",
      "Epoch 0[9590/17270] Time:0.233, Train Loss:0.4922660291194916\n",
      "Epoch 0[9591/17270] Time:0.237, Train Loss:0.5436824560165405\n",
      "Epoch 0[9592/17270] Time:0.239, Train Loss:1.2767468690872192\n",
      "Epoch 0[9593/17270] Time:0.239, Train Loss:0.48560062050819397\n",
      "Epoch 0[9594/17270] Time:0.243, Train Loss:0.4679830074310303\n",
      "Epoch 0[9595/17270] Time:0.235, Train Loss:1.5812121629714966\n",
      "Epoch 0[9596/17270] Time:0.233, Train Loss:0.4537216126918793\n",
      "Epoch 0[9597/17270] Time:0.238, Train Loss:0.8016841411590576\n",
      "Epoch 0[9598/17270] Time:0.238, Train Loss:1.0317721366882324\n",
      "Epoch 0[9599/17270] Time:0.247, Train Loss:0.3750429153442383\n",
      "Epoch 0[9600/17270] Time:0.232, Train Loss:0.7595937252044678\n",
      "Epoch 0[9601/17270] Time:0.233, Train Loss:0.49534881114959717\n",
      "Epoch 0[9602/17270] Time:0.225, Train Loss:0.39294564723968506\n",
      "Epoch 0[9603/17270] Time:0.236, Train Loss:0.4646165370941162\n",
      "Epoch 0[9604/17270] Time:0.232, Train Loss:0.5395449995994568\n",
      "Epoch 0[9605/17270] Time:0.23, Train Loss:0.2726539969444275\n",
      "Epoch 0[9606/17270] Time:0.238, Train Loss:0.735876739025116\n",
      "Epoch 0[9607/17270] Time:0.23, Train Loss:0.9308955669403076\n",
      "Epoch 0[9608/17270] Time:0.23, Train Loss:0.7076386213302612\n",
      "Epoch 0[9609/17270] Time:0.232, Train Loss:0.5178606510162354\n",
      "Epoch 0[9610/17270] Time:0.227, Train Loss:0.6052488684654236\n",
      "Epoch 0[9611/17270] Time:0.235, Train Loss:0.4513480067253113\n",
      "Epoch 0[9612/17270] Time:0.234, Train Loss:0.20775769650936127\n",
      "Epoch 0[9613/17270] Time:0.222, Train Loss:0.41656067967414856\n",
      "Epoch 0[9614/17270] Time:0.23, Train Loss:0.5439538955688477\n",
      "Epoch 0[9615/17270] Time:0.229, Train Loss:0.542247474193573\n",
      "Epoch 0[9616/17270] Time:0.231, Train Loss:0.6347941756248474\n",
      "Epoch 0[9617/17270] Time:0.236, Train Loss:0.7941160202026367\n",
      "Epoch 0[9618/17270] Time:0.239, Train Loss:0.5590149164199829\n",
      "Epoch 0[9619/17270] Time:0.238, Train Loss:0.6569293737411499\n",
      "Epoch 0[9620/17270] Time:0.242, Train Loss:0.32535839080810547\n",
      "Epoch 0[9621/17270] Time:0.228, Train Loss:0.5690876841545105\n",
      "Epoch 0[9622/17270] Time:0.236, Train Loss:0.4363454580307007\n",
      "Epoch 0[9623/17270] Time:0.223, Train Loss:0.33878982067108154\n",
      "Epoch 0[9624/17270] Time:0.239, Train Loss:0.6651170253753662\n",
      "Epoch 0[9625/17270] Time:0.232, Train Loss:0.7055185437202454\n",
      "Epoch 0[9626/17270] Time:0.233, Train Loss:0.6426964998245239\n",
      "Epoch 0[9627/17270] Time:0.232, Train Loss:0.4500288665294647\n",
      "Epoch 0[9628/17270] Time:0.231, Train Loss:0.38598427176475525\n",
      "Epoch 0[9629/17270] Time:0.239, Train Loss:0.43723437190055847\n",
      "Epoch 0[9630/17270] Time:0.235, Train Loss:0.4183087944984436\n",
      "Epoch 0[9631/17270] Time:0.234, Train Loss:0.6155824661254883\n",
      "Epoch 0[9632/17270] Time:0.226, Train Loss:0.493204802274704\n",
      "Epoch 0[9633/17270] Time:0.235, Train Loss:0.374650239944458\n",
      "Epoch 0[9634/17270] Time:0.238, Train Loss:1.1609526872634888\n",
      "Epoch 0[9635/17270] Time:0.234, Train Loss:0.8760354518890381\n",
      "Epoch 0[9636/17270] Time:0.245, Train Loss:0.39520636200904846\n",
      "Epoch 0[9637/17270] Time:0.231, Train Loss:0.7733641862869263\n",
      "Epoch 0[9638/17270] Time:0.222, Train Loss:0.8886334300041199\n",
      "Epoch 0[9639/17270] Time:0.246, Train Loss:0.8957253098487854\n",
      "Epoch 0[9640/17270] Time:0.222, Train Loss:0.3820147216320038\n",
      "Epoch 0[9641/17270] Time:0.235, Train Loss:1.2320623397827148\n",
      "Epoch 0[9642/17270] Time:0.237, Train Loss:0.44258224964141846\n",
      "Epoch 0[9643/17270] Time:0.233, Train Loss:0.4426874816417694\n",
      "Epoch 0[9644/17270] Time:0.224, Train Loss:0.9223880171775818\n",
      "Epoch 0[9645/17270] Time:0.235, Train Loss:0.8036011457443237\n",
      "Epoch 0[9646/17270] Time:0.224, Train Loss:0.48126670718193054\n",
      "Epoch 0[9647/17270] Time:0.24, Train Loss:0.4102911651134491\n",
      "Epoch 0[9648/17270] Time:0.235, Train Loss:0.7370290756225586\n",
      "Epoch 0[9649/17270] Time:0.241, Train Loss:0.4262670576572418\n",
      "Epoch 0[9650/17270] Time:0.227, Train Loss:0.38622015714645386\n",
      "Epoch 0[9651/17270] Time:0.236, Train Loss:0.4399234652519226\n",
      "Epoch 0[9652/17270] Time:0.23, Train Loss:0.6302992701530457\n",
      "Epoch 0[9653/17270] Time:0.233, Train Loss:0.46621260046958923\n",
      "Epoch 0[9654/17270] Time:0.234, Train Loss:0.5334816575050354\n",
      "Epoch 0[9655/17270] Time:0.238, Train Loss:0.7916728854179382\n",
      "Epoch 0[9656/17270] Time:0.23, Train Loss:0.5289696455001831\n",
      "Epoch 0[9657/17270] Time:0.234, Train Loss:0.6480380296707153\n",
      "Epoch 0[9658/17270] Time:0.233, Train Loss:0.7525057196617126\n",
      "Epoch 0[9659/17270] Time:0.231, Train Loss:0.6272182464599609\n",
      "Epoch 0[9660/17270] Time:0.229, Train Loss:0.6363855004310608\n",
      "Epoch 0[9661/17270] Time:0.24, Train Loss:1.0989691019058228\n",
      "Epoch 0[9662/17270] Time:0.231, Train Loss:0.43720483779907227\n",
      "Epoch 0[9663/17270] Time:0.242, Train Loss:0.3922702670097351\n",
      "Epoch 0[9664/17270] Time:0.23, Train Loss:1.2176228761672974\n",
      "Epoch 0[9665/17270] Time:0.241, Train Loss:0.4081786274909973\n",
      "Epoch 0[9666/17270] Time:0.231, Train Loss:0.5965315103530884\n",
      "Epoch 0[9667/17270] Time:0.244, Train Loss:0.7316638231277466\n",
      "Epoch 0[9668/17270] Time:0.239, Train Loss:0.3947807252407074\n",
      "Epoch 0[9669/17270] Time:0.231, Train Loss:0.5912334322929382\n",
      "Epoch 0[9670/17270] Time:0.234, Train Loss:0.926552414894104\n",
      "Epoch 0[9671/17270] Time:0.233, Train Loss:0.42710575461387634\n",
      "Epoch 0[9672/17270] Time:0.228, Train Loss:0.668084979057312\n",
      "Epoch 0[9673/17270] Time:0.238, Train Loss:1.2850537300109863\n",
      "Epoch 0[9674/17270] Time:0.245, Train Loss:0.3075551390647888\n",
      "Epoch 0[9675/17270] Time:0.225, Train Loss:0.6273341774940491\n",
      "Epoch 0[9676/17270] Time:0.235, Train Loss:0.6737971901893616\n",
      "Epoch 0[9677/17270] Time:0.235, Train Loss:0.5821846723556519\n",
      "Epoch 0[9678/17270] Time:0.238, Train Loss:0.5159457325935364\n",
      "Epoch 0[9679/17270] Time:0.241, Train Loss:0.44002634286880493\n",
      "Epoch 0[9680/17270] Time:0.229, Train Loss:0.8569434881210327\n",
      "Epoch 0[9681/17270] Time:0.228, Train Loss:0.7308597564697266\n",
      "Epoch 0[9682/17270] Time:0.233, Train Loss:0.6865622401237488\n",
      "Epoch 0[9683/17270] Time:0.227, Train Loss:0.4453538954257965\n",
      "Epoch 0[9684/17270] Time:0.231, Train Loss:0.7839880585670471\n",
      "Epoch 0[9685/17270] Time:0.237, Train Loss:0.5697874426841736\n",
      "Epoch 0[9686/17270] Time:0.23, Train Loss:0.4688461124897003\n",
      "Epoch 0[9687/17270] Time:0.24, Train Loss:0.651872456073761\n",
      "Epoch 0[9688/17270] Time:0.233, Train Loss:0.5408607125282288\n",
      "Epoch 0[9689/17270] Time:0.237, Train Loss:0.5357533097267151\n",
      "Epoch 0[9690/17270] Time:0.229, Train Loss:1.1276819705963135\n",
      "Epoch 0[9691/17270] Time:0.231, Train Loss:1.2001793384552002\n",
      "Epoch 0[9692/17270] Time:0.24, Train Loss:0.6588205099105835\n",
      "Epoch 0[9693/17270] Time:0.238, Train Loss:0.4983062744140625\n",
      "Epoch 0[9694/17270] Time:0.23, Train Loss:0.8908888697624207\n",
      "Epoch 0[9695/17270] Time:0.233, Train Loss:0.501950740814209\n",
      "Epoch 0[9696/17270] Time:0.235, Train Loss:0.898136556148529\n",
      "Epoch 0[9697/17270] Time:0.237, Train Loss:0.8315655589103699\n",
      "Epoch 0[9698/17270] Time:0.238, Train Loss:0.36888572573661804\n",
      "Epoch 0[9699/17270] Time:0.23, Train Loss:0.33553439378738403\n",
      "Epoch 0[9700/17270] Time:0.237, Train Loss:0.474864661693573\n",
      "Epoch 0[9701/17270] Time:0.238, Train Loss:0.42202937602996826\n",
      "Epoch 0[9702/17270] Time:0.235, Train Loss:0.41478824615478516\n",
      "Epoch 0[9703/17270] Time:0.238, Train Loss:0.32944566011428833\n",
      "Epoch 0[9704/17270] Time:0.233, Train Loss:0.5800446271896362\n",
      "Epoch 0[9705/17270] Time:0.241, Train Loss:0.52843177318573\n",
      "Epoch 0[9706/17270] Time:0.231, Train Loss:0.4344237148761749\n",
      "Epoch 0[9707/17270] Time:0.244, Train Loss:0.9598839282989502\n",
      "Epoch 0[9708/17270] Time:0.237, Train Loss:0.5032280683517456\n",
      "Epoch 0[9709/17270] Time:0.223, Train Loss:0.6119978427886963\n",
      "Epoch 0[9710/17270] Time:0.232, Train Loss:0.6505328416824341\n",
      "Epoch 0[9711/17270] Time:0.239, Train Loss:0.4673936069011688\n",
      "Epoch 0[9712/17270] Time:0.236, Train Loss:0.743190586566925\n",
      "Epoch 0[9713/17270] Time:0.231, Train Loss:0.5265052914619446\n",
      "Epoch 0[9714/17270] Time:0.223, Train Loss:1.1496530771255493\n",
      "Epoch 0[9715/17270] Time:0.237, Train Loss:0.6046848893165588\n",
      "Epoch 0[9716/17270] Time:0.244, Train Loss:0.6356012225151062\n",
      "Epoch 0[9717/17270] Time:0.235, Train Loss:0.5245230793952942\n",
      "Epoch 0[9718/17270] Time:0.229, Train Loss:0.7658083438873291\n",
      "Epoch 0[9719/17270] Time:0.248, Train Loss:0.46843382716178894\n",
      "Epoch 0[9720/17270] Time:0.237, Train Loss:0.3876374363899231\n",
      "Epoch 0[9721/17270] Time:0.232, Train Loss:0.47470179200172424\n",
      "Epoch 0[9722/17270] Time:0.247, Train Loss:0.5250205397605896\n",
      "Epoch 0[9723/17270] Time:0.224, Train Loss:1.6185567378997803\n",
      "Epoch 0[9724/17270] Time:0.248, Train Loss:0.48714303970336914\n",
      "Epoch 0[9725/17270] Time:0.239, Train Loss:0.9581239223480225\n",
      "Epoch 0[9726/17270] Time:0.236, Train Loss:0.547926664352417\n",
      "Epoch 0[9727/17270] Time:0.24, Train Loss:0.5273023247718811\n",
      "Epoch 0[9728/17270] Time:0.222, Train Loss:0.4855847954750061\n",
      "Epoch 0[9729/17270] Time:0.238, Train Loss:1.372278094291687\n",
      "Epoch 0[9730/17270] Time:0.232, Train Loss:0.7087885737419128\n",
      "Epoch 0[9731/17270] Time:0.224, Train Loss:0.4631800353527069\n",
      "Epoch 0[9732/17270] Time:0.23, Train Loss:0.4077184200286865\n",
      "Epoch 0[9733/17270] Time:0.23, Train Loss:0.3946697413921356\n",
      "Epoch 0[9734/17270] Time:0.232, Train Loss:0.3399973213672638\n",
      "Epoch 0[9735/17270] Time:0.238, Train Loss:1.2576414346694946\n",
      "Epoch 0[9736/17270] Time:0.236, Train Loss:0.49427270889282227\n",
      "Epoch 0[9737/17270] Time:0.239, Train Loss:0.30995431542396545\n",
      "Epoch 0[9738/17270] Time:0.233, Train Loss:0.4273088872432709\n",
      "Epoch 0[9739/17270] Time:0.231, Train Loss:0.5425402522087097\n",
      "Epoch 0[9740/17270] Time:0.223, Train Loss:0.6849348545074463\n",
      "Epoch 0[9741/17270] Time:0.231, Train Loss:0.6517468690872192\n",
      "Epoch 0[9742/17270] Time:0.233, Train Loss:0.47354838252067566\n",
      "Epoch 0[9743/17270] Time:0.242, Train Loss:0.6257409453392029\n",
      "Epoch 0[9744/17270] Time:0.234, Train Loss:0.7447478175163269\n",
      "Epoch 0[9745/17270] Time:0.237, Train Loss:0.4937264621257782\n",
      "Epoch 0[9746/17270] Time:0.237, Train Loss:0.3828084468841553\n",
      "Epoch 0[9747/17270] Time:0.236, Train Loss:0.3785693943500519\n",
      "Epoch 0[9748/17270] Time:0.222, Train Loss:0.42871609330177307\n",
      "Epoch 0[9749/17270] Time:0.239, Train Loss:0.5215057730674744\n",
      "Epoch 0[9750/17270] Time:0.245, Train Loss:0.5997686386108398\n",
      "Epoch 0[9751/17270] Time:0.238, Train Loss:0.7972961664199829\n",
      "Epoch 0[9752/17270] Time:0.233, Train Loss:0.6163529753684998\n",
      "Epoch 0[9753/17270] Time:0.222, Train Loss:0.4376710057258606\n",
      "Epoch 0[9754/17270] Time:0.228, Train Loss:0.6276074647903442\n",
      "Epoch 0[9755/17270] Time:0.241, Train Loss:0.7191529870033264\n",
      "Epoch 0[9756/17270] Time:0.24, Train Loss:0.5144724249839783\n",
      "Epoch 0[9757/17270] Time:0.233, Train Loss:0.6978257894515991\n",
      "Epoch 0[9758/17270] Time:0.223, Train Loss:1.936574101448059\n",
      "Epoch 0[9759/17270] Time:0.237, Train Loss:0.6385097503662109\n",
      "Epoch 0[9760/17270] Time:0.243, Train Loss:0.40858936309814453\n",
      "Epoch 0[9761/17270] Time:0.235, Train Loss:0.49338898062705994\n",
      "Epoch 0[9762/17270] Time:0.229, Train Loss:0.7699171900749207\n",
      "Epoch 0[9763/17270] Time:0.225, Train Loss:0.5504767894744873\n",
      "Epoch 0[9764/17270] Time:0.238, Train Loss:0.4458804428577423\n",
      "Epoch 0[9765/17270] Time:0.234, Train Loss:0.4034709632396698\n",
      "Epoch 0[9766/17270] Time:0.237, Train Loss:0.5951247811317444\n",
      "Epoch 0[9767/17270] Time:0.232, Train Loss:0.7296521067619324\n",
      "Epoch 0[9768/17270] Time:0.228, Train Loss:0.4173058271408081\n",
      "Epoch 0[9769/17270] Time:0.244, Train Loss:0.481289803981781\n",
      "Epoch 0[9770/17270] Time:0.229, Train Loss:0.6320808529853821\n",
      "Epoch 0[9771/17270] Time:0.219, Train Loss:0.45260098576545715\n",
      "Epoch 0[9772/17270] Time:0.239, Train Loss:0.4270417392253876\n",
      "Epoch 0[9773/17270] Time:0.22, Train Loss:0.61708003282547\n",
      "Epoch 0[9774/17270] Time:0.229, Train Loss:0.3539409339427948\n",
      "Epoch 0[9775/17270] Time:0.235, Train Loss:1.2504359483718872\n",
      "Epoch 0[9776/17270] Time:0.245, Train Loss:0.3887510299682617\n",
      "Epoch 0[9777/17270] Time:0.231, Train Loss:0.8102343082427979\n",
      "Epoch 0[9778/17270] Time:0.227, Train Loss:0.4769992232322693\n",
      "Epoch 0[9779/17270] Time:0.231, Train Loss:0.5842336416244507\n",
      "Epoch 0[9780/17270] Time:0.228, Train Loss:0.6256107687950134\n",
      "Epoch 0[9781/17270] Time:0.236, Train Loss:0.4807220697402954\n",
      "Epoch 0[9782/17270] Time:0.231, Train Loss:0.5611625909805298\n",
      "Epoch 0[9783/17270] Time:0.23, Train Loss:0.6144466400146484\n",
      "Epoch 0[9784/17270] Time:0.238, Train Loss:1.1432204246520996\n",
      "Epoch 0[9785/17270] Time:0.234, Train Loss:0.6721469163894653\n",
      "Epoch 0[9786/17270] Time:0.226, Train Loss:0.3326003849506378\n",
      "Epoch 0[9787/17270] Time:0.231, Train Loss:0.3808318078517914\n",
      "Epoch 0[9788/17270] Time:0.23, Train Loss:0.7517973184585571\n",
      "Epoch 0[9789/17270] Time:0.235, Train Loss:0.5177912712097168\n",
      "Epoch 0[9790/17270] Time:0.235, Train Loss:0.7160857915878296\n",
      "Epoch 0[9791/17270] Time:0.235, Train Loss:0.5853223204612732\n",
      "Epoch 0[9792/17270] Time:0.231, Train Loss:0.44189000129699707\n",
      "Epoch 0[9793/17270] Time:0.229, Train Loss:0.6773333549499512\n",
      "Epoch 0[9794/17270] Time:0.238, Train Loss:1.4357411861419678\n",
      "Epoch 0[9795/17270] Time:0.236, Train Loss:0.4040587842464447\n",
      "Epoch 0[9796/17270] Time:0.237, Train Loss:0.36052876710891724\n",
      "Epoch 0[9797/17270] Time:0.237, Train Loss:0.7710530161857605\n",
      "Epoch 0[9798/17270] Time:0.239, Train Loss:0.5941668748855591\n",
      "Epoch 0[9799/17270] Time:0.238, Train Loss:0.8999242186546326\n",
      "Epoch 0[9800/17270] Time:0.237, Train Loss:0.49293196201324463\n",
      "Epoch 0[9801/17270] Time:0.233, Train Loss:0.5515968799591064\n",
      "Epoch 0[9802/17270] Time:0.242, Train Loss:0.4434002637863159\n",
      "Epoch 0[9803/17270] Time:0.229, Train Loss:0.9927424788475037\n",
      "Epoch 0[9804/17270] Time:0.23, Train Loss:0.3121158480644226\n",
      "Epoch 0[9805/17270] Time:0.238, Train Loss:0.4681299030780792\n",
      "Epoch 0[9806/17270] Time:0.227, Train Loss:0.7097380757331848\n",
      "Epoch 0[9807/17270] Time:0.226, Train Loss:0.8675850033760071\n",
      "Epoch 0[9808/17270] Time:0.227, Train Loss:0.4119723439216614\n",
      "Epoch 0[9809/17270] Time:0.239, Train Loss:0.5253117680549622\n",
      "Epoch 0[9810/17270] Time:0.231, Train Loss:0.34930673241615295\n",
      "Epoch 0[9811/17270] Time:0.229, Train Loss:0.6988366842269897\n",
      "Epoch 0[9812/17270] Time:0.229, Train Loss:0.5362277626991272\n",
      "Epoch 0[9813/17270] Time:0.236, Train Loss:0.6922847032546997\n",
      "Epoch 0[9814/17270] Time:0.228, Train Loss:0.4990483522415161\n",
      "Epoch 0[9815/17270] Time:0.228, Train Loss:0.5055272579193115\n",
      "Epoch 0[9816/17270] Time:0.238, Train Loss:0.5700216889381409\n",
      "Epoch 0[9817/17270] Time:0.236, Train Loss:0.3970296382904053\n",
      "Epoch 0[9818/17270] Time:0.237, Train Loss:1.1037092208862305\n",
      "Epoch 0[9819/17270] Time:0.23, Train Loss:0.5172978043556213\n",
      "Epoch 0[9820/17270] Time:0.233, Train Loss:0.6926230788230896\n",
      "Epoch 0[9821/17270] Time:0.227, Train Loss:0.7525556087493896\n",
      "Epoch 0[9822/17270] Time:0.231, Train Loss:0.5730723142623901\n",
      "Epoch 0[9823/17270] Time:0.237, Train Loss:0.4478142559528351\n",
      "Epoch 0[9824/17270] Time:0.236, Train Loss:0.40475383400917053\n",
      "Epoch 0[9825/17270] Time:0.228, Train Loss:0.6633197069168091\n",
      "Epoch 0[9826/17270] Time:0.235, Train Loss:0.6935372948646545\n",
      "Epoch 0[9827/17270] Time:0.238, Train Loss:0.8256101608276367\n",
      "Epoch 0[9828/17270] Time:0.237, Train Loss:1.201247215270996\n",
      "Epoch 0[9829/17270] Time:0.236, Train Loss:0.6239414811134338\n",
      "Epoch 0[9830/17270] Time:0.226, Train Loss:0.8803111910820007\n",
      "Epoch 0[9831/17270] Time:0.23, Train Loss:0.550072431564331\n",
      "Epoch 0[9832/17270] Time:0.231, Train Loss:0.5833576321601868\n",
      "Epoch 0[9833/17270] Time:0.228, Train Loss:0.7169498205184937\n",
      "Epoch 0[9834/17270] Time:0.238, Train Loss:0.518220067024231\n",
      "Epoch 0[9835/17270] Time:0.228, Train Loss:0.5690294504165649\n",
      "Epoch 0[9836/17270] Time:0.235, Train Loss:0.28106504678726196\n",
      "Epoch 0[9837/17270] Time:0.232, Train Loss:0.7657880783081055\n",
      "Epoch 0[9838/17270] Time:0.23, Train Loss:0.45941242575645447\n",
      "Epoch 0[9839/17270] Time:0.235, Train Loss:0.37702974677085876\n",
      "Epoch 0[9840/17270] Time:0.223, Train Loss:0.6173231601715088\n",
      "Epoch 0[9841/17270] Time:0.236, Train Loss:0.7897903919219971\n",
      "Epoch 0[9842/17270] Time:0.238, Train Loss:0.899202287197113\n",
      "Epoch 0[9843/17270] Time:0.23, Train Loss:0.4373425841331482\n",
      "Epoch 0[9844/17270] Time:0.233, Train Loss:0.5712003707885742\n",
      "Epoch 0[9845/17270] Time:0.23, Train Loss:0.3951197862625122\n",
      "Epoch 0[9846/17270] Time:0.249, Train Loss:0.8817269206047058\n",
      "Epoch 0[9847/17270] Time:0.246, Train Loss:0.6131911873817444\n",
      "Epoch 0[9848/17270] Time:0.235, Train Loss:0.8457158207893372\n",
      "Epoch 0[9849/17270] Time:0.237, Train Loss:0.33693015575408936\n",
      "Epoch 0[9850/17270] Time:0.238, Train Loss:0.43102720379829407\n",
      "Epoch 0[9851/17270] Time:0.236, Train Loss:0.2977721095085144\n",
      "Epoch 0[9852/17270] Time:0.238, Train Loss:0.46664097905158997\n",
      "Epoch 0[9853/17270] Time:0.224, Train Loss:0.43128809332847595\n",
      "Epoch 0[9854/17270] Time:0.24, Train Loss:1.1311265230178833\n",
      "Epoch 0[9855/17270] Time:0.239, Train Loss:0.5841642022132874\n",
      "Epoch 0[9856/17270] Time:0.236, Train Loss:0.7579641342163086\n",
      "Epoch 0[9857/17270] Time:0.239, Train Loss:0.863008439540863\n",
      "Epoch 0[9858/17270] Time:0.236, Train Loss:0.20304569602012634\n",
      "Epoch 0[9859/17270] Time:0.237, Train Loss:0.5418779253959656\n",
      "Epoch 0[9860/17270] Time:0.227, Train Loss:0.6636380553245544\n",
      "Epoch 0[9861/17270] Time:0.239, Train Loss:0.5419631600379944\n",
      "Epoch 0[9862/17270] Time:0.238, Train Loss:0.3454606831073761\n",
      "Epoch 0[9863/17270] Time:0.226, Train Loss:0.38100913166999817\n",
      "Epoch 0[9864/17270] Time:0.248, Train Loss:0.3257158100605011\n",
      "Epoch 0[9865/17270] Time:0.226, Train Loss:0.3872753381729126\n",
      "Epoch 0[9866/17270] Time:0.232, Train Loss:0.7542162537574768\n",
      "Epoch 0[9867/17270] Time:0.226, Train Loss:0.5434730648994446\n",
      "Epoch 0[9868/17270] Time:0.244, Train Loss:0.5999744534492493\n",
      "Epoch 0[9869/17270] Time:0.233, Train Loss:0.48656967282295227\n",
      "Epoch 0[9870/17270] Time:0.232, Train Loss:0.25415390729904175\n",
      "Epoch 0[9871/17270] Time:0.236, Train Loss:0.6814846992492676\n",
      "Epoch 0[9872/17270] Time:0.236, Train Loss:0.6591116189956665\n",
      "Epoch 0[9873/17270] Time:0.24, Train Loss:0.6891484260559082\n",
      "Epoch 0[9874/17270] Time:0.253, Train Loss:0.34298238158226013\n",
      "Epoch 0[9875/17270] Time:0.224, Train Loss:0.44906091690063477\n",
      "Epoch 0[9876/17270] Time:0.248, Train Loss:0.5172202587127686\n",
      "Epoch 0[9877/17270] Time:0.246, Train Loss:0.30841198563575745\n",
      "Epoch 0[9878/17270] Time:0.235, Train Loss:1.0023521184921265\n",
      "Epoch 0[9879/17270] Time:0.239, Train Loss:0.7244791984558105\n",
      "Epoch 0[9880/17270] Time:0.232, Train Loss:0.6713539361953735\n",
      "Epoch 0[9881/17270] Time:0.228, Train Loss:0.6121442914009094\n",
      "Epoch 0[9882/17270] Time:0.232, Train Loss:0.4466053545475006\n",
      "Epoch 0[9883/17270] Time:0.237, Train Loss:0.5517300367355347\n",
      "Epoch 0[9884/17270] Time:0.23, Train Loss:0.49283090233802795\n",
      "Epoch 0[9885/17270] Time:0.222, Train Loss:0.5764226913452148\n",
      "Epoch 0[9886/17270] Time:0.227, Train Loss:0.2880541682243347\n",
      "Epoch 0[9887/17270] Time:0.241, Train Loss:0.5224526524543762\n",
      "Epoch 0[9888/17270] Time:0.225, Train Loss:1.184370517730713\n",
      "Epoch 0[9889/17270] Time:0.246, Train Loss:0.7938649654388428\n",
      "Epoch 0[9890/17270] Time:0.234, Train Loss:0.7052422761917114\n",
      "Epoch 0[9891/17270] Time:0.238, Train Loss:0.9401088356971741\n",
      "Epoch 0[9892/17270] Time:0.237, Train Loss:0.9653518199920654\n",
      "Epoch 0[9893/17270] Time:0.223, Train Loss:0.5831965804100037\n",
      "Epoch 0[9894/17270] Time:0.24, Train Loss:0.4072027802467346\n",
      "Epoch 0[9895/17270] Time:0.233, Train Loss:0.5401625633239746\n",
      "Epoch 0[9896/17270] Time:0.238, Train Loss:0.42871302366256714\n",
      "Epoch 0[9897/17270] Time:0.235, Train Loss:0.41808122396469116\n",
      "Epoch 0[9898/17270] Time:0.232, Train Loss:0.52644282579422\n",
      "Epoch 0[9899/17270] Time:0.237, Train Loss:0.9665608406066895\n",
      "Epoch 0[9900/17270] Time:0.234, Train Loss:0.4831515848636627\n",
      "Epoch 0[9901/17270] Time:0.233, Train Loss:0.7488682270050049\n",
      "Epoch 0[9902/17270] Time:0.233, Train Loss:0.6646386384963989\n",
      "Epoch 0[9903/17270] Time:0.232, Train Loss:0.3829823136329651\n",
      "Epoch 0[9904/17270] Time:0.225, Train Loss:0.4282549023628235\n",
      "Epoch 0[9905/17270] Time:0.229, Train Loss:0.5440992116928101\n",
      "Epoch 0[9906/17270] Time:0.236, Train Loss:2.1366682052612305\n",
      "Epoch 0[9907/17270] Time:0.231, Train Loss:0.7565104961395264\n",
      "Epoch 0[9908/17270] Time:0.232, Train Loss:0.5315653681755066\n",
      "Epoch 0[9909/17270] Time:0.244, Train Loss:0.39743685722351074\n",
      "Epoch 0[9910/17270] Time:0.237, Train Loss:0.6793892979621887\n",
      "Epoch 0[9911/17270] Time:0.23, Train Loss:1.002257227897644\n",
      "Epoch 0[9912/17270] Time:0.245, Train Loss:0.5112829804420471\n",
      "Epoch 0[9913/17270] Time:0.233, Train Loss:0.6313797831535339\n",
      "Epoch 0[9914/17270] Time:0.223, Train Loss:0.869185745716095\n",
      "Epoch 0[9915/17270] Time:0.241, Train Loss:0.5252645611763\n",
      "Epoch 0[9916/17270] Time:0.244, Train Loss:0.5019559860229492\n",
      "Epoch 0[9917/17270] Time:0.233, Train Loss:0.37218669056892395\n",
      "Epoch 0[9918/17270] Time:0.223, Train Loss:0.6151646971702576\n",
      "Epoch 0[9919/17270] Time:0.244, Train Loss:0.3842642605304718\n",
      "Epoch 0[9920/17270] Time:0.224, Train Loss:1.0471508502960205\n",
      "Epoch 0[9921/17270] Time:0.248, Train Loss:0.4324280917644501\n",
      "Epoch 0[9922/17270] Time:0.224, Train Loss:0.6314488053321838\n",
      "Epoch 0[9923/17270] Time:0.242, Train Loss:0.6181396842002869\n",
      "Epoch 0[9924/17270] Time:0.23, Train Loss:0.502017617225647\n",
      "Epoch 0[9925/17270] Time:0.23, Train Loss:0.49878960847854614\n",
      "Epoch 0[9926/17270] Time:0.241, Train Loss:1.0501744747161865\n",
      "Epoch 0[9927/17270] Time:0.227, Train Loss:0.408659964799881\n",
      "Epoch 0[9928/17270] Time:0.227, Train Loss:0.9365507364273071\n",
      "Epoch 0[9929/17270] Time:0.245, Train Loss:0.38177692890167236\n",
      "Epoch 0[9930/17270] Time:0.237, Train Loss:0.35070696473121643\n",
      "Epoch 0[9931/17270] Time:0.235, Train Loss:0.571998119354248\n",
      "Epoch 0[9932/17270] Time:0.229, Train Loss:0.4038802683353424\n",
      "Epoch 0[9933/17270] Time:0.235, Train Loss:0.34371218085289\n",
      "Epoch 0[9934/17270] Time:0.232, Train Loss:0.6782824397087097\n",
      "Epoch 0[9935/17270] Time:0.239, Train Loss:0.8933528065681458\n",
      "Epoch 0[9936/17270] Time:0.231, Train Loss:0.8484163880348206\n",
      "Epoch 0[9937/17270] Time:0.245, Train Loss:0.5903728008270264\n",
      "Epoch 0[9938/17270] Time:0.233, Train Loss:0.7531416416168213\n",
      "Epoch 0[9939/17270] Time:0.234, Train Loss:0.6947760581970215\n",
      "Epoch 0[9940/17270] Time:0.235, Train Loss:0.6178181767463684\n",
      "Epoch 0[9941/17270] Time:0.229, Train Loss:0.5637096166610718\n",
      "Epoch 0[9942/17270] Time:0.236, Train Loss:0.5448867082595825\n",
      "Epoch 0[9943/17270] Time:0.236, Train Loss:0.38317862153053284\n",
      "Epoch 0[9944/17270] Time:0.24, Train Loss:0.2917652130126953\n",
      "Epoch 0[9945/17270] Time:0.23, Train Loss:0.48810407519340515\n",
      "Epoch 0[9946/17270] Time:0.238, Train Loss:0.4258902966976166\n",
      "Epoch 0[9947/17270] Time:0.229, Train Loss:0.4333757162094116\n",
      "Epoch 0[9948/17270] Time:0.24, Train Loss:0.673843264579773\n",
      "Epoch 0[9949/17270] Time:0.237, Train Loss:0.6236410737037659\n",
      "Epoch 0[9950/17270] Time:0.229, Train Loss:0.4480644464492798\n",
      "Epoch 0[9951/17270] Time:0.238, Train Loss:1.0787304639816284\n",
      "Epoch 0[9952/17270] Time:0.236, Train Loss:0.5194196701049805\n",
      "Epoch 0[9953/17270] Time:0.239, Train Loss:0.3502659201622009\n",
      "Epoch 0[9954/17270] Time:0.231, Train Loss:0.7808738350868225\n",
      "Epoch 0[9955/17270] Time:0.228, Train Loss:0.47693943977355957\n",
      "Epoch 0[9956/17270] Time:0.238, Train Loss:0.3504219055175781\n",
      "Epoch 0[9957/17270] Time:0.256, Train Loss:0.47284674644470215\n",
      "Epoch 0[9958/17270] Time:0.236, Train Loss:0.43926262855529785\n",
      "Epoch 0[9959/17270] Time:0.226, Train Loss:0.4381437599658966\n",
      "Epoch 0[9960/17270] Time:0.222, Train Loss:0.47757577896118164\n",
      "Epoch 0[9961/17270] Time:0.232, Train Loss:1.1195017099380493\n",
      "Epoch 0[9962/17270] Time:0.236, Train Loss:0.45487070083618164\n",
      "Epoch 0[9963/17270] Time:0.23, Train Loss:0.48234841227531433\n",
      "Epoch 0[9964/17270] Time:0.239, Train Loss:0.977660059928894\n",
      "Epoch 0[9965/17270] Time:0.231, Train Loss:0.4537425935268402\n",
      "Epoch 0[9966/17270] Time:0.231, Train Loss:0.45131146907806396\n",
      "Epoch 0[9967/17270] Time:0.236, Train Loss:0.7576139569282532\n",
      "Epoch 0[9968/17270] Time:0.236, Train Loss:0.6176525354385376\n",
      "Epoch 0[9969/17270] Time:0.229, Train Loss:0.4032326638698578\n",
      "Epoch 0[9970/17270] Time:0.248, Train Loss:0.3895389139652252\n",
      "Epoch 0[9971/17270] Time:0.236, Train Loss:0.9796029329299927\n",
      "Epoch 0[9972/17270] Time:0.238, Train Loss:0.7190226316452026\n",
      "Epoch 0[9973/17270] Time:0.234, Train Loss:0.40135878324508667\n",
      "Epoch 0[9974/17270] Time:0.237, Train Loss:1.5005345344543457\n",
      "Epoch 0[9975/17270] Time:0.23, Train Loss:0.7036454081535339\n",
      "Epoch 0[9976/17270] Time:0.24, Train Loss:0.8571717739105225\n",
      "Epoch 0[9977/17270] Time:0.236, Train Loss:0.8984929919242859\n",
      "Epoch 0[9978/17270] Time:0.236, Train Loss:0.3966514468193054\n",
      "Epoch 0[9979/17270] Time:0.246, Train Loss:0.7520725131034851\n",
      "Epoch 0[9980/17270] Time:0.234, Train Loss:0.5677486658096313\n",
      "Epoch 0[9981/17270] Time:0.235, Train Loss:0.8895334601402283\n",
      "Epoch 0[9982/17270] Time:0.238, Train Loss:0.516278862953186\n",
      "Epoch 0[9983/17270] Time:0.233, Train Loss:0.4006536304950714\n",
      "Epoch 0[9984/17270] Time:0.236, Train Loss:1.5439600944519043\n",
      "Epoch 0[9985/17270] Time:0.228, Train Loss:0.6843742728233337\n",
      "Epoch 0[9986/17270] Time:0.239, Train Loss:0.9093488454818726\n",
      "Epoch 0[9987/17270] Time:0.227, Train Loss:0.4851455092430115\n",
      "Epoch 0[9988/17270] Time:0.237, Train Loss:0.47495025396347046\n",
      "Epoch 0[9989/17270] Time:0.23, Train Loss:0.9014391303062439\n",
      "Epoch 0[9990/17270] Time:0.237, Train Loss:0.7756785750389099\n",
      "Epoch 0[9991/17270] Time:0.229, Train Loss:0.4207133650779724\n",
      "Epoch 0[9992/17270] Time:0.229, Train Loss:0.737764298915863\n",
      "Epoch 0[9993/17270] Time:0.232, Train Loss:0.5118646621704102\n",
      "Epoch 0[9994/17270] Time:0.244, Train Loss:0.37509220838546753\n",
      "Epoch 0[9995/17270] Time:0.235, Train Loss:0.618749737739563\n",
      "Epoch 0[9996/17270] Time:0.242, Train Loss:0.5678057670593262\n",
      "Epoch 0[9997/17270] Time:0.238, Train Loss:0.59738689661026\n",
      "Epoch 0[9998/17270] Time:0.223, Train Loss:0.4370673596858978\n",
      "Epoch 0[9999/17270] Time:0.244, Train Loss:1.2193137407302856\n",
      "Epoch 0[10000/17270] Time:0.227, Train Loss:0.6769497990608215\n",
      "Epoch 0[10001/17270] Time:0.224, Train Loss:0.6873975992202759\n",
      "Epoch 0[10002/17270] Time:0.238, Train Loss:0.5636861324310303\n",
      "Epoch 0[10003/17270] Time:0.238, Train Loss:0.3521179258823395\n",
      "Epoch 0[10004/17270] Time:0.234, Train Loss:0.46368858218193054\n",
      "Epoch 0[10005/17270] Time:0.232, Train Loss:0.49735701084136963\n",
      "Epoch 0[10006/17270] Time:0.23, Train Loss:0.3841572701931\n",
      "Epoch 0[10007/17270] Time:0.234, Train Loss:0.5374602675437927\n",
      "Epoch 0[10008/17270] Time:0.236, Train Loss:0.4080354869365692\n",
      "Epoch 0[10009/17270] Time:0.232, Train Loss:0.9976781606674194\n",
      "Epoch 0[10010/17270] Time:0.232, Train Loss:0.24801288545131683\n",
      "Epoch 0[10011/17270] Time:0.229, Train Loss:0.7166354060173035\n",
      "Epoch 0[10012/17270] Time:0.238, Train Loss:0.5938423871994019\n",
      "Epoch 0[10013/17270] Time:0.231, Train Loss:0.6562721729278564\n",
      "Epoch 0[10014/17270] Time:0.233, Train Loss:0.46675002574920654\n",
      "Epoch 0[10015/17270] Time:0.227, Train Loss:0.47256365418434143\n",
      "Epoch 0[10016/17270] Time:0.239, Train Loss:0.5431053042411804\n",
      "Epoch 0[10017/17270] Time:0.23, Train Loss:0.38235142827033997\n",
      "Epoch 0[10018/17270] Time:0.236, Train Loss:0.6705796122550964\n",
      "Epoch 0[10019/17270] Time:0.238, Train Loss:0.4554356634616852\n",
      "Epoch 0[10020/17270] Time:0.238, Train Loss:0.4953557550907135\n",
      "Epoch 0[10021/17270] Time:0.235, Train Loss:0.44128233194351196\n",
      "Epoch 0[10022/17270] Time:0.238, Train Loss:0.42051592469215393\n",
      "Epoch 0[10023/17270] Time:0.238, Train Loss:0.4153081178665161\n",
      "Epoch 0[10024/17270] Time:0.242, Train Loss:0.8179876208305359\n",
      "Epoch 0[10025/17270] Time:0.228, Train Loss:0.7409376502037048\n",
      "Epoch 0[10026/17270] Time:0.227, Train Loss:0.3596959412097931\n",
      "Epoch 0[10027/17270] Time:0.246, Train Loss:0.5712956190109253\n",
      "Epoch 0[10028/17270] Time:0.238, Train Loss:0.48327144980430603\n",
      "Epoch 0[10029/17270] Time:0.235, Train Loss:0.6517450213432312\n",
      "Epoch 0[10030/17270] Time:0.236, Train Loss:0.5239450335502625\n",
      "Epoch 0[10031/17270] Time:0.24, Train Loss:0.3995364010334015\n",
      "Epoch 0[10032/17270] Time:0.235, Train Loss:0.6903597712516785\n",
      "Epoch 0[10033/17270] Time:0.228, Train Loss:0.4512821435928345\n",
      "Epoch 0[10034/17270] Time:0.244, Train Loss:0.504201352596283\n",
      "Epoch 0[10035/17270] Time:0.232, Train Loss:0.6665017008781433\n",
      "Epoch 0[10036/17270] Time:0.235, Train Loss:0.378523588180542\n",
      "Epoch 0[10037/17270] Time:0.233, Train Loss:0.3361808955669403\n",
      "Epoch 0[10038/17270] Time:0.243, Train Loss:0.5682576894760132\n",
      "Epoch 0[10039/17270] Time:0.232, Train Loss:0.3314044177532196\n",
      "Epoch 0[10040/17270] Time:0.234, Train Loss:0.31118202209472656\n",
      "Epoch 0[10041/17270] Time:0.231, Train Loss:0.7063794732093811\n",
      "Epoch 0[10042/17270] Time:0.242, Train Loss:0.40350839495658875\n",
      "Epoch 0[10043/17270] Time:0.233, Train Loss:0.37298405170440674\n",
      "Epoch 0[10044/17270] Time:0.236, Train Loss:0.6907634139060974\n",
      "Epoch 0[10045/17270] Time:0.232, Train Loss:0.3261811137199402\n",
      "Epoch 0[10046/17270] Time:0.234, Train Loss:0.457950234413147\n",
      "Epoch 0[10047/17270] Time:0.227, Train Loss:0.6477670073509216\n",
      "Epoch 0[10048/17270] Time:0.239, Train Loss:0.7230544090270996\n",
      "Epoch 0[10049/17270] Time:0.238, Train Loss:1.0861480236053467\n",
      "Epoch 0[10050/17270] Time:0.236, Train Loss:0.6179067492485046\n",
      "Epoch 0[10051/17270] Time:0.248, Train Loss:0.7351834177970886\n",
      "Epoch 0[10052/17270] Time:0.244, Train Loss:0.5550794005393982\n",
      "Epoch 0[10053/17270] Time:0.238, Train Loss:0.5302750468254089\n",
      "Epoch 0[10054/17270] Time:0.235, Train Loss:0.7248451113700867\n",
      "Epoch 0[10055/17270] Time:0.239, Train Loss:0.8943164348602295\n",
      "Epoch 0[10056/17270] Time:0.236, Train Loss:0.7843820452690125\n",
      "Epoch 0[10057/17270] Time:0.233, Train Loss:0.5390577912330627\n",
      "Epoch 0[10058/17270] Time:0.233, Train Loss:0.761605978012085\n",
      "Epoch 0[10059/17270] Time:0.231, Train Loss:0.7498123049736023\n",
      "Epoch 0[10060/17270] Time:0.237, Train Loss:0.3585638403892517\n",
      "Epoch 0[10061/17270] Time:0.226, Train Loss:0.4210769534111023\n",
      "Epoch 0[10062/17270] Time:0.232, Train Loss:0.562785267829895\n",
      "Epoch 0[10063/17270] Time:0.237, Train Loss:0.4104972183704376\n",
      "Epoch 0[10064/17270] Time:0.23, Train Loss:0.6367411017417908\n",
      "Epoch 0[10065/17270] Time:0.232, Train Loss:0.34773385524749756\n",
      "Epoch 0[10066/17270] Time:0.235, Train Loss:0.7252832651138306\n",
      "Epoch 0[10067/17270] Time:0.233, Train Loss:0.45343419909477234\n",
      "Epoch 0[10068/17270] Time:0.237, Train Loss:0.872245728969574\n",
      "Epoch 0[10069/17270] Time:0.228, Train Loss:0.5176178812980652\n",
      "Epoch 0[10070/17270] Time:0.226, Train Loss:0.39211490750312805\n",
      "Epoch 0[10071/17270] Time:0.227, Train Loss:0.32634666562080383\n",
      "Epoch 0[10072/17270] Time:0.238, Train Loss:0.36540231108665466\n",
      "Epoch 0[10073/17270] Time:0.237, Train Loss:0.36545389890670776\n",
      "Epoch 0[10074/17270] Time:0.237, Train Loss:0.6161465644836426\n",
      "Epoch 0[10075/17270] Time:0.229, Train Loss:0.7118194103240967\n",
      "Epoch 0[10076/17270] Time:0.259, Train Loss:0.4101213812828064\n",
      "Epoch 0[10077/17270] Time:0.231, Train Loss:1.1006242036819458\n",
      "Epoch 0[10078/17270] Time:0.231, Train Loss:0.8711546659469604\n",
      "Epoch 0[10079/17270] Time:0.225, Train Loss:0.7205941677093506\n",
      "Epoch 0[10080/17270] Time:0.245, Train Loss:1.2427709102630615\n",
      "Epoch 0[10081/17270] Time:0.246, Train Loss:0.6463682055473328\n",
      "Epoch 0[10082/17270] Time:0.232, Train Loss:0.4996446371078491\n",
      "Epoch 0[10083/17270] Time:0.223, Train Loss:0.9937961101531982\n",
      "Epoch 0[10084/17270] Time:0.226, Train Loss:0.3566511869430542\n",
      "Epoch 0[10085/17270] Time:0.231, Train Loss:0.7315607070922852\n",
      "Epoch 0[10086/17270] Time:0.237, Train Loss:0.5031509399414062\n",
      "Epoch 0[10087/17270] Time:0.227, Train Loss:0.5970897078514099\n",
      "Epoch 0[10088/17270] Time:0.229, Train Loss:0.5786275267601013\n",
      "Epoch 0[10089/17270] Time:0.248, Train Loss:0.6416228413581848\n",
      "Epoch 0[10090/17270] Time:0.239, Train Loss:0.5091602206230164\n",
      "Epoch 0[10091/17270] Time:0.222, Train Loss:1.5554684400558472\n",
      "Epoch 0[10092/17270] Time:0.225, Train Loss:0.6470483541488647\n",
      "Epoch 0[10093/17270] Time:0.228, Train Loss:0.39934471249580383\n",
      "Epoch 0[10094/17270] Time:0.238, Train Loss:0.4327254295349121\n",
      "Epoch 0[10095/17270] Time:0.229, Train Loss:0.6333025097846985\n",
      "Epoch 0[10096/17270] Time:0.228, Train Loss:0.8118364810943604\n",
      "Epoch 0[10097/17270] Time:0.237, Train Loss:0.590478241443634\n",
      "Epoch 0[10098/17270] Time:0.238, Train Loss:0.39633065462112427\n",
      "Epoch 0[10099/17270] Time:0.229, Train Loss:0.4863561987876892\n",
      "Epoch 0[10100/17270] Time:0.239, Train Loss:0.7184249758720398\n",
      "Epoch 0[10101/17270] Time:0.231, Train Loss:0.7330276966094971\n",
      "Epoch 0[10102/17270] Time:0.227, Train Loss:0.3221284747123718\n",
      "Epoch 0[10103/17270] Time:0.238, Train Loss:0.7103570103645325\n",
      "Epoch 0[10104/17270] Time:0.233, Train Loss:0.34926924109458923\n",
      "Epoch 0[10105/17270] Time:0.237, Train Loss:0.49442702531814575\n",
      "Epoch 0[10106/17270] Time:0.234, Train Loss:0.5617414712905884\n",
      "Epoch 0[10107/17270] Time:0.226, Train Loss:0.6546434760093689\n",
      "Epoch 0[10108/17270] Time:0.239, Train Loss:0.3679787516593933\n",
      "Epoch 0[10109/17270] Time:0.242, Train Loss:0.40474826097488403\n",
      "Epoch 0[10110/17270] Time:0.223, Train Loss:0.9844784140586853\n",
      "Epoch 0[10111/17270] Time:0.245, Train Loss:0.3887750208377838\n",
      "Epoch 0[10112/17270] Time:0.233, Train Loss:1.0202391147613525\n",
      "Epoch 0[10113/17270] Time:0.226, Train Loss:0.3960995078086853\n",
      "Epoch 0[10114/17270] Time:0.231, Train Loss:0.33945176005363464\n",
      "Epoch 0[10115/17270] Time:0.245, Train Loss:0.6215674877166748\n",
      "Epoch 0[10116/17270] Time:0.237, Train Loss:0.7287216186523438\n",
      "Epoch 0[10117/17270] Time:0.232, Train Loss:0.33380481600761414\n",
      "Epoch 0[10118/17270] Time:0.226, Train Loss:0.591729998588562\n",
      "Epoch 0[10119/17270] Time:0.245, Train Loss:0.3486020565032959\n",
      "Epoch 0[10120/17270] Time:0.236, Train Loss:0.4638691544532776\n",
      "Epoch 0[10121/17270] Time:0.235, Train Loss:0.46144258975982666\n",
      "Epoch 0[10122/17270] Time:0.235, Train Loss:0.3555428087711334\n",
      "Epoch 0[10123/17270] Time:0.236, Train Loss:0.42776355147361755\n",
      "Epoch 0[10124/17270] Time:0.24, Train Loss:0.3934076130390167\n",
      "Epoch 0[10125/17270] Time:0.234, Train Loss:1.2562274932861328\n",
      "Epoch 0[10126/17270] Time:0.24, Train Loss:0.6657480001449585\n",
      "Epoch 0[10127/17270] Time:0.235, Train Loss:0.6026022434234619\n",
      "Epoch 0[10128/17270] Time:0.233, Train Loss:0.36687320470809937\n",
      "Epoch 0[10129/17270] Time:0.241, Train Loss:0.4124191701412201\n",
      "Epoch 0[10130/17270] Time:0.244, Train Loss:0.41404837369918823\n",
      "Epoch 0[10131/17270] Time:0.235, Train Loss:0.9334229230880737\n",
      "Epoch 0[10132/17270] Time:0.245, Train Loss:1.0434033870697021\n",
      "Epoch 0[10133/17270] Time:0.239, Train Loss:1.0561631917953491\n",
      "Epoch 0[10134/17270] Time:0.233, Train Loss:0.7176101207733154\n",
      "Epoch 0[10135/17270] Time:0.235, Train Loss:0.540398895740509\n",
      "Epoch 0[10136/17270] Time:0.237, Train Loss:0.6957221031188965\n",
      "Epoch 0[10137/17270] Time:0.231, Train Loss:0.4116745591163635\n",
      "Epoch 0[10138/17270] Time:0.232, Train Loss:0.41764622926712036\n",
      "Epoch 0[10139/17270] Time:0.229, Train Loss:0.5250008702278137\n",
      "Epoch 0[10140/17270] Time:0.234, Train Loss:0.8665210604667664\n",
      "Epoch 0[10141/17270] Time:0.241, Train Loss:0.9664303660392761\n",
      "Epoch 0[10142/17270] Time:0.236, Train Loss:0.5994314551353455\n",
      "Epoch 0[10143/17270] Time:0.231, Train Loss:0.6691029071807861\n",
      "Epoch 0[10144/17270] Time:0.235, Train Loss:0.4557648301124573\n",
      "Epoch 0[10145/17270] Time:0.233, Train Loss:0.45548251271247864\n",
      "Epoch 0[10146/17270] Time:0.24, Train Loss:0.4576590657234192\n",
      "Epoch 0[10147/17270] Time:0.239, Train Loss:0.7580090165138245\n",
      "Epoch 0[10148/17270] Time:0.237, Train Loss:1.1072596311569214\n",
      "Epoch 0[10149/17270] Time:0.236, Train Loss:0.566564679145813\n",
      "Epoch 0[10150/17270] Time:0.237, Train Loss:0.43893489241600037\n",
      "Epoch 0[10151/17270] Time:0.23, Train Loss:0.4602321684360504\n",
      "Epoch 0[10152/17270] Time:0.237, Train Loss:0.9363714456558228\n",
      "Epoch 0[10153/17270] Time:0.238, Train Loss:0.5138190984725952\n",
      "Epoch 0[10154/17270] Time:0.238, Train Loss:0.682762086391449\n",
      "Epoch 0[10155/17270] Time:0.235, Train Loss:0.4463259279727936\n",
      "Epoch 0[10156/17270] Time:0.236, Train Loss:0.483150839805603\n",
      "Epoch 0[10157/17270] Time:0.239, Train Loss:1.5492265224456787\n",
      "Epoch 0[10158/17270] Time:0.235, Train Loss:0.6590420603752136\n",
      "Epoch 0[10159/17270] Time:0.239, Train Loss:0.47870945930480957\n",
      "Epoch 0[10160/17270] Time:0.237, Train Loss:0.6890683770179749\n",
      "Epoch 0[10161/17270] Time:0.23, Train Loss:0.5156030654907227\n",
      "Epoch 0[10162/17270] Time:0.232, Train Loss:0.4299275577068329\n",
      "Epoch 0[10163/17270] Time:0.237, Train Loss:1.0768752098083496\n",
      "Epoch 0[10164/17270] Time:0.237, Train Loss:0.5584768652915955\n",
      "Epoch 0[10165/17270] Time:0.23, Train Loss:0.3993229269981384\n",
      "Epoch 0[10166/17270] Time:0.243, Train Loss:0.525667667388916\n",
      "Epoch 0[10167/17270] Time:0.233, Train Loss:0.4305424690246582\n",
      "Epoch 0[10168/17270] Time:0.224, Train Loss:0.68817138671875\n",
      "Epoch 0[10169/17270] Time:0.249, Train Loss:0.6409763693809509\n",
      "Epoch 0[10170/17270] Time:0.241, Train Loss:0.5380616188049316\n",
      "Epoch 0[10171/17270] Time:0.24, Train Loss:0.683444082736969\n",
      "Epoch 0[10172/17270] Time:0.236, Train Loss:0.49759334325790405\n",
      "Epoch 0[10173/17270] Time:0.226, Train Loss:0.5147997140884399\n",
      "Epoch 0[10174/17270] Time:0.245, Train Loss:0.7089881300926208\n",
      "Epoch 0[10175/17270] Time:0.243, Train Loss:0.65119469165802\n",
      "Epoch 0[10176/17270] Time:0.234, Train Loss:0.3456748425960541\n",
      "Epoch 0[10177/17270] Time:0.219, Train Loss:0.6302202939987183\n",
      "Epoch 0[10178/17270] Time:0.228, Train Loss:0.5831119418144226\n",
      "Epoch 0[10179/17270] Time:0.247, Train Loss:0.5499798655509949\n",
      "Epoch 0[10180/17270] Time:0.236, Train Loss:0.3488462567329407\n",
      "Epoch 0[10181/17270] Time:0.229, Train Loss:0.4634060859680176\n",
      "Epoch 0[10182/17270] Time:0.232, Train Loss:0.3213652968406677\n",
      "Epoch 0[10183/17270] Time:0.238, Train Loss:0.4740707576274872\n",
      "Epoch 0[10184/17270] Time:0.236, Train Loss:0.35246115922927856\n",
      "Epoch 0[10185/17270] Time:0.239, Train Loss:0.4329080581665039\n",
      "Epoch 0[10186/17270] Time:0.242, Train Loss:0.9940855503082275\n",
      "Epoch 0[10187/17270] Time:0.235, Train Loss:0.41517946124076843\n",
      "Epoch 0[10188/17270] Time:0.235, Train Loss:0.4198058843612671\n",
      "Epoch 0[10189/17270] Time:0.24, Train Loss:0.5050792694091797\n",
      "Epoch 0[10190/17270] Time:0.234, Train Loss:0.5938244462013245\n",
      "Epoch 0[10191/17270] Time:0.223, Train Loss:0.26912927627563477\n",
      "Epoch 0[10192/17270] Time:0.232, Train Loss:0.462696373462677\n",
      "Epoch 0[10193/17270] Time:0.234, Train Loss:0.38706085085868835\n",
      "Epoch 0[10194/17270] Time:0.238, Train Loss:0.7800890803337097\n",
      "Epoch 0[10195/17270] Time:0.23, Train Loss:0.4927338659763336\n",
      "Epoch 0[10196/17270] Time:0.233, Train Loss:0.6102875471115112\n",
      "Epoch 0[10197/17270] Time:0.23, Train Loss:0.6030164957046509\n",
      "Epoch 0[10198/17270] Time:0.228, Train Loss:0.5133399963378906\n",
      "Epoch 0[10199/17270] Time:0.233, Train Loss:0.3408896327018738\n",
      "Epoch 0[10200/17270] Time:0.236, Train Loss:0.25947755575180054\n",
      "Epoch 0[10201/17270] Time:0.237, Train Loss:0.9460326433181763\n",
      "Epoch 0[10202/17270] Time:0.231, Train Loss:0.37878355383872986\n",
      "Epoch 0[10203/17270] Time:0.233, Train Loss:0.42117369174957275\n",
      "Epoch 0[10204/17270] Time:0.23, Train Loss:0.4448111653327942\n",
      "Epoch 0[10205/17270] Time:0.228, Train Loss:0.6660977005958557\n",
      "Epoch 0[10206/17270] Time:0.238, Train Loss:0.574815034866333\n",
      "Epoch 0[10207/17270] Time:0.23, Train Loss:0.7249681353569031\n",
      "Epoch 0[10208/17270] Time:0.229, Train Loss:0.4923349916934967\n",
      "Epoch 0[10209/17270] Time:0.237, Train Loss:0.7889470458030701\n",
      "Epoch 0[10210/17270] Time:0.238, Train Loss:0.502429723739624\n",
      "Epoch 0[10211/17270] Time:0.231, Train Loss:0.4563044011592865\n",
      "Epoch 0[10212/17270] Time:0.236, Train Loss:0.6371179819107056\n",
      "Epoch 0[10213/17270] Time:0.249, Train Loss:0.5005167126655579\n",
      "Epoch 0[10214/17270] Time:0.231, Train Loss:1.316446304321289\n",
      "Epoch 0[10215/17270] Time:0.238, Train Loss:0.34372952580451965\n",
      "Epoch 0[10216/17270] Time:0.229, Train Loss:0.41485875844955444\n",
      "Epoch 0[10217/17270] Time:0.229, Train Loss:0.5816894769668579\n",
      "Epoch 0[10218/17270] Time:0.231, Train Loss:0.5888686180114746\n",
      "Epoch 0[10219/17270] Time:0.227, Train Loss:0.4025826156139374\n",
      "Epoch 0[10220/17270] Time:0.232, Train Loss:0.4137396216392517\n",
      "Epoch 0[10221/17270] Time:0.228, Train Loss:0.5674872994422913\n",
      "Epoch 0[10222/17270] Time:0.232, Train Loss:0.4266280233860016\n",
      "Epoch 0[10223/17270] Time:0.237, Train Loss:0.6242431402206421\n",
      "Epoch 0[10224/17270] Time:0.236, Train Loss:0.7508301138877869\n",
      "Epoch 0[10225/17270] Time:0.236, Train Loss:0.5554326772689819\n",
      "Epoch 0[10226/17270] Time:0.231, Train Loss:0.6750553846359253\n",
      "Epoch 0[10227/17270] Time:0.228, Train Loss:0.8802700042724609\n",
      "Epoch 0[10228/17270] Time:0.238, Train Loss:0.24932324886322021\n",
      "Epoch 0[10229/17270] Time:0.228, Train Loss:0.35624754428863525\n",
      "Epoch 0[10230/17270] Time:0.231, Train Loss:0.5163581967353821\n",
      "Epoch 0[10231/17270] Time:0.238, Train Loss:0.5121451616287231\n",
      "Epoch 0[10232/17270] Time:0.233, Train Loss:0.8474889397621155\n",
      "Epoch 0[10233/17270] Time:0.227, Train Loss:0.42859143018722534\n",
      "Epoch 0[10234/17270] Time:0.237, Train Loss:0.5236797332763672\n",
      "Epoch 0[10235/17270] Time:0.226, Train Loss:0.4629330039024353\n",
      "Epoch 0[10236/17270] Time:0.234, Train Loss:0.5213286280632019\n",
      "Epoch 0[10237/17270] Time:0.228, Train Loss:0.3421984910964966\n",
      "Epoch 0[10238/17270] Time:0.237, Train Loss:0.639442503452301\n",
      "Epoch 0[10239/17270] Time:0.222, Train Loss:1.073477029800415\n",
      "Epoch 0[10240/17270] Time:0.242, Train Loss:0.3892955780029297\n",
      "Epoch 0[10241/17270] Time:0.24, Train Loss:0.24443471431732178\n",
      "Epoch 0[10242/17270] Time:0.234, Train Loss:0.587723433971405\n",
      "Epoch 0[10243/17270] Time:0.238, Train Loss:1.0297143459320068\n",
      "Epoch 0[10244/17270] Time:0.226, Train Loss:0.41102975606918335\n",
      "Epoch 0[10245/17270] Time:0.235, Train Loss:0.6046118140220642\n",
      "Epoch 0[10246/17270] Time:0.235, Train Loss:0.35105201601982117\n",
      "Epoch 0[10247/17270] Time:0.222, Train Loss:0.38189229369163513\n",
      "Epoch 0[10248/17270] Time:0.23, Train Loss:0.4260141849517822\n",
      "Epoch 0[10249/17270] Time:0.221, Train Loss:0.3364379405975342\n",
      "Epoch 0[10250/17270] Time:0.232, Train Loss:0.7645339965820312\n",
      "Epoch 0[10251/17270] Time:0.237, Train Loss:0.36919695138931274\n",
      "Epoch 0[10252/17270] Time:0.248, Train Loss:1.0182771682739258\n",
      "Epoch 0[10253/17270] Time:0.241, Train Loss:0.5928435325622559\n",
      "Epoch 0[10254/17270] Time:0.237, Train Loss:0.5555458068847656\n",
      "Epoch 0[10255/17270] Time:0.221, Train Loss:1.3940433263778687\n",
      "Epoch 0[10256/17270] Time:0.238, Train Loss:0.44133153557777405\n",
      "Epoch 0[10257/17270] Time:0.239, Train Loss:0.42679473757743835\n",
      "Epoch 0[10258/17270] Time:0.23, Train Loss:0.4572790563106537\n",
      "Epoch 0[10259/17270] Time:0.231, Train Loss:1.1476976871490479\n",
      "Epoch 0[10260/17270] Time:0.229, Train Loss:0.4986894428730011\n",
      "Epoch 0[10261/17270] Time:0.236, Train Loss:0.481884628534317\n",
      "Epoch 0[10262/17270] Time:0.239, Train Loss:0.5927582383155823\n",
      "Epoch 0[10263/17270] Time:0.235, Train Loss:1.0883305072784424\n",
      "Epoch 0[10264/17270] Time:0.237, Train Loss:0.4000706970691681\n",
      "Epoch 0[10265/17270] Time:0.241, Train Loss:0.6189208626747131\n",
      "Epoch 0[10266/17270] Time:0.229, Train Loss:0.7588877081871033\n",
      "Epoch 0[10267/17270] Time:0.238, Train Loss:0.4516381323337555\n",
      "Epoch 0[10268/17270] Time:0.243, Train Loss:1.3161967992782593\n",
      "Epoch 0[10269/17270] Time:0.233, Train Loss:0.30531734228134155\n",
      "Epoch 0[10270/17270] Time:0.233, Train Loss:0.3949796259403229\n",
      "Epoch 0[10271/17270] Time:0.235, Train Loss:0.5044736266136169\n",
      "Epoch 0[10272/17270] Time:0.233, Train Loss:0.6192965507507324\n",
      "Epoch 0[10273/17270] Time:0.233, Train Loss:1.031137228012085\n",
      "Epoch 0[10274/17270] Time:0.229, Train Loss:0.437900573015213\n",
      "Epoch 0[10275/17270] Time:0.235, Train Loss:1.3636173009872437\n",
      "Epoch 0[10276/17270] Time:0.234, Train Loss:0.4766107201576233\n",
      "Epoch 0[10277/17270] Time:0.238, Train Loss:1.0927202701568604\n",
      "Epoch 0[10278/17270] Time:0.229, Train Loss:0.38379669189453125\n",
      "Epoch 0[10279/17270] Time:0.234, Train Loss:0.5790713429450989\n",
      "Epoch 0[10280/17270] Time:0.236, Train Loss:0.8305701017379761\n",
      "Epoch 0[10281/17270] Time:0.237, Train Loss:0.35262811183929443\n",
      "Epoch 0[10282/17270] Time:0.239, Train Loss:0.6737259030342102\n",
      "Epoch 0[10283/17270] Time:0.231, Train Loss:0.9074179530143738\n",
      "Epoch 0[10284/17270] Time:0.229, Train Loss:0.5353390574455261\n",
      "Epoch 0[10285/17270] Time:0.227, Train Loss:0.5691073536872864\n",
      "Epoch 0[10286/17270] Time:0.238, Train Loss:0.48912516236305237\n",
      "Epoch 0[10287/17270] Time:0.235, Train Loss:0.6980728507041931\n",
      "Epoch 0[10288/17270] Time:0.238, Train Loss:0.5014543533325195\n",
      "Epoch 0[10289/17270] Time:0.23, Train Loss:0.4204096794128418\n",
      "Epoch 0[10290/17270] Time:0.236, Train Loss:0.6552732586860657\n",
      "Epoch 0[10291/17270] Time:0.236, Train Loss:0.42640742659568787\n",
      "Epoch 0[10292/17270] Time:0.237, Train Loss:1.2276216745376587\n",
      "Epoch 0[10293/17270] Time:0.231, Train Loss:0.9391412734985352\n",
      "Epoch 0[10294/17270] Time:0.231, Train Loss:0.6705290675163269\n",
      "Epoch 0[10295/17270] Time:0.239, Train Loss:0.4876357614994049\n",
      "Epoch 0[10296/17270] Time:0.228, Train Loss:0.6389085054397583\n",
      "Epoch 0[10297/17270] Time:0.236, Train Loss:0.7384692430496216\n",
      "Epoch 0[10298/17270] Time:0.232, Train Loss:0.6209771633148193\n",
      "Epoch 0[10299/17270] Time:0.234, Train Loss:0.7103856205940247\n",
      "Epoch 0[10300/17270] Time:0.238, Train Loss:0.530824601650238\n",
      "Epoch 0[10301/17270] Time:0.23, Train Loss:0.5017896890640259\n",
      "Epoch 0[10302/17270] Time:0.238, Train Loss:0.560610294342041\n",
      "Epoch 0[10303/17270] Time:0.231, Train Loss:0.7519756555557251\n",
      "Epoch 0[10304/17270] Time:0.233, Train Loss:0.7723417282104492\n",
      "Epoch 0[10305/17270] Time:0.23, Train Loss:0.6644173264503479\n",
      "Epoch 0[10306/17270] Time:0.239, Train Loss:0.3374063968658447\n",
      "Epoch 0[10307/17270] Time:0.23, Train Loss:0.73014235496521\n",
      "Epoch 0[10308/17270] Time:0.238, Train Loss:0.66401207447052\n",
      "Epoch 0[10309/17270] Time:0.231, Train Loss:0.8306778073310852\n",
      "Epoch 0[10310/17270] Time:0.229, Train Loss:0.8622463345527649\n",
      "Epoch 0[10311/17270] Time:0.237, Train Loss:0.9569311738014221\n",
      "Epoch 0[10312/17270] Time:0.239, Train Loss:0.48917508125305176\n",
      "Epoch 0[10313/17270] Time:0.232, Train Loss:0.6400461792945862\n",
      "Epoch 0[10314/17270] Time:0.236, Train Loss:0.45807337760925293\n",
      "Epoch 0[10315/17270] Time:0.228, Train Loss:0.6209897994995117\n",
      "Epoch 0[10316/17270] Time:0.237, Train Loss:0.3627520501613617\n",
      "Epoch 0[10317/17270] Time:0.239, Train Loss:0.6224715113639832\n",
      "Epoch 0[10318/17270] Time:0.235, Train Loss:0.5402414202690125\n",
      "Epoch 0[10319/17270] Time:0.238, Train Loss:0.8677918314933777\n",
      "Epoch 0[10320/17270] Time:0.236, Train Loss:0.5923396944999695\n",
      "Epoch 0[10321/17270] Time:0.233, Train Loss:0.3987477123737335\n",
      "Epoch 0[10322/17270] Time:0.239, Train Loss:0.48205146193504333\n",
      "Epoch 0[10323/17270] Time:0.247, Train Loss:0.5272944569587708\n",
      "Epoch 0[10324/17270] Time:0.23, Train Loss:0.6026996970176697\n",
      "Epoch 0[10325/17270] Time:0.241, Train Loss:0.520325243473053\n",
      "Epoch 0[10326/17270] Time:0.23, Train Loss:0.44324544072151184\n",
      "Epoch 0[10327/17270] Time:0.232, Train Loss:0.37939101457595825\n",
      "Epoch 0[10328/17270] Time:0.236, Train Loss:0.48764127492904663\n",
      "Epoch 0[10329/17270] Time:0.227, Train Loss:0.3938848078250885\n",
      "Epoch 0[10330/17270] Time:0.245, Train Loss:0.5580750703811646\n",
      "Epoch 0[10331/17270] Time:0.226, Train Loss:0.6148657202720642\n",
      "Epoch 0[10332/17270] Time:0.247, Train Loss:0.38466814160346985\n",
      "Epoch 0[10333/17270] Time:0.223, Train Loss:0.38169053196907043\n",
      "Epoch 0[10334/17270] Time:0.234, Train Loss:0.5940431952476501\n",
      "Epoch 0[10335/17270] Time:0.237, Train Loss:0.5322484374046326\n",
      "Epoch 0[10336/17270] Time:0.235, Train Loss:0.32524198293685913\n",
      "Epoch 0[10337/17270] Time:0.236, Train Loss:0.4963841140270233\n",
      "Epoch 0[10338/17270] Time:0.223, Train Loss:0.551986813545227\n",
      "Epoch 0[10339/17270] Time:0.239, Train Loss:0.38770025968551636\n",
      "Epoch 0[10340/17270] Time:0.237, Train Loss:0.7115475535392761\n",
      "Epoch 0[10341/17270] Time:0.237, Train Loss:0.4696132242679596\n",
      "Epoch 0[10342/17270] Time:0.239, Train Loss:0.32913443446159363\n",
      "Epoch 0[10343/17270] Time:0.24, Train Loss:0.6662579774856567\n",
      "Epoch 0[10344/17270] Time:0.229, Train Loss:0.5267850160598755\n",
      "Epoch 0[10345/17270] Time:0.238, Train Loss:1.0116782188415527\n",
      "Epoch 0[10346/17270] Time:0.226, Train Loss:0.2936347723007202\n",
      "Epoch 0[10347/17270] Time:0.225, Train Loss:0.6499246954917908\n",
      "Epoch 0[10348/17270] Time:0.23, Train Loss:0.5052617192268372\n",
      "Epoch 0[10349/17270] Time:0.226, Train Loss:0.8966092467308044\n",
      "Epoch 0[10350/17270] Time:0.237, Train Loss:0.6244174242019653\n",
      "Epoch 0[10351/17270] Time:0.23, Train Loss:0.7938374280929565\n",
      "Epoch 0[10352/17270] Time:0.228, Train Loss:0.9330123662948608\n",
      "Epoch 0[10353/17270] Time:0.229, Train Loss:0.7143881320953369\n",
      "Epoch 0[10354/17270] Time:0.228, Train Loss:0.45122167468070984\n",
      "Epoch 0[10355/17270] Time:0.229, Train Loss:0.425731360912323\n",
      "Epoch 0[10356/17270] Time:0.227, Train Loss:0.25874990224838257\n",
      "Epoch 0[10357/17270] Time:0.229, Train Loss:0.30012139678001404\n",
      "Epoch 0[10358/17270] Time:0.237, Train Loss:0.45745986700057983\n",
      "Epoch 0[10359/17270] Time:0.224, Train Loss:0.73332279920578\n",
      "Epoch 0[10360/17270] Time:0.229, Train Loss:0.37372440099716187\n",
      "Epoch 0[10361/17270] Time:0.236, Train Loss:0.5852749943733215\n",
      "Epoch 0[10362/17270] Time:0.235, Train Loss:0.6237205266952515\n",
      "Epoch 0[10363/17270] Time:0.234, Train Loss:0.505790114402771\n",
      "Epoch 0[10364/17270] Time:0.232, Train Loss:0.20223605632781982\n",
      "Epoch 0[10365/17270] Time:0.229, Train Loss:0.474265456199646\n",
      "Epoch 0[10366/17270] Time:0.238, Train Loss:0.5435879826545715\n",
      "Epoch 0[10367/17270] Time:0.238, Train Loss:2.168853998184204\n",
      "Epoch 0[10368/17270] Time:0.232, Train Loss:1.1376829147338867\n",
      "Epoch 0[10369/17270] Time:0.25, Train Loss:0.5071613788604736\n",
      "Epoch 0[10370/17270] Time:0.235, Train Loss:0.7905491590499878\n",
      "Epoch 0[10371/17270] Time:0.231, Train Loss:0.4610828161239624\n",
      "Epoch 0[10372/17270] Time:0.236, Train Loss:0.35783061385154724\n",
      "Epoch 0[10373/17270] Time:0.235, Train Loss:1.2018283605575562\n",
      "Epoch 0[10374/17270] Time:0.224, Train Loss:0.3047056198120117\n",
      "Epoch 0[10375/17270] Time:0.226, Train Loss:0.4742756485939026\n",
      "Epoch 0[10376/17270] Time:0.234, Train Loss:0.8249630928039551\n",
      "Epoch 0[10377/17270] Time:0.233, Train Loss:0.5505694150924683\n",
      "Epoch 0[10378/17270] Time:0.238, Train Loss:0.46651721000671387\n",
      "Epoch 0[10379/17270] Time:0.245, Train Loss:0.5827969908714294\n",
      "Epoch 0[10380/17270] Time:0.236, Train Loss:0.7794444561004639\n",
      "Epoch 0[10381/17270] Time:0.223, Train Loss:0.7239559888839722\n",
      "Epoch 0[10382/17270] Time:0.238, Train Loss:0.46969619393348694\n",
      "Epoch 0[10383/17270] Time:0.224, Train Loss:0.4639115631580353\n",
      "Epoch 0[10384/17270] Time:0.234, Train Loss:0.5658692717552185\n",
      "Epoch 0[10385/17270] Time:0.225, Train Loss:0.3355185389518738\n",
      "Epoch 0[10386/17270] Time:0.24, Train Loss:0.6419272422790527\n",
      "Epoch 0[10387/17270] Time:0.233, Train Loss:0.5162010192871094\n",
      "Epoch 0[10388/17270] Time:0.236, Train Loss:0.6298074126243591\n",
      "Epoch 0[10389/17270] Time:0.225, Train Loss:0.4881037175655365\n",
      "Epoch 0[10390/17270] Time:0.242, Train Loss:1.1971406936645508\n",
      "Epoch 0[10391/17270] Time:0.233, Train Loss:0.8453336358070374\n",
      "Epoch 0[10392/17270] Time:0.238, Train Loss:0.35387781262397766\n",
      "Epoch 0[10393/17270] Time:0.227, Train Loss:0.5087926387786865\n",
      "Epoch 0[10394/17270] Time:0.225, Train Loss:0.354051798582077\n",
      "Epoch 0[10395/17270] Time:0.237, Train Loss:0.5339376330375671\n",
      "Epoch 0[10396/17270] Time:0.236, Train Loss:0.7712435722351074\n",
      "Epoch 0[10397/17270] Time:0.233, Train Loss:0.42814338207244873\n",
      "Epoch 0[10398/17270] Time:0.237, Train Loss:0.6477453112602234\n",
      "Epoch 0[10399/17270] Time:0.228, Train Loss:0.6070922017097473\n",
      "Epoch 0[10400/17270] Time:0.232, Train Loss:0.5759401321411133\n",
      "Epoch 0[10401/17270] Time:0.229, Train Loss:0.5301356911659241\n",
      "Epoch 0[10402/17270] Time:0.237, Train Loss:0.6584641933441162\n",
      "Epoch 0[10403/17270] Time:0.236, Train Loss:0.592993438243866\n",
      "Epoch 0[10404/17270] Time:0.236, Train Loss:0.5562906861305237\n",
      "Epoch 0[10405/17270] Time:0.236, Train Loss:0.9870802164077759\n",
      "Epoch 0[10406/17270] Time:0.238, Train Loss:0.5899627804756165\n",
      "Epoch 0[10407/17270] Time:0.238, Train Loss:1.2702730894088745\n",
      "Epoch 0[10408/17270] Time:0.23, Train Loss:0.8810585737228394\n",
      "Epoch 0[10409/17270] Time:0.238, Train Loss:0.7783654928207397\n",
      "Epoch 0[10410/17270] Time:0.238, Train Loss:0.48993414640426636\n",
      "Epoch 0[10411/17270] Time:0.229, Train Loss:0.6122832894325256\n",
      "Epoch 0[10412/17270] Time:0.228, Train Loss:0.582843542098999\n",
      "Epoch 0[10413/17270] Time:0.237, Train Loss:0.6422240138053894\n",
      "Epoch 0[10414/17270] Time:0.235, Train Loss:0.3600432872772217\n",
      "Epoch 0[10415/17270] Time:0.237, Train Loss:1.0429943799972534\n",
      "Epoch 0[10416/17270] Time:0.236, Train Loss:0.29713210463523865\n",
      "Epoch 0[10417/17270] Time:0.244, Train Loss:0.34153616428375244\n",
      "Epoch 0[10418/17270] Time:0.233, Train Loss:0.45182937383651733\n",
      "Epoch 0[10419/17270] Time:0.237, Train Loss:0.5539929866790771\n",
      "Epoch 0[10420/17270] Time:0.242, Train Loss:0.35263457894325256\n",
      "Epoch 0[10421/17270] Time:0.239, Train Loss:0.542594313621521\n",
      "Epoch 0[10422/17270] Time:0.227, Train Loss:0.6074213981628418\n",
      "Epoch 0[10423/17270] Time:0.24, Train Loss:0.35269054770469666\n",
      "Epoch 0[10424/17270] Time:0.228, Train Loss:0.5968053936958313\n",
      "Epoch 0[10425/17270] Time:0.236, Train Loss:1.5100315809249878\n",
      "Epoch 0[10426/17270] Time:0.235, Train Loss:0.632597804069519\n",
      "Epoch 0[10427/17270] Time:0.222, Train Loss:0.8508479595184326\n",
      "Epoch 0[10428/17270] Time:0.236, Train Loss:0.5531936883926392\n",
      "Epoch 0[10429/17270] Time:0.229, Train Loss:0.7649167776107788\n",
      "Epoch 0[10430/17270] Time:0.238, Train Loss:0.5058916807174683\n",
      "Epoch 0[10431/17270] Time:0.235, Train Loss:0.5411760210990906\n",
      "Epoch 0[10432/17270] Time:0.236, Train Loss:0.36954671144485474\n",
      "Epoch 0[10433/17270] Time:0.236, Train Loss:0.5884860754013062\n",
      "Epoch 0[10434/17270] Time:0.24, Train Loss:0.8155938982963562\n",
      "Epoch 0[10435/17270] Time:0.229, Train Loss:1.3024225234985352\n",
      "Epoch 0[10436/17270] Time:0.233, Train Loss:0.5021079778671265\n",
      "Epoch 0[10437/17270] Time:0.232, Train Loss:0.5994598865509033\n",
      "Epoch 0[10438/17270] Time:0.237, Train Loss:0.4549606144428253\n",
      "Epoch 0[10439/17270] Time:0.227, Train Loss:0.4586261510848999\n",
      "Epoch 0[10440/17270] Time:0.231, Train Loss:0.8980197310447693\n",
      "Epoch 0[10441/17270] Time:0.227, Train Loss:1.0246665477752686\n",
      "Epoch 0[10442/17270] Time:0.24, Train Loss:0.41495490074157715\n",
      "Epoch 0[10443/17270] Time:0.232, Train Loss:0.494451642036438\n",
      "Epoch 0[10444/17270] Time:0.235, Train Loss:0.5923027992248535\n",
      "Epoch 0[10445/17270] Time:0.236, Train Loss:0.3040122985839844\n",
      "Epoch 0[10446/17270] Time:0.237, Train Loss:0.5828562378883362\n",
      "Epoch 0[10447/17270] Time:0.235, Train Loss:0.3543429672718048\n",
      "Epoch 0[10448/17270] Time:0.236, Train Loss:0.3838634490966797\n",
      "Epoch 0[10449/17270] Time:0.23, Train Loss:0.490936279296875\n",
      "Epoch 0[10450/17270] Time:0.239, Train Loss:0.6012027263641357\n",
      "Epoch 0[10451/17270] Time:0.237, Train Loss:0.5140241384506226\n",
      "Epoch 0[10452/17270] Time:0.232, Train Loss:0.4143800139427185\n",
      "Epoch 0[10453/17270] Time:0.236, Train Loss:0.6711589097976685\n",
      "Epoch 0[10454/17270] Time:0.229, Train Loss:0.586243212223053\n",
      "Epoch 0[10455/17270] Time:0.232, Train Loss:0.7646717429161072\n",
      "Epoch 0[10456/17270] Time:0.249, Train Loss:0.6968940496444702\n",
      "Epoch 0[10457/17270] Time:0.243, Train Loss:0.5061151385307312\n",
      "Epoch 0[10458/17270] Time:0.232, Train Loss:0.5238626599311829\n",
      "Epoch 0[10459/17270] Time:0.227, Train Loss:0.4226315915584564\n",
      "Epoch 0[10460/17270] Time:0.236, Train Loss:1.6803433895111084\n",
      "Epoch 0[10461/17270] Time:0.226, Train Loss:0.2270040512084961\n",
      "Epoch 0[10462/17270] Time:0.23, Train Loss:0.48326635360717773\n",
      "Epoch 0[10463/17270] Time:0.224, Train Loss:0.595162034034729\n",
      "Epoch 0[10464/17270] Time:0.245, Train Loss:0.6795394420623779\n",
      "Epoch 0[10465/17270] Time:0.227, Train Loss:0.7101060152053833\n",
      "Epoch 0[10466/17270] Time:0.228, Train Loss:0.4313352108001709\n",
      "Epoch 0[10467/17270] Time:0.246, Train Loss:0.4419599175453186\n",
      "Epoch 0[10468/17270] Time:0.237, Train Loss:0.4483484625816345\n",
      "Epoch 0[10469/17270] Time:0.229, Train Loss:0.5078513026237488\n",
      "Epoch 0[10470/17270] Time:0.236, Train Loss:0.4497663378715515\n",
      "Epoch 0[10471/17270] Time:0.234, Train Loss:0.39663442969322205\n",
      "Epoch 0[10472/17270] Time:0.251, Train Loss:0.46258896589279175\n",
      "Epoch 0[10473/17270] Time:0.225, Train Loss:0.4017288386821747\n",
      "Epoch 0[10474/17270] Time:0.227, Train Loss:0.5350489616394043\n",
      "Epoch 0[10475/17270] Time:0.243, Train Loss:0.2655058205127716\n",
      "Epoch 0[10476/17270] Time:0.248, Train Loss:0.7280025482177734\n",
      "Epoch 0[10477/17270] Time:0.229, Train Loss:0.3866952359676361\n",
      "Epoch 0[10478/17270] Time:0.231, Train Loss:0.5292845368385315\n",
      "Epoch 0[10479/17270] Time:0.228, Train Loss:0.5925281047821045\n",
      "Epoch 0[10480/17270] Time:0.229, Train Loss:0.3797967731952667\n",
      "Epoch 0[10481/17270] Time:0.23, Train Loss:0.8081212043762207\n",
      "Epoch 0[10482/17270] Time:0.229, Train Loss:0.6415724158287048\n",
      "Epoch 0[10483/17270] Time:0.238, Train Loss:0.5788694620132446\n",
      "Epoch 0[10484/17270] Time:0.227, Train Loss:0.4254568815231323\n",
      "Epoch 0[10485/17270] Time:0.232, Train Loss:0.5207066535949707\n",
      "Epoch 0[10486/17270] Time:0.254, Train Loss:0.5227292776107788\n",
      "Epoch 0[10487/17270] Time:0.233, Train Loss:0.44552183151245117\n",
      "Epoch 0[10488/17270] Time:0.234, Train Loss:0.48169052600860596\n",
      "Epoch 0[10489/17270] Time:0.234, Train Loss:1.6979471445083618\n",
      "Epoch 0[10490/17270] Time:0.237, Train Loss:0.6207587718963623\n",
      "Epoch 0[10491/17270] Time:0.234, Train Loss:0.4837692081928253\n",
      "Epoch 0[10492/17270] Time:0.234, Train Loss:0.5383957624435425\n",
      "Epoch 0[10493/17270] Time:0.233, Train Loss:0.4933187663555145\n",
      "Epoch 0[10494/17270] Time:0.232, Train Loss:0.9203514456748962\n",
      "Epoch 0[10495/17270] Time:0.233, Train Loss:0.37253376841545105\n",
      "Epoch 0[10496/17270] Time:0.233, Train Loss:0.7328221201896667\n",
      "Epoch 0[10497/17270] Time:0.241, Train Loss:0.5814098119735718\n",
      "Epoch 0[10498/17270] Time:0.222, Train Loss:0.473967045545578\n",
      "Epoch 0[10499/17270] Time:0.232, Train Loss:0.628013014793396\n",
      "Epoch 0[10500/17270] Time:0.243, Train Loss:0.5120840668678284\n",
      "Epoch 0[10501/17270] Time:0.238, Train Loss:0.4745560884475708\n",
      "Epoch 0[10502/17270] Time:0.227, Train Loss:0.4459781348705292\n",
      "Epoch 0[10503/17270] Time:0.246, Train Loss:0.52751624584198\n",
      "Epoch 0[10504/17270] Time:0.223, Train Loss:0.5888853669166565\n",
      "Epoch 0[10505/17270] Time:0.255, Train Loss:0.7113510370254517\n",
      "Epoch 0[10506/17270] Time:0.233, Train Loss:0.5623747706413269\n",
      "Epoch 0[10507/17270] Time:0.223, Train Loss:0.7336723208427429\n",
      "Epoch 0[10508/17270] Time:0.236, Train Loss:0.46608591079711914\n",
      "Epoch 0[10509/17270] Time:0.238, Train Loss:0.35554075241088867\n",
      "Epoch 0[10510/17270] Time:0.233, Train Loss:0.3805481195449829\n",
      "Epoch 0[10511/17270] Time:0.223, Train Loss:1.161872386932373\n",
      "Epoch 0[10512/17270] Time:0.235, Train Loss:0.44039082527160645\n",
      "Epoch 0[10513/17270] Time:0.227, Train Loss:0.7004310488700867\n",
      "Epoch 0[10514/17270] Time:0.229, Train Loss:0.4338729977607727\n",
      "Epoch 0[10515/17270] Time:0.233, Train Loss:0.6139274835586548\n",
      "Epoch 0[10516/17270] Time:0.232, Train Loss:0.25450676679611206\n",
      "Epoch 0[10517/17270] Time:0.254, Train Loss:0.5753911733627319\n",
      "Epoch 0[10518/17270] Time:0.221, Train Loss:1.344478964805603\n",
      "Epoch 0[10519/17270] Time:0.233, Train Loss:0.9450199604034424\n",
      "Epoch 0[10520/17270] Time:0.24, Train Loss:0.240959033370018\n",
      "Epoch 0[10521/17270] Time:0.231, Train Loss:0.8093838095664978\n",
      "Epoch 0[10522/17270] Time:0.233, Train Loss:0.7281827926635742\n",
      "Epoch 0[10523/17270] Time:0.231, Train Loss:0.37047210335731506\n",
      "Epoch 0[10524/17270] Time:0.238, Train Loss:0.3853042423725128\n",
      "Epoch 0[10525/17270] Time:0.234, Train Loss:0.6293541193008423\n",
      "Epoch 0[10526/17270] Time:0.233, Train Loss:1.184960126876831\n",
      "Epoch 0[10527/17270] Time:0.238, Train Loss:0.6916044354438782\n",
      "Epoch 0[10528/17270] Time:0.237, Train Loss:0.7536798119544983\n",
      "Epoch 0[10529/17270] Time:0.229, Train Loss:1.1141114234924316\n",
      "Epoch 0[10530/17270] Time:0.234, Train Loss:0.555899977684021\n",
      "Epoch 0[10531/17270] Time:0.236, Train Loss:0.569692850112915\n",
      "Epoch 0[10532/17270] Time:0.229, Train Loss:0.6480103731155396\n",
      "Epoch 0[10533/17270] Time:0.229, Train Loss:0.5340726375579834\n",
      "Epoch 0[10534/17270] Time:0.241, Train Loss:0.6680397391319275\n",
      "Epoch 0[10535/17270] Time:0.221, Train Loss:0.4523980915546417\n",
      "Epoch 0[10536/17270] Time:0.23, Train Loss:0.5769014954566956\n",
      "Epoch 0[10537/17270] Time:0.234, Train Loss:0.5560842752456665\n",
      "Epoch 0[10538/17270] Time:0.237, Train Loss:0.3657605051994324\n",
      "Epoch 0[10539/17270] Time:0.221, Train Loss:0.44703957438468933\n",
      "Epoch 0[10540/17270] Time:0.229, Train Loss:0.7258275151252747\n",
      "Epoch 0[10541/17270] Time:0.233, Train Loss:0.32736736536026\n",
      "Epoch 0[10542/17270] Time:0.229, Train Loss:0.6881057620048523\n",
      "Epoch 0[10543/17270] Time:0.237, Train Loss:0.39206063747406006\n",
      "Epoch 0[10544/17270] Time:0.232, Train Loss:0.4330936074256897\n",
      "Epoch 0[10545/17270] Time:0.23, Train Loss:0.37499627470970154\n",
      "Epoch 0[10546/17270] Time:0.228, Train Loss:0.7592170834541321\n",
      "Epoch 0[10547/17270] Time:0.233, Train Loss:0.6409187316894531\n",
      "Epoch 0[10548/17270] Time:0.237, Train Loss:0.2932145297527313\n",
      "Epoch 0[10549/17270] Time:0.23, Train Loss:0.6410249471664429\n",
      "Epoch 0[10550/17270] Time:0.232, Train Loss:0.4494595229625702\n",
      "Epoch 0[10551/17270] Time:0.248, Train Loss:0.7443903684616089\n",
      "Epoch 0[10552/17270] Time:0.241, Train Loss:0.48151901364326477\n",
      "Epoch 0[10553/17270] Time:0.224, Train Loss:0.7540882229804993\n",
      "Epoch 0[10554/17270] Time:0.248, Train Loss:1.31434166431427\n",
      "Epoch 0[10555/17270] Time:0.241, Train Loss:0.4391707479953766\n",
      "Epoch 0[10556/17270] Time:0.239, Train Loss:0.5100008249282837\n",
      "Epoch 0[10557/17270] Time:0.237, Train Loss:1.0188697576522827\n",
      "Epoch 0[10558/17270] Time:0.238, Train Loss:0.6203623414039612\n",
      "Epoch 0[10559/17270] Time:0.238, Train Loss:0.5806993246078491\n",
      "Epoch 0[10560/17270] Time:0.227, Train Loss:0.42078253626823425\n",
      "Epoch 0[10561/17270] Time:0.228, Train Loss:0.4298557639122009\n",
      "Epoch 0[10562/17270] Time:0.237, Train Loss:0.47530364990234375\n",
      "Epoch 0[10563/17270] Time:0.231, Train Loss:0.41335752606391907\n",
      "Epoch 0[10564/17270] Time:0.243, Train Loss:0.7799923419952393\n",
      "Epoch 0[10565/17270] Time:0.227, Train Loss:0.6323962211608887\n",
      "Epoch 0[10566/17270] Time:0.25, Train Loss:0.6322159767150879\n",
      "Epoch 0[10567/17270] Time:0.236, Train Loss:0.6207942962646484\n",
      "Epoch 0[10568/17270] Time:0.233, Train Loss:0.5399373173713684\n",
      "Epoch 0[10569/17270] Time:0.236, Train Loss:0.40276411175727844\n",
      "Epoch 0[10570/17270] Time:0.247, Train Loss:0.4568233788013458\n",
      "Epoch 0[10571/17270] Time:0.235, Train Loss:0.6843658685684204\n",
      "Epoch 0[10572/17270] Time:0.236, Train Loss:0.6933085322380066\n",
      "Epoch 0[10573/17270] Time:0.236, Train Loss:1.121614694595337\n",
      "Epoch 0[10574/17270] Time:0.224, Train Loss:0.3165383040904999\n",
      "Epoch 0[10575/17270] Time:0.228, Train Loss:0.7666746377944946\n",
      "Epoch 0[10576/17270] Time:0.228, Train Loss:0.7586244344711304\n",
      "Epoch 0[10577/17270] Time:0.233, Train Loss:0.28827086091041565\n",
      "Epoch 0[10578/17270] Time:0.24, Train Loss:0.6471665501594543\n",
      "Epoch 0[10579/17270] Time:0.227, Train Loss:0.4492959976196289\n",
      "Epoch 0[10580/17270] Time:0.239, Train Loss:0.570216953754425\n",
      "Epoch 0[10581/17270] Time:0.24, Train Loss:0.4418070912361145\n",
      "Epoch 0[10582/17270] Time:0.238, Train Loss:0.8539147973060608\n",
      "Epoch 0[10583/17270] Time:0.227, Train Loss:0.23673811554908752\n",
      "Epoch 0[10584/17270] Time:0.223, Train Loss:0.6875368356704712\n",
      "Epoch 0[10585/17270] Time:0.238, Train Loss:0.5237240195274353\n",
      "Epoch 0[10586/17270] Time:0.229, Train Loss:0.4383116662502289\n",
      "Epoch 0[10587/17270] Time:0.238, Train Loss:0.40337350964546204\n",
      "Epoch 0[10588/17270] Time:0.236, Train Loss:0.33885523676872253\n",
      "Epoch 0[10589/17270] Time:0.232, Train Loss:0.6441179513931274\n",
      "Epoch 0[10590/17270] Time:0.234, Train Loss:0.4031095504760742\n",
      "Epoch 0[10591/17270] Time:0.239, Train Loss:0.5406259894371033\n",
      "Epoch 0[10592/17270] Time:0.238, Train Loss:0.46679559350013733\n",
      "Epoch 0[10593/17270] Time:0.24, Train Loss:0.5954782962799072\n",
      "Epoch 0[10594/17270] Time:0.238, Train Loss:0.5434858798980713\n",
      "Epoch 0[10595/17270] Time:0.229, Train Loss:0.44414636492729187\n",
      "Epoch 0[10596/17270] Time:0.238, Train Loss:0.3711639940738678\n",
      "Epoch 0[10597/17270] Time:0.238, Train Loss:0.3495761752128601\n",
      "Epoch 0[10598/17270] Time:0.233, Train Loss:0.8851960897445679\n",
      "Epoch 0[10599/17270] Time:0.231, Train Loss:1.0414494276046753\n",
      "Epoch 0[10600/17270] Time:0.237, Train Loss:0.5953388214111328\n",
      "Epoch 0[10601/17270] Time:0.24, Train Loss:0.9479352831840515\n",
      "Epoch 0[10602/17270] Time:0.235, Train Loss:0.4405554533004761\n",
      "Epoch 0[10603/17270] Time:0.242, Train Loss:0.30938711762428284\n",
      "Epoch 0[10604/17270] Time:0.235, Train Loss:1.032240867614746\n",
      "Epoch 0[10605/17270] Time:0.232, Train Loss:0.3263112008571625\n",
      "Epoch 0[10606/17270] Time:0.235, Train Loss:0.6292495131492615\n",
      "Epoch 0[10607/17270] Time:0.258, Train Loss:0.7941869497299194\n",
      "Epoch 0[10608/17270] Time:0.241, Train Loss:0.5226593017578125\n",
      "Epoch 0[10609/17270] Time:0.229, Train Loss:0.44100603461265564\n",
      "Epoch 0[10610/17270] Time:0.238, Train Loss:0.4341300129890442\n",
      "Epoch 0[10611/17270] Time:0.235, Train Loss:1.1251345872879028\n",
      "Epoch 0[10612/17270] Time:0.237, Train Loss:0.5869393944740295\n",
      "Epoch 0[10613/17270] Time:0.238, Train Loss:0.407317578792572\n",
      "Epoch 0[10614/17270] Time:0.235, Train Loss:0.5897685289382935\n",
      "Epoch 0[10615/17270] Time:0.237, Train Loss:0.40913906693458557\n",
      "Epoch 0[10616/17270] Time:0.248, Train Loss:0.3327098488807678\n",
      "Epoch 0[10617/17270] Time:0.239, Train Loss:0.6008615493774414\n",
      "Epoch 0[10618/17270] Time:0.225, Train Loss:0.3999115824699402\n",
      "Epoch 0[10619/17270] Time:0.234, Train Loss:0.6767602562904358\n",
      "Epoch 0[10620/17270] Time:0.229, Train Loss:0.4538319706916809\n",
      "Epoch 0[10621/17270] Time:0.23, Train Loss:0.4030892550945282\n",
      "Epoch 0[10622/17270] Time:0.232, Train Loss:0.4884011149406433\n",
      "Epoch 0[10623/17270] Time:0.229, Train Loss:0.8129276633262634\n",
      "Epoch 0[10624/17270] Time:0.228, Train Loss:0.7261481881141663\n",
      "Epoch 0[10625/17270] Time:0.231, Train Loss:0.6451870203018188\n",
      "Epoch 0[10626/17270] Time:0.237, Train Loss:0.8230015635490417\n",
      "Epoch 0[10627/17270] Time:0.236, Train Loss:0.37047937512397766\n",
      "Epoch 0[10628/17270] Time:0.233, Train Loss:0.44143471121788025\n",
      "Epoch 0[10629/17270] Time:0.243, Train Loss:0.2774302065372467\n",
      "Epoch 0[10630/17270] Time:0.233, Train Loss:0.7141110301017761\n",
      "Epoch 0[10631/17270] Time:0.229, Train Loss:0.2914358079433441\n",
      "Epoch 0[10632/17270] Time:0.231, Train Loss:0.5597269535064697\n",
      "Epoch 0[10633/17270] Time:0.231, Train Loss:0.391801118850708\n",
      "Epoch 0[10634/17270] Time:0.23, Train Loss:0.7259954810142517\n",
      "Epoch 0[10635/17270] Time:0.239, Train Loss:0.5577475428581238\n",
      "Epoch 0[10636/17270] Time:0.229, Train Loss:0.4521492123603821\n",
      "Epoch 0[10637/17270] Time:0.239, Train Loss:0.7103556394577026\n",
      "Epoch 0[10638/17270] Time:0.226, Train Loss:0.5362304449081421\n",
      "Epoch 0[10639/17270] Time:0.243, Train Loss:0.6530588269233704\n",
      "Epoch 0[10640/17270] Time:0.241, Train Loss:0.5198434591293335\n",
      "Epoch 0[10641/17270] Time:0.224, Train Loss:1.157051920890808\n",
      "Epoch 0[10642/17270] Time:0.242, Train Loss:0.33085402846336365\n",
      "Epoch 0[10643/17270] Time:0.241, Train Loss:0.6216884255409241\n",
      "Epoch 0[10644/17270] Time:0.236, Train Loss:0.5273190140724182\n",
      "Epoch 0[10645/17270] Time:0.229, Train Loss:1.1119753122329712\n",
      "Epoch 0[10646/17270] Time:0.236, Train Loss:0.39711830019950867\n",
      "Epoch 0[10647/17270] Time:0.228, Train Loss:0.5772172212600708\n",
      "Epoch 0[10648/17270] Time:0.252, Train Loss:1.8130000829696655\n",
      "Epoch 0[10649/17270] Time:0.224, Train Loss:0.5330948233604431\n",
      "Epoch 0[10650/17270] Time:0.242, Train Loss:0.8316015601158142\n",
      "Epoch 0[10651/17270] Time:0.224, Train Loss:0.5749647617340088\n",
      "Epoch 0[10652/17270] Time:0.244, Train Loss:0.7280676364898682\n",
      "Epoch 0[10653/17270] Time:0.245, Train Loss:0.595495343208313\n",
      "Epoch 0[10654/17270] Time:0.236, Train Loss:0.3716563880443573\n",
      "Epoch 0[10655/17270] Time:0.234, Train Loss:0.5411735773086548\n",
      "Epoch 0[10656/17270] Time:0.229, Train Loss:0.31314876675605774\n",
      "Epoch 0[10657/17270] Time:0.235, Train Loss:0.3957858681678772\n",
      "Epoch 0[10658/17270] Time:0.236, Train Loss:0.4813981354236603\n",
      "Epoch 0[10659/17270] Time:0.231, Train Loss:0.3634791970252991\n",
      "Epoch 0[10660/17270] Time:0.234, Train Loss:0.8923966288566589\n",
      "Epoch 0[10661/17270] Time:0.228, Train Loss:0.3418838083744049\n",
      "Epoch 0[10662/17270] Time:0.232, Train Loss:0.6913152933120728\n",
      "Epoch 0[10663/17270] Time:0.242, Train Loss:0.3587411046028137\n",
      "Epoch 0[10664/17270] Time:0.236, Train Loss:0.5704026818275452\n",
      "Epoch 0[10665/17270] Time:0.233, Train Loss:0.4941008985042572\n",
      "Epoch 0[10666/17270] Time:0.233, Train Loss:0.5156773924827576\n",
      "Epoch 0[10667/17270] Time:0.236, Train Loss:0.3893890380859375\n",
      "Epoch 0[10668/17270] Time:0.24, Train Loss:0.3594632148742676\n",
      "Epoch 0[10669/17270] Time:0.241, Train Loss:0.6313871741294861\n",
      "Epoch 0[10670/17270] Time:0.239, Train Loss:0.39124736189842224\n",
      "Epoch 0[10671/17270] Time:0.235, Train Loss:0.36393406987190247\n",
      "Epoch 0[10672/17270] Time:0.235, Train Loss:0.6149717569351196\n",
      "Epoch 0[10673/17270] Time:0.233, Train Loss:0.37432166934013367\n",
      "Epoch 0[10674/17270] Time:0.241, Train Loss:0.5250048041343689\n",
      "Epoch 0[10675/17270] Time:0.237, Train Loss:0.45818424224853516\n",
      "Epoch 0[10676/17270] Time:0.224, Train Loss:0.9663237929344177\n",
      "Epoch 0[10677/17270] Time:0.25, Train Loss:0.7967208027839661\n",
      "Epoch 0[10678/17270] Time:0.236, Train Loss:0.5003241896629333\n",
      "Epoch 0[10679/17270] Time:0.232, Train Loss:0.5103884935379028\n",
      "Epoch 0[10680/17270] Time:0.224, Train Loss:0.41827377676963806\n",
      "Epoch 0[10681/17270] Time:0.223, Train Loss:0.5834216475486755\n",
      "Epoch 0[10682/17270] Time:0.228, Train Loss:0.5973360538482666\n",
      "Epoch 0[10683/17270] Time:0.226, Train Loss:0.3562925457954407\n",
      "Epoch 0[10684/17270] Time:0.23, Train Loss:0.7583069801330566\n",
      "Epoch 0[10685/17270] Time:0.258, Train Loss:0.5749844312667847\n",
      "Epoch 0[10686/17270] Time:0.229, Train Loss:0.6244754791259766\n",
      "Epoch 0[10687/17270] Time:0.25, Train Loss:0.4394010007381439\n",
      "Epoch 0[10688/17270] Time:0.23, Train Loss:0.5877489447593689\n",
      "Epoch 0[10689/17270] Time:0.237, Train Loss:0.620990514755249\n",
      "Epoch 0[10690/17270] Time:0.249, Train Loss:0.5357999205589294\n",
      "Epoch 0[10691/17270] Time:0.231, Train Loss:0.8437811136245728\n",
      "Epoch 0[10692/17270] Time:0.245, Train Loss:0.42873045802116394\n",
      "Epoch 0[10693/17270] Time:0.241, Train Loss:0.5208410620689392\n",
      "Epoch 0[10694/17270] Time:0.236, Train Loss:0.5370506644248962\n",
      "Epoch 0[10695/17270] Time:0.233, Train Loss:0.4878946542739868\n",
      "Epoch 0[10696/17270] Time:0.224, Train Loss:0.9423738718032837\n",
      "Epoch 0[10697/17270] Time:0.238, Train Loss:0.5619868040084839\n",
      "Epoch 0[10698/17270] Time:0.233, Train Loss:0.4827795624732971\n",
      "Epoch 0[10699/17270] Time:0.233, Train Loss:0.45713773369789124\n",
      "Epoch 0[10700/17270] Time:0.236, Train Loss:0.7138561606407166\n",
      "Epoch 0[10701/17270] Time:0.233, Train Loss:0.646812915802002\n",
      "Epoch 0[10702/17270] Time:0.255, Train Loss:0.536658763885498\n",
      "Epoch 0[10703/17270] Time:0.235, Train Loss:0.25322726368904114\n",
      "Epoch 0[10704/17270] Time:0.233, Train Loss:0.42786356806755066\n",
      "Epoch 0[10705/17270] Time:0.227, Train Loss:0.5761927366256714\n",
      "Epoch 0[10706/17270] Time:0.25, Train Loss:0.9921085834503174\n",
      "Epoch 0[10707/17270] Time:0.235, Train Loss:0.544633150100708\n",
      "Epoch 0[10708/17270] Time:0.226, Train Loss:0.27366334199905396\n",
      "Epoch 0[10709/17270] Time:0.234, Train Loss:0.554361879825592\n",
      "Epoch 0[10710/17270] Time:0.235, Train Loss:0.4629611372947693\n",
      "Epoch 0[10711/17270] Time:0.228, Train Loss:0.4981834888458252\n",
      "Epoch 0[10712/17270] Time:0.245, Train Loss:0.5402591228485107\n",
      "Epoch 0[10713/17270] Time:0.227, Train Loss:0.3798357844352722\n",
      "Epoch 0[10714/17270] Time:0.234, Train Loss:0.3363247811794281\n",
      "Epoch 0[10715/17270] Time:0.253, Train Loss:0.6021081209182739\n",
      "Epoch 0[10716/17270] Time:0.233, Train Loss:0.4106193780899048\n",
      "Epoch 0[10717/17270] Time:0.245, Train Loss:0.32188519835472107\n",
      "Epoch 0[10718/17270] Time:0.233, Train Loss:0.5169259309768677\n",
      "Epoch 0[10719/17270] Time:0.224, Train Loss:1.3570636510849\n",
      "Epoch 0[10720/17270] Time:0.244, Train Loss:0.36653369665145874\n",
      "Epoch 0[10721/17270] Time:0.225, Train Loss:0.8774521350860596\n",
      "Epoch 0[10722/17270] Time:0.245, Train Loss:0.7469125390052795\n",
      "Epoch 0[10723/17270] Time:0.239, Train Loss:0.9206292033195496\n",
      "Epoch 0[10724/17270] Time:0.222, Train Loss:0.6349530816078186\n",
      "Epoch 0[10725/17270] Time:0.229, Train Loss:0.25357934832572937\n",
      "Epoch 0[10726/17270] Time:0.24, Train Loss:0.4607029855251312\n",
      "Epoch 0[10727/17270] Time:0.23, Train Loss:0.6119483113288879\n",
      "Epoch 0[10728/17270] Time:0.247, Train Loss:0.3465132415294647\n",
      "Epoch 0[10729/17270] Time:0.235, Train Loss:0.4737018048763275\n",
      "Epoch 0[10730/17270] Time:0.228, Train Loss:0.4698752760887146\n",
      "Epoch 0[10731/17270] Time:0.243, Train Loss:0.5796197056770325\n",
      "Epoch 0[10732/17270] Time:0.238, Train Loss:1.1927458047866821\n",
      "Epoch 0[10733/17270] Time:0.222, Train Loss:0.3617344796657562\n",
      "Epoch 0[10734/17270] Time:0.247, Train Loss:0.30838921666145325\n",
      "Epoch 0[10735/17270] Time:0.242, Train Loss:0.6186795234680176\n",
      "Epoch 0[10736/17270] Time:0.236, Train Loss:0.6733445525169373\n",
      "Epoch 0[10737/17270] Time:0.222, Train Loss:0.44509392976760864\n",
      "Epoch 0[10738/17270] Time:0.249, Train Loss:0.5102893114089966\n",
      "Epoch 0[10739/17270] Time:0.234, Train Loss:0.37841901183128357\n",
      "Epoch 0[10740/17270] Time:0.248, Train Loss:0.6332045197486877\n",
      "Epoch 0[10741/17270] Time:0.222, Train Loss:0.5825049877166748\n",
      "Epoch 0[10742/17270] Time:0.248, Train Loss:1.1386796236038208\n",
      "Epoch 0[10743/17270] Time:0.233, Train Loss:0.5324375033378601\n",
      "Epoch 0[10744/17270] Time:0.238, Train Loss:0.3783242702484131\n",
      "Epoch 0[10745/17270] Time:0.226, Train Loss:1.1465874910354614\n",
      "Epoch 0[10746/17270] Time:0.243, Train Loss:0.9163213968276978\n",
      "Epoch 0[10747/17270] Time:0.226, Train Loss:0.5452994704246521\n",
      "Epoch 0[10748/17270] Time:0.231, Train Loss:0.48158934712409973\n",
      "Epoch 0[10749/17270] Time:0.24, Train Loss:0.9254155158996582\n",
      "Epoch 0[10750/17270] Time:0.25, Train Loss:0.6222567558288574\n",
      "Epoch 0[10751/17270] Time:0.247, Train Loss:0.6287175416946411\n",
      "Epoch 0[10752/17270] Time:0.234, Train Loss:1.2719846963882446\n",
      "Epoch 0[10753/17270] Time:0.221, Train Loss:0.5476862192153931\n",
      "Epoch 0[10754/17270] Time:0.237, Train Loss:0.4783168137073517\n",
      "Epoch 0[10755/17270] Time:0.238, Train Loss:0.35266607999801636\n",
      "Epoch 0[10756/17270] Time:0.229, Train Loss:0.5873302221298218\n",
      "Epoch 0[10757/17270] Time:0.227, Train Loss:0.5985060930252075\n",
      "Epoch 0[10758/17270] Time:0.23, Train Loss:0.7975704669952393\n",
      "Epoch 0[10759/17270] Time:0.236, Train Loss:0.6883185505867004\n",
      "Epoch 0[10760/17270] Time:0.223, Train Loss:0.24294118583202362\n",
      "Epoch 0[10761/17270] Time:0.242, Train Loss:0.5297178626060486\n",
      "Epoch 0[10762/17270] Time:0.232, Train Loss:0.2999882996082306\n",
      "Epoch 0[10763/17270] Time:0.223, Train Loss:0.6495941281318665\n",
      "Epoch 0[10764/17270] Time:0.242, Train Loss:0.6442300081253052\n",
      "Epoch 0[10765/17270] Time:0.233, Train Loss:0.5834411978721619\n",
      "Epoch 0[10766/17270] Time:0.233, Train Loss:0.5499566197395325\n",
      "Epoch 0[10767/17270] Time:0.257, Train Loss:0.8355501890182495\n",
      "Epoch 0[10768/17270] Time:0.235, Train Loss:0.6221963763237\n",
      "Epoch 0[10769/17270] Time:0.228, Train Loss:1.1161913871765137\n",
      "Epoch 0[10770/17270] Time:0.23, Train Loss:0.6817142963409424\n",
      "Epoch 0[10771/17270] Time:0.247, Train Loss:0.3879599869251251\n",
      "Epoch 0[10772/17270] Time:0.234, Train Loss:0.9336562156677246\n",
      "Epoch 0[10773/17270] Time:0.23, Train Loss:0.8797429800033569\n",
      "Epoch 0[10774/17270] Time:0.223, Train Loss:0.7624659538269043\n",
      "Epoch 0[10775/17270] Time:0.238, Train Loss:0.9337457418441772\n",
      "Epoch 0[10776/17270] Time:0.241, Train Loss:0.3945199251174927\n",
      "Epoch 0[10777/17270] Time:0.232, Train Loss:0.6486621499061584\n",
      "Epoch 0[10778/17270] Time:0.227, Train Loss:0.6448766589164734\n",
      "Epoch 0[10779/17270] Time:0.244, Train Loss:0.2605409026145935\n",
      "Epoch 0[10780/17270] Time:0.234, Train Loss:0.3247884511947632\n",
      "Epoch 0[10781/17270] Time:0.247, Train Loss:0.5877313017845154\n",
      "Epoch 0[10782/17270] Time:0.231, Train Loss:0.40851694345474243\n",
      "Epoch 0[10783/17270] Time:0.248, Train Loss:0.8088560700416565\n",
      "Epoch 0[10784/17270] Time:0.233, Train Loss:0.552760899066925\n",
      "Epoch 0[10785/17270] Time:0.233, Train Loss:0.4901566505432129\n",
      "Epoch 0[10786/17270] Time:0.254, Train Loss:0.7046909928321838\n",
      "Epoch 0[10787/17270] Time:0.242, Train Loss:0.7424914240837097\n",
      "Epoch 0[10788/17270] Time:0.243, Train Loss:0.5795427560806274\n",
      "Epoch 0[10789/17270] Time:0.226, Train Loss:0.3044698238372803\n",
      "Epoch 0[10790/17270] Time:0.231, Train Loss:0.4995107352733612\n",
      "Epoch 0[10791/17270] Time:0.244, Train Loss:0.5829554796218872\n",
      "Epoch 0[10792/17270] Time:0.227, Train Loss:0.2844739556312561\n",
      "Epoch 0[10793/17270] Time:0.247, Train Loss:0.25963154435157776\n",
      "Epoch 0[10794/17270] Time:0.232, Train Loss:0.541825532913208\n",
      "Epoch 0[10795/17270] Time:0.246, Train Loss:0.5579968094825745\n",
      "Epoch 0[10796/17270] Time:0.244, Train Loss:0.4015655815601349\n",
      "Epoch 0[10797/17270] Time:0.216, Train Loss:0.6569995880126953\n",
      "Epoch 0[10798/17270] Time:0.232, Train Loss:0.30860456824302673\n",
      "Epoch 0[10799/17270] Time:0.234, Train Loss:0.5511059165000916\n",
      "Epoch 0[10800/17270] Time:0.228, Train Loss:0.9666042923927307\n",
      "Epoch 0[10801/17270] Time:0.239, Train Loss:0.6090946197509766\n",
      "Epoch 0[10802/17270] Time:0.239, Train Loss:0.44362667202949524\n",
      "Epoch 0[10803/17270] Time:0.238, Train Loss:0.42839422821998596\n",
      "Epoch 0[10804/17270] Time:0.248, Train Loss:0.6447564363479614\n",
      "Epoch 0[10805/17270] Time:0.228, Train Loss:0.7724807858467102\n",
      "Epoch 0[10806/17270] Time:0.23, Train Loss:0.5414978265762329\n",
      "Epoch 0[10807/17270] Time:0.229, Train Loss:0.5833470821380615\n",
      "Epoch 0[10808/17270] Time:0.237, Train Loss:0.7940731048583984\n",
      "Epoch 0[10809/17270] Time:0.225, Train Loss:0.6563984751701355\n",
      "Epoch 0[10810/17270] Time:0.227, Train Loss:0.7463186979293823\n",
      "Epoch 0[10811/17270] Time:0.238, Train Loss:0.4349709749221802\n",
      "Epoch 0[10812/17270] Time:0.243, Train Loss:0.5844875574111938\n",
      "Epoch 0[10813/17270] Time:0.236, Train Loss:0.4391450583934784\n",
      "Epoch 0[10814/17270] Time:0.23, Train Loss:0.7915530204772949\n",
      "Epoch 0[10815/17270] Time:0.243, Train Loss:0.5632378458976746\n",
      "Epoch 0[10816/17270] Time:0.234, Train Loss:0.7310776114463806\n",
      "Epoch 0[10817/17270] Time:0.245, Train Loss:0.7656067609786987\n",
      "Epoch 0[10818/17270] Time:0.229, Train Loss:0.411205917596817\n",
      "Epoch 0[10819/17270] Time:0.232, Train Loss:0.6011638641357422\n",
      "Epoch 0[10820/17270] Time:0.237, Train Loss:0.4577752649784088\n",
      "Epoch 0[10821/17270] Time:0.239, Train Loss:0.5511895418167114\n",
      "Epoch 0[10822/17270] Time:0.23, Train Loss:0.2791944742202759\n",
      "Epoch 0[10823/17270] Time:0.238, Train Loss:0.3874675929546356\n",
      "Epoch 0[10824/17270] Time:0.235, Train Loss:0.40160974860191345\n",
      "Epoch 0[10825/17270] Time:0.237, Train Loss:0.40723925828933716\n",
      "Epoch 0[10826/17270] Time:0.24, Train Loss:0.5661283731460571\n",
      "Epoch 0[10827/17270] Time:0.233, Train Loss:0.6111023426055908\n",
      "Epoch 0[10828/17270] Time:0.237, Train Loss:0.5541929602622986\n",
      "Epoch 0[10829/17270] Time:0.232, Train Loss:0.6119173765182495\n",
      "Epoch 0[10830/17270] Time:0.23, Train Loss:0.6920954585075378\n",
      "Epoch 0[10831/17270] Time:0.237, Train Loss:0.6279715895652771\n",
      "Epoch 0[10832/17270] Time:0.236, Train Loss:0.4893396198749542\n",
      "Epoch 0[10833/17270] Time:0.238, Train Loss:0.6142460703849792\n",
      "Epoch 0[10834/17270] Time:0.24, Train Loss:0.6977126598358154\n",
      "Epoch 0[10835/17270] Time:0.237, Train Loss:0.3483964502811432\n",
      "Epoch 0[10836/17270] Time:0.239, Train Loss:0.39500001072883606\n",
      "Epoch 0[10837/17270] Time:0.23, Train Loss:0.39595770835876465\n",
      "Epoch 0[10838/17270] Time:0.234, Train Loss:0.6421228051185608\n",
      "Epoch 0[10839/17270] Time:0.234, Train Loss:0.5623301863670349\n",
      "Epoch 0[10840/17270] Time:0.238, Train Loss:0.5386326313018799\n",
      "Epoch 0[10841/17270] Time:0.234, Train Loss:0.2549060583114624\n",
      "Epoch 0[10842/17270] Time:0.234, Train Loss:0.663568377494812\n",
      "Epoch 0[10843/17270] Time:0.232, Train Loss:0.5523248910903931\n",
      "Epoch 0[10844/17270] Time:0.232, Train Loss:0.5609035491943359\n",
      "Epoch 0[10845/17270] Time:0.225, Train Loss:0.4284334182739258\n",
      "Epoch 0[10846/17270] Time:0.241, Train Loss:1.020838975906372\n",
      "Epoch 0[10847/17270] Time:0.241, Train Loss:0.49016234278678894\n",
      "Epoch 0[10848/17270] Time:0.239, Train Loss:0.3920736312866211\n",
      "Epoch 0[10849/17270] Time:0.232, Train Loss:0.6634203195571899\n",
      "Epoch 0[10850/17270] Time:0.225, Train Loss:0.5824669599533081\n",
      "Epoch 0[10851/17270] Time:0.229, Train Loss:0.6397911906242371\n",
      "Epoch 0[10852/17270] Time:0.239, Train Loss:0.4724661409854889\n",
      "Epoch 0[10853/17270] Time:0.238, Train Loss:0.38094452023506165\n",
      "Epoch 0[10854/17270] Time:0.23, Train Loss:0.6269946098327637\n",
      "Epoch 0[10855/17270] Time:0.232, Train Loss:0.7827743291854858\n",
      "Epoch 0[10856/17270] Time:0.237, Train Loss:0.4580515921115875\n",
      "Epoch 0[10857/17270] Time:0.238, Train Loss:0.47941601276397705\n",
      "Epoch 0[10858/17270] Time:0.238, Train Loss:0.5339280962944031\n",
      "Epoch 0[10859/17270] Time:0.23, Train Loss:0.5302242040634155\n",
      "Epoch 0[10860/17270] Time:0.231, Train Loss:1.1300770044326782\n",
      "Epoch 0[10861/17270] Time:0.228, Train Loss:0.56378573179245\n",
      "Epoch 0[10862/17270] Time:0.235, Train Loss:0.6992304921150208\n",
      "Epoch 0[10863/17270] Time:0.231, Train Loss:0.5480931997299194\n",
      "Epoch 0[10864/17270] Time:0.236, Train Loss:1.3331975936889648\n",
      "Epoch 0[10865/17270] Time:0.241, Train Loss:0.30299484729766846\n",
      "Epoch 0[10866/17270] Time:0.232, Train Loss:0.5541424751281738\n",
      "Epoch 0[10867/17270] Time:0.228, Train Loss:0.5910177826881409\n",
      "Epoch 0[10868/17270] Time:0.248, Train Loss:0.33142274618148804\n",
      "Epoch 0[10869/17270] Time:0.23, Train Loss:0.7415266633033752\n",
      "Epoch 0[10870/17270] Time:0.238, Train Loss:0.31331244111061096\n",
      "Epoch 0[10871/17270] Time:0.233, Train Loss:0.7142717838287354\n",
      "Epoch 0[10872/17270] Time:0.238, Train Loss:0.6697593927383423\n",
      "Epoch 0[10873/17270] Time:0.232, Train Loss:0.5261279940605164\n",
      "Epoch 0[10874/17270] Time:0.244, Train Loss:0.3156071901321411\n",
      "Epoch 0[10875/17270] Time:0.24, Train Loss:0.35691380500793457\n",
      "Epoch 0[10876/17270] Time:0.235, Train Loss:0.45223569869995117\n",
      "Epoch 0[10877/17270] Time:0.224, Train Loss:0.8878918290138245\n",
      "Epoch 0[10878/17270] Time:0.238, Train Loss:0.39558231830596924\n",
      "Epoch 0[10879/17270] Time:0.222, Train Loss:0.4107067883014679\n",
      "Epoch 0[10880/17270] Time:0.237, Train Loss:0.4410099983215332\n",
      "Epoch 0[10881/17270] Time:0.233, Train Loss:0.6727288961410522\n",
      "Epoch 0[10882/17270] Time:0.227, Train Loss:0.2776643931865692\n",
      "Epoch 0[10883/17270] Time:0.237, Train Loss:1.1185332536697388\n",
      "Epoch 0[10884/17270] Time:0.237, Train Loss:0.8678271770477295\n",
      "Epoch 0[10885/17270] Time:0.236, Train Loss:0.7183615565299988\n",
      "Epoch 0[10886/17270] Time:0.239, Train Loss:0.36808890104293823\n",
      "Epoch 0[10887/17270] Time:0.232, Train Loss:0.6058930158615112\n",
      "Epoch 0[10888/17270] Time:0.239, Train Loss:1.1381820440292358\n",
      "Epoch 0[10889/17270] Time:0.231, Train Loss:0.6325445175170898\n",
      "Epoch 0[10890/17270] Time:0.249, Train Loss:0.5417050719261169\n",
      "Epoch 0[10891/17270] Time:0.232, Train Loss:1.539900779724121\n",
      "Epoch 0[10892/17270] Time:0.246, Train Loss:0.5900024771690369\n",
      "Epoch 0[10893/17270] Time:0.234, Train Loss:0.5501773357391357\n",
      "Epoch 0[10894/17270] Time:0.248, Train Loss:0.4586125612258911\n",
      "Epoch 0[10895/17270] Time:0.239, Train Loss:0.3292032778263092\n",
      "Epoch 0[10896/17270] Time:0.233, Train Loss:0.6640660762786865\n",
      "Epoch 0[10897/17270] Time:0.226, Train Loss:0.48607176542282104\n",
      "Epoch 0[10898/17270] Time:0.24, Train Loss:0.7837205529212952\n",
      "Epoch 0[10899/17270] Time:0.218, Train Loss:0.7715243697166443\n",
      "Epoch 0[10900/17270] Time:0.232, Train Loss:0.6278188228607178\n",
      "Epoch 0[10901/17270] Time:0.231, Train Loss:0.36870238184928894\n",
      "Epoch 0[10902/17270] Time:0.228, Train Loss:0.42646679282188416\n",
      "Epoch 0[10903/17270] Time:0.228, Train Loss:0.5760929584503174\n",
      "Epoch 0[10904/17270] Time:0.23, Train Loss:0.5170382261276245\n",
      "Epoch 0[10905/17270] Time:0.238, Train Loss:0.34203609824180603\n",
      "Epoch 0[10906/17270] Time:0.233, Train Loss:0.46635115146636963\n",
      "Epoch 0[10907/17270] Time:0.237, Train Loss:0.7984313368797302\n",
      "Epoch 0[10908/17270] Time:0.236, Train Loss:0.36821362376213074\n",
      "Epoch 0[10909/17270] Time:0.224, Train Loss:0.6240295767784119\n",
      "Epoch 0[10910/17270] Time:0.234, Train Loss:0.6173893213272095\n",
      "Epoch 0[10911/17270] Time:0.236, Train Loss:0.42150747776031494\n",
      "Epoch 0[10912/17270] Time:0.237, Train Loss:0.5511059165000916\n",
      "Epoch 0[10913/17270] Time:0.237, Train Loss:0.49726930260658264\n",
      "Epoch 0[10914/17270] Time:0.221, Train Loss:0.4146084487438202\n",
      "Epoch 0[10915/17270] Time:0.232, Train Loss:0.7364842295646667\n",
      "Epoch 0[10916/17270] Time:0.236, Train Loss:0.47826042771339417\n",
      "Epoch 0[10917/17270] Time:0.237, Train Loss:0.6049929857254028\n",
      "Epoch 0[10918/17270] Time:0.238, Train Loss:0.4420640468597412\n",
      "Epoch 0[10919/17270] Time:0.232, Train Loss:0.6543339490890503\n",
      "Epoch 0[10920/17270] Time:0.231, Train Loss:1.0876516103744507\n",
      "Epoch 0[10921/17270] Time:0.23, Train Loss:0.843690812587738\n",
      "Epoch 0[10922/17270] Time:0.243, Train Loss:0.44167405366897583\n",
      "Epoch 0[10923/17270] Time:0.241, Train Loss:0.41634035110473633\n",
      "Epoch 0[10924/17270] Time:0.255, Train Loss:0.5685617923736572\n",
      "Epoch 0[10925/17270] Time:0.229, Train Loss:0.4431126117706299\n",
      "Epoch 0[10926/17270] Time:0.244, Train Loss:1.393076777458191\n",
      "Epoch 0[10927/17270] Time:0.238, Train Loss:1.2362064123153687\n",
      "Epoch 0[10928/17270] Time:0.232, Train Loss:0.49245685338974\n",
      "Epoch 0[10929/17270] Time:0.246, Train Loss:0.6034621596336365\n",
      "Epoch 0[10930/17270] Time:0.238, Train Loss:0.841385543346405\n",
      "Epoch 0[10931/17270] Time:0.23, Train Loss:0.4933939278125763\n",
      "Epoch 0[10932/17270] Time:0.23, Train Loss:0.4864836037158966\n",
      "Epoch 0[10933/17270] Time:0.245, Train Loss:0.4819082021713257\n",
      "Epoch 0[10934/17270] Time:0.248, Train Loss:0.3894559442996979\n",
      "Epoch 0[10935/17270] Time:0.238, Train Loss:0.5469246506690979\n",
      "Epoch 0[10936/17270] Time:0.243, Train Loss:0.6275408267974854\n",
      "Epoch 0[10937/17270] Time:0.243, Train Loss:0.7216777801513672\n",
      "Epoch 0[10938/17270] Time:0.242, Train Loss:0.65315842628479\n",
      "Epoch 0[10939/17270] Time:0.231, Train Loss:0.5553445219993591\n",
      "Epoch 0[10940/17270] Time:0.244, Train Loss:0.6224322319030762\n",
      "Epoch 0[10941/17270] Time:0.247, Train Loss:1.1963196992874146\n",
      "Epoch 0[10942/17270] Time:0.232, Train Loss:0.35397061705589294\n",
      "Epoch 0[10943/17270] Time:0.242, Train Loss:0.6354371905326843\n",
      "Epoch 0[10944/17270] Time:0.232, Train Loss:0.6990025043487549\n",
      "Epoch 0[10945/17270] Time:0.227, Train Loss:0.6587546467781067\n",
      "Epoch 0[10946/17270] Time:0.24, Train Loss:0.3121635317802429\n",
      "Epoch 0[10947/17270] Time:0.241, Train Loss:0.8489620089530945\n",
      "Epoch 0[10948/17270] Time:0.233, Train Loss:0.8353819251060486\n",
      "Epoch 0[10949/17270] Time:0.231, Train Loss:0.6000896096229553\n",
      "Epoch 0[10950/17270] Time:0.234, Train Loss:0.5836856365203857\n",
      "Epoch 0[10951/17270] Time:0.233, Train Loss:0.81397944688797\n",
      "Epoch 0[10952/17270] Time:0.229, Train Loss:0.5371906757354736\n",
      "Epoch 0[10953/17270] Time:0.231, Train Loss:0.639670193195343\n",
      "Epoch 0[10954/17270] Time:0.237, Train Loss:0.30894917249679565\n",
      "Epoch 0[10955/17270] Time:0.23, Train Loss:0.5635315179824829\n",
      "Epoch 0[10956/17270] Time:0.234, Train Loss:0.6284341216087341\n",
      "Epoch 0[10957/17270] Time:0.242, Train Loss:1.163144826889038\n",
      "Epoch 0[10958/17270] Time:0.232, Train Loss:0.5632007122039795\n",
      "Epoch 0[10959/17270] Time:0.247, Train Loss:1.0953927040100098\n",
      "Epoch 0[10960/17270] Time:0.241, Train Loss:0.4890655279159546\n",
      "Epoch 0[10961/17270] Time:0.223, Train Loss:0.4343462586402893\n",
      "Epoch 0[10962/17270] Time:0.226, Train Loss:0.5252490043640137\n",
      "Epoch 0[10963/17270] Time:0.229, Train Loss:0.5928471684455872\n",
      "Epoch 0[10964/17270] Time:0.239, Train Loss:0.4067806303501129\n",
      "Epoch 0[10965/17270] Time:0.238, Train Loss:0.7022185921669006\n",
      "Epoch 0[10966/17270] Time:0.236, Train Loss:0.33794277906417847\n",
      "Epoch 0[10967/17270] Time:0.227, Train Loss:0.8259139657020569\n",
      "Epoch 0[10968/17270] Time:0.235, Train Loss:1.013157844543457\n",
      "Epoch 0[10969/17270] Time:0.237, Train Loss:0.792678713798523\n",
      "Epoch 0[10970/17270] Time:0.237, Train Loss:0.44603997468948364\n",
      "Epoch 0[10971/17270] Time:0.237, Train Loss:0.5213935971260071\n",
      "Epoch 0[10972/17270] Time:0.237, Train Loss:0.39741116762161255\n",
      "Epoch 0[10973/17270] Time:0.232, Train Loss:0.6736654043197632\n",
      "Epoch 0[10974/17270] Time:0.233, Train Loss:0.5325329899787903\n",
      "Epoch 0[10975/17270] Time:0.232, Train Loss:0.5372822880744934\n",
      "Epoch 0[10976/17270] Time:0.231, Train Loss:0.4630929231643677\n",
      "Epoch 0[10977/17270] Time:0.231, Train Loss:0.710226833820343\n",
      "Epoch 0[10978/17270] Time:0.236, Train Loss:0.5052622556686401\n",
      "Epoch 0[10979/17270] Time:0.232, Train Loss:0.3636285364627838\n",
      "Epoch 0[10980/17270] Time:0.229, Train Loss:0.6013148427009583\n",
      "Epoch 0[10981/17270] Time:0.226, Train Loss:0.6070558428764343\n",
      "Epoch 0[10982/17270] Time:0.236, Train Loss:0.43298056721687317\n",
      "Epoch 0[10983/17270] Time:0.237, Train Loss:0.35059013962745667\n",
      "Epoch 0[10984/17270] Time:0.242, Train Loss:0.2983646094799042\n",
      "Epoch 0[10985/17270] Time:0.234, Train Loss:0.47049039602279663\n",
      "Epoch 0[10986/17270] Time:0.232, Train Loss:0.42199239134788513\n",
      "Epoch 0[10987/17270] Time:0.237, Train Loss:0.3484858572483063\n",
      "Epoch 0[10988/17270] Time:0.238, Train Loss:0.4777943789958954\n",
      "Epoch 0[10989/17270] Time:0.237, Train Loss:0.32597094774246216\n",
      "Epoch 0[10990/17270] Time:0.228, Train Loss:0.5858855843544006\n",
      "Epoch 0[10991/17270] Time:0.234, Train Loss:1.1489633321762085\n",
      "Epoch 0[10992/17270] Time:0.237, Train Loss:0.3448311388492584\n",
      "Epoch 0[10993/17270] Time:0.231, Train Loss:1.026294469833374\n",
      "Epoch 0[10994/17270] Time:0.217, Train Loss:0.35561737418174744\n",
      "Epoch 0[10995/17270] Time:0.244, Train Loss:0.35502108931541443\n",
      "Epoch 0[10996/17270] Time:0.237, Train Loss:0.41570091247558594\n",
      "Epoch 0[10997/17270] Time:0.242, Train Loss:0.5891537666320801\n",
      "Epoch 0[10998/17270] Time:0.24, Train Loss:0.30932098627090454\n",
      "Epoch 0[10999/17270] Time:0.235, Train Loss:0.48736199736595154\n",
      "Epoch 0[11000/17270] Time:0.237, Train Loss:0.23863662779331207\n",
      "Epoch 0[11001/17270] Time:0.231, Train Loss:0.5195732712745667\n",
      "Epoch 0[11002/17270] Time:0.252, Train Loss:0.6772753000259399\n",
      "Epoch 0[11003/17270] Time:0.231, Train Loss:0.6044827103614807\n",
      "Epoch 0[11004/17270] Time:0.233, Train Loss:0.43068331480026245\n",
      "Epoch 0[11005/17270] Time:0.233, Train Loss:0.5161852240562439\n",
      "Epoch 0[11006/17270] Time:0.246, Train Loss:0.5200403332710266\n",
      "Epoch 0[11007/17270] Time:0.24, Train Loss:0.5390437841415405\n",
      "Epoch 0[11008/17270] Time:0.229, Train Loss:0.6036015748977661\n",
      "Epoch 0[11009/17270] Time:0.234, Train Loss:0.5024241805076599\n",
      "Epoch 0[11010/17270] Time:0.223, Train Loss:0.6205546259880066\n",
      "Epoch 0[11011/17270] Time:0.237, Train Loss:0.5428303480148315\n",
      "Epoch 0[11012/17270] Time:0.238, Train Loss:0.6500653624534607\n",
      "Epoch 0[11013/17270] Time:0.242, Train Loss:0.5167213678359985\n",
      "Epoch 0[11014/17270] Time:0.232, Train Loss:0.2464752197265625\n",
      "Epoch 0[11015/17270] Time:0.236, Train Loss:0.7838491797447205\n",
      "Epoch 0[11016/17270] Time:0.228, Train Loss:0.6039561033248901\n",
      "Epoch 0[11017/17270] Time:0.24, Train Loss:0.4403023421764374\n",
      "Epoch 0[11018/17270] Time:0.224, Train Loss:0.6317574381828308\n",
      "Epoch 0[11019/17270] Time:0.228, Train Loss:0.4106753468513489\n",
      "Epoch 0[11020/17270] Time:0.232, Train Loss:0.4163256287574768\n",
      "Epoch 0[11021/17270] Time:0.233, Train Loss:0.698610246181488\n",
      "Epoch 0[11022/17270] Time:0.234, Train Loss:1.1825605630874634\n",
      "Epoch 0[11023/17270] Time:0.233, Train Loss:0.6562654972076416\n",
      "Epoch 0[11024/17270] Time:0.228, Train Loss:0.7833213806152344\n",
      "Epoch 0[11025/17270] Time:0.228, Train Loss:0.5809020400047302\n",
      "Epoch 0[11026/17270] Time:0.228, Train Loss:0.6122451424598694\n",
      "Epoch 0[11027/17270] Time:0.233, Train Loss:1.4573860168457031\n",
      "Epoch 0[11028/17270] Time:0.235, Train Loss:0.6859366297721863\n",
      "Epoch 0[11029/17270] Time:0.238, Train Loss:0.4765024483203888\n",
      "Epoch 0[11030/17270] Time:0.243, Train Loss:0.7357948422431946\n",
      "Epoch 0[11031/17270] Time:0.239, Train Loss:0.4756242334842682\n",
      "Epoch 0[11032/17270] Time:0.23, Train Loss:0.5234026908874512\n",
      "Epoch 0[11033/17270] Time:0.221, Train Loss:0.4376508891582489\n",
      "Epoch 0[11034/17270] Time:0.237, Train Loss:0.3028235137462616\n",
      "Epoch 0[11035/17270] Time:0.234, Train Loss:0.4022585451602936\n",
      "Epoch 0[11036/17270] Time:0.232, Train Loss:0.8062748312950134\n",
      "Epoch 0[11037/17270] Time:0.237, Train Loss:0.5266304016113281\n",
      "Epoch 0[11038/17270] Time:0.226, Train Loss:0.7221108078956604\n",
      "Epoch 0[11039/17270] Time:0.231, Train Loss:0.6586710214614868\n",
      "Epoch 0[11040/17270] Time:0.239, Train Loss:0.38777217268943787\n",
      "Epoch 0[11041/17270] Time:0.238, Train Loss:0.846437394618988\n",
      "Epoch 0[11042/17270] Time:0.239, Train Loss:0.5773677229881287\n",
      "Epoch 0[11043/17270] Time:0.232, Train Loss:0.5712542533874512\n",
      "Epoch 0[11044/17270] Time:0.238, Train Loss:1.1482574939727783\n",
      "Epoch 0[11045/17270] Time:0.233, Train Loss:0.9103193283081055\n",
      "Epoch 0[11046/17270] Time:0.244, Train Loss:0.3783222734928131\n",
      "Epoch 0[11047/17270] Time:0.235, Train Loss:0.4102625250816345\n",
      "Epoch 0[11048/17270] Time:0.247, Train Loss:0.3459470570087433\n",
      "Epoch 0[11049/17270] Time:0.226, Train Loss:0.6056376695632935\n",
      "Epoch 0[11050/17270] Time:0.225, Train Loss:0.45544180274009705\n",
      "Epoch 0[11051/17270] Time:0.235, Train Loss:0.5668568015098572\n",
      "Epoch 0[11052/17270] Time:0.238, Train Loss:0.6443975567817688\n",
      "Epoch 0[11053/17270] Time:0.231, Train Loss:0.4807923138141632\n",
      "Epoch 0[11054/17270] Time:0.229, Train Loss:0.4056960344314575\n",
      "Epoch 0[11055/17270] Time:0.23, Train Loss:1.0136250257492065\n",
      "Epoch 0[11056/17270] Time:0.234, Train Loss:0.6008707880973816\n",
      "Epoch 0[11057/17270] Time:0.236, Train Loss:0.9339089393615723\n",
      "Epoch 0[11058/17270] Time:0.245, Train Loss:0.6809655427932739\n",
      "Epoch 0[11059/17270] Time:0.23, Train Loss:0.44824302196502686\n",
      "Epoch 0[11060/17270] Time:0.247, Train Loss:0.40928342938423157\n",
      "Epoch 0[11061/17270] Time:0.234, Train Loss:0.679975688457489\n",
      "Epoch 0[11062/17270] Time:0.237, Train Loss:0.7514893412590027\n",
      "Epoch 0[11063/17270] Time:0.237, Train Loss:0.44669148325920105\n",
      "Epoch 0[11064/17270] Time:0.232, Train Loss:0.722979724407196\n",
      "Epoch 0[11065/17270] Time:0.237, Train Loss:1.029725193977356\n",
      "Epoch 0[11066/17270] Time:0.238, Train Loss:0.4347601532936096\n",
      "Epoch 0[11067/17270] Time:0.234, Train Loss:0.660519540309906\n",
      "Epoch 0[11068/17270] Time:0.238, Train Loss:0.5667402148246765\n",
      "Epoch 0[11069/17270] Time:0.223, Train Loss:0.5430313348770142\n",
      "Epoch 0[11070/17270] Time:0.226, Train Loss:0.6274136900901794\n",
      "Epoch 0[11071/17270] Time:0.239, Train Loss:0.8164656162261963\n",
      "Epoch 0[11072/17270] Time:0.222, Train Loss:0.5349258184432983\n",
      "Epoch 0[11073/17270] Time:0.243, Train Loss:0.4101112186908722\n",
      "Epoch 0[11074/17270] Time:0.235, Train Loss:0.5144899487495422\n",
      "Epoch 0[11075/17270] Time:0.237, Train Loss:0.36506155133247375\n",
      "Epoch 0[11076/17270] Time:0.231, Train Loss:0.770538866519928\n",
      "Epoch 0[11077/17270] Time:0.242, Train Loss:0.634793221950531\n",
      "Epoch 0[11078/17270] Time:0.227, Train Loss:0.849941074848175\n",
      "Epoch 0[11079/17270] Time:0.233, Train Loss:1.0540698766708374\n",
      "Epoch 0[11080/17270] Time:0.231, Train Loss:0.49671265482902527\n",
      "Epoch 0[11081/17270] Time:0.23, Train Loss:0.5811179280281067\n",
      "Epoch 0[11082/17270] Time:0.226, Train Loss:0.4791029393672943\n",
      "Epoch 0[11083/17270] Time:0.245, Train Loss:0.7464928030967712\n",
      "Epoch 0[11084/17270] Time:0.232, Train Loss:0.5526733994483948\n",
      "Epoch 0[11085/17270] Time:0.235, Train Loss:0.6123666167259216\n",
      "Epoch 0[11086/17270] Time:0.231, Train Loss:0.5047025680541992\n",
      "Epoch 0[11087/17270] Time:0.225, Train Loss:0.762502908706665\n",
      "Epoch 0[11088/17270] Time:0.237, Train Loss:0.4210990071296692\n",
      "Epoch 0[11089/17270] Time:0.236, Train Loss:0.5820534229278564\n",
      "Epoch 0[11090/17270] Time:0.234, Train Loss:0.5777843594551086\n",
      "Epoch 0[11091/17270] Time:0.223, Train Loss:0.4218818247318268\n",
      "Epoch 0[11092/17270] Time:0.24, Train Loss:0.48352640867233276\n",
      "Epoch 0[11093/17270] Time:0.232, Train Loss:0.79224693775177\n",
      "Epoch 0[11094/17270] Time:0.235, Train Loss:0.8082844614982605\n",
      "Epoch 0[11095/17270] Time:0.237, Train Loss:0.42775145173072815\n",
      "Epoch 0[11096/17270] Time:0.233, Train Loss:1.913212776184082\n",
      "Epoch 0[11097/17270] Time:0.234, Train Loss:0.5215185284614563\n",
      "Epoch 0[11098/17270] Time:0.235, Train Loss:0.5422368049621582\n",
      "Epoch 0[11099/17270] Time:0.223, Train Loss:0.6530079245567322\n",
      "Epoch 0[11100/17270] Time:0.244, Train Loss:0.6332641243934631\n",
      "Epoch 0[11101/17270] Time:0.233, Train Loss:0.275835782289505\n",
      "Epoch 0[11102/17270] Time:0.24, Train Loss:0.7328779697418213\n",
      "Epoch 0[11103/17270] Time:0.234, Train Loss:0.6244539022445679\n",
      "Epoch 0[11104/17270] Time:0.238, Train Loss:0.41308140754699707\n",
      "Epoch 0[11105/17270] Time:0.234, Train Loss:0.4727008640766144\n",
      "Epoch 0[11106/17270] Time:0.235, Train Loss:0.6772122383117676\n",
      "Epoch 0[11107/17270] Time:0.235, Train Loss:0.5608869194984436\n",
      "Epoch 0[11108/17270] Time:0.221, Train Loss:0.8334328532218933\n",
      "Epoch 0[11109/17270] Time:0.232, Train Loss:0.536266565322876\n",
      "Epoch 0[11110/17270] Time:0.247, Train Loss:0.37356725335121155\n",
      "Epoch 0[11111/17270] Time:0.233, Train Loss:1.1694639921188354\n",
      "Epoch 0[11112/17270] Time:0.237, Train Loss:0.32271358370780945\n",
      "Epoch 0[11113/17270] Time:0.231, Train Loss:0.5474342703819275\n",
      "Epoch 0[11114/17270] Time:0.233, Train Loss:0.594530463218689\n",
      "Epoch 0[11115/17270] Time:0.232, Train Loss:0.7957743406295776\n",
      "Epoch 0[11116/17270] Time:0.243, Train Loss:0.38221660256385803\n",
      "Epoch 0[11117/17270] Time:0.238, Train Loss:0.4919078052043915\n",
      "Epoch 0[11118/17270] Time:0.242, Train Loss:0.3974272906780243\n",
      "Epoch 0[11119/17270] Time:0.232, Train Loss:0.5427737236022949\n",
      "Epoch 0[11120/17270] Time:0.239, Train Loss:0.3766818046569824\n",
      "Epoch 0[11121/17270] Time:0.234, Train Loss:0.5602720975875854\n",
      "Epoch 0[11122/17270] Time:0.24, Train Loss:0.4896868169307709\n",
      "Epoch 0[11123/17270] Time:0.235, Train Loss:1.0066384077072144\n",
      "Epoch 0[11124/17270] Time:0.237, Train Loss:0.30730804800987244\n",
      "Epoch 0[11125/17270] Time:0.249, Train Loss:0.3095315396785736\n",
      "Epoch 0[11126/17270] Time:0.25, Train Loss:0.6857364177703857\n",
      "Epoch 0[11127/17270] Time:0.223, Train Loss:0.33098170161247253\n",
      "Epoch 0[11128/17270] Time:0.245, Train Loss:0.618659257888794\n",
      "Epoch 0[11129/17270] Time:0.243, Train Loss:0.5150991678237915\n",
      "Epoch 0[11130/17270] Time:0.24, Train Loss:0.9275594353675842\n",
      "Epoch 0[11131/17270] Time:0.222, Train Loss:0.47750693559646606\n",
      "Epoch 0[11132/17270] Time:0.231, Train Loss:0.5672760605812073\n",
      "Epoch 0[11133/17270] Time:0.239, Train Loss:0.5934232473373413\n",
      "Epoch 0[11134/17270] Time:0.237, Train Loss:0.5565893054008484\n",
      "Epoch 0[11135/17270] Time:0.227, Train Loss:0.7486666440963745\n",
      "Epoch 0[11136/17270] Time:0.23, Train Loss:0.4716344177722931\n",
      "Epoch 0[11137/17270] Time:0.25, Train Loss:0.7440487146377563\n",
      "Epoch 0[11138/17270] Time:0.245, Train Loss:0.5303046703338623\n",
      "Epoch 0[11139/17270] Time:0.223, Train Loss:0.5256338715553284\n",
      "Epoch 0[11140/17270] Time:0.234, Train Loss:0.4827655255794525\n",
      "Epoch 0[11141/17270] Time:0.233, Train Loss:0.663466215133667\n",
      "Epoch 0[11142/17270] Time:0.24, Train Loss:0.5356400012969971\n",
      "Epoch 0[11143/17270] Time:0.233, Train Loss:0.9448023438453674\n",
      "Epoch 0[11144/17270] Time:0.242, Train Loss:0.8306168913841248\n",
      "Epoch 0[11145/17270] Time:0.236, Train Loss:0.32025405764579773\n",
      "Epoch 0[11146/17270] Time:0.22, Train Loss:0.6288319826126099\n",
      "Epoch 0[11147/17270] Time:0.243, Train Loss:0.6371632814407349\n",
      "Epoch 0[11148/17270] Time:0.226, Train Loss:0.8439244031906128\n",
      "Epoch 0[11149/17270] Time:0.246, Train Loss:0.3613291382789612\n",
      "Epoch 0[11150/17270] Time:0.234, Train Loss:0.3809637427330017\n",
      "Epoch 0[11151/17270] Time:0.229, Train Loss:0.4757539629936218\n",
      "Epoch 0[11152/17270] Time:0.227, Train Loss:0.4840989112854004\n",
      "Epoch 0[11153/17270] Time:0.226, Train Loss:0.3039991557598114\n",
      "Epoch 0[11154/17270] Time:0.237, Train Loss:0.4458742141723633\n",
      "Epoch 0[11155/17270] Time:0.239, Train Loss:0.4611005187034607\n",
      "Epoch 0[11156/17270] Time:0.227, Train Loss:0.49528735876083374\n",
      "Epoch 0[11157/17270] Time:0.237, Train Loss:1.1966081857681274\n",
      "Epoch 0[11158/17270] Time:0.227, Train Loss:0.6615222692489624\n",
      "Epoch 0[11159/17270] Time:0.232, Train Loss:0.38960257172584534\n",
      "Epoch 0[11160/17270] Time:0.229, Train Loss:0.6358658075332642\n",
      "Epoch 0[11161/17270] Time:0.245, Train Loss:0.6379090547561646\n",
      "Epoch 0[11162/17270] Time:0.238, Train Loss:0.3384981155395508\n",
      "Epoch 0[11163/17270] Time:0.243, Train Loss:0.24608443677425385\n",
      "Epoch 0[11164/17270] Time:0.228, Train Loss:0.49898022413253784\n",
      "Epoch 0[11165/17270] Time:0.238, Train Loss:2.01953387260437\n",
      "Epoch 0[11166/17270] Time:0.236, Train Loss:1.1326098442077637\n",
      "Epoch 0[11167/17270] Time:0.244, Train Loss:0.9046511054039001\n",
      "Epoch 0[11168/17270] Time:0.236, Train Loss:1.0621238946914673\n",
      "Epoch 0[11169/17270] Time:0.247, Train Loss:0.6878154873847961\n",
      "Epoch 0[11170/17270] Time:0.236, Train Loss:0.7233976721763611\n",
      "Epoch 0[11171/17270] Time:0.226, Train Loss:0.44825902581214905\n",
      "Epoch 0[11172/17270] Time:0.223, Train Loss:1.1275602579116821\n",
      "Epoch 0[11173/17270] Time:0.229, Train Loss:0.32646551728248596\n",
      "Epoch 0[11174/17270] Time:0.229, Train Loss:0.5531039237976074\n",
      "Epoch 0[11175/17270] Time:0.229, Train Loss:0.7433047294616699\n",
      "Epoch 0[11176/17270] Time:0.227, Train Loss:0.5358790755271912\n",
      "Epoch 0[11177/17270] Time:0.233, Train Loss:1.5001579523086548\n",
      "Epoch 0[11178/17270] Time:0.235, Train Loss:0.47402164340019226\n",
      "Epoch 0[11179/17270] Time:0.233, Train Loss:0.4982892870903015\n",
      "Epoch 0[11180/17270] Time:0.238, Train Loss:0.42264118790626526\n",
      "Epoch 0[11181/17270] Time:0.235, Train Loss:0.4196909964084625\n",
      "Epoch 0[11182/17270] Time:0.238, Train Loss:0.6878916025161743\n",
      "Epoch 0[11183/17270] Time:0.229, Train Loss:0.9252282381057739\n",
      "Epoch 0[11184/17270] Time:0.238, Train Loss:0.9883065819740295\n",
      "Epoch 0[11185/17270] Time:0.229, Train Loss:0.4301137328147888\n",
      "Epoch 0[11186/17270] Time:0.236, Train Loss:0.5310463309288025\n",
      "Epoch 0[11187/17270] Time:0.238, Train Loss:0.4698178172111511\n",
      "Epoch 0[11188/17270] Time:0.235, Train Loss:0.41283559799194336\n",
      "Epoch 0[11189/17270] Time:0.236, Train Loss:0.6930729150772095\n",
      "Epoch 0[11190/17270] Time:0.232, Train Loss:0.5900202989578247\n",
      "Epoch 0[11191/17270] Time:0.24, Train Loss:0.6804965138435364\n",
      "Epoch 0[11192/17270] Time:0.237, Train Loss:0.49787864089012146\n",
      "Epoch 0[11193/17270] Time:0.244, Train Loss:0.416022926568985\n",
      "Epoch 0[11194/17270] Time:0.233, Train Loss:0.6775533556938171\n",
      "Epoch 0[11195/17270] Time:0.234, Train Loss:0.4752694368362427\n",
      "Epoch 0[11196/17270] Time:0.237, Train Loss:0.8548279404640198\n",
      "Epoch 0[11197/17270] Time:0.241, Train Loss:0.29239416122436523\n",
      "Epoch 0[11198/17270] Time:0.249, Train Loss:0.8455551862716675\n",
      "Epoch 0[11199/17270] Time:0.225, Train Loss:1.047818660736084\n",
      "Epoch 0[11200/17270] Time:0.231, Train Loss:0.3921195864677429\n",
      "Epoch 0[11201/17270] Time:0.239, Train Loss:0.38421830534935\n",
      "Epoch 0[11202/17270] Time:0.248, Train Loss:0.9637024402618408\n",
      "Epoch 0[11203/17270] Time:0.226, Train Loss:0.44184911251068115\n",
      "Epoch 0[11204/17270] Time:0.236, Train Loss:0.4239265024662018\n",
      "Epoch 0[11205/17270] Time:0.238, Train Loss:0.6801753640174866\n",
      "Epoch 0[11206/17270] Time:0.24, Train Loss:0.5639009475708008\n",
      "Epoch 0[11207/17270] Time:0.232, Train Loss:0.5351287126541138\n",
      "Epoch 0[11208/17270] Time:0.235, Train Loss:0.6061839461326599\n",
      "Epoch 0[11209/17270] Time:0.236, Train Loss:0.8945506811141968\n",
      "Epoch 0[11210/17270] Time:0.234, Train Loss:0.932460367679596\n",
      "Epoch 0[11211/17270] Time:0.233, Train Loss:0.5376625061035156\n",
      "Epoch 0[11212/17270] Time:0.247, Train Loss:0.5991920828819275\n",
      "Epoch 0[11213/17270] Time:0.234, Train Loss:0.6812784075737\n",
      "Epoch 0[11214/17270] Time:0.239, Train Loss:0.40103259682655334\n",
      "Epoch 0[11215/17270] Time:0.239, Train Loss:0.5096588134765625\n",
      "Epoch 0[11216/17270] Time:0.239, Train Loss:0.793779194355011\n",
      "Epoch 0[11217/17270] Time:0.238, Train Loss:1.2687163352966309\n",
      "Epoch 0[11218/17270] Time:0.239, Train Loss:0.7713034749031067\n",
      "Epoch 0[11219/17270] Time:0.239, Train Loss:0.6441454291343689\n",
      "Epoch 0[11220/17270] Time:0.232, Train Loss:0.8579081892967224\n",
      "Epoch 0[11221/17270] Time:0.236, Train Loss:1.0611810684204102\n",
      "Epoch 0[11222/17270] Time:0.226, Train Loss:0.42737019062042236\n",
      "Epoch 0[11223/17270] Time:0.23, Train Loss:0.7024663090705872\n",
      "Epoch 0[11224/17270] Time:0.241, Train Loss:0.7007492184638977\n",
      "Epoch 0[11225/17270] Time:0.238, Train Loss:0.5743552446365356\n",
      "Epoch 0[11226/17270] Time:0.231, Train Loss:0.584701418876648\n",
      "Epoch 0[11227/17270] Time:0.237, Train Loss:0.42856866121292114\n",
      "Epoch 0[11228/17270] Time:0.23, Train Loss:0.7754049897193909\n",
      "Epoch 0[11229/17270] Time:0.233, Train Loss:0.564423680305481\n",
      "Epoch 0[11230/17270] Time:0.231, Train Loss:0.6361908316612244\n",
      "Epoch 0[11231/17270] Time:0.248, Train Loss:0.3975992500782013\n",
      "Epoch 0[11232/17270] Time:0.235, Train Loss:0.9017046093940735\n",
      "Epoch 0[11233/17270] Time:0.228, Train Loss:0.5806863307952881\n",
      "Epoch 0[11234/17270] Time:0.244, Train Loss:0.5747519135475159\n",
      "Epoch 0[11235/17270] Time:0.244, Train Loss:0.4598865211009979\n",
      "Epoch 0[11236/17270] Time:0.24, Train Loss:0.27805590629577637\n",
      "Epoch 0[11237/17270] Time:0.237, Train Loss:0.3960838317871094\n",
      "Epoch 0[11238/17270] Time:0.242, Train Loss:0.5051935911178589\n",
      "Epoch 0[11239/17270] Time:0.227, Train Loss:0.6400214433670044\n",
      "Epoch 0[11240/17270] Time:0.23, Train Loss:0.36623141169548035\n",
      "Epoch 0[11241/17270] Time:0.24, Train Loss:0.5191550254821777\n",
      "Epoch 0[11242/17270] Time:0.25, Train Loss:0.4224879741668701\n",
      "Epoch 0[11243/17270] Time:0.233, Train Loss:1.0993049144744873\n",
      "Epoch 0[11244/17270] Time:0.233, Train Loss:0.5283063054084778\n",
      "Epoch 0[11245/17270] Time:0.243, Train Loss:0.6974305510520935\n",
      "Epoch 0[11246/17270] Time:0.236, Train Loss:1.2625404596328735\n",
      "Epoch 0[11247/17270] Time:0.226, Train Loss:0.4793573319911957\n",
      "Epoch 0[11248/17270] Time:0.226, Train Loss:0.31055158376693726\n",
      "Epoch 0[11249/17270] Time:0.228, Train Loss:0.32017865777015686\n",
      "Epoch 0[11250/17270] Time:0.237, Train Loss:0.555813193321228\n",
      "Epoch 0[11251/17270] Time:0.233, Train Loss:0.5139365196228027\n",
      "Epoch 0[11252/17270] Time:0.232, Train Loss:0.4118245542049408\n",
      "Epoch 0[11253/17270] Time:0.228, Train Loss:0.6890718936920166\n",
      "Epoch 0[11254/17270] Time:0.231, Train Loss:0.32273876667022705\n",
      "Epoch 0[11255/17270] Time:0.232, Train Loss:0.4097261428833008\n",
      "Epoch 0[11256/17270] Time:0.234, Train Loss:0.4019612967967987\n",
      "Epoch 0[11257/17270] Time:0.23, Train Loss:0.9702361822128296\n",
      "Epoch 0[11258/17270] Time:0.24, Train Loss:0.9073156714439392\n",
      "Epoch 0[11259/17270] Time:0.235, Train Loss:0.6855071187019348\n",
      "Epoch 0[11260/17270] Time:0.233, Train Loss:0.6467028856277466\n",
      "Epoch 0[11261/17270] Time:0.23, Train Loss:0.4342038333415985\n",
      "Epoch 0[11262/17270] Time:0.239, Train Loss:0.43916940689086914\n",
      "Epoch 0[11263/17270] Time:0.23, Train Loss:0.6685534715652466\n",
      "Epoch 0[11264/17270] Time:0.236, Train Loss:0.599810004234314\n",
      "Epoch 0[11265/17270] Time:0.229, Train Loss:0.9171284437179565\n",
      "Epoch 0[11266/17270] Time:0.239, Train Loss:0.6715025305747986\n",
      "Epoch 0[11267/17270] Time:0.23, Train Loss:1.0264261960983276\n",
      "Epoch 0[11268/17270] Time:0.239, Train Loss:0.3245941400527954\n",
      "Epoch 0[11269/17270] Time:0.232, Train Loss:0.701718807220459\n",
      "Epoch 0[11270/17270] Time:0.228, Train Loss:0.4384019672870636\n",
      "Epoch 0[11271/17270] Time:0.228, Train Loss:1.0532715320587158\n",
      "Epoch 0[11272/17270] Time:0.231, Train Loss:0.9713866710662842\n",
      "Epoch 0[11273/17270] Time:0.229, Train Loss:0.5684424638748169\n",
      "Epoch 0[11274/17270] Time:0.236, Train Loss:0.5101913809776306\n",
      "Epoch 0[11275/17270] Time:0.236, Train Loss:1.4889850616455078\n",
      "Epoch 0[11276/17270] Time:0.239, Train Loss:0.6227081418037415\n",
      "Epoch 0[11277/17270] Time:0.239, Train Loss:0.6188716292381287\n",
      "Epoch 0[11278/17270] Time:0.23, Train Loss:0.9496651291847229\n",
      "Epoch 0[11279/17270] Time:0.241, Train Loss:0.6295955181121826\n",
      "Epoch 0[11280/17270] Time:0.233, Train Loss:0.7166305184364319\n",
      "Epoch 0[11281/17270] Time:0.237, Train Loss:0.3548356592655182\n",
      "Epoch 0[11282/17270] Time:0.248, Train Loss:0.6598744988441467\n",
      "Epoch 0[11283/17270] Time:0.238, Train Loss:0.40641912817955017\n",
      "Epoch 0[11284/17270] Time:0.235, Train Loss:0.5220020413398743\n",
      "Epoch 0[11285/17270] Time:0.233, Train Loss:0.6894287467002869\n",
      "Epoch 0[11286/17270] Time:0.236, Train Loss:0.5299519896507263\n",
      "Epoch 0[11287/17270] Time:0.236, Train Loss:0.4298675060272217\n",
      "Epoch 0[11288/17270] Time:0.241, Train Loss:0.4754563271999359\n",
      "Epoch 0[11289/17270] Time:0.239, Train Loss:0.3997151851654053\n",
      "Epoch 0[11290/17270] Time:0.235, Train Loss:0.5253015160560608\n",
      "Epoch 0[11291/17270] Time:0.236, Train Loss:0.3851385712623596\n",
      "Epoch 0[11292/17270] Time:0.225, Train Loss:0.5256009101867676\n",
      "Epoch 0[11293/17270] Time:0.245, Train Loss:0.7482035160064697\n",
      "Epoch 0[11294/17270] Time:0.23, Train Loss:0.4901007115840912\n",
      "Epoch 0[11295/17270] Time:0.235, Train Loss:0.32477009296417236\n",
      "Epoch 0[11296/17270] Time:0.23, Train Loss:0.5384567379951477\n",
      "Epoch 0[11297/17270] Time:0.238, Train Loss:0.6092920303344727\n",
      "Epoch 0[11298/17270] Time:0.238, Train Loss:0.6649866700172424\n",
      "Epoch 0[11299/17270] Time:0.236, Train Loss:0.839480996131897\n",
      "Epoch 0[11300/17270] Time:0.231, Train Loss:0.52032870054245\n",
      "Epoch 0[11301/17270] Time:0.23, Train Loss:1.077221393585205\n",
      "Epoch 0[11302/17270] Time:0.237, Train Loss:0.5474412441253662\n",
      "Epoch 0[11303/17270] Time:0.237, Train Loss:0.5344352126121521\n",
      "Epoch 0[11304/17270] Time:0.235, Train Loss:0.8013827204704285\n",
      "Epoch 0[11305/17270] Time:0.237, Train Loss:0.2694810926914215\n",
      "Epoch 0[11306/17270] Time:0.234, Train Loss:0.6789252758026123\n",
      "Epoch 0[11307/17270] Time:0.238, Train Loss:0.5797241926193237\n",
      "Epoch 0[11308/17270] Time:0.233, Train Loss:0.47867441177368164\n",
      "Epoch 0[11309/17270] Time:0.237, Train Loss:0.8923448920249939\n",
      "Epoch 0[11310/17270] Time:0.225, Train Loss:0.37551215291023254\n",
      "Epoch 0[11311/17270] Time:0.228, Train Loss:0.4599341154098511\n",
      "Epoch 0[11312/17270] Time:0.241, Train Loss:0.6239351034164429\n",
      "Epoch 0[11313/17270] Time:0.224, Train Loss:1.0331627130508423\n",
      "Epoch 0[11314/17270] Time:0.239, Train Loss:0.3950052857398987\n",
      "Epoch 0[11315/17270] Time:0.231, Train Loss:0.5299643874168396\n",
      "Epoch 0[11316/17270] Time:0.234, Train Loss:0.7094665169715881\n",
      "Epoch 0[11317/17270] Time:0.214, Train Loss:0.5236104726791382\n",
      "Epoch 0[11318/17270] Time:0.247, Train Loss:0.7981841564178467\n",
      "Epoch 0[11319/17270] Time:0.24, Train Loss:0.43401122093200684\n",
      "Epoch 0[11320/17270] Time:0.235, Train Loss:0.5190454125404358\n",
      "Epoch 0[11321/17270] Time:0.242, Train Loss:0.5905541181564331\n",
      "Epoch 0[11322/17270] Time:0.238, Train Loss:0.5543599724769592\n",
      "Epoch 0[11323/17270] Time:0.237, Train Loss:0.4486132264137268\n",
      "Epoch 0[11324/17270] Time:0.243, Train Loss:0.36635732650756836\n",
      "Epoch 0[11325/17270] Time:0.233, Train Loss:0.42574894428253174\n",
      "Epoch 0[11326/17270] Time:0.251, Train Loss:0.6139187216758728\n",
      "Epoch 0[11327/17270] Time:0.223, Train Loss:0.47151750326156616\n",
      "Epoch 0[11328/17270] Time:0.239, Train Loss:0.46501097083091736\n",
      "Epoch 0[11329/17270] Time:0.23, Train Loss:0.3052756190299988\n",
      "Epoch 0[11330/17270] Time:0.237, Train Loss:0.5276991128921509\n",
      "Epoch 0[11331/17270] Time:0.234, Train Loss:0.4708821475505829\n",
      "Epoch 0[11332/17270] Time:0.229, Train Loss:2.0461697578430176\n",
      "Epoch 0[11333/17270] Time:0.234, Train Loss:0.7006228566169739\n",
      "Epoch 0[11334/17270] Time:0.243, Train Loss:0.9587519764900208\n",
      "Epoch 0[11335/17270] Time:0.238, Train Loss:0.4530458450317383\n",
      "Epoch 0[11336/17270] Time:0.234, Train Loss:1.5230674743652344\n",
      "Epoch 0[11337/17270] Time:0.224, Train Loss:1.1520800590515137\n",
      "Epoch 0[11338/17270] Time:0.241, Train Loss:0.6597847938537598\n",
      "Epoch 0[11339/17270] Time:0.228, Train Loss:0.37871304154396057\n",
      "Epoch 0[11340/17270] Time:0.23, Train Loss:0.7982511520385742\n",
      "Epoch 0[11341/17270] Time:0.225, Train Loss:0.4949089586734772\n",
      "Epoch 0[11342/17270] Time:0.235, Train Loss:0.32568326592445374\n",
      "Epoch 0[11343/17270] Time:0.228, Train Loss:0.46670830249786377\n",
      "Epoch 0[11344/17270] Time:0.233, Train Loss:0.568749725818634\n",
      "Epoch 0[11345/17270] Time:0.234, Train Loss:0.6352766752243042\n",
      "Epoch 0[11346/17270] Time:0.24, Train Loss:0.46327248215675354\n",
      "Epoch 0[11347/17270] Time:0.234, Train Loss:0.41086506843566895\n",
      "Epoch 0[11348/17270] Time:0.234, Train Loss:0.5183602571487427\n",
      "Epoch 0[11349/17270] Time:0.234, Train Loss:0.688679575920105\n",
      "Epoch 0[11350/17270] Time:0.231, Train Loss:0.6074461936950684\n",
      "Epoch 0[11351/17270] Time:0.231, Train Loss:0.3862971365451813\n",
      "Epoch 0[11352/17270] Time:0.233, Train Loss:0.5715349912643433\n",
      "Epoch 0[11353/17270] Time:0.234, Train Loss:0.7187539935112\n",
      "Epoch 0[11354/17270] Time:0.239, Train Loss:0.27758920192718506\n",
      "Epoch 0[11355/17270] Time:0.222, Train Loss:0.9941623210906982\n",
      "Epoch 0[11356/17270] Time:0.237, Train Loss:0.6241983771324158\n",
      "Epoch 0[11357/17270] Time:0.229, Train Loss:0.5048549771308899\n",
      "Epoch 0[11358/17270] Time:0.23, Train Loss:0.41210559010505676\n",
      "Epoch 0[11359/17270] Time:0.227, Train Loss:0.5356469750404358\n",
      "Epoch 0[11360/17270] Time:0.227, Train Loss:0.7269041538238525\n",
      "Epoch 0[11361/17270] Time:0.259, Train Loss:0.5370542407035828\n",
      "Epoch 0[11362/17270] Time:0.225, Train Loss:0.8093087673187256\n",
      "Epoch 0[11363/17270] Time:0.241, Train Loss:0.4734250009059906\n",
      "Epoch 0[11364/17270] Time:0.231, Train Loss:0.36994776129722595\n",
      "Epoch 0[11365/17270] Time:0.232, Train Loss:0.5896158218383789\n",
      "Epoch 0[11366/17270] Time:0.232, Train Loss:0.47211283445358276\n",
      "Epoch 0[11367/17270] Time:0.23, Train Loss:0.44085627794265747\n",
      "Epoch 0[11368/17270] Time:0.238, Train Loss:0.36142659187316895\n",
      "Epoch 0[11369/17270] Time:0.236, Train Loss:0.45157691836357117\n",
      "Epoch 0[11370/17270] Time:0.242, Train Loss:0.4614667594432831\n",
      "Epoch 0[11371/17270] Time:0.233, Train Loss:0.2498106062412262\n",
      "Epoch 0[11372/17270] Time:0.236, Train Loss:0.24946069717407227\n",
      "Epoch 0[11373/17270] Time:0.23, Train Loss:0.31061023473739624\n",
      "Epoch 0[11374/17270] Time:0.228, Train Loss:0.3603754937648773\n",
      "Epoch 0[11375/17270] Time:0.239, Train Loss:0.4998857080936432\n",
      "Epoch 0[11376/17270] Time:0.231, Train Loss:0.5880616307258606\n",
      "Epoch 0[11377/17270] Time:0.227, Train Loss:0.2253085821866989\n",
      "Epoch 0[11378/17270] Time:0.238, Train Loss:0.3783174455165863\n",
      "Epoch 0[11379/17270] Time:0.232, Train Loss:0.5619587898254395\n",
      "Epoch 0[11380/17270] Time:0.23, Train Loss:1.3421729803085327\n",
      "Epoch 0[11381/17270] Time:0.239, Train Loss:0.4242519438266754\n",
      "Epoch 0[11382/17270] Time:0.233, Train Loss:0.792143702507019\n",
      "Epoch 0[11383/17270] Time:0.237, Train Loss:0.4561740756034851\n",
      "Epoch 0[11384/17270] Time:0.239, Train Loss:0.4230092763900757\n",
      "Epoch 0[11385/17270] Time:0.237, Train Loss:1.2367451190948486\n",
      "Epoch 0[11386/17270] Time:0.237, Train Loss:1.2920267581939697\n",
      "Epoch 0[11387/17270] Time:0.237, Train Loss:0.5976640582084656\n",
      "Epoch 0[11388/17270] Time:0.236, Train Loss:0.5620574355125427\n",
      "Epoch 0[11389/17270] Time:0.236, Train Loss:0.6420724391937256\n",
      "Epoch 0[11390/17270] Time:0.239, Train Loss:0.6706459522247314\n",
      "Epoch 0[11391/17270] Time:0.235, Train Loss:0.6474673748016357\n",
      "Epoch 0[11392/17270] Time:0.226, Train Loss:0.7993636131286621\n",
      "Epoch 0[11393/17270] Time:0.229, Train Loss:1.0496050119400024\n",
      "Epoch 0[11394/17270] Time:0.229, Train Loss:0.5933548808097839\n",
      "Epoch 0[11395/17270] Time:0.229, Train Loss:0.587111234664917\n",
      "Epoch 0[11396/17270] Time:0.228, Train Loss:0.5293106436729431\n",
      "Epoch 0[11397/17270] Time:0.227, Train Loss:0.5738853812217712\n",
      "Epoch 0[11398/17270] Time:0.229, Train Loss:0.37109997868537903\n",
      "Epoch 0[11399/17270] Time:0.237, Train Loss:0.564868688583374\n",
      "Epoch 0[11400/17270] Time:0.237, Train Loss:0.38319501280784607\n",
      "Epoch 0[11401/17270] Time:0.238, Train Loss:0.39569538831710815\n",
      "Epoch 0[11402/17270] Time:0.248, Train Loss:0.9533321857452393\n",
      "Epoch 0[11403/17270] Time:0.231, Train Loss:1.106640100479126\n",
      "Epoch 0[11404/17270] Time:0.24, Train Loss:0.2444511204957962\n",
      "Epoch 0[11405/17270] Time:0.222, Train Loss:0.36635324358940125\n",
      "Epoch 0[11406/17270] Time:0.232, Train Loss:0.46213439106941223\n",
      "Epoch 0[11407/17270] Time:0.235, Train Loss:0.8599521517753601\n",
      "Epoch 0[11408/17270] Time:0.237, Train Loss:0.5322328209877014\n",
      "Epoch 0[11409/17270] Time:0.238, Train Loss:0.7102471590042114\n",
      "Epoch 0[11410/17270] Time:0.231, Train Loss:0.47625860571861267\n",
      "Epoch 0[11411/17270] Time:0.237, Train Loss:0.5606766939163208\n",
      "Epoch 0[11412/17270] Time:0.239, Train Loss:0.5800929665565491\n",
      "Epoch 0[11413/17270] Time:0.248, Train Loss:1.39313805103302\n",
      "Epoch 0[11414/17270] Time:0.229, Train Loss:0.842803418636322\n",
      "Epoch 0[11415/17270] Time:0.235, Train Loss:0.2140897512435913\n",
      "Epoch 0[11416/17270] Time:0.246, Train Loss:0.564332127571106\n",
      "Epoch 0[11417/17270] Time:0.233, Train Loss:0.43303051590919495\n",
      "Epoch 0[11418/17270] Time:0.219, Train Loss:0.5491581559181213\n",
      "Epoch 0[11419/17270] Time:0.238, Train Loss:0.3664539158344269\n",
      "Epoch 0[11420/17270] Time:0.23, Train Loss:1.0020356178283691\n",
      "Epoch 0[11421/17270] Time:0.25, Train Loss:0.6704540848731995\n",
      "Epoch 0[11422/17270] Time:0.228, Train Loss:0.5593618154525757\n",
      "Epoch 0[11423/17270] Time:0.237, Train Loss:0.6334407925605774\n",
      "Epoch 0[11424/17270] Time:0.238, Train Loss:0.6355491876602173\n",
      "Epoch 0[11425/17270] Time:0.242, Train Loss:0.4492383301258087\n",
      "Epoch 0[11426/17270] Time:0.236, Train Loss:0.4753265678882599\n",
      "Epoch 0[11427/17270] Time:0.252, Train Loss:0.3893548846244812\n",
      "Epoch 0[11428/17270] Time:0.236, Train Loss:0.5175591707229614\n",
      "Epoch 0[11429/17270] Time:0.235, Train Loss:0.2723120152950287\n",
      "Epoch 0[11430/17270] Time:0.231, Train Loss:0.45708757638931274\n",
      "Epoch 0[11431/17270] Time:0.238, Train Loss:0.4686093330383301\n",
      "Epoch 0[11432/17270] Time:0.23, Train Loss:0.8194157481193542\n",
      "Epoch 0[11433/17270] Time:0.229, Train Loss:0.6409621834754944\n",
      "Epoch 0[11434/17270] Time:0.234, Train Loss:0.42632779479026794\n",
      "Epoch 0[11435/17270] Time:0.23, Train Loss:0.31481513381004333\n",
      "Epoch 0[11436/17270] Time:0.231, Train Loss:0.5508798956871033\n",
      "Epoch 0[11437/17270] Time:0.238, Train Loss:0.4451247453689575\n",
      "Epoch 0[11438/17270] Time:0.23, Train Loss:0.37645405530929565\n",
      "Epoch 0[11439/17270] Time:0.238, Train Loss:0.6107520461082458\n",
      "Epoch 0[11440/17270] Time:0.237, Train Loss:0.46997109055519104\n",
      "Epoch 0[11441/17270] Time:0.233, Train Loss:0.8989862203598022\n",
      "Epoch 0[11442/17270] Time:0.236, Train Loss:0.6268205642700195\n",
      "Epoch 0[11443/17270] Time:0.245, Train Loss:0.9578062295913696\n",
      "Epoch 0[11444/17270] Time:0.228, Train Loss:0.3499305546283722\n",
      "Epoch 0[11445/17270] Time:0.233, Train Loss:0.3777735233306885\n",
      "Epoch 0[11446/17270] Time:0.23, Train Loss:0.3037155866622925\n",
      "Epoch 0[11447/17270] Time:0.234, Train Loss:0.4745354950428009\n",
      "Epoch 0[11448/17270] Time:0.228, Train Loss:1.031171202659607\n",
      "Epoch 0[11449/17270] Time:0.23, Train Loss:0.3465386629104614\n",
      "Epoch 0[11450/17270] Time:0.239, Train Loss:0.5507648587226868\n",
      "Epoch 0[11451/17270] Time:0.239, Train Loss:1.316169261932373\n",
      "Epoch 0[11452/17270] Time:0.259, Train Loss:0.4972793459892273\n",
      "Epoch 0[11453/17270] Time:0.231, Train Loss:0.5913369059562683\n",
      "Epoch 0[11454/17270] Time:0.227, Train Loss:0.5126748085021973\n",
      "Epoch 0[11455/17270] Time:0.224, Train Loss:0.3850002586841583\n",
      "Epoch 0[11456/17270] Time:0.238, Train Loss:0.7275557518005371\n",
      "Epoch 0[11457/17270] Time:0.236, Train Loss:0.3157251477241516\n",
      "Epoch 0[11458/17270] Time:0.23, Train Loss:0.572553277015686\n",
      "Epoch 0[11459/17270] Time:0.23, Train Loss:0.6475863456726074\n",
      "Epoch 0[11460/17270] Time:0.235, Train Loss:0.452762633562088\n",
      "Epoch 0[11461/17270] Time:0.236, Train Loss:0.6169858574867249\n",
      "Epoch 0[11462/17270] Time:0.238, Train Loss:0.5550717711448669\n",
      "Epoch 0[11463/17270] Time:0.236, Train Loss:0.3929499089717865\n",
      "Epoch 0[11464/17270] Time:0.225, Train Loss:0.6615108251571655\n",
      "Epoch 0[11465/17270] Time:0.239, Train Loss:0.35541537404060364\n",
      "Epoch 0[11466/17270] Time:0.243, Train Loss:0.451325386762619\n",
      "Epoch 0[11467/17270] Time:0.233, Train Loss:0.4378533363342285\n",
      "Epoch 0[11468/17270] Time:0.23, Train Loss:0.36861518025398254\n",
      "Epoch 0[11469/17270] Time:0.238, Train Loss:0.6446115374565125\n",
      "Epoch 0[11470/17270] Time:0.225, Train Loss:0.39207157492637634\n",
      "Epoch 0[11471/17270] Time:0.237, Train Loss:0.3289535939693451\n",
      "Epoch 0[11472/17270] Time:0.23, Train Loss:0.6188017129898071\n",
      "Epoch 0[11473/17270] Time:0.239, Train Loss:0.42231282591819763\n",
      "Epoch 0[11474/17270] Time:0.236, Train Loss:0.62300705909729\n",
      "Epoch 0[11475/17270] Time:0.236, Train Loss:0.4431522488594055\n",
      "Epoch 0[11476/17270] Time:0.257, Train Loss:0.2432336062192917\n",
      "Epoch 0[11477/17270] Time:0.226, Train Loss:0.2187805026769638\n",
      "Epoch 0[11478/17270] Time:0.247, Train Loss:0.3548702597618103\n",
      "Epoch 0[11479/17270] Time:0.224, Train Loss:0.7776889204978943\n",
      "Epoch 0[11480/17270] Time:0.246, Train Loss:0.6761989593505859\n",
      "Epoch 0[11481/17270] Time:0.237, Train Loss:1.1421798467636108\n",
      "Epoch 0[11482/17270] Time:0.223, Train Loss:0.3908751606941223\n",
      "Epoch 0[11483/17270] Time:0.243, Train Loss:0.2749386429786682\n",
      "Epoch 0[11484/17270] Time:0.243, Train Loss:0.45232683420181274\n",
      "Epoch 0[11485/17270] Time:0.227, Train Loss:0.28736045956611633\n",
      "Epoch 0[11486/17270] Time:0.228, Train Loss:0.327551007270813\n",
      "Epoch 0[11487/17270] Time:0.241, Train Loss:0.6449459791183472\n",
      "Epoch 0[11488/17270] Time:0.243, Train Loss:0.516484797000885\n",
      "Epoch 0[11489/17270] Time:0.234, Train Loss:0.4037206470966339\n",
      "Epoch 0[11490/17270] Time:0.222, Train Loss:0.4724794924259186\n",
      "Epoch 0[11491/17270] Time:0.237, Train Loss:0.9761512875556946\n",
      "Epoch 0[11492/17270] Time:0.24, Train Loss:0.5535549521446228\n",
      "Epoch 0[11493/17270] Time:0.233, Train Loss:0.5333544015884399\n",
      "Epoch 0[11494/17270] Time:0.243, Train Loss:0.3314594328403473\n",
      "Epoch 0[11495/17270] Time:0.229, Train Loss:0.5828120708465576\n",
      "Epoch 0[11496/17270] Time:0.242, Train Loss:0.3152417540550232\n",
      "Epoch 0[11497/17270] Time:0.241, Train Loss:0.6079902648925781\n",
      "Epoch 0[11498/17270] Time:0.254, Train Loss:0.6961581110954285\n",
      "Epoch 0[11499/17270] Time:0.227, Train Loss:0.43239226937294006\n",
      "Epoch 0[11500/17270] Time:0.229, Train Loss:0.46540895104408264\n",
      "Epoch 0[11501/17270] Time:0.226, Train Loss:0.4005975127220154\n",
      "Epoch 0[11502/17270] Time:0.236, Train Loss:0.746559202671051\n",
      "Epoch 0[11503/17270] Time:0.234, Train Loss:0.4008468985557556\n",
      "Epoch 0[11504/17270] Time:0.236, Train Loss:0.393557608127594\n",
      "Epoch 0[11505/17270] Time:0.247, Train Loss:0.6394338011741638\n",
      "Epoch 0[11506/17270] Time:0.225, Train Loss:0.430029034614563\n",
      "Epoch 0[11507/17270] Time:0.242, Train Loss:0.5746979117393494\n",
      "Epoch 0[11508/17270] Time:0.241, Train Loss:0.5179564952850342\n",
      "Epoch 0[11509/17270] Time:0.232, Train Loss:0.7843608856201172\n",
      "Epoch 0[11510/17270] Time:0.237, Train Loss:0.2971673011779785\n",
      "Epoch 0[11511/17270] Time:0.233, Train Loss:0.3748273253440857\n",
      "Epoch 0[11512/17270] Time:0.233, Train Loss:0.4348946511745453\n",
      "Epoch 0[11513/17270] Time:0.233, Train Loss:0.41260579228401184\n",
      "Epoch 0[11514/17270] Time:0.236, Train Loss:0.4084050953388214\n",
      "Epoch 0[11515/17270] Time:0.241, Train Loss:0.5839515328407288\n",
      "Epoch 0[11516/17270] Time:0.23, Train Loss:0.48930883407592773\n",
      "Epoch 0[11517/17270] Time:0.236, Train Loss:1.0072470903396606\n",
      "Epoch 0[11518/17270] Time:0.236, Train Loss:0.8357807397842407\n",
      "Epoch 0[11519/17270] Time:0.238, Train Loss:0.7263922691345215\n",
      "Epoch 0[11520/17270] Time:0.242, Train Loss:0.8075835704803467\n",
      "Epoch 0[11521/17270] Time:0.227, Train Loss:0.34844520688056946\n",
      "Epoch 0[11522/17270] Time:0.238, Train Loss:0.4123865067958832\n",
      "Epoch 0[11523/17270] Time:0.241, Train Loss:0.39635518193244934\n",
      "Epoch 0[11524/17270] Time:0.226, Train Loss:0.3007526099681854\n",
      "Epoch 0[11525/17270] Time:0.236, Train Loss:0.9499412775039673\n",
      "Epoch 0[11526/17270] Time:0.234, Train Loss:0.7415734529495239\n",
      "Epoch 0[11527/17270] Time:0.235, Train Loss:0.47529441118240356\n",
      "Epoch 0[11528/17270] Time:0.233, Train Loss:0.46138429641723633\n",
      "Epoch 0[11529/17270] Time:0.241, Train Loss:0.5447468757629395\n",
      "Epoch 0[11530/17270] Time:0.233, Train Loss:0.8070574402809143\n",
      "Epoch 0[11531/17270] Time:0.23, Train Loss:0.4040547311306\n",
      "Epoch 0[11532/17270] Time:0.234, Train Loss:0.6192449331283569\n",
      "Epoch 0[11533/17270] Time:0.23, Train Loss:0.4364427328109741\n",
      "Epoch 0[11534/17270] Time:0.234, Train Loss:0.3255595862865448\n",
      "Epoch 0[11535/17270] Time:0.236, Train Loss:0.7830854654312134\n",
      "Epoch 0[11536/17270] Time:0.23, Train Loss:0.4439339339733124\n",
      "Epoch 0[11537/17270] Time:0.234, Train Loss:0.6973841786384583\n",
      "Epoch 0[11538/17270] Time:0.228, Train Loss:0.5829055905342102\n",
      "Epoch 0[11539/17270] Time:0.247, Train Loss:0.8157566785812378\n",
      "Epoch 0[11540/17270] Time:0.232, Train Loss:1.1807795763015747\n",
      "Epoch 0[11541/17270] Time:0.232, Train Loss:0.6163648366928101\n",
      "Epoch 0[11542/17270] Time:0.232, Train Loss:0.8274293541908264\n",
      "Epoch 0[11543/17270] Time:0.232, Train Loss:0.5990868210792542\n",
      "Epoch 0[11544/17270] Time:0.234, Train Loss:0.8045457601547241\n",
      "Epoch 0[11545/17270] Time:0.232, Train Loss:0.4564635157585144\n",
      "Epoch 0[11546/17270] Time:0.241, Train Loss:0.4838581085205078\n",
      "Epoch 0[11547/17270] Time:0.224, Train Loss:0.5723916888237\n",
      "Epoch 0[11548/17270] Time:0.244, Train Loss:0.6291150450706482\n",
      "Epoch 0[11549/17270] Time:0.229, Train Loss:0.704482913017273\n",
      "Epoch 0[11550/17270] Time:0.229, Train Loss:0.39567410945892334\n",
      "Epoch 0[11551/17270] Time:0.252, Train Loss:0.3601350784301758\n",
      "Epoch 0[11552/17270] Time:0.223, Train Loss:0.49496355652809143\n",
      "Epoch 0[11553/17270] Time:0.248, Train Loss:0.44882431626319885\n",
      "Epoch 0[11554/17270] Time:0.244, Train Loss:0.2867701053619385\n",
      "Epoch 0[11555/17270] Time:0.233, Train Loss:0.7005245685577393\n",
      "Epoch 0[11556/17270] Time:0.233, Train Loss:0.5477631092071533\n",
      "Epoch 0[11557/17270] Time:0.235, Train Loss:0.8628694415092468\n",
      "Epoch 0[11558/17270] Time:0.235, Train Loss:0.5673011541366577\n",
      "Epoch 0[11559/17270] Time:0.235, Train Loss:0.44135621190071106\n",
      "Epoch 0[11560/17270] Time:0.233, Train Loss:0.36907848715782166\n",
      "Epoch 0[11561/17270] Time:0.234, Train Loss:0.7935348749160767\n",
      "Epoch 0[11562/17270] Time:0.232, Train Loss:1.0835111141204834\n",
      "Epoch 0[11563/17270] Time:0.234, Train Loss:0.47489574551582336\n",
      "Epoch 0[11564/17270] Time:0.222, Train Loss:0.4470060467720032\n",
      "Epoch 0[11565/17270] Time:0.247, Train Loss:0.4927223026752472\n",
      "Epoch 0[11566/17270] Time:0.232, Train Loss:0.7763462662696838\n",
      "Epoch 0[11567/17270] Time:0.233, Train Loss:0.4994184672832489\n",
      "Epoch 0[11568/17270] Time:0.238, Train Loss:0.43434226512908936\n",
      "Epoch 0[11569/17270] Time:0.244, Train Loss:0.3132987320423126\n",
      "Epoch 0[11570/17270] Time:0.237, Train Loss:1.255640983581543\n",
      "Epoch 0[11571/17270] Time:0.233, Train Loss:0.5835793018341064\n",
      "Epoch 0[11572/17270] Time:0.242, Train Loss:0.8792625665664673\n",
      "Epoch 0[11573/17270] Time:0.232, Train Loss:0.6350823640823364\n",
      "Epoch 0[11574/17270] Time:0.233, Train Loss:0.5014116764068604\n",
      "Epoch 0[11575/17270] Time:0.24, Train Loss:0.5378299951553345\n",
      "Epoch 0[11576/17270] Time:0.24, Train Loss:0.5485512614250183\n",
      "Epoch 0[11577/17270] Time:0.233, Train Loss:0.6745717525482178\n",
      "Epoch 0[11578/17270] Time:0.23, Train Loss:0.6401246190071106\n",
      "Epoch 0[11579/17270] Time:0.234, Train Loss:0.37033772468566895\n",
      "Epoch 0[11580/17270] Time:0.234, Train Loss:0.6767969727516174\n",
      "Epoch 0[11581/17270] Time:0.225, Train Loss:0.46964216232299805\n",
      "Epoch 0[11582/17270] Time:0.239, Train Loss:0.5098787546157837\n",
      "Epoch 0[11583/17270] Time:0.232, Train Loss:0.43195536732673645\n",
      "Epoch 0[11584/17270] Time:0.243, Train Loss:0.558837890625\n",
      "Epoch 0[11585/17270] Time:0.243, Train Loss:0.298713743686676\n",
      "Epoch 0[11586/17270] Time:0.234, Train Loss:0.38511428236961365\n",
      "Epoch 0[11587/17270] Time:0.237, Train Loss:1.4099491834640503\n",
      "Epoch 0[11588/17270] Time:0.233, Train Loss:0.3907051384449005\n",
      "Epoch 0[11589/17270] Time:0.232, Train Loss:0.5874078869819641\n",
      "Epoch 0[11590/17270] Time:0.237, Train Loss:0.45076483488082886\n",
      "Epoch 0[11591/17270] Time:0.233, Train Loss:0.522575318813324\n",
      "Epoch 0[11592/17270] Time:0.235, Train Loss:0.46139925718307495\n",
      "Epoch 0[11593/17270] Time:0.234, Train Loss:0.44686609506607056\n",
      "Epoch 0[11594/17270] Time:0.238, Train Loss:0.3943377435207367\n",
      "Epoch 0[11595/17270] Time:0.221, Train Loss:0.7566325068473816\n",
      "Epoch 0[11596/17270] Time:0.231, Train Loss:0.6412190198898315\n",
      "Epoch 0[11597/17270] Time:0.241, Train Loss:0.32444146275520325\n",
      "Epoch 0[11598/17270] Time:0.252, Train Loss:0.6837896704673767\n",
      "Epoch 0[11599/17270] Time:0.233, Train Loss:0.3119841516017914\n",
      "Epoch 0[11600/17270] Time:0.233, Train Loss:0.3988471031188965\n",
      "Epoch 0[11601/17270] Time:0.231, Train Loss:0.9762796759605408\n",
      "Epoch 0[11602/17270] Time:0.238, Train Loss:0.42231932282447815\n",
      "Epoch 0[11603/17270] Time:0.235, Train Loss:1.1157867908477783\n",
      "Epoch 0[11604/17270] Time:0.235, Train Loss:0.5164968371391296\n",
      "Epoch 0[11605/17270] Time:0.232, Train Loss:0.5595415234565735\n",
      "Epoch 0[11606/17270] Time:0.234, Train Loss:0.4161207973957062\n",
      "Epoch 0[11607/17270] Time:0.233, Train Loss:0.4694916009902954\n",
      "Epoch 0[11608/17270] Time:0.245, Train Loss:0.4194463789463043\n",
      "Epoch 0[11609/17270] Time:0.218, Train Loss:1.072400450706482\n",
      "Epoch 0[11610/17270] Time:0.246, Train Loss:0.8136963248252869\n",
      "Epoch 0[11611/17270] Time:0.237, Train Loss:0.5068371295928955\n",
      "Epoch 0[11612/17270] Time:0.234, Train Loss:0.3500185012817383\n",
      "Epoch 0[11613/17270] Time:0.228, Train Loss:0.38973379135131836\n",
      "Epoch 0[11614/17270] Time:0.234, Train Loss:0.6162667274475098\n",
      "Epoch 0[11615/17270] Time:0.236, Train Loss:0.40872374176979065\n",
      "Epoch 0[11616/17270] Time:0.235, Train Loss:0.823043942451477\n",
      "Epoch 0[11617/17270] Time:0.23, Train Loss:0.36274445056915283\n",
      "Epoch 0[11618/17270] Time:0.246, Train Loss:0.5966099500656128\n",
      "Epoch 0[11619/17270] Time:0.237, Train Loss:0.4879511296749115\n",
      "Epoch 0[11620/17270] Time:0.233, Train Loss:0.6258453726768494\n",
      "Epoch 0[11621/17270] Time:0.237, Train Loss:0.45220625400543213\n",
      "Epoch 0[11622/17270] Time:0.237, Train Loss:1.1096760034561157\n",
      "Epoch 0[11623/17270] Time:0.232, Train Loss:0.5558600425720215\n",
      "Epoch 0[11624/17270] Time:0.224, Train Loss:0.5953643321990967\n",
      "Epoch 0[11625/17270] Time:0.244, Train Loss:0.3410952091217041\n",
      "Epoch 0[11626/17270] Time:0.238, Train Loss:1.2434797286987305\n",
      "Epoch 0[11627/17270] Time:0.224, Train Loss:1.0551472902297974\n",
      "Epoch 0[11628/17270] Time:0.244, Train Loss:0.3950461447238922\n",
      "Epoch 0[11629/17270] Time:0.238, Train Loss:1.1349012851715088\n",
      "Epoch 0[11630/17270] Time:0.237, Train Loss:0.6117629408836365\n",
      "Epoch 0[11631/17270] Time:0.234, Train Loss:0.40705257654190063\n",
      "Epoch 0[11632/17270] Time:0.234, Train Loss:0.4019436240196228\n",
      "Epoch 0[11633/17270] Time:0.238, Train Loss:0.40037238597869873\n",
      "Epoch 0[11634/17270] Time:0.227, Train Loss:0.7129509449005127\n",
      "Epoch 0[11635/17270] Time:0.233, Train Loss:0.6952660083770752\n",
      "Epoch 0[11636/17270] Time:0.251, Train Loss:0.44713181257247925\n",
      "Epoch 0[11637/17270] Time:0.234, Train Loss:0.6918716430664062\n",
      "Epoch 0[11638/17270] Time:0.239, Train Loss:0.383202463388443\n",
      "Epoch 0[11639/17270] Time:0.228, Train Loss:0.7733657956123352\n",
      "Epoch 0[11640/17270] Time:0.236, Train Loss:0.5279720425605774\n",
      "Epoch 0[11641/17270] Time:0.231, Train Loss:0.4428095519542694\n",
      "Epoch 0[11642/17270] Time:0.231, Train Loss:0.49509575963020325\n",
      "Epoch 0[11643/17270] Time:0.234, Train Loss:1.505257487297058\n",
      "Epoch 0[11644/17270] Time:0.224, Train Loss:0.7865244746208191\n",
      "Epoch 0[11645/17270] Time:0.245, Train Loss:0.33892568945884705\n",
      "Epoch 0[11646/17270] Time:0.234, Train Loss:0.5153225064277649\n",
      "Epoch 0[11647/17270] Time:0.228, Train Loss:1.318534016609192\n",
      "Epoch 0[11648/17270] Time:0.23, Train Loss:1.3986122608184814\n",
      "Epoch 0[11649/17270] Time:0.241, Train Loss:0.5840122103691101\n",
      "Epoch 0[11650/17270] Time:0.239, Train Loss:0.38422051072120667\n",
      "Epoch 0[11651/17270] Time:0.234, Train Loss:0.6872331500053406\n",
      "Epoch 0[11652/17270] Time:0.235, Train Loss:0.5280399322509766\n",
      "Epoch 0[11653/17270] Time:0.234, Train Loss:0.6620098352432251\n",
      "Epoch 0[11654/17270] Time:0.244, Train Loss:1.0083047151565552\n",
      "Epoch 0[11655/17270] Time:0.226, Train Loss:0.5410882830619812\n",
      "Epoch 0[11656/17270] Time:0.235, Train Loss:0.3865276873111725\n",
      "Epoch 0[11657/17270] Time:0.238, Train Loss:0.5446731448173523\n",
      "Epoch 0[11658/17270] Time:0.243, Train Loss:0.7093293070793152\n",
      "Epoch 0[11659/17270] Time:0.24, Train Loss:0.7327678799629211\n",
      "Epoch 0[11660/17270] Time:0.228, Train Loss:0.7342910766601562\n",
      "Epoch 0[11661/17270] Time:0.237, Train Loss:0.5454215407371521\n",
      "Epoch 0[11662/17270] Time:0.237, Train Loss:0.5142974257469177\n",
      "Epoch 0[11663/17270] Time:0.236, Train Loss:0.5470715761184692\n",
      "Epoch 0[11664/17270] Time:0.236, Train Loss:0.5224190950393677\n",
      "Epoch 0[11665/17270] Time:0.247, Train Loss:0.4039657413959503\n",
      "Epoch 0[11666/17270] Time:0.234, Train Loss:0.6366642713546753\n",
      "Epoch 0[11667/17270] Time:0.241, Train Loss:1.0323009490966797\n",
      "Epoch 0[11668/17270] Time:0.229, Train Loss:0.6066681742668152\n",
      "Epoch 0[11669/17270] Time:0.249, Train Loss:0.5852647423744202\n",
      "Epoch 0[11670/17270] Time:0.237, Train Loss:0.5011603236198425\n",
      "Epoch 0[11671/17270] Time:0.242, Train Loss:0.36427709460258484\n",
      "Epoch 0[11672/17270] Time:0.24, Train Loss:0.6406086683273315\n",
      "Epoch 0[11673/17270] Time:0.236, Train Loss:0.855366051197052\n",
      "Epoch 0[11674/17270] Time:0.234, Train Loss:0.6680423021316528\n",
      "Epoch 0[11675/17270] Time:0.233, Train Loss:0.5844786763191223\n",
      "Epoch 0[11676/17270] Time:0.234, Train Loss:0.6774951219558716\n",
      "Epoch 0[11677/17270] Time:0.234, Train Loss:0.5461173057556152\n",
      "Epoch 0[11678/17270] Time:0.233, Train Loss:0.5269782543182373\n",
      "Epoch 0[11679/17270] Time:0.236, Train Loss:0.5369967222213745\n",
      "Epoch 0[11680/17270] Time:0.233, Train Loss:0.36010292172431946\n",
      "Epoch 0[11681/17270] Time:0.234, Train Loss:0.5450631380081177\n",
      "Epoch 0[11682/17270] Time:0.227, Train Loss:0.5525228977203369\n",
      "Epoch 0[11683/17270] Time:0.248, Train Loss:0.4908420443534851\n",
      "Epoch 0[11684/17270] Time:0.239, Train Loss:0.6275049448013306\n",
      "Epoch 0[11685/17270] Time:0.233, Train Loss:0.5248702764511108\n",
      "Epoch 0[11686/17270] Time:0.24, Train Loss:0.4213305115699768\n",
      "Epoch 0[11687/17270] Time:0.234, Train Loss:0.9456743597984314\n",
      "Epoch 0[11688/17270] Time:0.229, Train Loss:0.4988115131855011\n",
      "Epoch 0[11689/17270] Time:0.259, Train Loss:0.4005163908004761\n",
      "Epoch 0[11690/17270] Time:0.233, Train Loss:0.5968017578125\n",
      "Epoch 0[11691/17270] Time:0.227, Train Loss:0.6067646145820618\n",
      "Epoch 0[11692/17270] Time:0.228, Train Loss:0.5101310014724731\n",
      "Epoch 0[11693/17270] Time:0.23, Train Loss:0.6377913951873779\n",
      "Epoch 0[11694/17270] Time:0.232, Train Loss:0.6555050611495972\n",
      "Epoch 0[11695/17270] Time:0.232, Train Loss:0.6587578654289246\n",
      "Epoch 0[11696/17270] Time:0.24, Train Loss:0.5634264349937439\n",
      "Epoch 0[11697/17270] Time:0.236, Train Loss:0.8137106895446777\n",
      "Epoch 0[11698/17270] Time:0.232, Train Loss:0.3449191153049469\n",
      "Epoch 0[11699/17270] Time:0.232, Train Loss:0.5932077169418335\n",
      "Epoch 0[11700/17270] Time:0.245, Train Loss:0.4544261693954468\n",
      "Epoch 0[11701/17270] Time:0.224, Train Loss:0.41792336106300354\n",
      "Epoch 0[11702/17270] Time:0.238, Train Loss:0.9917860627174377\n",
      "Epoch 0[11703/17270] Time:0.238, Train Loss:0.46443477272987366\n",
      "Epoch 0[11704/17270] Time:0.238, Train Loss:0.33547037839889526\n",
      "Epoch 0[11705/17270] Time:0.231, Train Loss:1.1502219438552856\n",
      "Epoch 0[11706/17270] Time:0.233, Train Loss:0.5020233392715454\n",
      "Epoch 0[11707/17270] Time:0.241, Train Loss:1.4544466733932495\n",
      "Epoch 0[11708/17270] Time:0.234, Train Loss:0.6987650394439697\n",
      "Epoch 0[11709/17270] Time:0.229, Train Loss:0.6790770292282104\n",
      "Epoch 0[11710/17270] Time:0.239, Train Loss:0.9818841814994812\n",
      "Epoch 0[11711/17270] Time:0.232, Train Loss:0.5771777033805847\n",
      "Epoch 0[11712/17270] Time:0.24, Train Loss:0.5268440246582031\n",
      "Epoch 0[11713/17270] Time:0.237, Train Loss:0.3182564079761505\n",
      "Epoch 0[11714/17270] Time:0.239, Train Loss:0.3786289095878601\n",
      "Epoch 0[11715/17270] Time:0.231, Train Loss:0.5899921655654907\n",
      "Epoch 0[11716/17270] Time:0.234, Train Loss:0.7518712282180786\n",
      "Epoch 0[11717/17270] Time:0.236, Train Loss:0.4174186885356903\n",
      "Epoch 0[11718/17270] Time:0.237, Train Loss:1.5232245922088623\n",
      "Epoch 0[11719/17270] Time:0.231, Train Loss:0.4886758327484131\n",
      "Epoch 0[11720/17270] Time:0.235, Train Loss:0.588539719581604\n",
      "Epoch 0[11721/17270] Time:0.236, Train Loss:0.5283076763153076\n",
      "Epoch 0[11722/17270] Time:0.232, Train Loss:0.5289855599403381\n",
      "Epoch 0[11723/17270] Time:0.235, Train Loss:0.3660736382007599\n",
      "Epoch 0[11724/17270] Time:0.231, Train Loss:0.6172835230827332\n",
      "Epoch 0[11725/17270] Time:0.235, Train Loss:0.5382466912269592\n",
      "Epoch 0[11726/17270] Time:0.232, Train Loss:0.7624331116676331\n",
      "Epoch 0[11727/17270] Time:0.235, Train Loss:1.0281323194503784\n",
      "Epoch 0[11728/17270] Time:0.232, Train Loss:0.8526687622070312\n",
      "Epoch 0[11729/17270] Time:0.237, Train Loss:0.44390586018562317\n",
      "Epoch 0[11730/17270] Time:0.247, Train Loss:0.5631064176559448\n",
      "Epoch 0[11731/17270] Time:0.229, Train Loss:0.35392969846725464\n",
      "Epoch 0[11732/17270] Time:0.226, Train Loss:0.7569946646690369\n",
      "Epoch 0[11733/17270] Time:0.244, Train Loss:0.3315523862838745\n",
      "Epoch 0[11734/17270] Time:0.233, Train Loss:0.4907188415527344\n",
      "Epoch 0[11735/17270] Time:0.234, Train Loss:0.5784593820571899\n",
      "Epoch 0[11736/17270] Time:0.237, Train Loss:0.42131826281547546\n",
      "Epoch 0[11737/17270] Time:0.235, Train Loss:0.5789378881454468\n",
      "Epoch 0[11738/17270] Time:0.228, Train Loss:0.35850974917411804\n",
      "Epoch 0[11739/17270] Time:0.24, Train Loss:0.538167417049408\n",
      "Epoch 0[11740/17270] Time:0.243, Train Loss:0.22812753915786743\n",
      "Epoch 0[11741/17270] Time:0.234, Train Loss:0.48193085193634033\n",
      "Epoch 0[11742/17270] Time:0.234, Train Loss:0.513349175453186\n",
      "Epoch 0[11743/17270] Time:0.238, Train Loss:0.48943474888801575\n",
      "Epoch 0[11744/17270] Time:0.247, Train Loss:0.43282029032707214\n",
      "Epoch 0[11745/17270] Time:0.234, Train Loss:0.30341047048568726\n",
      "Epoch 0[11746/17270] Time:0.233, Train Loss:0.3282721936702728\n",
      "Epoch 0[11747/17270] Time:0.233, Train Loss:0.2793644964694977\n",
      "Epoch 0[11748/17270] Time:0.231, Train Loss:0.2607826292514801\n",
      "Epoch 0[11749/17270] Time:0.238, Train Loss:0.7219085693359375\n",
      "Epoch 0[11750/17270] Time:0.234, Train Loss:0.5164797306060791\n",
      "Epoch 0[11751/17270] Time:0.229, Train Loss:0.718237578868866\n",
      "Epoch 0[11752/17270] Time:0.228, Train Loss:1.3130581378936768\n",
      "Epoch 0[11753/17270] Time:0.23, Train Loss:0.3825145661830902\n",
      "Epoch 0[11754/17270] Time:0.231, Train Loss:0.502509355545044\n",
      "Epoch 0[11755/17270] Time:0.231, Train Loss:1.1078312397003174\n",
      "Epoch 0[11756/17270] Time:0.238, Train Loss:0.38012322783470154\n",
      "Epoch 0[11757/17270] Time:0.245, Train Loss:0.5438423752784729\n",
      "Epoch 0[11758/17270] Time:0.233, Train Loss:0.6323810815811157\n",
      "Epoch 0[11759/17270] Time:0.234, Train Loss:0.7747859954833984\n",
      "Epoch 0[11760/17270] Time:0.232, Train Loss:0.3444017469882965\n",
      "Epoch 0[11761/17270] Time:0.234, Train Loss:1.0753929615020752\n",
      "Epoch 0[11762/17270] Time:0.233, Train Loss:0.34555742144584656\n",
      "Epoch 0[11763/17270] Time:0.233, Train Loss:0.4364392161369324\n",
      "Epoch 0[11764/17270] Time:0.238, Train Loss:0.3604845404624939\n",
      "Epoch 0[11765/17270] Time:0.237, Train Loss:0.8083487153053284\n",
      "Epoch 0[11766/17270] Time:0.227, Train Loss:0.591834545135498\n",
      "Epoch 0[11767/17270] Time:0.223, Train Loss:0.30472105741500854\n",
      "Epoch 0[11768/17270] Time:0.23, Train Loss:1.3338252305984497\n",
      "Epoch 0[11769/17270] Time:0.232, Train Loss:0.6312881708145142\n",
      "Epoch 0[11770/17270] Time:0.229, Train Loss:1.6623587608337402\n",
      "Epoch 0[11771/17270] Time:0.23, Train Loss:0.4474292993545532\n",
      "Epoch 0[11772/17270] Time:0.235, Train Loss:0.7461885809898376\n",
      "Epoch 0[11773/17270] Time:0.229, Train Loss:0.4392586350440979\n",
      "Epoch 0[11774/17270] Time:0.233, Train Loss:0.34267374873161316\n",
      "Epoch 0[11775/17270] Time:0.232, Train Loss:0.8007606863975525\n",
      "Epoch 0[11776/17270] Time:0.238, Train Loss:0.5781689882278442\n",
      "Epoch 0[11777/17270] Time:0.232, Train Loss:0.6855482459068298\n",
      "Epoch 0[11778/17270] Time:0.228, Train Loss:0.3353599011898041\n",
      "Epoch 0[11779/17270] Time:0.233, Train Loss:0.5347433686256409\n",
      "Epoch 0[11780/17270] Time:0.235, Train Loss:1.2209787368774414\n",
      "Epoch 0[11781/17270] Time:0.232, Train Loss:0.6466532945632935\n",
      "Epoch 0[11782/17270] Time:0.231, Train Loss:0.3224451243877411\n",
      "Epoch 0[11783/17270] Time:0.233, Train Loss:0.6494026780128479\n",
      "Epoch 0[11784/17270] Time:0.231, Train Loss:0.5334859490394592\n",
      "Epoch 0[11785/17270] Time:0.24, Train Loss:0.44688716530799866\n",
      "Epoch 0[11786/17270] Time:0.234, Train Loss:0.6285607814788818\n",
      "Epoch 0[11787/17270] Time:0.227, Train Loss:0.4891165792942047\n",
      "Epoch 0[11788/17270] Time:0.233, Train Loss:1.0191774368286133\n",
      "Epoch 0[11789/17270] Time:0.241, Train Loss:0.5634127259254456\n",
      "Epoch 0[11790/17270] Time:0.225, Train Loss:0.6416792273521423\n",
      "Epoch 0[11791/17270] Time:0.243, Train Loss:0.4919665455818176\n",
      "Epoch 0[11792/17270] Time:0.241, Train Loss:0.5585638880729675\n",
      "Epoch 0[11793/17270] Time:0.236, Train Loss:0.651186466217041\n",
      "Epoch 0[11794/17270] Time:0.232, Train Loss:0.7879959344863892\n",
      "Epoch 0[11795/17270] Time:0.236, Train Loss:0.5638206601142883\n",
      "Epoch 0[11796/17270] Time:0.239, Train Loss:0.3618433475494385\n",
      "Epoch 0[11797/17270] Time:0.235, Train Loss:0.6606562733650208\n",
      "Epoch 0[11798/17270] Time:0.237, Train Loss:0.5250288248062134\n",
      "Epoch 0[11799/17270] Time:0.234, Train Loss:0.6880778670310974\n",
      "Epoch 0[11800/17270] Time:0.236, Train Loss:0.5303641557693481\n",
      "Epoch 0[11801/17270] Time:0.229, Train Loss:0.7830060124397278\n",
      "Epoch 0[11802/17270] Time:0.229, Train Loss:0.48212945461273193\n",
      "Epoch 0[11803/17270] Time:0.232, Train Loss:0.707405149936676\n",
      "Epoch 0[11804/17270] Time:0.232, Train Loss:0.41561877727508545\n",
      "Epoch 0[11805/17270] Time:0.23, Train Loss:0.44988152384757996\n",
      "Epoch 0[11806/17270] Time:0.232, Train Loss:0.4277452230453491\n",
      "Epoch 0[11807/17270] Time:0.231, Train Loss:0.6112149953842163\n",
      "Epoch 0[11808/17270] Time:0.237, Train Loss:0.5579047203063965\n",
      "Epoch 0[11809/17270] Time:0.234, Train Loss:1.2715510129928589\n",
      "Epoch 0[11810/17270] Time:0.227, Train Loss:0.8553152680397034\n",
      "Epoch 0[11811/17270] Time:0.247, Train Loss:0.42350974678993225\n",
      "Epoch 0[11812/17270] Time:0.244, Train Loss:0.448386549949646\n",
      "Epoch 0[11813/17270] Time:0.237, Train Loss:0.4176092743873596\n",
      "Epoch 0[11814/17270] Time:0.232, Train Loss:0.3569389879703522\n",
      "Epoch 0[11815/17270] Time:0.238, Train Loss:0.3206321895122528\n",
      "Epoch 0[11816/17270] Time:0.24, Train Loss:0.46070560812950134\n",
      "Epoch 0[11817/17270] Time:0.243, Train Loss:0.5416226983070374\n",
      "Epoch 0[11818/17270] Time:0.239, Train Loss:0.689003586769104\n",
      "Epoch 0[11819/17270] Time:0.244, Train Loss:0.6574742197990417\n",
      "Epoch 0[11820/17270] Time:0.229, Train Loss:0.6596676707267761\n",
      "Epoch 0[11821/17270] Time:0.234, Train Loss:0.5559718012809753\n",
      "Epoch 0[11822/17270] Time:0.243, Train Loss:0.5113885402679443\n",
      "Epoch 0[11823/17270] Time:0.236, Train Loss:0.8410089015960693\n",
      "Epoch 0[11824/17270] Time:0.232, Train Loss:0.6336819529533386\n",
      "Epoch 0[11825/17270] Time:0.237, Train Loss:0.3503338694572449\n",
      "Epoch 0[11826/17270] Time:0.246, Train Loss:0.7581824064254761\n",
      "Epoch 0[11827/17270] Time:0.236, Train Loss:0.6830800175666809\n",
      "Epoch 0[11828/17270] Time:0.234, Train Loss:0.46889355778694153\n",
      "Epoch 0[11829/17270] Time:0.234, Train Loss:0.7963232398033142\n",
      "Epoch 0[11830/17270] Time:0.237, Train Loss:0.726371705532074\n",
      "Epoch 0[11831/17270] Time:0.228, Train Loss:0.6645316481590271\n",
      "Epoch 0[11832/17270] Time:0.235, Train Loss:0.42545178532600403\n",
      "Epoch 0[11833/17270] Time:0.246, Train Loss:0.3763071596622467\n",
      "Epoch 0[11834/17270] Time:0.236, Train Loss:0.4912177324295044\n",
      "Epoch 0[11835/17270] Time:0.231, Train Loss:0.6051648259162903\n",
      "Epoch 0[11836/17270] Time:0.246, Train Loss:0.6807745099067688\n",
      "Epoch 0[11837/17270] Time:0.225, Train Loss:0.4829694926738739\n",
      "Epoch 0[11838/17270] Time:0.24, Train Loss:0.8348119854927063\n",
      "Epoch 0[11839/17270] Time:0.239, Train Loss:0.5276346802711487\n",
      "Epoch 0[11840/17270] Time:0.231, Train Loss:0.3883740305900574\n",
      "Epoch 0[11841/17270] Time:0.234, Train Loss:0.5285650491714478\n",
      "Epoch 0[11842/17270] Time:0.232, Train Loss:0.7080252170562744\n",
      "Epoch 0[11843/17270] Time:0.226, Train Loss:0.782513439655304\n",
      "Epoch 0[11844/17270] Time:0.237, Train Loss:0.38572990894317627\n",
      "Epoch 0[11845/17270] Time:0.242, Train Loss:0.37384048104286194\n",
      "Epoch 0[11846/17270] Time:0.244, Train Loss:0.599522590637207\n",
      "Epoch 0[11847/17270] Time:0.238, Train Loss:0.5617380142211914\n",
      "Epoch 0[11848/17270] Time:0.239, Train Loss:0.904132068157196\n",
      "Epoch 0[11849/17270] Time:0.235, Train Loss:0.9367654919624329\n",
      "Epoch 0[11850/17270] Time:0.241, Train Loss:0.39924007654190063\n",
      "Epoch 0[11851/17270] Time:0.228, Train Loss:0.6051552891731262\n",
      "Epoch 0[11852/17270] Time:0.243, Train Loss:0.4748598635196686\n",
      "Epoch 0[11853/17270] Time:0.236, Train Loss:0.41003936529159546\n",
      "Epoch 0[11854/17270] Time:0.23, Train Loss:0.7151315212249756\n",
      "Epoch 0[11855/17270] Time:0.232, Train Loss:0.4427088499069214\n",
      "Epoch 0[11856/17270] Time:0.228, Train Loss:0.644606351852417\n",
      "Epoch 0[11857/17270] Time:0.228, Train Loss:0.256193608045578\n",
      "Epoch 0[11858/17270] Time:0.241, Train Loss:1.2297755479812622\n",
      "Epoch 0[11859/17270] Time:0.225, Train Loss:0.3718794286251068\n",
      "Epoch 0[11860/17270] Time:0.229, Train Loss:0.46961379051208496\n",
      "Epoch 0[11861/17270] Time:0.238, Train Loss:0.880313515663147\n",
      "Epoch 0[11862/17270] Time:0.224, Train Loss:0.5856135487556458\n",
      "Epoch 0[11863/17270] Time:0.228, Train Loss:0.4884856343269348\n",
      "Epoch 0[11864/17270] Time:0.23, Train Loss:0.3539729416370392\n",
      "Epoch 0[11865/17270] Time:0.233, Train Loss:0.5872482657432556\n",
      "Epoch 0[11866/17270] Time:0.24, Train Loss:0.5479943752288818\n",
      "Epoch 0[11867/17270] Time:0.245, Train Loss:0.543307363986969\n",
      "Epoch 0[11868/17270] Time:0.234, Train Loss:0.7514092326164246\n",
      "Epoch 0[11869/17270] Time:0.232, Train Loss:0.3721734285354614\n",
      "Epoch 0[11870/17270] Time:0.224, Train Loss:0.5704152584075928\n",
      "Epoch 0[11871/17270] Time:0.246, Train Loss:0.3630223572254181\n",
      "Epoch 0[11872/17270] Time:0.239, Train Loss:0.46583661437034607\n",
      "Epoch 0[11873/17270] Time:0.232, Train Loss:0.699288010597229\n",
      "Epoch 0[11874/17270] Time:0.226, Train Loss:0.7994359731674194\n",
      "Epoch 0[11875/17270] Time:0.242, Train Loss:0.6113247871398926\n",
      "Epoch 0[11876/17270] Time:0.236, Train Loss:0.5667248368263245\n",
      "Epoch 0[11877/17270] Time:0.237, Train Loss:0.38211145997047424\n",
      "Epoch 0[11878/17270] Time:0.237, Train Loss:0.6612440347671509\n",
      "Epoch 0[11879/17270] Time:0.248, Train Loss:0.7370657324790955\n",
      "Epoch 0[11880/17270] Time:0.24, Train Loss:0.4545275568962097\n",
      "Epoch 0[11881/17270] Time:0.226, Train Loss:0.436185747385025\n",
      "Epoch 0[11882/17270] Time:0.229, Train Loss:0.35719117522239685\n",
      "Epoch 0[11883/17270] Time:0.229, Train Loss:0.5356485843658447\n",
      "Epoch 0[11884/17270] Time:0.237, Train Loss:0.6003414392471313\n",
      "Epoch 0[11885/17270] Time:0.236, Train Loss:0.630331814289093\n",
      "Epoch 0[11886/17270] Time:0.235, Train Loss:0.5354520082473755\n",
      "Epoch 0[11887/17270] Time:0.236, Train Loss:0.600841760635376\n",
      "Epoch 0[11888/17270] Time:0.225, Train Loss:0.695619523525238\n",
      "Epoch 0[11889/17270] Time:0.248, Train Loss:0.3084680438041687\n",
      "Epoch 0[11890/17270] Time:0.219, Train Loss:0.6607109904289246\n",
      "Epoch 0[11891/17270] Time:0.241, Train Loss:0.7414141297340393\n",
      "Epoch 0[11892/17270] Time:0.238, Train Loss:0.34408295154571533\n",
      "Epoch 0[11893/17270] Time:0.241, Train Loss:0.7717519998550415\n",
      "Epoch 0[11894/17270] Time:0.238, Train Loss:0.5664891004562378\n",
      "Epoch 0[11895/17270] Time:0.23, Train Loss:0.6085678935050964\n",
      "Epoch 0[11896/17270] Time:0.229, Train Loss:0.3471777141094208\n",
      "Epoch 0[11897/17270] Time:0.235, Train Loss:0.3781507909297943\n",
      "Epoch 0[11898/17270] Time:0.234, Train Loss:0.3809627592563629\n",
      "Epoch 0[11899/17270] Time:0.242, Train Loss:0.3609514534473419\n",
      "Epoch 0[11900/17270] Time:0.251, Train Loss:0.7476247549057007\n",
      "Epoch 0[11901/17270] Time:0.231, Train Loss:0.5448201894760132\n",
      "Epoch 0[11902/17270] Time:0.234, Train Loss:1.0044059753417969\n",
      "Epoch 0[11903/17270] Time:0.233, Train Loss:0.4568255543708801\n",
      "Epoch 0[11904/17270] Time:0.238, Train Loss:0.806549072265625\n",
      "Epoch 0[11905/17270] Time:0.231, Train Loss:0.35503217577934265\n",
      "Epoch 0[11906/17270] Time:0.232, Train Loss:0.41960039734840393\n",
      "Epoch 0[11907/17270] Time:0.234, Train Loss:0.4573485255241394\n",
      "Epoch 0[11908/17270] Time:0.225, Train Loss:0.6521663665771484\n",
      "Epoch 0[11909/17270] Time:0.236, Train Loss:0.3761002719402313\n",
      "Epoch 0[11910/17270] Time:0.233, Train Loss:0.5559911131858826\n",
      "Epoch 0[11911/17270] Time:0.225, Train Loss:0.7627043724060059\n",
      "Epoch 0[11912/17270] Time:0.24, Train Loss:0.32933202385902405\n",
      "Epoch 0[11913/17270] Time:0.237, Train Loss:0.4622604548931122\n",
      "Epoch 0[11914/17270] Time:0.223, Train Loss:0.3517884314060211\n",
      "Epoch 0[11915/17270] Time:0.232, Train Loss:0.4570056200027466\n",
      "Epoch 0[11916/17270] Time:0.246, Train Loss:0.5933881402015686\n",
      "Epoch 0[11917/17270] Time:0.236, Train Loss:0.4793492257595062\n",
      "Epoch 0[11918/17270] Time:0.243, Train Loss:0.3257216215133667\n",
      "Epoch 0[11919/17270] Time:0.235, Train Loss:0.4915161728858948\n",
      "Epoch 0[11920/17270] Time:0.237, Train Loss:0.737304151058197\n",
      "Epoch 0[11921/17270] Time:0.243, Train Loss:1.06541907787323\n",
      "Epoch 0[11922/17270] Time:0.237, Train Loss:0.9622528553009033\n",
      "Epoch 0[11923/17270] Time:0.242, Train Loss:0.48507145047187805\n",
      "Epoch 0[11924/17270] Time:0.251, Train Loss:0.5318025350570679\n",
      "Epoch 0[11925/17270] Time:0.225, Train Loss:0.33860570192337036\n",
      "Epoch 0[11926/17270] Time:0.245, Train Loss:0.7275739312171936\n",
      "Epoch 0[11927/17270] Time:0.231, Train Loss:0.7093842029571533\n",
      "Epoch 0[11928/17270] Time:0.242, Train Loss:0.43911629915237427\n",
      "Epoch 0[11929/17270] Time:0.232, Train Loss:1.0130491256713867\n",
      "Epoch 0[11930/17270] Time:0.233, Train Loss:0.36907172203063965\n",
      "Epoch 0[11931/17270] Time:0.226, Train Loss:0.7100346684455872\n",
      "Epoch 0[11932/17270] Time:0.227, Train Loss:0.7220509648323059\n",
      "Epoch 0[11933/17270] Time:0.239, Train Loss:0.755009114742279\n",
      "Epoch 0[11934/17270] Time:0.232, Train Loss:0.49274829030036926\n",
      "Epoch 0[11935/17270] Time:0.235, Train Loss:0.4397166967391968\n",
      "Epoch 0[11936/17270] Time:0.225, Train Loss:0.5521695017814636\n",
      "Epoch 0[11937/17270] Time:0.247, Train Loss:0.6093589067459106\n",
      "Epoch 0[11938/17270] Time:0.227, Train Loss:0.47213810682296753\n",
      "Epoch 0[11939/17270] Time:0.24, Train Loss:0.48091205954551697\n",
      "Epoch 0[11940/17270] Time:0.227, Train Loss:0.5647888779640198\n",
      "Epoch 0[11941/17270] Time:0.23, Train Loss:0.44286561012268066\n",
      "Epoch 0[11942/17270] Time:0.229, Train Loss:0.8949679732322693\n",
      "Epoch 0[11943/17270] Time:0.243, Train Loss:0.47873061895370483\n",
      "Epoch 0[11944/17270] Time:0.233, Train Loss:0.8122588992118835\n",
      "Epoch 0[11945/17270] Time:0.228, Train Loss:0.4604033827781677\n",
      "Epoch 0[11946/17270] Time:0.239, Train Loss:0.35736751556396484\n",
      "Epoch 0[11947/17270] Time:0.245, Train Loss:1.0890690088272095\n",
      "Epoch 0[11948/17270] Time:0.233, Train Loss:0.6415958404541016\n",
      "Epoch 0[11949/17270] Time:0.237, Train Loss:0.54165118932724\n",
      "Epoch 0[11950/17270] Time:0.234, Train Loss:0.7257818579673767\n",
      "Epoch 0[11951/17270] Time:0.242, Train Loss:0.527110755443573\n",
      "Epoch 0[11952/17270] Time:0.234, Train Loss:0.8345895409584045\n",
      "Epoch 0[11953/17270] Time:0.233, Train Loss:2.0698554515838623\n",
      "Epoch 0[11954/17270] Time:0.231, Train Loss:1.8560136556625366\n",
      "Epoch 0[11955/17270] Time:0.242, Train Loss:0.5675157904624939\n",
      "Epoch 0[11956/17270] Time:0.231, Train Loss:0.6427538990974426\n",
      "Epoch 0[11957/17270] Time:0.248, Train Loss:0.517876923084259\n",
      "Epoch 0[11958/17270] Time:0.235, Train Loss:0.5187188982963562\n",
      "Epoch 0[11959/17270] Time:0.228, Train Loss:0.38509035110473633\n",
      "Epoch 0[11960/17270] Time:0.245, Train Loss:0.9683257937431335\n",
      "Epoch 0[11961/17270] Time:0.229, Train Loss:0.5054102540016174\n",
      "Epoch 0[11962/17270] Time:0.248, Train Loss:1.1177934408187866\n",
      "Epoch 0[11963/17270] Time:0.234, Train Loss:0.8380340337753296\n",
      "Epoch 0[11964/17270] Time:0.235, Train Loss:0.7288125157356262\n",
      "Epoch 0[11965/17270] Time:0.251, Train Loss:0.3045618534088135\n",
      "Epoch 0[11966/17270] Time:0.228, Train Loss:0.4109315574169159\n",
      "Epoch 0[11967/17270] Time:0.241, Train Loss:0.6865020990371704\n",
      "Epoch 0[11968/17270] Time:0.238, Train Loss:0.6421802639961243\n",
      "Epoch 0[11969/17270] Time:0.244, Train Loss:0.5885807871818542\n",
      "Epoch 0[11970/17270] Time:0.225, Train Loss:0.5782495737075806\n",
      "Epoch 0[11971/17270] Time:0.24, Train Loss:1.1554628610610962\n",
      "Epoch 0[11972/17270] Time:0.234, Train Loss:0.6701033115386963\n",
      "Epoch 0[11973/17270] Time:0.23, Train Loss:0.5016573667526245\n",
      "Epoch 0[11974/17270] Time:0.235, Train Loss:0.5030737519264221\n",
      "Epoch 0[11975/17270] Time:0.238, Train Loss:0.9151627421379089\n",
      "Epoch 0[11976/17270] Time:0.238, Train Loss:0.6368882060050964\n",
      "Epoch 0[11977/17270] Time:0.238, Train Loss:0.5322498679161072\n",
      "Epoch 0[11978/17270] Time:0.224, Train Loss:0.5876142978668213\n",
      "Epoch 0[11979/17270] Time:0.23, Train Loss:0.8533161878585815\n",
      "Epoch 0[11980/17270] Time:0.249, Train Loss:0.4029640257358551\n",
      "Epoch 0[11981/17270] Time:0.235, Train Loss:0.5932010412216187\n",
      "Epoch 0[11982/17270] Time:0.233, Train Loss:0.4411912262439728\n",
      "Epoch 0[11983/17270] Time:0.233, Train Loss:0.4646601378917694\n",
      "Epoch 0[11984/17270] Time:0.233, Train Loss:0.32097455859184265\n",
      "Epoch 0[11985/17270] Time:0.234, Train Loss:0.32552775740623474\n",
      "Epoch 0[11986/17270] Time:0.249, Train Loss:1.103907585144043\n",
      "Epoch 0[11987/17270] Time:0.233, Train Loss:0.5400445461273193\n",
      "Epoch 0[11988/17270] Time:0.236, Train Loss:0.5420970320701599\n",
      "Epoch 0[11989/17270] Time:0.254, Train Loss:1.1469476222991943\n",
      "Epoch 0[11990/17270] Time:0.229, Train Loss:0.4481985569000244\n",
      "Epoch 0[11991/17270] Time:0.228, Train Loss:0.6031094193458557\n",
      "Epoch 0[11992/17270] Time:0.229, Train Loss:0.46707576513290405\n",
      "Epoch 0[11993/17270] Time:0.238, Train Loss:0.5085665583610535\n",
      "Epoch 0[11994/17270] Time:0.237, Train Loss:0.6153725981712341\n",
      "Epoch 0[11995/17270] Time:0.233, Train Loss:0.44462043046951294\n",
      "Epoch 0[11996/17270] Time:0.237, Train Loss:0.781293511390686\n",
      "Epoch 0[11997/17270] Time:0.238, Train Loss:0.44067031145095825\n",
      "Epoch 0[11998/17270] Time:0.228, Train Loss:0.46959754824638367\n",
      "Epoch 0[11999/17270] Time:0.239, Train Loss:0.6135959625244141\n",
      "Epoch 0[12000/17270] Time:0.239, Train Loss:0.5170102119445801\n",
      "Epoch 0[12001/17270] Time:0.229, Train Loss:0.49037402868270874\n",
      "Epoch 0[12002/17270] Time:0.237, Train Loss:0.38759565353393555\n",
      "Epoch 0[12003/17270] Time:0.236, Train Loss:0.5651015639305115\n",
      "Epoch 0[12004/17270] Time:0.239, Train Loss:0.7177004814147949\n",
      "Epoch 0[12005/17270] Time:0.236, Train Loss:0.472758948802948\n",
      "Epoch 0[12006/17270] Time:0.241, Train Loss:0.4198582172393799\n",
      "Epoch 0[12007/17270] Time:0.237, Train Loss:0.3678663671016693\n",
      "Epoch 0[12008/17270] Time:0.23, Train Loss:0.5030938982963562\n",
      "Epoch 0[12009/17270] Time:0.233, Train Loss:0.5592899918556213\n",
      "Epoch 0[12010/17270] Time:0.23, Train Loss:0.45571550726890564\n",
      "Epoch 0[12011/17270] Time:0.239, Train Loss:0.550102710723877\n",
      "Epoch 0[12012/17270] Time:0.239, Train Loss:0.563312292098999\n",
      "Epoch 0[12013/17270] Time:0.247, Train Loss:0.4767419397830963\n",
      "Epoch 0[12014/17270] Time:0.22, Train Loss:0.3248857855796814\n",
      "Epoch 0[12015/17270] Time:0.242, Train Loss:0.48380038142204285\n",
      "Epoch 0[12016/17270] Time:0.245, Train Loss:0.5590702295303345\n",
      "Epoch 0[12017/17270] Time:0.247, Train Loss:0.5196903944015503\n",
      "Epoch 0[12018/17270] Time:0.233, Train Loss:0.40378814935684204\n",
      "Epoch 0[12019/17270] Time:0.23, Train Loss:0.45742228627204895\n",
      "Epoch 0[12020/17270] Time:0.232, Train Loss:0.9965912103652954\n",
      "Epoch 0[12021/17270] Time:0.241, Train Loss:0.4735501706600189\n",
      "Epoch 0[12022/17270] Time:0.24, Train Loss:0.6513779163360596\n",
      "Epoch 0[12023/17270] Time:0.243, Train Loss:0.5980578064918518\n",
      "Epoch 0[12024/17270] Time:0.238, Train Loss:1.4517807960510254\n",
      "Epoch 0[12025/17270] Time:0.249, Train Loss:0.4532865285873413\n",
      "Epoch 0[12026/17270] Time:0.242, Train Loss:1.0101139545440674\n",
      "Epoch 0[12027/17270] Time:0.243, Train Loss:0.43729257583618164\n",
      "Epoch 0[12028/17270] Time:0.236, Train Loss:0.43057703971862793\n",
      "Epoch 0[12029/17270] Time:0.23, Train Loss:0.4683528542518616\n",
      "Epoch 0[12030/17270] Time:0.242, Train Loss:0.6210330128669739\n",
      "Epoch 0[12031/17270] Time:0.237, Train Loss:0.4228280186653137\n",
      "Epoch 0[12032/17270] Time:0.237, Train Loss:0.7722041010856628\n",
      "Epoch 0[12033/17270] Time:0.236, Train Loss:0.4989154636859894\n",
      "Epoch 0[12034/17270] Time:0.239, Train Loss:0.6999069452285767\n",
      "Epoch 0[12035/17270] Time:0.238, Train Loss:0.9753362536430359\n",
      "Epoch 0[12036/17270] Time:0.229, Train Loss:0.3948953449726105\n",
      "Epoch 0[12037/17270] Time:0.229, Train Loss:0.5325251817703247\n",
      "Epoch 0[12038/17270] Time:0.241, Train Loss:0.3611389398574829\n",
      "Epoch 0[12039/17270] Time:0.238, Train Loss:0.29129043221473694\n",
      "Epoch 0[12040/17270] Time:0.231, Train Loss:0.40116456151008606\n",
      "Epoch 0[12041/17270] Time:0.24, Train Loss:0.7368245720863342\n",
      "Epoch 0[12042/17270] Time:0.238, Train Loss:0.5067480802536011\n",
      "Epoch 0[12043/17270] Time:0.242, Train Loss:0.5964024066925049\n",
      "Epoch 0[12044/17270] Time:0.246, Train Loss:0.5947016477584839\n",
      "Epoch 0[12045/17270] Time:0.228, Train Loss:0.6901649236679077\n",
      "Epoch 0[12046/17270] Time:0.243, Train Loss:0.39530202746391296\n",
      "Epoch 0[12047/17270] Time:0.225, Train Loss:1.100713849067688\n",
      "Epoch 0[12048/17270] Time:0.252, Train Loss:0.4920431971549988\n",
      "Epoch 0[12049/17270] Time:0.239, Train Loss:0.48471131920814514\n",
      "Epoch 0[12050/17270] Time:0.232, Train Loss:0.30863264203071594\n",
      "Epoch 0[12051/17270] Time:0.252, Train Loss:0.3668982982635498\n",
      "Epoch 0[12052/17270] Time:0.248, Train Loss:0.5665679574012756\n",
      "Epoch 0[12053/17270] Time:0.238, Train Loss:0.44909968972206116\n",
      "Epoch 0[12054/17270] Time:0.244, Train Loss:0.5505391359329224\n",
      "Epoch 0[12055/17270] Time:0.229, Train Loss:0.46453186869621277\n",
      "Epoch 0[12056/17270] Time:0.244, Train Loss:0.41791462898254395\n",
      "Epoch 0[12057/17270] Time:0.248, Train Loss:0.5125565528869629\n",
      "Epoch 0[12058/17270] Time:0.231, Train Loss:0.8731133937835693\n",
      "Epoch 0[12059/17270] Time:0.239, Train Loss:0.3697398006916046\n",
      "Epoch 0[12060/17270] Time:0.239, Train Loss:0.3345300853252411\n",
      "Epoch 0[12061/17270] Time:0.237, Train Loss:0.46562933921813965\n",
      "Epoch 0[12062/17270] Time:0.229, Train Loss:0.2898971438407898\n",
      "Epoch 0[12063/17270] Time:0.238, Train Loss:0.4993489682674408\n",
      "Epoch 0[12064/17270] Time:0.236, Train Loss:0.40628692507743835\n",
      "Epoch 0[12065/17270] Time:0.228, Train Loss:0.7417503595352173\n",
      "Epoch 0[12066/17270] Time:0.241, Train Loss:0.3778691589832306\n",
      "Epoch 0[12067/17270] Time:0.228, Train Loss:0.6272712349891663\n",
      "Epoch 0[12068/17270] Time:0.244, Train Loss:0.5703534483909607\n",
      "Epoch 0[12069/17270] Time:0.221, Train Loss:0.31364285945892334\n",
      "Epoch 0[12070/17270] Time:0.24, Train Loss:0.39228561520576477\n",
      "Epoch 0[12071/17270] Time:0.222, Train Loss:1.5916484594345093\n",
      "Epoch 0[12072/17270] Time:0.228, Train Loss:0.30351367592811584\n",
      "Epoch 0[12073/17270] Time:0.233, Train Loss:1.2942450046539307\n",
      "Epoch 0[12074/17270] Time:0.23, Train Loss:0.4015730321407318\n",
      "Epoch 0[12075/17270] Time:0.235, Train Loss:0.5915625095367432\n",
      "Epoch 0[12076/17270] Time:0.24, Train Loss:0.29870569705963135\n",
      "Epoch 0[12077/17270] Time:0.23, Train Loss:0.5039988160133362\n",
      "Epoch 0[12078/17270] Time:0.241, Train Loss:0.9246137738227844\n",
      "Epoch 0[12079/17270] Time:0.229, Train Loss:0.6012446284294128\n",
      "Epoch 0[12080/17270] Time:0.232, Train Loss:0.5001592040061951\n",
      "Epoch 0[12081/17270] Time:0.227, Train Loss:0.5760281682014465\n",
      "Epoch 0[12082/17270] Time:0.231, Train Loss:0.4572637677192688\n",
      "Epoch 0[12083/17270] Time:0.232, Train Loss:0.5938132405281067\n",
      "Epoch 0[12084/17270] Time:0.229, Train Loss:0.7675138711929321\n",
      "Epoch 0[12085/17270] Time:0.232, Train Loss:0.4594056010246277\n",
      "Epoch 0[12086/17270] Time:0.236, Train Loss:0.3891865611076355\n",
      "Epoch 0[12087/17270] Time:0.227, Train Loss:0.8972659111022949\n",
      "Epoch 0[12088/17270] Time:0.243, Train Loss:0.5675813555717468\n",
      "Epoch 0[12089/17270] Time:0.227, Train Loss:0.6680431962013245\n",
      "Epoch 0[12090/17270] Time:0.24, Train Loss:0.9434847831726074\n",
      "Epoch 0[12091/17270] Time:0.233, Train Loss:0.33883628249168396\n",
      "Epoch 0[12092/17270] Time:0.235, Train Loss:0.8170576691627502\n",
      "Epoch 0[12093/17270] Time:0.233, Train Loss:0.48605552315711975\n",
      "Epoch 0[12094/17270] Time:0.232, Train Loss:0.6441789865493774\n",
      "Epoch 0[12095/17270] Time:0.225, Train Loss:0.4105457067489624\n",
      "Epoch 0[12096/17270] Time:0.242, Train Loss:1.3152964115142822\n",
      "Epoch 0[12097/17270] Time:0.227, Train Loss:0.3642805814743042\n",
      "Epoch 0[12098/17270] Time:0.231, Train Loss:0.33390721678733826\n",
      "Epoch 0[12099/17270] Time:0.228, Train Loss:0.820030152797699\n",
      "Epoch 0[12100/17270] Time:0.241, Train Loss:0.5264172554016113\n",
      "Epoch 0[12101/17270] Time:0.233, Train Loss:1.1344577074050903\n",
      "Epoch 0[12102/17270] Time:0.236, Train Loss:0.6670467853546143\n",
      "Epoch 0[12103/17270] Time:0.228, Train Loss:0.36558642983436584\n",
      "Epoch 0[12104/17270] Time:0.241, Train Loss:0.525611937046051\n",
      "Epoch 0[12105/17270] Time:0.223, Train Loss:0.7990937829017639\n",
      "Epoch 0[12106/17270] Time:0.246, Train Loss:0.5735363960266113\n",
      "Epoch 0[12107/17270] Time:0.221, Train Loss:0.9235426187515259\n",
      "Epoch 0[12108/17270] Time:0.232, Train Loss:0.556206226348877\n",
      "Epoch 0[12109/17270] Time:0.24, Train Loss:0.42842692136764526\n",
      "Epoch 0[12110/17270] Time:0.237, Train Loss:1.0409481525421143\n",
      "Epoch 0[12111/17270] Time:0.228, Train Loss:0.5168086886405945\n",
      "Epoch 0[12112/17270] Time:0.23, Train Loss:0.3663594126701355\n",
      "Epoch 0[12113/17270] Time:0.24, Train Loss:1.2552648782730103\n",
      "Epoch 0[12114/17270] Time:0.234, Train Loss:0.5726994872093201\n",
      "Epoch 0[12115/17270] Time:0.242, Train Loss:0.3876489996910095\n",
      "Epoch 0[12116/17270] Time:0.228, Train Loss:0.6605152487754822\n",
      "Epoch 0[12117/17270] Time:0.242, Train Loss:0.4329583942890167\n",
      "Epoch 0[12118/17270] Time:0.235, Train Loss:0.5591321587562561\n",
      "Epoch 0[12119/17270] Time:0.232, Train Loss:0.28018835186958313\n",
      "Epoch 0[12120/17270] Time:0.245, Train Loss:0.3711446225643158\n",
      "Epoch 0[12121/17270] Time:0.225, Train Loss:0.756219208240509\n",
      "Epoch 0[12122/17270] Time:0.243, Train Loss:0.5443150401115417\n",
      "Epoch 0[12123/17270] Time:0.235, Train Loss:0.6319900155067444\n",
      "Epoch 0[12124/17270] Time:0.233, Train Loss:0.486905962228775\n",
      "Epoch 0[12125/17270] Time:0.227, Train Loss:0.9035008549690247\n",
      "Epoch 0[12126/17270] Time:0.24, Train Loss:1.0311570167541504\n",
      "Epoch 0[12127/17270] Time:0.237, Train Loss:0.6603058576583862\n",
      "Epoch 0[12128/17270] Time:0.236, Train Loss:0.46277034282684326\n",
      "Epoch 0[12129/17270] Time:0.256, Train Loss:0.4804697036743164\n",
      "Epoch 0[12130/17270] Time:0.235, Train Loss:0.8952759504318237\n",
      "Epoch 0[12131/17270] Time:0.231, Train Loss:0.5649226903915405\n",
      "Epoch 0[12132/17270] Time:0.232, Train Loss:0.42353585362434387\n",
      "Epoch 0[12133/17270] Time:0.238, Train Loss:1.5761303901672363\n",
      "Epoch 0[12134/17270] Time:0.234, Train Loss:0.578899085521698\n",
      "Epoch 0[12135/17270] Time:0.231, Train Loss:0.5031365156173706\n",
      "Epoch 0[12136/17270] Time:0.235, Train Loss:1.0141972303390503\n",
      "Epoch 0[12137/17270] Time:0.229, Train Loss:0.46467217803001404\n",
      "Epoch 0[12138/17270] Time:0.237, Train Loss:0.36882010102272034\n",
      "Epoch 0[12139/17270] Time:0.238, Train Loss:0.4465901255607605\n",
      "Epoch 0[12140/17270] Time:0.243, Train Loss:0.3538629710674286\n",
      "Epoch 0[12141/17270] Time:0.239, Train Loss:0.7649025917053223\n",
      "Epoch 0[12142/17270] Time:0.237, Train Loss:0.4209311604499817\n",
      "Epoch 0[12143/17270] Time:0.242, Train Loss:0.4045593738555908\n",
      "Epoch 0[12144/17270] Time:0.228, Train Loss:0.4394155740737915\n",
      "Epoch 0[12145/17270] Time:0.232, Train Loss:0.5834059119224548\n",
      "Epoch 0[12146/17270] Time:0.238, Train Loss:0.3135520815849304\n",
      "Epoch 0[12147/17270] Time:0.228, Train Loss:0.42149457335472107\n",
      "Epoch 0[12148/17270] Time:0.239, Train Loss:0.5615037679672241\n",
      "Epoch 0[12149/17270] Time:0.237, Train Loss:0.7731124758720398\n",
      "Epoch 0[12150/17270] Time:0.227, Train Loss:0.5505082607269287\n",
      "Epoch 0[12151/17270] Time:0.234, Train Loss:1.3549824953079224\n",
      "Epoch 0[12152/17270] Time:0.239, Train Loss:0.45791390538215637\n",
      "Epoch 0[12153/17270] Time:0.233, Train Loss:0.4416852295398712\n",
      "Epoch 0[12154/17270] Time:0.229, Train Loss:0.48802611231803894\n",
      "Epoch 0[12155/17270] Time:0.231, Train Loss:0.6161899566650391\n",
      "Epoch 0[12156/17270] Time:0.24, Train Loss:0.2999788522720337\n",
      "Epoch 0[12157/17270] Time:0.231, Train Loss:1.0888844728469849\n",
      "Epoch 0[12158/17270] Time:0.233, Train Loss:0.7796844244003296\n",
      "Epoch 0[12159/17270] Time:0.236, Train Loss:0.7114774584770203\n",
      "Epoch 0[12160/17270] Time:0.236, Train Loss:1.2808903455734253\n",
      "Epoch 0[12161/17270] Time:0.236, Train Loss:1.3460173606872559\n",
      "Epoch 0[12162/17270] Time:0.229, Train Loss:0.4807211458683014\n",
      "Epoch 0[12163/17270] Time:0.237, Train Loss:0.60078364610672\n",
      "Epoch 0[12164/17270] Time:0.228, Train Loss:0.43936261534690857\n",
      "Epoch 0[12165/17270] Time:0.234, Train Loss:0.5198982357978821\n",
      "Epoch 0[12166/17270] Time:0.237, Train Loss:0.5222135782241821\n",
      "Epoch 0[12167/17270] Time:0.237, Train Loss:0.5973919034004211\n",
      "Epoch 0[12168/17270] Time:0.234, Train Loss:0.5134350061416626\n",
      "Epoch 0[12169/17270] Time:0.228, Train Loss:0.4470808207988739\n",
      "Epoch 0[12170/17270] Time:0.239, Train Loss:1.0918735265731812\n",
      "Epoch 0[12171/17270] Time:0.241, Train Loss:0.6232814788818359\n",
      "Epoch 0[12172/17270] Time:0.237, Train Loss:0.7350907921791077\n",
      "Epoch 0[12173/17270] Time:0.236, Train Loss:0.8109776377677917\n",
      "Epoch 0[12174/17270] Time:0.241, Train Loss:0.6684982180595398\n",
      "Epoch 0[12175/17270] Time:0.237, Train Loss:1.2321311235427856\n",
      "Epoch 0[12176/17270] Time:0.227, Train Loss:0.42673105001449585\n",
      "Epoch 0[12177/17270] Time:0.23, Train Loss:0.47401195764541626\n",
      "Epoch 0[12178/17270] Time:0.244, Train Loss:0.9636673331260681\n",
      "Epoch 0[12179/17270] Time:0.226, Train Loss:0.973892331123352\n",
      "Epoch 0[12180/17270] Time:0.238, Train Loss:0.6635118722915649\n",
      "Epoch 0[12181/17270] Time:0.245, Train Loss:0.5214865803718567\n",
      "Epoch 0[12182/17270] Time:0.24, Train Loss:0.7103496789932251\n",
      "Epoch 0[12183/17270] Time:0.233, Train Loss:0.7019574046134949\n",
      "Epoch 0[12184/17270] Time:0.233, Train Loss:0.3391748070716858\n",
      "Epoch 0[12185/17270] Time:0.231, Train Loss:0.7222763895988464\n",
      "Epoch 0[12186/17270] Time:0.232, Train Loss:0.5379278063774109\n",
      "Epoch 0[12187/17270] Time:0.225, Train Loss:0.7715911865234375\n",
      "Epoch 0[12188/17270] Time:0.229, Train Loss:0.4983168840408325\n",
      "Epoch 0[12189/17270] Time:0.235, Train Loss:0.6943944096565247\n",
      "Epoch 0[12190/17270] Time:0.236, Train Loss:0.5671283602714539\n",
      "Epoch 0[12191/17270] Time:0.239, Train Loss:1.0437836647033691\n",
      "Epoch 0[12192/17270] Time:0.231, Train Loss:0.5200592875480652\n",
      "Epoch 0[12193/17270] Time:0.228, Train Loss:0.485975444316864\n",
      "Epoch 0[12194/17270] Time:0.233, Train Loss:0.4623888432979584\n",
      "Epoch 0[12195/17270] Time:0.23, Train Loss:0.7830305099487305\n",
      "Epoch 0[12196/17270] Time:0.236, Train Loss:0.6351975202560425\n",
      "Epoch 0[12197/17270] Time:0.237, Train Loss:0.6671984791755676\n",
      "Epoch 0[12198/17270] Time:0.236, Train Loss:0.5223228931427002\n",
      "Epoch 0[12199/17270] Time:0.24, Train Loss:0.34214547276496887\n",
      "Epoch 0[12200/17270] Time:0.226, Train Loss:0.41186144948005676\n",
      "Epoch 0[12201/17270] Time:0.237, Train Loss:0.40781447291374207\n",
      "Epoch 0[12202/17270] Time:0.226, Train Loss:0.720940887928009\n",
      "Epoch 0[12203/17270] Time:0.237, Train Loss:0.8920624852180481\n",
      "Epoch 0[12204/17270] Time:0.23, Train Loss:0.312737375497818\n",
      "Epoch 0[12205/17270] Time:0.23, Train Loss:0.35311299562454224\n",
      "Epoch 0[12206/17270] Time:0.236, Train Loss:0.4000595510005951\n",
      "Epoch 0[12207/17270] Time:0.222, Train Loss:0.5904112458229065\n",
      "Epoch 0[12208/17270] Time:0.232, Train Loss:0.7850902080535889\n",
      "Epoch 0[12209/17270] Time:0.241, Train Loss:0.7964025139808655\n",
      "Epoch 0[12210/17270] Time:0.237, Train Loss:0.7332326173782349\n",
      "Epoch 0[12211/17270] Time:0.233, Train Loss:0.45346230268478394\n",
      "Epoch 0[12212/17270] Time:0.225, Train Loss:0.34824422001838684\n",
      "Epoch 0[12213/17270] Time:0.239, Train Loss:0.2965816557407379\n",
      "Epoch 0[12214/17270] Time:0.237, Train Loss:0.8468853831291199\n",
      "Epoch 0[12215/17270] Time:0.232, Train Loss:0.6565333604812622\n",
      "Epoch 0[12216/17270] Time:0.235, Train Loss:0.4032800793647766\n",
      "Epoch 0[12217/17270] Time:0.233, Train Loss:1.017697811126709\n",
      "Epoch 0[12218/17270] Time:0.231, Train Loss:0.27465927600860596\n",
      "Epoch 0[12219/17270] Time:0.223, Train Loss:0.626872181892395\n",
      "Epoch 0[12220/17270] Time:0.229, Train Loss:0.5489166975021362\n",
      "Epoch 0[12221/17270] Time:0.241, Train Loss:0.37104278802871704\n",
      "Epoch 0[12222/17270] Time:0.235, Train Loss:0.5904385447502136\n",
      "Epoch 0[12223/17270] Time:0.241, Train Loss:0.47126638889312744\n",
      "Epoch 0[12224/17270] Time:0.237, Train Loss:0.4683350622653961\n",
      "Epoch 0[12225/17270] Time:0.233, Train Loss:0.3798028528690338\n",
      "Epoch 0[12226/17270] Time:0.233, Train Loss:0.8585062623023987\n",
      "Epoch 0[12227/17270] Time:0.242, Train Loss:0.39168304204940796\n",
      "Epoch 0[12228/17270] Time:0.234, Train Loss:0.9793294668197632\n",
      "Epoch 0[12229/17270] Time:0.237, Train Loss:0.5864695906639099\n",
      "Epoch 0[12230/17270] Time:0.242, Train Loss:0.4402298927307129\n",
      "Epoch 0[12231/17270] Time:0.239, Train Loss:0.5372581481933594\n",
      "Epoch 0[12232/17270] Time:0.239, Train Loss:0.4069342315196991\n",
      "Epoch 0[12233/17270] Time:0.234, Train Loss:0.3086260259151459\n",
      "Epoch 0[12234/17270] Time:0.242, Train Loss:0.9773900508880615\n",
      "Epoch 0[12235/17270] Time:0.246, Train Loss:0.9774826169013977\n",
      "Epoch 0[12236/17270] Time:0.241, Train Loss:0.6977377533912659\n",
      "Epoch 0[12237/17270] Time:0.236, Train Loss:0.3114064931869507\n",
      "Epoch 0[12238/17270] Time:0.242, Train Loss:0.42358940839767456\n",
      "Epoch 0[12239/17270] Time:0.227, Train Loss:0.4978468716144562\n",
      "Epoch 0[12240/17270] Time:0.245, Train Loss:0.7072767019271851\n",
      "Epoch 0[12241/17270] Time:0.236, Train Loss:0.5926355123519897\n",
      "Epoch 0[12242/17270] Time:0.241, Train Loss:0.3102816641330719\n",
      "Epoch 0[12243/17270] Time:0.242, Train Loss:0.4690863788127899\n",
      "Epoch 0[12244/17270] Time:0.238, Train Loss:1.2610794305801392\n",
      "Epoch 0[12245/17270] Time:0.249, Train Loss:0.5636759996414185\n",
      "Epoch 0[12246/17270] Time:0.232, Train Loss:0.48222124576568604\n",
      "Epoch 0[12247/17270] Time:0.245, Train Loss:0.6701645255088806\n",
      "Epoch 0[12248/17270] Time:0.243, Train Loss:0.37654733657836914\n",
      "Epoch 0[12249/17270] Time:0.231, Train Loss:0.4577823281288147\n",
      "Epoch 0[12250/17270] Time:0.246, Train Loss:0.6255914568901062\n",
      "Epoch 0[12251/17270] Time:0.22, Train Loss:0.8701098561286926\n",
      "Epoch 0[12252/17270] Time:0.247, Train Loss:0.5188044309616089\n",
      "Epoch 0[12253/17270] Time:0.223, Train Loss:0.32285216450691223\n",
      "Epoch 0[12254/17270] Time:0.231, Train Loss:0.4649316370487213\n",
      "Epoch 0[12255/17270] Time:0.235, Train Loss:0.5689694285392761\n",
      "Epoch 0[12256/17270] Time:0.236, Train Loss:1.3088983297348022\n",
      "Epoch 0[12257/17270] Time:0.228, Train Loss:0.2213401347398758\n",
      "Epoch 0[12258/17270] Time:0.222, Train Loss:0.5364941358566284\n",
      "Epoch 0[12259/17270] Time:0.235, Train Loss:0.6115697026252747\n",
      "Epoch 0[12260/17270] Time:0.239, Train Loss:0.4937814176082611\n",
      "Epoch 0[12261/17270] Time:0.238, Train Loss:0.4138111472129822\n",
      "Epoch 0[12262/17270] Time:0.23, Train Loss:0.5712610483169556\n",
      "Epoch 0[12263/17270] Time:0.234, Train Loss:0.5158976912498474\n",
      "Epoch 0[12264/17270] Time:0.244, Train Loss:0.4954574704170227\n",
      "Epoch 0[12265/17270] Time:0.237, Train Loss:0.4770224988460541\n",
      "Epoch 0[12266/17270] Time:0.235, Train Loss:0.8114104270935059\n",
      "Epoch 0[12267/17270] Time:0.238, Train Loss:0.665730893611908\n",
      "Epoch 0[12268/17270] Time:0.225, Train Loss:0.6199379563331604\n",
      "Epoch 0[12269/17270] Time:0.236, Train Loss:0.6201976537704468\n",
      "Epoch 0[12270/17270] Time:0.229, Train Loss:0.4765559136867523\n",
      "Epoch 0[12271/17270] Time:0.23, Train Loss:1.1071833372116089\n",
      "Epoch 0[12272/17270] Time:0.241, Train Loss:0.36540690064430237\n",
      "Epoch 0[12273/17270] Time:0.231, Train Loss:1.5016345977783203\n",
      "Epoch 0[12274/17270] Time:0.232, Train Loss:0.4870169162750244\n",
      "Epoch 0[12275/17270] Time:0.229, Train Loss:0.7331639528274536\n",
      "Epoch 0[12276/17270] Time:0.233, Train Loss:0.6043599843978882\n",
      "Epoch 0[12277/17270] Time:0.232, Train Loss:1.1263009309768677\n",
      "Epoch 0[12278/17270] Time:0.223, Train Loss:0.5169879794120789\n",
      "Epoch 0[12279/17270] Time:0.238, Train Loss:0.2841739356517792\n",
      "Epoch 0[12280/17270] Time:0.233, Train Loss:0.7197710871696472\n",
      "Epoch 0[12281/17270] Time:0.237, Train Loss:0.4325718283653259\n",
      "Epoch 0[12282/17270] Time:0.231, Train Loss:0.3096804618835449\n",
      "Epoch 0[12283/17270] Time:0.233, Train Loss:0.5196207761764526\n",
      "Epoch 0[12284/17270] Time:0.243, Train Loss:0.6581586599349976\n",
      "Epoch 0[12285/17270] Time:0.226, Train Loss:0.29243192076683044\n",
      "Epoch 0[12286/17270] Time:0.228, Train Loss:0.6345809698104858\n",
      "Epoch 0[12287/17270] Time:0.239, Train Loss:0.4779193699359894\n",
      "Epoch 0[12288/17270] Time:0.228, Train Loss:0.8787584900856018\n",
      "Epoch 0[12289/17270] Time:0.232, Train Loss:0.41557666659355164\n",
      "Epoch 0[12290/17270] Time:0.229, Train Loss:0.6030995845794678\n",
      "Epoch 0[12291/17270] Time:0.23, Train Loss:0.9557561874389648\n",
      "Epoch 0[12292/17270] Time:0.234, Train Loss:0.611454963684082\n",
      "Epoch 0[12293/17270] Time:0.221, Train Loss:0.729313850402832\n",
      "Epoch 0[12294/17270] Time:0.232, Train Loss:0.48088258504867554\n",
      "Epoch 0[12295/17270] Time:0.24, Train Loss:0.45601949095726013\n",
      "Epoch 0[12296/17270] Time:0.242, Train Loss:1.2834060192108154\n",
      "Epoch 0[12297/17270] Time:0.221, Train Loss:0.5909124612808228\n",
      "Epoch 0[12298/17270] Time:0.246, Train Loss:0.8437178134918213\n",
      "Epoch 0[12299/17270] Time:0.232, Train Loss:0.6403639316558838\n",
      "Epoch 0[12300/17270] Time:0.224, Train Loss:0.8998565077781677\n",
      "Epoch 0[12301/17270] Time:0.235, Train Loss:0.5788711309432983\n",
      "Epoch 0[12302/17270] Time:0.235, Train Loss:0.6320732831954956\n",
      "Epoch 0[12303/17270] Time:0.238, Train Loss:0.6939430236816406\n",
      "Epoch 0[12304/17270] Time:0.246, Train Loss:0.5942648649215698\n",
      "Epoch 0[12305/17270] Time:0.226, Train Loss:1.5135529041290283\n",
      "Epoch 0[12306/17270] Time:0.24, Train Loss:0.7960324883460999\n",
      "Epoch 0[12307/17270] Time:0.233, Train Loss:0.7584313750267029\n",
      "Epoch 0[12308/17270] Time:0.226, Train Loss:0.4836900234222412\n",
      "Epoch 0[12309/17270] Time:0.24, Train Loss:0.7963934540748596\n",
      "Epoch 0[12310/17270] Time:0.228, Train Loss:0.6920919418334961\n",
      "Epoch 0[12311/17270] Time:0.232, Train Loss:0.8588311076164246\n",
      "Epoch 0[12312/17270] Time:0.23, Train Loss:0.4936926066875458\n",
      "Epoch 0[12313/17270] Time:0.232, Train Loss:0.472190260887146\n",
      "Epoch 0[12314/17270] Time:0.248, Train Loss:0.5029622316360474\n",
      "Epoch 0[12315/17270] Time:0.225, Train Loss:0.4693887531757355\n",
      "Epoch 0[12316/17270] Time:0.231, Train Loss:0.37165528535842896\n",
      "Epoch 0[12317/17270] Time:0.234, Train Loss:0.9552382826805115\n",
      "Epoch 0[12318/17270] Time:0.236, Train Loss:0.6975049376487732\n",
      "Epoch 0[12319/17270] Time:0.236, Train Loss:0.6784623861312866\n",
      "Epoch 0[12320/17270] Time:0.24, Train Loss:0.7576242089271545\n",
      "Epoch 0[12321/17270] Time:0.23, Train Loss:0.5473757386207581\n",
      "Epoch 0[12322/17270] Time:0.24, Train Loss:1.2033822536468506\n",
      "Epoch 0[12323/17270] Time:0.233, Train Loss:0.4061189591884613\n",
      "Epoch 0[12324/17270] Time:0.23, Train Loss:0.27363163232803345\n",
      "Epoch 0[12325/17270] Time:0.233, Train Loss:0.5149932503700256\n",
      "Epoch 0[12326/17270] Time:0.222, Train Loss:0.6732430458068848\n",
      "Epoch 0[12327/17270] Time:0.235, Train Loss:0.5083284378051758\n",
      "Epoch 0[12328/17270] Time:0.233, Train Loss:0.34034836292266846\n",
      "Epoch 0[12329/17270] Time:0.234, Train Loss:0.5585488677024841\n",
      "Epoch 0[12330/17270] Time:0.233, Train Loss:0.45450881123542786\n",
      "Epoch 0[12331/17270] Time:0.231, Train Loss:0.9603624939918518\n",
      "Epoch 0[12332/17270] Time:0.238, Train Loss:0.6389467120170593\n",
      "Epoch 0[12333/17270] Time:0.237, Train Loss:0.6141707301139832\n",
      "Epoch 0[12334/17270] Time:0.24, Train Loss:0.3602692186832428\n",
      "Epoch 0[12335/17270] Time:0.228, Train Loss:0.5145246386528015\n",
      "Epoch 0[12336/17270] Time:0.23, Train Loss:0.5189787745475769\n",
      "Epoch 0[12337/17270] Time:0.233, Train Loss:0.4563957154750824\n",
      "Epoch 0[12338/17270] Time:0.234, Train Loss:0.6131067276000977\n",
      "Epoch 0[12339/17270] Time:0.233, Train Loss:0.4939332604408264\n",
      "Epoch 0[12340/17270] Time:0.237, Train Loss:0.46505674719810486\n",
      "Epoch 0[12341/17270] Time:0.239, Train Loss:0.6642434000968933\n",
      "Epoch 0[12342/17270] Time:0.233, Train Loss:0.5948560833930969\n",
      "Epoch 0[12343/17270] Time:0.23, Train Loss:0.33480191230773926\n",
      "Epoch 0[12344/17270] Time:0.243, Train Loss:0.36244744062423706\n",
      "Epoch 0[12345/17270] Time:0.238, Train Loss:0.4063097834587097\n",
      "Epoch 0[12346/17270] Time:0.244, Train Loss:0.4414227306842804\n",
      "Epoch 0[12347/17270] Time:0.231, Train Loss:0.8793450593948364\n",
      "Epoch 0[12348/17270] Time:0.234, Train Loss:0.5210461020469666\n",
      "Epoch 0[12349/17270] Time:0.226, Train Loss:0.22837644815444946\n",
      "Epoch 0[12350/17270] Time:0.236, Train Loss:0.8168904781341553\n",
      "Epoch 0[12351/17270] Time:0.231, Train Loss:0.4265821576118469\n",
      "Epoch 0[12352/17270] Time:0.232, Train Loss:0.532633900642395\n",
      "Epoch 0[12353/17270] Time:0.245, Train Loss:0.39702078700065613\n",
      "Epoch 0[12354/17270] Time:0.246, Train Loss:0.4838443696498871\n",
      "Epoch 0[12355/17270] Time:0.238, Train Loss:0.8994235992431641\n",
      "Epoch 0[12356/17270] Time:0.238, Train Loss:0.5773705840110779\n",
      "Epoch 0[12357/17270] Time:0.218, Train Loss:0.6351566910743713\n",
      "Epoch 0[12358/17270] Time:0.228, Train Loss:0.5157981514930725\n",
      "Epoch 0[12359/17270] Time:0.249, Train Loss:0.39763134717941284\n",
      "Epoch 0[12360/17270] Time:0.236, Train Loss:0.7130510807037354\n",
      "Epoch 0[12361/17270] Time:0.236, Train Loss:0.38562434911727905\n",
      "Epoch 0[12362/17270] Time:0.224, Train Loss:0.33435583114624023\n",
      "Epoch 0[12363/17270] Time:0.228, Train Loss:1.1507091522216797\n",
      "Epoch 0[12364/17270] Time:0.237, Train Loss:0.3802547752857208\n",
      "Epoch 0[12365/17270] Time:0.24, Train Loss:0.40465158224105835\n",
      "Epoch 0[12366/17270] Time:0.23, Train Loss:0.6510565280914307\n",
      "Epoch 0[12367/17270] Time:0.23, Train Loss:0.4476574957370758\n",
      "Epoch 0[12368/17270] Time:0.237, Train Loss:0.3140902817249298\n",
      "Epoch 0[12369/17270] Time:0.238, Train Loss:0.639006495475769\n",
      "Epoch 0[12370/17270] Time:0.24, Train Loss:0.4259244203567505\n",
      "Epoch 0[12371/17270] Time:0.229, Train Loss:0.5828962326049805\n",
      "Epoch 0[12372/17270] Time:0.237, Train Loss:0.332622766494751\n",
      "Epoch 0[12373/17270] Time:0.238, Train Loss:0.5269442796707153\n",
      "Epoch 0[12374/17270] Time:0.228, Train Loss:0.43472328782081604\n",
      "Epoch 0[12375/17270] Time:0.235, Train Loss:0.6029554009437561\n",
      "Epoch 0[12376/17270] Time:0.227, Train Loss:0.3600066900253296\n",
      "Epoch 0[12377/17270] Time:0.228, Train Loss:0.38304755091667175\n",
      "Epoch 0[12378/17270] Time:0.229, Train Loss:0.9451559782028198\n",
      "Epoch 0[12379/17270] Time:0.229, Train Loss:0.6197710037231445\n",
      "Epoch 0[12380/17270] Time:0.234, Train Loss:0.40031537413597107\n",
      "Epoch 0[12381/17270] Time:0.229, Train Loss:0.5145950317382812\n",
      "Epoch 0[12382/17270] Time:0.235, Train Loss:0.696540355682373\n",
      "Epoch 0[12383/17270] Time:0.238, Train Loss:0.4525739550590515\n",
      "Epoch 0[12384/17270] Time:0.25, Train Loss:0.32439565658569336\n",
      "Epoch 0[12385/17270] Time:0.226, Train Loss:0.6560159921646118\n",
      "Epoch 0[12386/17270] Time:0.238, Train Loss:0.7665222883224487\n",
      "Epoch 0[12387/17270] Time:0.224, Train Loss:0.5209392309188843\n",
      "Epoch 0[12388/17270] Time:0.243, Train Loss:0.5479872822761536\n",
      "Epoch 0[12389/17270] Time:0.233, Train Loss:0.4920046925544739\n",
      "Epoch 0[12390/17270] Time:0.232, Train Loss:0.3582153916358948\n",
      "Epoch 0[12391/17270] Time:0.225, Train Loss:0.7101768255233765\n",
      "Epoch 0[12392/17270] Time:0.238, Train Loss:0.6416393518447876\n",
      "Epoch 0[12393/17270] Time:0.238, Train Loss:0.7643563747406006\n",
      "Epoch 0[12394/17270] Time:0.237, Train Loss:0.35014691948890686\n",
      "Epoch 0[12395/17270] Time:0.232, Train Loss:0.423575222492218\n",
      "Epoch 0[12396/17270] Time:0.232, Train Loss:0.853188157081604\n",
      "Epoch 0[12397/17270] Time:0.232, Train Loss:0.3759324848651886\n",
      "Epoch 0[12398/17270] Time:0.235, Train Loss:0.47199493646621704\n",
      "Epoch 0[12399/17270] Time:0.238, Train Loss:0.3420049250125885\n",
      "Epoch 0[12400/17270] Time:0.235, Train Loss:0.378406822681427\n",
      "Epoch 0[12401/17270] Time:0.238, Train Loss:0.5198726654052734\n",
      "Epoch 0[12402/17270] Time:0.234, Train Loss:0.3575907349586487\n",
      "Epoch 0[12403/17270] Time:0.238, Train Loss:0.39978495240211487\n",
      "Epoch 0[12404/17270] Time:0.234, Train Loss:0.4827330708503723\n",
      "Epoch 0[12405/17270] Time:0.227, Train Loss:0.7049488425254822\n",
      "Epoch 0[12406/17270] Time:0.233, Train Loss:0.574391782283783\n",
      "Epoch 0[12407/17270] Time:0.235, Train Loss:0.7088736295700073\n",
      "Epoch 0[12408/17270] Time:0.254, Train Loss:0.5240041613578796\n",
      "Epoch 0[12409/17270] Time:0.226, Train Loss:1.0810868740081787\n",
      "Epoch 0[12410/17270] Time:0.244, Train Loss:0.40656888484954834\n",
      "Epoch 0[12411/17270] Time:0.235, Train Loss:0.8770619630813599\n",
      "Epoch 0[12412/17270] Time:0.262, Train Loss:0.3788686692714691\n",
      "Epoch 0[12413/17270] Time:0.238, Train Loss:0.6502350568771362\n",
      "Epoch 0[12414/17270] Time:0.226, Train Loss:0.5937538743019104\n",
      "Epoch 0[12415/17270] Time:0.234, Train Loss:0.7195351123809814\n",
      "Epoch 0[12416/17270] Time:0.238, Train Loss:0.4655861258506775\n",
      "Epoch 0[12417/17270] Time:0.227, Train Loss:0.46159887313842773\n",
      "Epoch 0[12418/17270] Time:0.226, Train Loss:0.5527099967002869\n",
      "Epoch 0[12419/17270] Time:0.225, Train Loss:0.481102854013443\n",
      "Epoch 0[12420/17270] Time:0.228, Train Loss:0.6951647996902466\n",
      "Epoch 0[12421/17270] Time:0.232, Train Loss:0.34274542331695557\n",
      "Epoch 0[12422/17270] Time:0.228, Train Loss:0.40898191928863525\n",
      "Epoch 0[12423/17270] Time:0.236, Train Loss:0.3701819181442261\n",
      "Epoch 0[12424/17270] Time:0.235, Train Loss:0.6890751719474792\n",
      "Epoch 0[12425/17270] Time:0.235, Train Loss:0.5935109853744507\n",
      "Epoch 0[12426/17270] Time:0.235, Train Loss:0.7025768756866455\n",
      "Epoch 0[12427/17270] Time:0.228, Train Loss:0.44662240147590637\n",
      "Epoch 0[12428/17270] Time:0.254, Train Loss:0.9300132393836975\n",
      "Epoch 0[12429/17270] Time:0.235, Train Loss:0.534792423248291\n",
      "Epoch 0[12430/17270] Time:0.246, Train Loss:0.5287487506866455\n",
      "Epoch 0[12431/17270] Time:0.25, Train Loss:0.4745650291442871\n",
      "Epoch 0[12432/17270] Time:0.233, Train Loss:0.43381375074386597\n",
      "Epoch 0[12433/17270] Time:0.248, Train Loss:0.4846203625202179\n",
      "Epoch 0[12434/17270] Time:0.237, Train Loss:0.5289414525032043\n",
      "Epoch 0[12435/17270] Time:0.238, Train Loss:0.6396307945251465\n",
      "Epoch 0[12436/17270] Time:0.236, Train Loss:0.35648322105407715\n",
      "Epoch 0[12437/17270] Time:0.231, Train Loss:0.45851993560791016\n",
      "Epoch 0[12438/17270] Time:0.233, Train Loss:0.28217074275016785\n",
      "Epoch 0[12439/17270] Time:0.242, Train Loss:0.3663065731525421\n",
      "Epoch 0[12440/17270] Time:0.226, Train Loss:0.3910259008407593\n",
      "Epoch 0[12441/17270] Time:0.233, Train Loss:0.7150992155075073\n",
      "Epoch 0[12442/17270] Time:0.23, Train Loss:0.5350605845451355\n",
      "Epoch 0[12443/17270] Time:0.241, Train Loss:0.9030008912086487\n",
      "Epoch 0[12444/17270] Time:0.234, Train Loss:0.4479721784591675\n",
      "Epoch 0[12445/17270] Time:0.224, Train Loss:0.7073193788528442\n",
      "Epoch 0[12446/17270] Time:0.229, Train Loss:0.9318410158157349\n",
      "Epoch 0[12447/17270] Time:0.232, Train Loss:0.31515905261039734\n",
      "Epoch 0[12448/17270] Time:0.237, Train Loss:0.40066081285476685\n",
      "Epoch 0[12449/17270] Time:0.23, Train Loss:0.8097543120384216\n",
      "Epoch 0[12450/17270] Time:0.242, Train Loss:0.4409913718700409\n",
      "Epoch 0[12451/17270] Time:0.236, Train Loss:0.891063928604126\n",
      "Epoch 0[12452/17270] Time:0.24, Train Loss:0.47821488976478577\n",
      "Epoch 0[12453/17270] Time:0.237, Train Loss:0.761199951171875\n",
      "Epoch 0[12454/17270] Time:0.236, Train Loss:0.37728968262672424\n",
      "Epoch 0[12455/17270] Time:0.232, Train Loss:0.6669155955314636\n",
      "Epoch 0[12456/17270] Time:0.236, Train Loss:0.5254666805267334\n",
      "Epoch 0[12457/17270] Time:0.23, Train Loss:0.31775540113449097\n",
      "Epoch 0[12458/17270] Time:0.235, Train Loss:0.5276808738708496\n",
      "Epoch 0[12459/17270] Time:0.229, Train Loss:0.4519110321998596\n",
      "Epoch 0[12460/17270] Time:0.23, Train Loss:0.6303402185440063\n",
      "Epoch 0[12461/17270] Time:0.229, Train Loss:0.6162446737289429\n",
      "Epoch 0[12462/17270] Time:0.235, Train Loss:0.6698392033576965\n",
      "Epoch 0[12463/17270] Time:0.239, Train Loss:0.6149328351020813\n",
      "Epoch 0[12464/17270] Time:0.235, Train Loss:0.5150661468505859\n",
      "Epoch 0[12465/17270] Time:0.232, Train Loss:0.8119210004806519\n",
      "Epoch 0[12466/17270] Time:0.23, Train Loss:0.43818870186805725\n",
      "Epoch 0[12467/17270] Time:0.232, Train Loss:0.41443681716918945\n",
      "Epoch 0[12468/17270] Time:0.232, Train Loss:0.38671407103538513\n",
      "Epoch 0[12469/17270] Time:0.237, Train Loss:0.3020365834236145\n",
      "Epoch 0[12470/17270] Time:0.231, Train Loss:0.41529685258865356\n",
      "Epoch 0[12471/17270] Time:0.245, Train Loss:0.38688555359840393\n",
      "Epoch 0[12472/17270] Time:0.236, Train Loss:0.5213967561721802\n",
      "Epoch 0[12473/17270] Time:0.241, Train Loss:0.536846935749054\n",
      "Epoch 0[12474/17270] Time:0.247, Train Loss:0.4647602438926697\n",
      "Epoch 0[12475/17270] Time:0.231, Train Loss:0.5133278369903564\n",
      "Epoch 0[12476/17270] Time:0.233, Train Loss:0.6306097507476807\n",
      "Epoch 0[12477/17270] Time:0.251, Train Loss:1.0137832164764404\n",
      "Epoch 0[12478/17270] Time:0.229, Train Loss:0.38898158073425293\n",
      "Epoch 0[12479/17270] Time:0.238, Train Loss:0.6122735738754272\n",
      "Epoch 0[12480/17270] Time:0.238, Train Loss:0.6461485028266907\n",
      "Epoch 0[12481/17270] Time:0.226, Train Loss:0.3749511241912842\n",
      "Epoch 0[12482/17270] Time:0.24, Train Loss:0.38017985224723816\n",
      "Epoch 0[12483/17270] Time:0.23, Train Loss:0.47772216796875\n",
      "Epoch 0[12484/17270] Time:0.246, Train Loss:0.6255471110343933\n",
      "Epoch 0[12485/17270] Time:0.225, Train Loss:0.6315127015113831\n",
      "Epoch 0[12486/17270] Time:0.242, Train Loss:1.3467087745666504\n",
      "Epoch 0[12487/17270] Time:0.239, Train Loss:0.5261273384094238\n",
      "Epoch 0[12488/17270] Time:0.223, Train Loss:0.5995433330535889\n",
      "Epoch 0[12489/17270] Time:0.241, Train Loss:0.43010058999061584\n",
      "Epoch 0[12490/17270] Time:0.241, Train Loss:0.7290744781494141\n",
      "Epoch 0[12491/17270] Time:0.234, Train Loss:0.4996602535247803\n",
      "Epoch 0[12492/17270] Time:0.233, Train Loss:0.5776923894882202\n",
      "Epoch 0[12493/17270] Time:0.237, Train Loss:0.34081873297691345\n",
      "Epoch 0[12494/17270] Time:0.233, Train Loss:0.7701968550682068\n",
      "Epoch 0[12495/17270] Time:0.243, Train Loss:0.6932968497276306\n",
      "Epoch 0[12496/17270] Time:0.239, Train Loss:0.41142961382865906\n",
      "Epoch 0[12497/17270] Time:0.23, Train Loss:0.44221487641334534\n",
      "Epoch 0[12498/17270] Time:0.242, Train Loss:0.5137551426887512\n",
      "Epoch 0[12499/17270] Time:0.225, Train Loss:0.29009687900543213\n",
      "Epoch 0[12500/17270] Time:0.255, Train Loss:0.46502596139907837\n",
      "Epoch 0[12501/17270] Time:0.245, Train Loss:0.3309677243232727\n",
      "Epoch 0[12502/17270] Time:0.241, Train Loss:1.1757731437683105\n",
      "Epoch 0[12503/17270] Time:0.244, Train Loss:1.0069974660873413\n",
      "Epoch 0[12504/17270] Time:0.231, Train Loss:0.7788825035095215\n",
      "Epoch 0[12505/17270] Time:0.237, Train Loss:0.47611546516418457\n",
      "Epoch 0[12506/17270] Time:0.247, Train Loss:0.6721676588058472\n",
      "Epoch 0[12507/17270] Time:0.232, Train Loss:0.3884277641773224\n",
      "Epoch 0[12508/17270] Time:0.235, Train Loss:0.5069010257720947\n",
      "Epoch 0[12509/17270] Time:0.243, Train Loss:1.0031929016113281\n",
      "Epoch 0[12510/17270] Time:0.242, Train Loss:0.5665130019187927\n",
      "Epoch 0[12511/17270] Time:0.242, Train Loss:0.8218679428100586\n",
      "Epoch 0[12512/17270] Time:0.232, Train Loss:0.4107668399810791\n",
      "Epoch 0[12513/17270] Time:0.231, Train Loss:0.4216587543487549\n",
      "Epoch 0[12514/17270] Time:0.238, Train Loss:0.4803304374217987\n",
      "Epoch 0[12515/17270] Time:0.228, Train Loss:0.401450514793396\n",
      "Epoch 0[12516/17270] Time:0.237, Train Loss:1.0346421003341675\n",
      "Epoch 0[12517/17270] Time:0.237, Train Loss:0.4522131681442261\n",
      "Epoch 0[12518/17270] Time:0.233, Train Loss:0.5351661443710327\n",
      "Epoch 0[12519/17270] Time:0.24, Train Loss:0.48301419615745544\n",
      "Epoch 0[12520/17270] Time:0.223, Train Loss:0.383820503950119\n",
      "Epoch 0[12521/17270] Time:0.242, Train Loss:0.7926072478294373\n",
      "Epoch 0[12522/17270] Time:0.237, Train Loss:0.4121361970901489\n",
      "Epoch 0[12523/17270] Time:0.23, Train Loss:0.4122820794582367\n",
      "Epoch 0[12524/17270] Time:0.239, Train Loss:0.35701557993888855\n",
      "Epoch 0[12525/17270] Time:0.248, Train Loss:0.45600372552871704\n",
      "Epoch 0[12526/17270] Time:0.237, Train Loss:0.964097797870636\n",
      "Epoch 0[12527/17270] Time:0.231, Train Loss:0.49720388650894165\n",
      "Epoch 0[12528/17270] Time:0.236, Train Loss:0.5503739714622498\n",
      "Epoch 0[12529/17270] Time:0.238, Train Loss:0.44411760568618774\n",
      "Epoch 0[12530/17270] Time:0.229, Train Loss:0.915682852268219\n",
      "Epoch 0[12531/17270] Time:0.238, Train Loss:1.0771617889404297\n",
      "Epoch 0[12532/17270] Time:0.232, Train Loss:0.9527142643928528\n",
      "Epoch 0[12533/17270] Time:0.238, Train Loss:0.49815452098846436\n",
      "Epoch 0[12534/17270] Time:0.237, Train Loss:1.140834927558899\n",
      "Epoch 0[12535/17270] Time:0.23, Train Loss:0.6744707822799683\n",
      "Epoch 0[12536/17270] Time:0.239, Train Loss:0.2935572862625122\n",
      "Epoch 0[12537/17270] Time:0.231, Train Loss:0.7173563241958618\n",
      "Epoch 0[12538/17270] Time:0.237, Train Loss:0.4250734746456146\n",
      "Epoch 0[12539/17270] Time:0.237, Train Loss:0.30338093638420105\n",
      "Epoch 0[12540/17270] Time:0.229, Train Loss:0.4954036772251129\n",
      "Epoch 0[12541/17270] Time:0.228, Train Loss:0.2876494824886322\n",
      "Epoch 0[12542/17270] Time:0.226, Train Loss:0.4406757950782776\n",
      "Epoch 0[12543/17270] Time:0.229, Train Loss:0.8739727735519409\n",
      "Epoch 0[12544/17270] Time:0.232, Train Loss:0.614266574382782\n",
      "Epoch 0[12545/17270] Time:0.236, Train Loss:0.855754017829895\n",
      "Epoch 0[12546/17270] Time:0.228, Train Loss:0.5420194268226624\n",
      "Epoch 0[12547/17270] Time:0.238, Train Loss:0.33231377601623535\n",
      "Epoch 0[12548/17270] Time:0.227, Train Loss:0.5462034940719604\n",
      "Epoch 0[12549/17270] Time:0.235, Train Loss:0.6560496687889099\n",
      "Epoch 0[12550/17270] Time:0.237, Train Loss:0.47845807671546936\n",
      "Epoch 0[12551/17270] Time:0.239, Train Loss:0.5725033283233643\n",
      "Epoch 0[12552/17270] Time:0.223, Train Loss:0.5238174200057983\n",
      "Epoch 0[12553/17270] Time:0.228, Train Loss:0.3380086123943329\n",
      "Epoch 0[12554/17270] Time:0.231, Train Loss:0.4606262147426605\n",
      "Epoch 0[12555/17270] Time:0.229, Train Loss:0.42879050970077515\n",
      "Epoch 0[12556/17270] Time:0.23, Train Loss:0.6397815942764282\n",
      "Epoch 0[12557/17270] Time:0.229, Train Loss:0.5195798277854919\n",
      "Epoch 0[12558/17270] Time:0.247, Train Loss:0.6271171569824219\n",
      "Epoch 0[12559/17270] Time:0.231, Train Loss:0.4277275502681732\n",
      "Epoch 0[12560/17270] Time:0.241, Train Loss:0.5177215337753296\n",
      "Epoch 0[12561/17270] Time:0.223, Train Loss:0.3960249423980713\n",
      "Epoch 0[12562/17270] Time:0.233, Train Loss:0.7742727994918823\n",
      "Epoch 0[12563/17270] Time:0.228, Train Loss:0.2972254753112793\n",
      "Epoch 0[12564/17270] Time:0.244, Train Loss:0.5458902716636658\n",
      "Epoch 0[12565/17270] Time:0.221, Train Loss:0.7697767615318298\n",
      "Epoch 0[12566/17270] Time:0.236, Train Loss:0.5550948977470398\n",
      "Epoch 0[12567/17270] Time:0.232, Train Loss:0.7941006422042847\n",
      "Epoch 0[12568/17270] Time:0.236, Train Loss:0.2605925500392914\n",
      "Epoch 0[12569/17270] Time:0.23, Train Loss:0.6794488430023193\n",
      "Epoch 0[12570/17270] Time:0.233, Train Loss:0.4146936237812042\n",
      "Epoch 0[12571/17270] Time:0.238, Train Loss:0.6630186438560486\n",
      "Epoch 0[12572/17270] Time:0.231, Train Loss:0.39320334792137146\n",
      "Epoch 0[12573/17270] Time:0.226, Train Loss:0.5546539425849915\n",
      "Epoch 0[12574/17270] Time:0.23, Train Loss:0.5196638107299805\n",
      "Epoch 0[12575/17270] Time:0.241, Train Loss:0.34010592103004456\n",
      "Epoch 0[12576/17270] Time:0.23, Train Loss:0.4904240071773529\n",
      "Epoch 0[12577/17270] Time:0.242, Train Loss:0.5425257086753845\n",
      "Epoch 0[12578/17270] Time:0.223, Train Loss:0.2841683030128479\n",
      "Epoch 0[12579/17270] Time:0.237, Train Loss:0.6013934016227722\n",
      "Epoch 0[12580/17270] Time:0.226, Train Loss:0.809090793132782\n",
      "Epoch 0[12581/17270] Time:0.242, Train Loss:0.63130122423172\n",
      "Epoch 0[12582/17270] Time:0.247, Train Loss:0.7445507049560547\n",
      "Epoch 0[12583/17270] Time:0.24, Train Loss:0.5736238360404968\n",
      "Epoch 0[12584/17270] Time:0.227, Train Loss:0.495189905166626\n",
      "Epoch 0[12585/17270] Time:0.234, Train Loss:0.3714079260826111\n",
      "Epoch 0[12586/17270] Time:0.23, Train Loss:0.9124787449836731\n",
      "Epoch 0[12587/17270] Time:0.227, Train Loss:0.4172165095806122\n",
      "Epoch 0[12588/17270] Time:0.227, Train Loss:0.31796950101852417\n",
      "Epoch 0[12589/17270] Time:0.243, Train Loss:0.5703147053718567\n",
      "Epoch 0[12590/17270] Time:0.219, Train Loss:0.6675304174423218\n",
      "Epoch 0[12591/17270] Time:0.235, Train Loss:0.6041417717933655\n",
      "Epoch 0[12592/17270] Time:0.252, Train Loss:0.492256224155426\n",
      "Epoch 0[12593/17270] Time:0.251, Train Loss:0.3341900706291199\n",
      "Epoch 0[12594/17270] Time:0.232, Train Loss:0.5806282162666321\n",
      "Epoch 0[12595/17270] Time:0.23, Train Loss:0.6199619174003601\n",
      "Epoch 0[12596/17270] Time:0.242, Train Loss:0.3972080647945404\n",
      "Epoch 0[12597/17270] Time:0.225, Train Loss:0.4760715663433075\n",
      "Epoch 0[12598/17270] Time:0.258, Train Loss:0.9099768996238708\n",
      "Epoch 0[12599/17270] Time:0.233, Train Loss:0.7778438329696655\n",
      "Epoch 0[12600/17270] Time:0.244, Train Loss:0.8070409297943115\n",
      "Epoch 0[12601/17270] Time:0.245, Train Loss:1.0409480333328247\n",
      "Epoch 0[12602/17270] Time:0.238, Train Loss:0.5613827109336853\n",
      "Epoch 0[12603/17270] Time:0.238, Train Loss:0.2965582013130188\n",
      "Epoch 0[12604/17270] Time:0.247, Train Loss:0.3502315580844879\n",
      "Epoch 0[12605/17270] Time:0.252, Train Loss:0.41849297285079956\n",
      "Epoch 0[12606/17270] Time:0.235, Train Loss:0.45683303475379944\n",
      "Epoch 0[12607/17270] Time:0.247, Train Loss:0.3372691869735718\n",
      "Epoch 0[12608/17270] Time:0.235, Train Loss:0.48793017864227295\n",
      "Epoch 0[12609/17270] Time:0.235, Train Loss:0.2904416024684906\n",
      "Epoch 0[12610/17270] Time:0.23, Train Loss:0.48945140838623047\n",
      "Epoch 0[12611/17270] Time:0.232, Train Loss:0.8097289204597473\n",
      "Epoch 0[12612/17270] Time:0.243, Train Loss:0.3978513181209564\n",
      "Epoch 0[12613/17270] Time:0.223, Train Loss:0.43028345704078674\n",
      "Epoch 0[12614/17270] Time:0.239, Train Loss:0.6016749143600464\n",
      "Epoch 0[12615/17270] Time:0.251, Train Loss:1.7621783018112183\n",
      "Epoch 0[12616/17270] Time:0.239, Train Loss:0.676436722278595\n",
      "Epoch 0[12617/17270] Time:0.225, Train Loss:0.6463364958763123\n",
      "Epoch 0[12618/17270] Time:0.236, Train Loss:0.38212910294532776\n",
      "Epoch 0[12619/17270] Time:0.235, Train Loss:0.4083193838596344\n",
      "Epoch 0[12620/17270] Time:0.225, Train Loss:0.5082734227180481\n",
      "Epoch 0[12621/17270] Time:0.245, Train Loss:0.7622089982032776\n",
      "Epoch 0[12622/17270] Time:0.232, Train Loss:0.6278794407844543\n",
      "Epoch 0[12623/17270] Time:0.226, Train Loss:0.45170319080352783\n",
      "Epoch 0[12624/17270] Time:0.233, Train Loss:0.8730900287628174\n",
      "Epoch 0[12625/17270] Time:0.244, Train Loss:0.526822566986084\n",
      "Epoch 0[12626/17270] Time:0.239, Train Loss:0.5302549004554749\n",
      "Epoch 0[12627/17270] Time:0.234, Train Loss:0.5426688194274902\n",
      "Epoch 0[12628/17270] Time:0.232, Train Loss:0.3791821300983429\n",
      "Epoch 0[12629/17270] Time:0.246, Train Loss:0.5669452548027039\n",
      "Epoch 0[12630/17270] Time:0.226, Train Loss:0.6681581139564514\n",
      "Epoch 0[12631/17270] Time:0.23, Train Loss:0.4461097717285156\n",
      "Epoch 0[12632/17270] Time:0.253, Train Loss:0.2975039482116699\n",
      "Epoch 0[12633/17270] Time:0.234, Train Loss:0.9458867907524109\n",
      "Epoch 0[12634/17270] Time:0.232, Train Loss:0.31632137298583984\n",
      "Epoch 0[12635/17270] Time:0.231, Train Loss:0.7968577146530151\n",
      "Epoch 0[12636/17270] Time:0.235, Train Loss:0.3582235872745514\n",
      "Epoch 0[12637/17270] Time:0.233, Train Loss:0.5585306286811829\n",
      "Epoch 0[12638/17270] Time:0.237, Train Loss:1.468778133392334\n",
      "Epoch 0[12639/17270] Time:0.239, Train Loss:0.653204619884491\n",
      "Epoch 0[12640/17270] Time:0.233, Train Loss:1.4080594778060913\n",
      "Epoch 0[12641/17270] Time:0.234, Train Loss:0.6110960245132446\n",
      "Epoch 0[12642/17270] Time:0.228, Train Loss:0.4581459164619446\n",
      "Epoch 0[12643/17270] Time:0.233, Train Loss:0.4974363148212433\n",
      "Epoch 0[12644/17270] Time:0.23, Train Loss:0.9916175007820129\n",
      "Epoch 0[12645/17270] Time:0.237, Train Loss:0.8478728532791138\n",
      "Epoch 0[12646/17270] Time:0.238, Train Loss:0.5152754187583923\n",
      "Epoch 0[12647/17270] Time:0.238, Train Loss:0.5237001180648804\n",
      "Epoch 0[12648/17270] Time:0.23, Train Loss:1.098936915397644\n",
      "Epoch 0[12649/17270] Time:0.233, Train Loss:0.779437243938446\n",
      "Epoch 0[12650/17270] Time:0.229, Train Loss:0.5028846859931946\n",
      "Epoch 0[12651/17270] Time:0.238, Train Loss:0.7888304591178894\n",
      "Epoch 0[12652/17270] Time:0.238, Train Loss:0.33645153045654297\n",
      "Epoch 0[12653/17270] Time:0.231, Train Loss:0.37521636486053467\n",
      "Epoch 0[12654/17270] Time:0.232, Train Loss:0.5327092409133911\n",
      "Epoch 0[12655/17270] Time:0.23, Train Loss:0.6569213271141052\n",
      "Epoch 0[12656/17270] Time:0.239, Train Loss:0.44275331497192383\n",
      "Epoch 0[12657/17270] Time:0.232, Train Loss:0.7155873775482178\n",
      "Epoch 0[12658/17270] Time:0.23, Train Loss:1.2204245328903198\n",
      "Epoch 0[12659/17270] Time:0.241, Train Loss:0.4434528052806854\n",
      "Epoch 0[12660/17270] Time:0.24, Train Loss:0.48872673511505127\n",
      "Epoch 0[12661/17270] Time:0.237, Train Loss:0.5030566453933716\n",
      "Epoch 0[12662/17270] Time:0.248, Train Loss:0.4462011158466339\n",
      "Epoch 0[12663/17270] Time:0.221, Train Loss:0.5854698419570923\n",
      "Epoch 0[12664/17270] Time:0.231, Train Loss:0.5985926985740662\n",
      "Epoch 0[12665/17270] Time:0.236, Train Loss:0.42293474078178406\n",
      "Epoch 0[12666/17270] Time:0.239, Train Loss:0.968395471572876\n",
      "Epoch 0[12667/17270] Time:0.234, Train Loss:0.648725688457489\n",
      "Epoch 0[12668/17270] Time:0.239, Train Loss:0.6333632469177246\n",
      "Epoch 0[12669/17270] Time:0.23, Train Loss:0.5222323536872864\n",
      "Epoch 0[12670/17270] Time:0.228, Train Loss:0.47251471877098083\n",
      "Epoch 0[12671/17270] Time:0.23, Train Loss:0.58751380443573\n",
      "Epoch 0[12672/17270] Time:0.233, Train Loss:0.9959965944290161\n",
      "Epoch 0[12673/17270] Time:0.234, Train Loss:0.894315779209137\n",
      "Epoch 0[12674/17270] Time:0.23, Train Loss:0.577148973941803\n",
      "Epoch 0[12675/17270] Time:0.223, Train Loss:0.9838358759880066\n",
      "Epoch 0[12676/17270] Time:0.245, Train Loss:0.5661745667457581\n",
      "Epoch 0[12677/17270] Time:0.241, Train Loss:0.3294179141521454\n",
      "Epoch 0[12678/17270] Time:0.228, Train Loss:0.42478200793266296\n",
      "Epoch 0[12679/17270] Time:0.236, Train Loss:0.3602178990840912\n",
      "Epoch 0[12680/17270] Time:0.237, Train Loss:0.6476290822029114\n",
      "Epoch 0[12681/17270] Time:0.237, Train Loss:0.5059804916381836\n",
      "Epoch 0[12682/17270] Time:0.241, Train Loss:0.5986093878746033\n",
      "Epoch 0[12683/17270] Time:0.236, Train Loss:0.6224068403244019\n",
      "Epoch 0[12684/17270] Time:0.227, Train Loss:0.5276904702186584\n",
      "Epoch 0[12685/17270] Time:0.23, Train Loss:0.4147868752479553\n",
      "Epoch 0[12686/17270] Time:0.234, Train Loss:0.5293745398521423\n",
      "Epoch 0[12687/17270] Time:0.236, Train Loss:0.6137130856513977\n",
      "Epoch 0[12688/17270] Time:0.237, Train Loss:0.5180813074111938\n",
      "Epoch 0[12689/17270] Time:0.232, Train Loss:0.8282570242881775\n",
      "Epoch 0[12690/17270] Time:0.233, Train Loss:0.5897914171218872\n",
      "Epoch 0[12691/17270] Time:0.231, Train Loss:0.8712030053138733\n",
      "Epoch 0[12692/17270] Time:0.235, Train Loss:1.322340965270996\n",
      "Epoch 0[12693/17270] Time:0.232, Train Loss:0.8130741715431213\n",
      "Epoch 0[12694/17270] Time:0.23, Train Loss:0.4231918454170227\n",
      "Epoch 0[12695/17270] Time:0.231, Train Loss:0.34389346837997437\n",
      "Epoch 0[12696/17270] Time:0.257, Train Loss:0.6583095788955688\n",
      "Epoch 0[12697/17270] Time:0.235, Train Loss:0.6080557703971863\n",
      "Epoch 0[12698/17270] Time:0.223, Train Loss:0.4932718276977539\n",
      "Epoch 0[12699/17270] Time:0.236, Train Loss:0.5573577880859375\n",
      "Epoch 0[12700/17270] Time:0.237, Train Loss:0.2729525566101074\n",
      "Epoch 0[12701/17270] Time:0.231, Train Loss:0.3956995904445648\n",
      "Epoch 0[12702/17270] Time:0.24, Train Loss:1.5095282793045044\n",
      "Epoch 0[12703/17270] Time:0.232, Train Loss:0.8296682834625244\n",
      "Epoch 0[12704/17270] Time:0.237, Train Loss:1.1282776594161987\n",
      "Epoch 0[12705/17270] Time:0.235, Train Loss:0.39451292157173157\n",
      "Epoch 0[12706/17270] Time:0.231, Train Loss:0.49524742364883423\n",
      "Epoch 0[12707/17270] Time:0.235, Train Loss:0.4389937222003937\n",
      "Epoch 0[12708/17270] Time:0.238, Train Loss:0.6693727970123291\n",
      "Epoch 0[12709/17270] Time:0.23, Train Loss:1.3417720794677734\n",
      "Epoch 0[12710/17270] Time:0.239, Train Loss:0.6354982256889343\n",
      "Epoch 0[12711/17270] Time:0.229, Train Loss:0.6746698021888733\n",
      "Epoch 0[12712/17270] Time:0.238, Train Loss:0.4639735817909241\n",
      "Epoch 0[12713/17270] Time:0.229, Train Loss:0.8729183673858643\n",
      "Epoch 0[12714/17270] Time:0.235, Train Loss:0.6151241660118103\n",
      "Epoch 0[12715/17270] Time:0.23, Train Loss:0.5867632627487183\n",
      "Epoch 0[12716/17270] Time:0.242, Train Loss:0.5817755460739136\n",
      "Epoch 0[12717/17270] Time:0.234, Train Loss:0.4688626825809479\n",
      "Epoch 0[12718/17270] Time:0.237, Train Loss:1.1086394786834717\n",
      "Epoch 0[12719/17270] Time:0.233, Train Loss:0.43921810388565063\n",
      "Epoch 0[12720/17270] Time:0.233, Train Loss:0.4273633360862732\n",
      "Epoch 0[12721/17270] Time:0.234, Train Loss:0.6764011979103088\n",
      "Epoch 0[12722/17270] Time:0.239, Train Loss:0.5734487771987915\n",
      "Epoch 0[12723/17270] Time:0.229, Train Loss:0.5713640451431274\n",
      "Epoch 0[12724/17270] Time:0.248, Train Loss:0.5432634949684143\n",
      "Epoch 0[12725/17270] Time:0.245, Train Loss:0.5375982522964478\n",
      "Epoch 0[12726/17270] Time:0.237, Train Loss:0.8187080025672913\n",
      "Epoch 0[12727/17270] Time:0.221, Train Loss:0.6642746925354004\n",
      "Epoch 0[12728/17270] Time:0.241, Train Loss:0.3968624770641327\n",
      "Epoch 0[12729/17270] Time:0.245, Train Loss:0.4969538152217865\n",
      "Epoch 0[12730/17270] Time:0.247, Train Loss:0.32134681940078735\n",
      "Epoch 0[12731/17270] Time:0.241, Train Loss:0.38767653703689575\n",
      "Epoch 0[12732/17270] Time:0.234, Train Loss:0.7146440744400024\n",
      "Epoch 0[12733/17270] Time:0.238, Train Loss:0.7000787258148193\n",
      "Epoch 0[12734/17270] Time:0.24, Train Loss:0.7485600113868713\n",
      "Epoch 0[12735/17270] Time:0.236, Train Loss:0.45375943183898926\n",
      "Epoch 0[12736/17270] Time:0.223, Train Loss:0.41324424743652344\n",
      "Epoch 0[12737/17270] Time:0.241, Train Loss:0.455185204744339\n",
      "Epoch 0[12738/17270] Time:0.236, Train Loss:0.4121537208557129\n",
      "Epoch 0[12739/17270] Time:0.223, Train Loss:0.695000171661377\n",
      "Epoch 0[12740/17270] Time:0.232, Train Loss:0.5421701073646545\n",
      "Epoch 0[12741/17270] Time:0.238, Train Loss:0.31953534483909607\n",
      "Epoch 0[12742/17270] Time:0.236, Train Loss:0.6727489233016968\n",
      "Epoch 0[12743/17270] Time:0.237, Train Loss:0.6095690131187439\n",
      "Epoch 0[12744/17270] Time:0.239, Train Loss:0.44182196259498596\n",
      "Epoch 0[12745/17270] Time:0.232, Train Loss:0.41493621468544006\n",
      "Epoch 0[12746/17270] Time:0.239, Train Loss:0.4410872757434845\n",
      "Epoch 0[12747/17270] Time:0.231, Train Loss:0.9118136167526245\n",
      "Epoch 0[12748/17270] Time:0.23, Train Loss:0.8512658476829529\n",
      "Epoch 0[12749/17270] Time:0.238, Train Loss:0.5237240195274353\n",
      "Epoch 0[12750/17270] Time:0.239, Train Loss:0.2614911198616028\n",
      "Epoch 0[12751/17270] Time:0.234, Train Loss:0.49671292304992676\n",
      "Epoch 0[12752/17270] Time:0.25, Train Loss:0.4391990303993225\n",
      "Epoch 0[12753/17270] Time:0.23, Train Loss:0.26215842366218567\n",
      "Epoch 0[12754/17270] Time:0.238, Train Loss:0.6611974835395813\n",
      "Epoch 0[12755/17270] Time:0.221, Train Loss:0.32959702610969543\n",
      "Epoch 0[12756/17270] Time:0.239, Train Loss:0.9044256806373596\n",
      "Epoch 0[12757/17270] Time:0.239, Train Loss:0.7390329241752625\n",
      "Epoch 0[12758/17270] Time:0.237, Train Loss:0.4252781271934509\n",
      "Epoch 0[12759/17270] Time:0.244, Train Loss:0.5818656086921692\n",
      "Epoch 0[12760/17270] Time:0.24, Train Loss:0.6380415558815002\n",
      "Epoch 0[12761/17270] Time:0.227, Train Loss:0.44451960921287537\n",
      "Epoch 0[12762/17270] Time:0.23, Train Loss:0.43308135867118835\n",
      "Epoch 0[12763/17270] Time:0.247, Train Loss:1.1550008058547974\n",
      "Epoch 0[12764/17270] Time:0.233, Train Loss:0.5183387994766235\n",
      "Epoch 0[12765/17270] Time:0.239, Train Loss:0.3764289915561676\n",
      "Epoch 0[12766/17270] Time:0.225, Train Loss:0.28188827633857727\n",
      "Epoch 0[12767/17270] Time:0.233, Train Loss:0.6622319221496582\n",
      "Epoch 0[12768/17270] Time:0.245, Train Loss:0.36282119154930115\n",
      "Epoch 0[12769/17270] Time:0.235, Train Loss:0.5351195335388184\n",
      "Epoch 0[12770/17270] Time:0.221, Train Loss:0.7644377946853638\n",
      "Epoch 0[12771/17270] Time:0.23, Train Loss:0.33297857642173767\n",
      "Epoch 0[12772/17270] Time:0.24, Train Loss:0.5274280309677124\n",
      "Epoch 0[12773/17270] Time:0.234, Train Loss:0.45868784189224243\n",
      "Epoch 0[12774/17270] Time:0.227, Train Loss:0.38516849279403687\n",
      "Epoch 0[12775/17270] Time:0.23, Train Loss:0.5643928647041321\n",
      "Epoch 0[12776/17270] Time:0.23, Train Loss:0.3511126637458801\n",
      "Epoch 0[12777/17270] Time:0.24, Train Loss:0.7111590504646301\n",
      "Epoch 0[12778/17270] Time:0.234, Train Loss:0.20271210372447968\n",
      "Epoch 0[12779/17270] Time:0.237, Train Loss:0.4103391766548157\n",
      "Epoch 0[12780/17270] Time:0.238, Train Loss:0.7666361927986145\n",
      "Epoch 0[12781/17270] Time:0.23, Train Loss:0.38269299268722534\n",
      "Epoch 0[12782/17270] Time:0.239, Train Loss:0.41999921202659607\n",
      "Epoch 0[12783/17270] Time:0.233, Train Loss:0.23527756333351135\n",
      "Epoch 0[12784/17270] Time:0.238, Train Loss:0.4573499262332916\n",
      "Epoch 0[12785/17270] Time:0.234, Train Loss:0.3618330955505371\n",
      "Epoch 0[12786/17270] Time:0.237, Train Loss:0.2970689833164215\n",
      "Epoch 0[12787/17270] Time:0.237, Train Loss:0.6425395607948303\n",
      "Epoch 0[12788/17270] Time:0.229, Train Loss:0.4115484654903412\n",
      "Epoch 0[12789/17270] Time:0.23, Train Loss:1.081166386604309\n",
      "Epoch 0[12790/17270] Time:0.238, Train Loss:0.6751449704170227\n",
      "Epoch 0[12791/17270] Time:0.228, Train Loss:0.7038232088088989\n",
      "Epoch 0[12792/17270] Time:0.23, Train Loss:0.3727250099182129\n",
      "Epoch 0[12793/17270] Time:0.23, Train Loss:0.8834884166717529\n",
      "Epoch 0[12794/17270] Time:0.229, Train Loss:0.32874414324760437\n",
      "Epoch 0[12795/17270] Time:0.235, Train Loss:0.5910687446594238\n",
      "Epoch 0[12796/17270] Time:0.239, Train Loss:1.052809476852417\n",
      "Epoch 0[12797/17270] Time:0.241, Train Loss:0.3773077726364136\n",
      "Epoch 0[12798/17270] Time:0.228, Train Loss:0.5517973899841309\n",
      "Epoch 0[12799/17270] Time:0.238, Train Loss:0.5515283942222595\n",
      "Epoch 0[12800/17270] Time:0.23, Train Loss:0.7333480715751648\n",
      "Epoch 0[12801/17270] Time:0.236, Train Loss:0.5601693987846375\n",
      "Epoch 0[12802/17270] Time:0.23, Train Loss:0.4080999493598938\n",
      "Epoch 0[12803/17270] Time:0.228, Train Loss:0.2940099537372589\n",
      "Epoch 0[12804/17270] Time:0.229, Train Loss:0.6036645770072937\n",
      "Epoch 0[12805/17270] Time:0.238, Train Loss:0.3784140348434448\n",
      "Epoch 0[12806/17270] Time:0.235, Train Loss:0.49659138917922974\n",
      "Epoch 0[12807/17270] Time:0.239, Train Loss:1.0731834173202515\n",
      "Epoch 0[12808/17270] Time:0.231, Train Loss:0.41099414229393005\n",
      "Epoch 0[12809/17270] Time:0.23, Train Loss:0.6813305020332336\n",
      "Epoch 0[12810/17270] Time:0.228, Train Loss:0.555732250213623\n",
      "Epoch 0[12811/17270] Time:0.232, Train Loss:0.5718815326690674\n",
      "Epoch 0[12812/17270] Time:0.237, Train Loss:0.718182384967804\n",
      "Epoch 0[12813/17270] Time:0.226, Train Loss:2.715677499771118\n",
      "Epoch 0[12814/17270] Time:0.227, Train Loss:0.5961136221885681\n",
      "Epoch 0[12815/17270] Time:0.238, Train Loss:0.9391131401062012\n",
      "Epoch 0[12816/17270] Time:0.238, Train Loss:0.3532784879207611\n",
      "Epoch 0[12817/17270] Time:0.231, Train Loss:0.620414674282074\n",
      "Epoch 0[12818/17270] Time:0.23, Train Loss:0.41032665967941284\n",
      "Epoch 0[12819/17270] Time:0.229, Train Loss:0.7722710371017456\n",
      "Epoch 0[12820/17270] Time:0.228, Train Loss:0.35509565472602844\n",
      "Epoch 0[12821/17270] Time:0.238, Train Loss:0.4855700731277466\n",
      "Epoch 0[12822/17270] Time:0.233, Train Loss:0.63202965259552\n",
      "Epoch 0[12823/17270] Time:0.229, Train Loss:0.6149386763572693\n",
      "Epoch 0[12824/17270] Time:0.236, Train Loss:0.5639966726303101\n",
      "Epoch 0[12825/17270] Time:0.236, Train Loss:0.48819997906684875\n",
      "Epoch 0[12826/17270] Time:0.236, Train Loss:0.6072812080383301\n",
      "Epoch 0[12827/17270] Time:0.239, Train Loss:0.6420304775238037\n",
      "Epoch 0[12828/17270] Time:0.23, Train Loss:0.6406627893447876\n",
      "Epoch 0[12829/17270] Time:0.231, Train Loss:0.6678164005279541\n",
      "Epoch 0[12830/17270] Time:0.229, Train Loss:0.4852313995361328\n",
      "Epoch 0[12831/17270] Time:0.235, Train Loss:1.2547905445098877\n",
      "Epoch 0[12832/17270] Time:0.239, Train Loss:0.5263429880142212\n",
      "Epoch 0[12833/17270] Time:0.229, Train Loss:0.7469990253448486\n",
      "Epoch 0[12834/17270] Time:0.238, Train Loss:0.8081566691398621\n",
      "Epoch 0[12835/17270] Time:0.239, Train Loss:0.41495800018310547\n",
      "Epoch 0[12836/17270] Time:0.23, Train Loss:1.1823219060897827\n",
      "Epoch 0[12837/17270] Time:0.24, Train Loss:0.6842949986457825\n",
      "Epoch 0[12838/17270] Time:0.245, Train Loss:0.36626681685447693\n",
      "Epoch 0[12839/17270] Time:0.235, Train Loss:1.0936847925186157\n",
      "Epoch 0[12840/17270] Time:0.224, Train Loss:0.629048764705658\n",
      "Epoch 0[12841/17270] Time:0.249, Train Loss:0.40779969096183777\n",
      "Epoch 0[12842/17270] Time:0.226, Train Loss:0.587217390537262\n",
      "Epoch 0[12843/17270] Time:0.238, Train Loss:0.6750769019126892\n",
      "Epoch 0[12844/17270] Time:0.249, Train Loss:0.6189365386962891\n",
      "Epoch 0[12845/17270] Time:0.236, Train Loss:0.5192170739173889\n",
      "Epoch 0[12846/17270] Time:0.238, Train Loss:0.7038475871086121\n",
      "Epoch 0[12847/17270] Time:0.234, Train Loss:0.5689741373062134\n",
      "Epoch 0[12848/17270] Time:0.246, Train Loss:0.5651667714118958\n",
      "Epoch 0[12849/17270] Time:0.233, Train Loss:0.5786699056625366\n",
      "Epoch 0[12850/17270] Time:0.224, Train Loss:0.5310655832290649\n",
      "Epoch 0[12851/17270] Time:0.224, Train Loss:0.4471796452999115\n",
      "Epoch 0[12852/17270] Time:0.232, Train Loss:0.7093505859375\n",
      "Epoch 0[12853/17270] Time:0.24, Train Loss:0.6027711033821106\n",
      "Epoch 0[12854/17270] Time:0.218, Train Loss:0.5258469581604004\n",
      "Epoch 0[12855/17270] Time:0.244, Train Loss:0.42347052693367004\n",
      "Epoch 0[12856/17270] Time:0.236, Train Loss:0.44025033712387085\n",
      "Epoch 0[12857/17270] Time:0.234, Train Loss:0.4069021940231323\n",
      "Epoch 0[12858/17270] Time:0.23, Train Loss:0.49189022183418274\n",
      "Epoch 0[12859/17270] Time:0.223, Train Loss:0.6703134775161743\n",
      "Epoch 0[12860/17270] Time:0.26, Train Loss:0.4403643310070038\n",
      "Epoch 0[12861/17270] Time:0.235, Train Loss:0.4713037610054016\n",
      "Epoch 0[12862/17270] Time:0.226, Train Loss:0.2974681258201599\n",
      "Epoch 0[12863/17270] Time:0.221, Train Loss:0.5800454616546631\n",
      "Epoch 0[12864/17270] Time:0.25, Train Loss:0.40432560443878174\n",
      "Epoch 0[12865/17270] Time:0.24, Train Loss:0.4120068848133087\n",
      "Epoch 0[12866/17270] Time:0.236, Train Loss:0.6994916200637817\n",
      "Epoch 0[12867/17270] Time:0.238, Train Loss:0.31788545846939087\n",
      "Epoch 0[12868/17270] Time:0.233, Train Loss:0.2936599850654602\n",
      "Epoch 0[12869/17270] Time:0.232, Train Loss:0.5992246270179749\n",
      "Epoch 0[12870/17270] Time:0.227, Train Loss:0.23625092208385468\n",
      "Epoch 0[12871/17270] Time:0.228, Train Loss:0.3973046839237213\n",
      "Epoch 0[12872/17270] Time:0.237, Train Loss:0.4380178451538086\n",
      "Epoch 0[12873/17270] Time:0.238, Train Loss:0.516731858253479\n",
      "Epoch 0[12874/17270] Time:0.229, Train Loss:0.3784935176372528\n",
      "Epoch 0[12875/17270] Time:0.237, Train Loss:0.5473931431770325\n",
      "Epoch 0[12876/17270] Time:0.239, Train Loss:0.5719417333602905\n",
      "Epoch 0[12877/17270] Time:0.23, Train Loss:0.6307480335235596\n",
      "Epoch 0[12878/17270] Time:0.229, Train Loss:1.1019887924194336\n",
      "Epoch 0[12879/17270] Time:0.234, Train Loss:0.4883032441139221\n",
      "Epoch 0[12880/17270] Time:0.229, Train Loss:0.4289383590221405\n",
      "Epoch 0[12881/17270] Time:0.251, Train Loss:0.5457016229629517\n",
      "Epoch 0[12882/17270] Time:0.231, Train Loss:0.5440862774848938\n",
      "Epoch 0[12883/17270] Time:0.249, Train Loss:0.48354828357696533\n",
      "Epoch 0[12884/17270] Time:0.238, Train Loss:0.3347587287425995\n",
      "Epoch 0[12885/17270] Time:0.241, Train Loss:0.6160359978675842\n",
      "Epoch 0[12886/17270] Time:0.241, Train Loss:0.5689957737922668\n",
      "Epoch 0[12887/17270] Time:0.241, Train Loss:0.43703973293304443\n",
      "Epoch 0[12888/17270] Time:0.235, Train Loss:0.3149959146976471\n",
      "Epoch 0[12889/17270] Time:0.252, Train Loss:0.5963031649589539\n",
      "Epoch 0[12890/17270] Time:0.238, Train Loss:0.3763570487499237\n",
      "Epoch 0[12891/17270] Time:0.225, Train Loss:0.3955845236778259\n",
      "Epoch 0[12892/17270] Time:0.233, Train Loss:0.618462324142456\n",
      "Epoch 0[12893/17270] Time:0.228, Train Loss:0.36202704906463623\n",
      "Epoch 0[12894/17270] Time:0.238, Train Loss:0.4641784727573395\n",
      "Epoch 0[12895/17270] Time:0.23, Train Loss:0.328942209482193\n",
      "Epoch 0[12896/17270] Time:0.237, Train Loss:0.4266369938850403\n",
      "Epoch 0[12897/17270] Time:0.239, Train Loss:1.5296763181686401\n",
      "Epoch 0[12898/17270] Time:0.23, Train Loss:0.38362011313438416\n",
      "Epoch 0[12899/17270] Time:0.237, Train Loss:0.47277286648750305\n",
      "Epoch 0[12900/17270] Time:0.235, Train Loss:0.4308426082134247\n",
      "Epoch 0[12901/17270] Time:0.238, Train Loss:0.4489905834197998\n",
      "Epoch 0[12902/17270] Time:0.233, Train Loss:0.4503798186779022\n",
      "Epoch 0[12903/17270] Time:0.236, Train Loss:0.6528644561767578\n",
      "Epoch 0[12904/17270] Time:0.237, Train Loss:0.39215579628944397\n",
      "Epoch 0[12905/17270] Time:0.24, Train Loss:0.8533305525779724\n",
      "Epoch 0[12906/17270] Time:0.24, Train Loss:0.3241664469242096\n",
      "Epoch 0[12907/17270] Time:0.235, Train Loss:0.5239620804786682\n",
      "Epoch 0[12908/17270] Time:0.236, Train Loss:0.30051782727241516\n",
      "Epoch 0[12909/17270] Time:0.253, Train Loss:1.0330647230148315\n",
      "Epoch 0[12910/17270] Time:0.238, Train Loss:0.6943756937980652\n",
      "Epoch 0[12911/17270] Time:0.237, Train Loss:0.4375888705253601\n",
      "Epoch 0[12912/17270] Time:0.231, Train Loss:0.47470295429229736\n",
      "Epoch 0[12913/17270] Time:0.25, Train Loss:0.5349915623664856\n",
      "Epoch 0[12914/17270] Time:0.235, Train Loss:0.3956888020038605\n",
      "Epoch 0[12915/17270] Time:0.243, Train Loss:0.5836693644523621\n",
      "Epoch 0[12916/17270] Time:0.237, Train Loss:0.39396223425865173\n",
      "Epoch 0[12917/17270] Time:0.233, Train Loss:0.45996129512786865\n",
      "Epoch 0[12918/17270] Time:0.227, Train Loss:0.27973297238349915\n",
      "Epoch 0[12919/17270] Time:0.239, Train Loss:0.5155988335609436\n",
      "Epoch 0[12920/17270] Time:0.233, Train Loss:0.33544307947158813\n",
      "Epoch 0[12921/17270] Time:0.242, Train Loss:0.2868155241012573\n",
      "Epoch 0[12922/17270] Time:0.223, Train Loss:0.858929455280304\n",
      "Epoch 0[12923/17270] Time:0.233, Train Loss:0.8719441294670105\n",
      "Epoch 0[12924/17270] Time:0.248, Train Loss:0.44539788365364075\n",
      "Epoch 0[12925/17270] Time:0.244, Train Loss:0.673438310623169\n",
      "Epoch 0[12926/17270] Time:0.243, Train Loss:0.40788716077804565\n",
      "Epoch 0[12927/17270] Time:0.242, Train Loss:0.5000568628311157\n",
      "Epoch 0[12928/17270] Time:0.238, Train Loss:0.6526302695274353\n",
      "Epoch 0[12929/17270] Time:0.237, Train Loss:0.2923415005207062\n",
      "Epoch 0[12930/17270] Time:0.233, Train Loss:1.0639463663101196\n",
      "Epoch 0[12931/17270] Time:0.235, Train Loss:0.394554078578949\n",
      "Epoch 0[12932/17270] Time:0.236, Train Loss:0.6418892741203308\n",
      "Epoch 0[12933/17270] Time:0.224, Train Loss:0.49030211567878723\n",
      "Epoch 0[12934/17270] Time:0.229, Train Loss:0.5989605784416199\n",
      "Epoch 0[12935/17270] Time:0.229, Train Loss:0.36572134494781494\n",
      "Epoch 0[12936/17270] Time:0.238, Train Loss:0.3374813497066498\n",
      "Epoch 0[12937/17270] Time:0.226, Train Loss:0.3845446705818176\n",
      "Epoch 0[12938/17270] Time:0.233, Train Loss:0.7194450497627258\n",
      "Epoch 0[12939/17270] Time:0.238, Train Loss:0.6004862189292908\n",
      "Epoch 0[12940/17270] Time:0.239, Train Loss:0.551486611366272\n",
      "Epoch 0[12941/17270] Time:0.237, Train Loss:0.29984965920448303\n",
      "Epoch 0[12942/17270] Time:0.23, Train Loss:0.31462594866752625\n",
      "Epoch 0[12943/17270] Time:0.236, Train Loss:2.1882941722869873\n",
      "Epoch 0[12944/17270] Time:0.24, Train Loss:1.553770661354065\n",
      "Epoch 0[12945/17270] Time:0.237, Train Loss:0.4141543209552765\n",
      "Epoch 0[12946/17270] Time:0.238, Train Loss:0.41041162610054016\n",
      "Epoch 0[12947/17270] Time:0.23, Train Loss:0.6173207759857178\n",
      "Epoch 0[12948/17270] Time:0.238, Train Loss:0.7227182984352112\n",
      "Epoch 0[12949/17270] Time:0.239, Train Loss:0.5297900438308716\n",
      "Epoch 0[12950/17270] Time:0.233, Train Loss:0.4695795774459839\n",
      "Epoch 0[12951/17270] Time:0.238, Train Loss:0.5128363966941833\n",
      "Epoch 0[12952/17270] Time:0.231, Train Loss:0.5042154788970947\n",
      "Epoch 0[12953/17270] Time:0.233, Train Loss:0.5062444806098938\n",
      "Epoch 0[12954/17270] Time:0.232, Train Loss:0.8871851563453674\n",
      "Epoch 0[12955/17270] Time:0.237, Train Loss:0.41826435923576355\n",
      "Epoch 0[12956/17270] Time:0.238, Train Loss:0.6689813733100891\n",
      "Epoch 0[12957/17270] Time:0.228, Train Loss:0.39585748314857483\n",
      "Epoch 0[12958/17270] Time:0.235, Train Loss:0.498526930809021\n",
      "Epoch 0[12959/17270] Time:0.239, Train Loss:0.5434213876724243\n",
      "Epoch 0[12960/17270] Time:0.236, Train Loss:1.5038330554962158\n",
      "Epoch 0[12961/17270] Time:0.245, Train Loss:0.41016459465026855\n",
      "Epoch 0[12962/17270] Time:0.234, Train Loss:0.72715824842453\n",
      "Epoch 0[12963/17270] Time:0.238, Train Loss:0.7554460167884827\n",
      "Epoch 0[12964/17270] Time:0.245, Train Loss:0.48076266050338745\n",
      "Epoch 0[12965/17270] Time:0.236, Train Loss:0.4794386625289917\n",
      "Epoch 0[12966/17270] Time:0.229, Train Loss:0.5776057243347168\n",
      "Epoch 0[12967/17270] Time:0.225, Train Loss:0.5686822533607483\n",
      "Epoch 0[12968/17270] Time:0.232, Train Loss:0.44748300313949585\n",
      "Epoch 0[12969/17270] Time:0.238, Train Loss:0.45039790868759155\n",
      "Epoch 0[12970/17270] Time:0.229, Train Loss:0.39811888337135315\n",
      "Epoch 0[12971/17270] Time:0.231, Train Loss:0.6905686855316162\n",
      "Epoch 0[12972/17270] Time:0.23, Train Loss:0.5725201964378357\n",
      "Epoch 0[12973/17270] Time:0.231, Train Loss:0.6195603609085083\n",
      "Epoch 0[12974/17270] Time:0.233, Train Loss:0.3727123737335205\n",
      "Epoch 0[12975/17270] Time:0.228, Train Loss:0.6462546586990356\n",
      "Epoch 0[12976/17270] Time:0.234, Train Loss:0.6746906638145447\n",
      "Epoch 0[12977/17270] Time:0.236, Train Loss:0.3348882794380188\n",
      "Epoch 0[12978/17270] Time:0.244, Train Loss:0.4253198504447937\n",
      "Epoch 0[12979/17270] Time:0.245, Train Loss:0.7066841125488281\n",
      "Epoch 0[12980/17270] Time:0.235, Train Loss:0.3557387888431549\n",
      "Epoch 0[12981/17270] Time:0.23, Train Loss:0.6234740018844604\n",
      "Epoch 0[12982/17270] Time:0.244, Train Loss:0.43817582726478577\n",
      "Epoch 0[12983/17270] Time:0.245, Train Loss:0.515614926815033\n",
      "Epoch 0[12984/17270] Time:0.234, Train Loss:0.472393274307251\n",
      "Epoch 0[12985/17270] Time:0.227, Train Loss:0.38759416341781616\n",
      "Epoch 0[12986/17270] Time:0.233, Train Loss:0.4295768141746521\n",
      "Epoch 0[12987/17270] Time:0.232, Train Loss:0.6603013277053833\n",
      "Epoch 0[12988/17270] Time:0.232, Train Loss:0.457554429769516\n",
      "Epoch 0[12989/17270] Time:0.238, Train Loss:0.3027387261390686\n",
      "Epoch 0[12990/17270] Time:0.225, Train Loss:0.39067238569259644\n",
      "Epoch 0[12991/17270] Time:0.239, Train Loss:0.8551288843154907\n",
      "Epoch 0[12992/17270] Time:0.229, Train Loss:0.5677372813224792\n",
      "Epoch 0[12993/17270] Time:0.227, Train Loss:0.9974105358123779\n",
      "Epoch 0[12994/17270] Time:0.231, Train Loss:1.2753077745437622\n",
      "Epoch 0[12995/17270] Time:0.229, Train Loss:1.0289196968078613\n",
      "Epoch 0[12996/17270] Time:0.227, Train Loss:0.3523341715335846\n",
      "Epoch 0[12997/17270] Time:0.238, Train Loss:0.44827795028686523\n",
      "Epoch 0[12998/17270] Time:0.231, Train Loss:0.4333433210849762\n",
      "Epoch 0[12999/17270] Time:0.236, Train Loss:0.41796404123306274\n",
      "Epoch 0[13000/17270] Time:0.232, Train Loss:0.304745614528656\n",
      "Epoch 0[13001/17270] Time:0.231, Train Loss:0.6645742654800415\n",
      "Epoch 0[13002/17270] Time:0.231, Train Loss:1.0790438652038574\n",
      "Epoch 0[13003/17270] Time:0.229, Train Loss:0.41316404938697815\n",
      "Epoch 0[13004/17270] Time:0.234, Train Loss:0.4206204116344452\n",
      "Epoch 0[13005/17270] Time:0.228, Train Loss:0.5285215973854065\n",
      "Epoch 0[13006/17270] Time:0.237, Train Loss:0.9535643458366394\n",
      "Epoch 0[13007/17270] Time:0.235, Train Loss:0.9920139312744141\n",
      "Epoch 0[13008/17270] Time:0.249, Train Loss:0.8010081052780151\n",
      "Epoch 0[13009/17270] Time:0.23, Train Loss:0.518681526184082\n",
      "Epoch 0[13010/17270] Time:0.236, Train Loss:0.36445707082748413\n",
      "Epoch 0[13011/17270] Time:0.228, Train Loss:1.008787989616394\n",
      "Epoch 0[13012/17270] Time:0.238, Train Loss:0.3838137090206146\n",
      "Epoch 0[13013/17270] Time:0.23, Train Loss:0.5820896029472351\n",
      "Epoch 0[13014/17270] Time:0.238, Train Loss:0.4741745591163635\n",
      "Epoch 0[13015/17270] Time:0.237, Train Loss:0.3592357933521271\n",
      "Epoch 0[13016/17270] Time:0.231, Train Loss:0.30070120096206665\n",
      "Epoch 0[13017/17270] Time:0.235, Train Loss:1.0084290504455566\n",
      "Epoch 0[13018/17270] Time:0.23, Train Loss:0.7072784304618835\n",
      "Epoch 0[13019/17270] Time:0.242, Train Loss:0.7487179636955261\n",
      "Epoch 0[13020/17270] Time:0.231, Train Loss:0.4128939211368561\n",
      "Epoch 0[13021/17270] Time:0.23, Train Loss:0.6637424826622009\n",
      "Epoch 0[13022/17270] Time:0.231, Train Loss:0.33380794525146484\n",
      "Epoch 0[13023/17270] Time:0.231, Train Loss:0.6796166300773621\n",
      "Epoch 0[13024/17270] Time:0.232, Train Loss:0.5684257745742798\n",
      "Epoch 0[13025/17270] Time:0.23, Train Loss:0.49207690358161926\n",
      "Epoch 0[13026/17270] Time:0.236, Train Loss:0.24777749180793762\n",
      "Epoch 0[13027/17270] Time:0.233, Train Loss:0.7260212302207947\n",
      "Epoch 0[13028/17270] Time:0.228, Train Loss:0.4442301392555237\n",
      "Epoch 0[13029/17270] Time:0.236, Train Loss:0.7048691511154175\n",
      "Epoch 0[13030/17270] Time:0.238, Train Loss:0.7046247720718384\n",
      "Epoch 0[13031/17270] Time:0.244, Train Loss:1.0688133239746094\n",
      "Epoch 0[13032/17270] Time:0.226, Train Loss:0.5931457877159119\n",
      "Epoch 0[13033/17270] Time:0.228, Train Loss:0.5034026503562927\n",
      "Epoch 0[13034/17270] Time:0.228, Train Loss:1.1396375894546509\n",
      "Epoch 0[13035/17270] Time:0.236, Train Loss:0.7448793649673462\n",
      "Epoch 0[13036/17270] Time:0.238, Train Loss:0.48547840118408203\n",
      "Epoch 0[13037/17270] Time:0.234, Train Loss:0.4537011981010437\n",
      "Epoch 0[13038/17270] Time:0.238, Train Loss:0.976826012134552\n",
      "Epoch 0[13039/17270] Time:0.23, Train Loss:0.7735174894332886\n",
      "Epoch 0[13040/17270] Time:0.231, Train Loss:0.7840543389320374\n",
      "Epoch 0[13041/17270] Time:0.232, Train Loss:0.5538327693939209\n",
      "Epoch 0[13042/17270] Time:0.25, Train Loss:0.7095737457275391\n",
      "Epoch 0[13043/17270] Time:0.233, Train Loss:0.7618300914764404\n",
      "Epoch 0[13044/17270] Time:0.226, Train Loss:0.4531877338886261\n",
      "Epoch 0[13045/17270] Time:0.228, Train Loss:0.49420928955078125\n",
      "Epoch 0[13046/17270] Time:0.226, Train Loss:1.0453277826309204\n",
      "Epoch 0[13047/17270] Time:0.249, Train Loss:0.7971749305725098\n",
      "Epoch 0[13048/17270] Time:0.245, Train Loss:0.943224310874939\n",
      "Epoch 0[13049/17270] Time:0.251, Train Loss:0.4105228781700134\n",
      "Epoch 0[13050/17270] Time:0.234, Train Loss:0.6206203103065491\n",
      "Epoch 0[13051/17270] Time:0.228, Train Loss:0.3830420672893524\n",
      "Epoch 0[13052/17270] Time:0.238, Train Loss:0.35700681805610657\n",
      "Epoch 0[13053/17270] Time:0.252, Train Loss:0.4911840856075287\n",
      "Epoch 0[13054/17270] Time:0.25, Train Loss:0.5215456485748291\n",
      "Epoch 0[13055/17270] Time:0.244, Train Loss:0.5462321639060974\n",
      "Epoch 0[13056/17270] Time:0.237, Train Loss:0.8864319324493408\n",
      "Epoch 0[13057/17270] Time:0.231, Train Loss:0.5489873886108398\n",
      "Epoch 0[13058/17270] Time:0.237, Train Loss:0.4215525686740875\n",
      "Epoch 0[13059/17270] Time:0.238, Train Loss:0.5988827347755432\n",
      "Epoch 0[13060/17270] Time:0.254, Train Loss:0.696193277835846\n",
      "Epoch 0[13061/17270] Time:0.237, Train Loss:0.5495640635490417\n",
      "Epoch 0[13062/17270] Time:0.229, Train Loss:0.3914719820022583\n",
      "Epoch 0[13063/17270] Time:0.228, Train Loss:0.3577061593532562\n",
      "Epoch 0[13064/17270] Time:0.252, Train Loss:0.9764198064804077\n",
      "Epoch 0[13065/17270] Time:0.231, Train Loss:0.699691116809845\n",
      "Epoch 0[13066/17270] Time:0.232, Train Loss:0.549111545085907\n",
      "Epoch 0[13067/17270] Time:0.234, Train Loss:1.032220482826233\n",
      "Epoch 0[13068/17270] Time:0.238, Train Loss:0.40391218662261963\n",
      "Epoch 0[13069/17270] Time:0.233, Train Loss:0.6212277412414551\n",
      "Epoch 0[13070/17270] Time:0.231, Train Loss:0.42981377243995667\n",
      "Epoch 0[13071/17270] Time:0.245, Train Loss:0.44776836037635803\n",
      "Epoch 0[13072/17270] Time:0.235, Train Loss:0.4744572043418884\n",
      "Epoch 0[13073/17270] Time:0.234, Train Loss:0.25737470388412476\n",
      "Epoch 0[13074/17270] Time:0.235, Train Loss:0.8276050090789795\n",
      "Epoch 0[13075/17270] Time:0.231, Train Loss:0.4326906204223633\n",
      "Epoch 0[13076/17270] Time:0.226, Train Loss:0.7992134094238281\n",
      "Epoch 0[13077/17270] Time:0.229, Train Loss:0.7075492143630981\n",
      "Epoch 0[13078/17270] Time:0.229, Train Loss:0.671227216720581\n",
      "Epoch 0[13079/17270] Time:0.237, Train Loss:0.6009434461593628\n",
      "Epoch 0[13080/17270] Time:0.23, Train Loss:0.697974443435669\n",
      "Epoch 0[13081/17270] Time:0.236, Train Loss:0.4275490343570709\n",
      "Epoch 0[13082/17270] Time:0.235, Train Loss:0.7362992167472839\n",
      "Epoch 0[13083/17270] Time:0.237, Train Loss:0.5710825324058533\n",
      "Epoch 0[13084/17270] Time:0.236, Train Loss:0.5307548642158508\n",
      "Epoch 0[13085/17270] Time:0.234, Train Loss:0.7673720121383667\n",
      "Epoch 0[13086/17270] Time:0.235, Train Loss:0.520179271697998\n",
      "Epoch 0[13087/17270] Time:0.237, Train Loss:0.6834027767181396\n",
      "Epoch 0[13088/17270] Time:0.232, Train Loss:0.8557226657867432\n",
      "Epoch 0[13089/17270] Time:0.23, Train Loss:0.6657114624977112\n",
      "Epoch 0[13090/17270] Time:0.232, Train Loss:0.4381117522716522\n",
      "Epoch 0[13091/17270] Time:0.236, Train Loss:0.7960547208786011\n",
      "Epoch 0[13092/17270] Time:0.234, Train Loss:0.6143097281455994\n",
      "Epoch 0[13093/17270] Time:0.235, Train Loss:0.4451383054256439\n",
      "Epoch 0[13094/17270] Time:0.236, Train Loss:0.3445126414299011\n",
      "Epoch 0[13095/17270] Time:0.231, Train Loss:0.47419795393943787\n",
      "Epoch 0[13096/17270] Time:0.235, Train Loss:0.353019118309021\n",
      "Epoch 0[13097/17270] Time:0.237, Train Loss:0.445105642080307\n",
      "Epoch 0[13098/17270] Time:0.234, Train Loss:0.6011901497840881\n",
      "Epoch 0[13099/17270] Time:0.234, Train Loss:0.6819707751274109\n",
      "Epoch 0[13100/17270] Time:0.235, Train Loss:0.7375149130821228\n",
      "Epoch 0[13101/17270] Time:0.23, Train Loss:0.4171850383281708\n",
      "Epoch 0[13102/17270] Time:0.235, Train Loss:0.5565974116325378\n",
      "Epoch 0[13103/17270] Time:0.228, Train Loss:0.5865611433982849\n",
      "Epoch 0[13104/17270] Time:0.235, Train Loss:0.41567522287368774\n",
      "Epoch 0[13105/17270] Time:0.221, Train Loss:0.5614522695541382\n",
      "Epoch 0[13106/17270] Time:0.243, Train Loss:0.35426709055900574\n",
      "Epoch 0[13107/17270] Time:0.229, Train Loss:0.5709038972854614\n",
      "Epoch 0[13108/17270] Time:0.234, Train Loss:0.4430401921272278\n",
      "Epoch 0[13109/17270] Time:0.223, Train Loss:0.32205140590667725\n",
      "Epoch 0[13110/17270] Time:0.242, Train Loss:0.6571898460388184\n",
      "Epoch 0[13111/17270] Time:0.234, Train Loss:0.7343592643737793\n",
      "Epoch 0[13112/17270] Time:0.233, Train Loss:0.3304201066493988\n",
      "Epoch 0[13113/17270] Time:0.243, Train Loss:0.5958003401756287\n",
      "Epoch 0[13114/17270] Time:0.246, Train Loss:0.2827829420566559\n",
      "Epoch 0[13115/17270] Time:0.233, Train Loss:0.5619388222694397\n",
      "Epoch 0[13116/17270] Time:0.233, Train Loss:0.289715975522995\n",
      "Epoch 0[13117/17270] Time:0.232, Train Loss:0.653119683265686\n",
      "Epoch 0[13118/17270] Time:0.244, Train Loss:0.6879602074623108\n",
      "Epoch 0[13119/17270] Time:0.222, Train Loss:0.5372708439826965\n",
      "Epoch 0[13120/17270] Time:0.241, Train Loss:0.531639814376831\n",
      "Epoch 0[13121/17270] Time:0.23, Train Loss:1.6878949403762817\n",
      "Epoch 0[13122/17270] Time:0.236, Train Loss:0.3976682722568512\n",
      "Epoch 0[13123/17270] Time:0.237, Train Loss:0.32179978489875793\n",
      "Epoch 0[13124/17270] Time:0.235, Train Loss:0.29316169023513794\n",
      "Epoch 0[13125/17270] Time:0.238, Train Loss:0.36262497305870056\n",
      "Epoch 0[13126/17270] Time:0.22, Train Loss:0.7029321789741516\n",
      "Epoch 0[13127/17270] Time:0.234, Train Loss:0.43355658650398254\n",
      "Epoch 0[13128/17270] Time:0.233, Train Loss:0.5442706346511841\n",
      "Epoch 0[13129/17270] Time:0.223, Train Loss:0.7266674041748047\n",
      "Epoch 0[13130/17270] Time:0.234, Train Loss:0.6857457160949707\n",
      "Epoch 0[13131/17270] Time:0.251, Train Loss:0.6764980554580688\n",
      "Epoch 0[13132/17270] Time:0.233, Train Loss:1.0846009254455566\n",
      "Epoch 0[13133/17270] Time:0.231, Train Loss:0.2667621970176697\n",
      "Epoch 0[13134/17270] Time:0.24, Train Loss:1.5818365812301636\n",
      "Epoch 0[13135/17270] Time:0.235, Train Loss:0.52458256483078\n",
      "Epoch 0[13136/17270] Time:0.244, Train Loss:0.8907545804977417\n",
      "Epoch 0[13137/17270] Time:0.232, Train Loss:0.45789673924446106\n",
      "Epoch 0[13138/17270] Time:0.242, Train Loss:0.22403062880039215\n",
      "Epoch 0[13139/17270] Time:0.234, Train Loss:0.5067906975746155\n",
      "Epoch 0[13140/17270] Time:0.242, Train Loss:0.3336785137653351\n",
      "Epoch 0[13141/17270] Time:0.24, Train Loss:0.30671021342277527\n",
      "Epoch 0[13142/17270] Time:0.231, Train Loss:0.40507078170776367\n",
      "Epoch 0[13143/17270] Time:0.225, Train Loss:0.36129599809646606\n",
      "Epoch 0[13144/17270] Time:0.241, Train Loss:0.6768502593040466\n",
      "Epoch 0[13145/17270] Time:0.24, Train Loss:0.8918970227241516\n",
      "Epoch 0[13146/17270] Time:0.234, Train Loss:0.846878170967102\n",
      "Epoch 0[13147/17270] Time:0.233, Train Loss:0.27046120166778564\n",
      "Epoch 0[13148/17270] Time:0.229, Train Loss:0.4805544912815094\n",
      "Epoch 0[13149/17270] Time:0.232, Train Loss:0.544806182384491\n",
      "Epoch 0[13150/17270] Time:0.236, Train Loss:0.670744001865387\n",
      "Epoch 0[13151/17270] Time:0.233, Train Loss:0.6846810579299927\n",
      "Epoch 0[13152/17270] Time:0.237, Train Loss:0.5401865839958191\n",
      "Epoch 0[13153/17270] Time:0.229, Train Loss:0.8025345206260681\n",
      "Epoch 0[13154/17270] Time:0.245, Train Loss:0.5776974558830261\n",
      "Epoch 0[13155/17270] Time:0.231, Train Loss:0.6785091757774353\n",
      "Epoch 0[13156/17270] Time:0.236, Train Loss:0.4490075707435608\n",
      "Epoch 0[13157/17270] Time:0.229, Train Loss:0.3989720046520233\n",
      "Epoch 0[13158/17270] Time:0.236, Train Loss:0.5449022054672241\n",
      "Epoch 0[13159/17270] Time:0.229, Train Loss:0.4451024532318115\n",
      "Epoch 0[13160/17270] Time:0.238, Train Loss:0.4689021110534668\n",
      "Epoch 0[13161/17270] Time:0.23, Train Loss:0.5515822172164917\n",
      "Epoch 0[13162/17270] Time:0.233, Train Loss:1.3647133111953735\n",
      "Epoch 0[13163/17270] Time:0.231, Train Loss:0.42771244049072266\n",
      "Epoch 0[13164/17270] Time:0.23, Train Loss:0.7042413949966431\n",
      "Epoch 0[13165/17270] Time:0.23, Train Loss:0.6686780452728271\n",
      "Epoch 0[13166/17270] Time:0.241, Train Loss:0.8975518345832825\n",
      "Epoch 0[13167/17270] Time:0.235, Train Loss:0.8059582710266113\n",
      "Epoch 0[13168/17270] Time:0.237, Train Loss:0.2800784707069397\n",
      "Epoch 0[13169/17270] Time:0.225, Train Loss:0.614850640296936\n",
      "Epoch 0[13170/17270] Time:0.236, Train Loss:0.5611201524734497\n",
      "Epoch 0[13171/17270] Time:0.231, Train Loss:0.4621036648750305\n",
      "Epoch 0[13172/17270] Time:0.237, Train Loss:0.36297962069511414\n",
      "Epoch 0[13173/17270] Time:0.226, Train Loss:0.3561953604221344\n",
      "Epoch 0[13174/17270] Time:0.229, Train Loss:0.4106486737728119\n",
      "Epoch 0[13175/17270] Time:0.239, Train Loss:0.4817829430103302\n",
      "Epoch 0[13176/17270] Time:0.234, Train Loss:0.5234391093254089\n",
      "Epoch 0[13177/17270] Time:0.234, Train Loss:0.45811426639556885\n",
      "Epoch 0[13178/17270] Time:0.224, Train Loss:1.18832266330719\n",
      "Epoch 0[13179/17270] Time:0.236, Train Loss:0.9136788845062256\n",
      "Epoch 0[13180/17270] Time:0.236, Train Loss:0.4228343963623047\n",
      "Epoch 0[13181/17270] Time:0.225, Train Loss:0.6081019639968872\n",
      "Epoch 0[13182/17270] Time:0.233, Train Loss:0.6770046949386597\n",
      "Epoch 0[13183/17270] Time:0.237, Train Loss:0.830059289932251\n",
      "Epoch 0[13184/17270] Time:0.248, Train Loss:0.26004472374916077\n",
      "Epoch 0[13185/17270] Time:0.238, Train Loss:0.3072604835033417\n",
      "Epoch 0[13186/17270] Time:0.224, Train Loss:0.6990726590156555\n",
      "Epoch 0[13187/17270] Time:0.247, Train Loss:0.5513014197349548\n",
      "Epoch 0[13188/17270] Time:0.239, Train Loss:0.9866927862167358\n",
      "Epoch 0[13189/17270] Time:0.225, Train Loss:1.038714051246643\n",
      "Epoch 0[13190/17270] Time:0.238, Train Loss:0.4746798872947693\n",
      "Epoch 0[13191/17270] Time:0.236, Train Loss:0.5608804821968079\n",
      "Epoch 0[13192/17270] Time:0.234, Train Loss:0.5964271426200867\n",
      "Epoch 0[13193/17270] Time:0.238, Train Loss:0.5674300789833069\n",
      "Epoch 0[13194/17270] Time:0.227, Train Loss:0.7091158628463745\n",
      "Epoch 0[13195/17270] Time:0.245, Train Loss:0.3436231017112732\n",
      "Epoch 0[13196/17270] Time:0.243, Train Loss:0.27046188712120056\n",
      "Epoch 0[13197/17270] Time:0.233, Train Loss:0.5681853294372559\n",
      "Epoch 0[13198/17270] Time:0.233, Train Loss:0.28518354892730713\n",
      "Epoch 0[13199/17270] Time:0.233, Train Loss:0.5883920192718506\n",
      "Epoch 0[13200/17270] Time:0.225, Train Loss:0.577038049697876\n",
      "Epoch 0[13201/17270] Time:0.24, Train Loss:0.7972536087036133\n",
      "Epoch 0[13202/17270] Time:0.237, Train Loss:0.4692329466342926\n",
      "Epoch 0[13203/17270] Time:0.225, Train Loss:0.30078497529029846\n",
      "Epoch 0[13204/17270] Time:0.241, Train Loss:0.6183706521987915\n",
      "Epoch 0[13205/17270] Time:0.233, Train Loss:0.6228273510932922\n",
      "Epoch 0[13206/17270] Time:0.225, Train Loss:0.47974473237991333\n",
      "Epoch 0[13207/17270] Time:0.251, Train Loss:0.575858473777771\n",
      "Epoch 0[13208/17270] Time:0.234, Train Loss:0.6826486587524414\n",
      "Epoch 0[13209/17270] Time:0.237, Train Loss:0.5102921724319458\n",
      "Epoch 0[13210/17270] Time:0.227, Train Loss:0.5483436584472656\n",
      "Epoch 0[13211/17270] Time:0.231, Train Loss:0.36851954460144043\n",
      "Epoch 0[13212/17270] Time:0.246, Train Loss:0.7530584931373596\n",
      "Epoch 0[13213/17270] Time:0.239, Train Loss:0.4920560121536255\n",
      "Epoch 0[13214/17270] Time:0.227, Train Loss:0.48530086874961853\n",
      "Epoch 0[13215/17270] Time:0.237, Train Loss:1.3653712272644043\n",
      "Epoch 0[13216/17270] Time:0.231, Train Loss:0.5017069578170776\n",
      "Epoch 0[13217/17270] Time:0.231, Train Loss:0.4294205904006958\n",
      "Epoch 0[13218/17270] Time:0.227, Train Loss:0.31706830859184265\n",
      "Epoch 0[13219/17270] Time:0.238, Train Loss:0.3649444878101349\n",
      "Epoch 0[13220/17270] Time:0.228, Train Loss:0.8012233972549438\n",
      "Epoch 0[13221/17270] Time:0.236, Train Loss:0.5114914178848267\n",
      "Epoch 0[13222/17270] Time:0.239, Train Loss:0.9764490723609924\n",
      "Epoch 0[13223/17270] Time:0.227, Train Loss:0.5053566694259644\n",
      "Epoch 0[13224/17270] Time:0.238, Train Loss:0.4540088176727295\n",
      "Epoch 0[13225/17270] Time:0.229, Train Loss:1.472211480140686\n",
      "Epoch 0[13226/17270] Time:0.238, Train Loss:0.6723371744155884\n",
      "Epoch 0[13227/17270] Time:0.238, Train Loss:0.4802095592021942\n",
      "Epoch 0[13228/17270] Time:0.23, Train Loss:0.31662046909332275\n",
      "Epoch 0[13229/17270] Time:0.24, Train Loss:0.5422950983047485\n",
      "Epoch 0[13230/17270] Time:0.24, Train Loss:0.9173944592475891\n",
      "Epoch 0[13231/17270] Time:0.23, Train Loss:0.5877887606620789\n",
      "Epoch 0[13232/17270] Time:0.231, Train Loss:0.7966567277908325\n",
      "Epoch 0[13233/17270] Time:0.236, Train Loss:0.28994062542915344\n",
      "Epoch 0[13234/17270] Time:0.23, Train Loss:0.5346558094024658\n",
      "Epoch 0[13235/17270] Time:0.229, Train Loss:0.42541128396987915\n",
      "Epoch 0[13236/17270] Time:0.237, Train Loss:0.4981883764266968\n",
      "Epoch 0[13237/17270] Time:0.237, Train Loss:0.40088412165641785\n",
      "Epoch 0[13238/17270] Time:0.225, Train Loss:0.6020243763923645\n",
      "Epoch 0[13239/17270] Time:0.237, Train Loss:1.1397525072097778\n",
      "Epoch 0[13240/17270] Time:0.229, Train Loss:0.3539723753929138\n",
      "Epoch 0[13241/17270] Time:0.245, Train Loss:0.5358147025108337\n",
      "Epoch 0[13242/17270] Time:0.234, Train Loss:1.0096509456634521\n",
      "Epoch 0[13243/17270] Time:0.238, Train Loss:0.5538927316665649\n",
      "Epoch 0[13244/17270] Time:0.24, Train Loss:0.4051685333251953\n",
      "Epoch 0[13245/17270] Time:0.222, Train Loss:0.8445602655410767\n",
      "Epoch 0[13246/17270] Time:0.228, Train Loss:0.5148658156394958\n",
      "Epoch 0[13247/17270] Time:0.244, Train Loss:0.38683024048805237\n",
      "Epoch 0[13248/17270] Time:0.231, Train Loss:0.3341502249240875\n",
      "Epoch 0[13249/17270] Time:0.234, Train Loss:0.6253872513771057\n",
      "Epoch 0[13250/17270] Time:0.233, Train Loss:0.3162061274051666\n",
      "Epoch 0[13251/17270] Time:0.231, Train Loss:0.5745307803153992\n",
      "Epoch 0[13252/17270] Time:0.235, Train Loss:0.6533638834953308\n",
      "Epoch 0[13253/17270] Time:0.236, Train Loss:0.4759674370288849\n",
      "Epoch 0[13254/17270] Time:0.237, Train Loss:0.49860766530036926\n",
      "Epoch 0[13255/17270] Time:0.236, Train Loss:0.46116533875465393\n",
      "Epoch 0[13256/17270] Time:0.237, Train Loss:0.5069689750671387\n",
      "Epoch 0[13257/17270] Time:0.236, Train Loss:0.471164345741272\n",
      "Epoch 0[13258/17270] Time:0.23, Train Loss:1.1210260391235352\n",
      "Epoch 0[13259/17270] Time:0.236, Train Loss:0.3770830035209656\n",
      "Epoch 0[13260/17270] Time:0.238, Train Loss:0.34072163701057434\n",
      "Epoch 0[13261/17270] Time:0.237, Train Loss:0.4213404655456543\n",
      "Epoch 0[13262/17270] Time:0.236, Train Loss:0.36025574803352356\n",
      "Epoch 0[13263/17270] Time:0.231, Train Loss:0.25486335158348083\n",
      "Epoch 0[13264/17270] Time:0.235, Train Loss:0.528383731842041\n",
      "Epoch 0[13265/17270] Time:0.236, Train Loss:0.4436611831188202\n",
      "Epoch 0[13266/17270] Time:0.229, Train Loss:0.34524038434028625\n",
      "Epoch 0[13267/17270] Time:0.237, Train Loss:1.1284586191177368\n",
      "Epoch 0[13268/17270] Time:0.236, Train Loss:0.5178638696670532\n",
      "Epoch 0[13269/17270] Time:0.25, Train Loss:0.36168763041496277\n",
      "Epoch 0[13270/17270] Time:0.223, Train Loss:0.5206261873245239\n",
      "Epoch 0[13271/17270] Time:0.247, Train Loss:0.36957433819770813\n",
      "Epoch 0[13272/17270] Time:0.238, Train Loss:0.4976886510848999\n",
      "Epoch 0[13273/17270] Time:0.237, Train Loss:0.3981272876262665\n",
      "Epoch 0[13274/17270] Time:0.234, Train Loss:0.7795641422271729\n",
      "Epoch 0[13275/17270] Time:0.234, Train Loss:0.26283031702041626\n",
      "Epoch 0[13276/17270] Time:0.243, Train Loss:0.7081626653671265\n",
      "Epoch 0[13277/17270] Time:0.234, Train Loss:0.5634741187095642\n",
      "Epoch 0[13278/17270] Time:0.243, Train Loss:0.7429322004318237\n",
      "Epoch 0[13279/17270] Time:0.237, Train Loss:0.5148081183433533\n",
      "Epoch 0[13280/17270] Time:0.235, Train Loss:0.49053773283958435\n",
      "Epoch 0[13281/17270] Time:0.233, Train Loss:0.4256240427494049\n",
      "Epoch 0[13282/17270] Time:0.233, Train Loss:0.47568583488464355\n",
      "Epoch 0[13283/17270] Time:0.247, Train Loss:0.22786661982536316\n",
      "Epoch 0[13284/17270] Time:0.234, Train Loss:0.41451600193977356\n",
      "Epoch 0[13285/17270] Time:0.245, Train Loss:0.2966093122959137\n",
      "Epoch 0[13286/17270] Time:0.236, Train Loss:0.5279079079627991\n",
      "Epoch 0[13287/17270] Time:0.221, Train Loss:0.8844495415687561\n",
      "Epoch 0[13288/17270] Time:0.263, Train Loss:0.44603654742240906\n",
      "Epoch 0[13289/17270] Time:0.238, Train Loss:0.35179367661476135\n",
      "Epoch 0[13290/17270] Time:0.237, Train Loss:0.5500875115394592\n",
      "Epoch 0[13291/17270] Time:0.232, Train Loss:0.555997908115387\n",
      "Epoch 0[13292/17270] Time:0.239, Train Loss:0.5092812180519104\n",
      "Epoch 0[13293/17270] Time:0.247, Train Loss:0.3938114643096924\n",
      "Epoch 0[13294/17270] Time:0.222, Train Loss:0.5177568197250366\n",
      "Epoch 0[13295/17270] Time:0.229, Train Loss:0.45119643211364746\n",
      "Epoch 0[13296/17270] Time:0.23, Train Loss:0.409538596868515\n",
      "Epoch 0[13297/17270] Time:0.233, Train Loss:0.6406875848770142\n",
      "Epoch 0[13298/17270] Time:0.241, Train Loss:0.3751949965953827\n",
      "Epoch 0[13299/17270] Time:0.246, Train Loss:0.35916438698768616\n",
      "Epoch 0[13300/17270] Time:0.235, Train Loss:0.6742599606513977\n",
      "Epoch 0[13301/17270] Time:0.254, Train Loss:0.7918873429298401\n",
      "Epoch 0[13302/17270] Time:0.239, Train Loss:0.32685884833335876\n",
      "Epoch 0[13303/17270] Time:0.242, Train Loss:0.6596347689628601\n",
      "Epoch 0[13304/17270] Time:0.236, Train Loss:0.639695942401886\n",
      "Epoch 0[13305/17270] Time:0.225, Train Loss:0.49757465720176697\n",
      "Epoch 0[13306/17270] Time:0.225, Train Loss:0.7882596850395203\n",
      "Epoch 0[13307/17270] Time:0.23, Train Loss:0.4666746258735657\n",
      "Epoch 0[13308/17270] Time:0.233, Train Loss:0.33072951436042786\n",
      "Epoch 0[13309/17270] Time:0.235, Train Loss:0.3840281367301941\n",
      "Epoch 0[13310/17270] Time:0.238, Train Loss:0.3679867088794708\n",
      "Epoch 0[13311/17270] Time:0.237, Train Loss:0.4042378067970276\n",
      "Epoch 0[13312/17270] Time:0.226, Train Loss:0.4033088684082031\n",
      "Epoch 0[13313/17270] Time:0.227, Train Loss:0.5434070825576782\n",
      "Epoch 0[13314/17270] Time:0.231, Train Loss:1.2155981063842773\n",
      "Epoch 0[13315/17270] Time:0.237, Train Loss:0.4155966639518738\n",
      "Epoch 0[13316/17270] Time:0.248, Train Loss:0.568835973739624\n",
      "Epoch 0[13317/17270] Time:0.239, Train Loss:0.37751224637031555\n",
      "Epoch 0[13318/17270] Time:0.239, Train Loss:0.36239853501319885\n",
      "Epoch 0[13319/17270] Time:0.236, Train Loss:0.5771371722221375\n",
      "Epoch 0[13320/17270] Time:0.226, Train Loss:0.4930218756198883\n",
      "Epoch 0[13321/17270] Time:0.227, Train Loss:0.46106815338134766\n",
      "Epoch 0[13322/17270] Time:0.233, Train Loss:0.7114529609680176\n",
      "Epoch 0[13323/17270] Time:0.233, Train Loss:0.2988334596157074\n",
      "Epoch 0[13324/17270] Time:0.238, Train Loss:1.1664369106292725\n",
      "Epoch 0[13325/17270] Time:0.227, Train Loss:1.1862927675247192\n",
      "Epoch 0[13326/17270] Time:0.224, Train Loss:0.43773284554481506\n",
      "Epoch 0[13327/17270] Time:0.236, Train Loss:0.5611506104469299\n",
      "Epoch 0[13328/17270] Time:0.236, Train Loss:0.5223355889320374\n",
      "Epoch 0[13329/17270] Time:0.227, Train Loss:0.3897859454154968\n",
      "Epoch 0[13330/17270] Time:0.242, Train Loss:0.5953253507614136\n",
      "Epoch 0[13331/17270] Time:0.227, Train Loss:0.24427326023578644\n",
      "Epoch 0[13332/17270] Time:0.248, Train Loss:0.43306800723075867\n",
      "Epoch 0[13333/17270] Time:0.23, Train Loss:0.9023469090461731\n",
      "Epoch 0[13334/17270] Time:0.234, Train Loss:0.3867662847042084\n",
      "Epoch 0[13335/17270] Time:0.247, Train Loss:0.48839059472084045\n",
      "Epoch 0[13336/17270] Time:0.238, Train Loss:0.6714239716529846\n",
      "Epoch 0[13337/17270] Time:0.226, Train Loss:0.5040537714958191\n",
      "Epoch 0[13338/17270] Time:0.236, Train Loss:0.47445347905158997\n",
      "Epoch 0[13339/17270] Time:0.229, Train Loss:0.8693884611129761\n",
      "Epoch 0[13340/17270] Time:0.236, Train Loss:0.37727323174476624\n",
      "Epoch 0[13341/17270] Time:0.233, Train Loss:1.992774248123169\n",
      "Epoch 0[13342/17270] Time:0.227, Train Loss:0.4159068763256073\n",
      "Epoch 0[13343/17270] Time:0.233, Train Loss:0.9660763740539551\n",
      "Epoch 0[13344/17270] Time:0.234, Train Loss:0.6273247599601746\n",
      "Epoch 0[13345/17270] Time:0.23, Train Loss:0.6790496706962585\n",
      "Epoch 0[13346/17270] Time:0.237, Train Loss:0.6893328428268433\n",
      "Epoch 0[13347/17270] Time:0.235, Train Loss:0.6001004576683044\n",
      "Epoch 0[13348/17270] Time:0.236, Train Loss:0.46751460433006287\n",
      "Epoch 0[13349/17270] Time:0.237, Train Loss:1.1231249570846558\n",
      "Epoch 0[13350/17270] Time:0.229, Train Loss:0.9634565711021423\n",
      "Epoch 0[13351/17270] Time:0.239, Train Loss:0.3905949890613556\n",
      "Epoch 0[13352/17270] Time:0.234, Train Loss:0.4904477298259735\n",
      "Epoch 0[13353/17270] Time:0.229, Train Loss:0.3971964716911316\n",
      "Epoch 0[13354/17270] Time:0.231, Train Loss:0.45034119486808777\n",
      "Epoch 0[13355/17270] Time:0.23, Train Loss:1.0904978513717651\n",
      "Epoch 0[13356/17270] Time:0.24, Train Loss:0.7200879454612732\n",
      "Epoch 0[13357/17270] Time:0.231, Train Loss:0.6032547354698181\n",
      "Epoch 0[13358/17270] Time:0.235, Train Loss:1.1297900676727295\n",
      "Epoch 0[13359/17270] Time:0.23, Train Loss:0.48404377698898315\n",
      "Epoch 0[13360/17270] Time:0.249, Train Loss:1.371621012687683\n",
      "Epoch 0[13361/17270] Time:0.241, Train Loss:0.8215586543083191\n",
      "Epoch 0[13362/17270] Time:0.233, Train Loss:0.5764340758323669\n",
      "Epoch 0[13363/17270] Time:0.226, Train Loss:0.5728975534439087\n",
      "Epoch 0[13364/17270] Time:0.24, Train Loss:0.3992728292942047\n",
      "Epoch 0[13365/17270] Time:0.22, Train Loss:0.6237097382545471\n",
      "Epoch 0[13366/17270] Time:0.234, Train Loss:0.5691156983375549\n",
      "Epoch 0[13367/17270] Time:0.232, Train Loss:0.5027691721916199\n",
      "Epoch 0[13368/17270] Time:0.234, Train Loss:0.8170375227928162\n",
      "Epoch 0[13369/17270] Time:0.241, Train Loss:0.5300090909004211\n",
      "Epoch 0[13370/17270] Time:0.237, Train Loss:0.4512573778629303\n",
      "Epoch 0[13371/17270] Time:0.242, Train Loss:0.7517918944358826\n",
      "Epoch 0[13372/17270] Time:0.232, Train Loss:0.3291858434677124\n",
      "Epoch 0[13373/17270] Time:0.228, Train Loss:0.6555532217025757\n",
      "Epoch 0[13374/17270] Time:0.235, Train Loss:0.6124821901321411\n",
      "Epoch 0[13375/17270] Time:0.243, Train Loss:0.38162922859191895\n",
      "Epoch 0[13376/17270] Time:0.243, Train Loss:0.7433466911315918\n",
      "Epoch 0[13377/17270] Time:0.223, Train Loss:0.4470292031764984\n",
      "Epoch 0[13378/17270] Time:0.247, Train Loss:0.4538576602935791\n",
      "Epoch 0[13379/17270] Time:0.234, Train Loss:0.5374662280082703\n",
      "Epoch 0[13380/17270] Time:0.232, Train Loss:0.44012150168418884\n",
      "Epoch 0[13381/17270] Time:0.247, Train Loss:0.2480556070804596\n",
      "Epoch 0[13382/17270] Time:0.225, Train Loss:0.3217114806175232\n",
      "Epoch 0[13383/17270] Time:0.238, Train Loss:1.5076314210891724\n",
      "Epoch 0[13384/17270] Time:0.237, Train Loss:0.5433381199836731\n",
      "Epoch 0[13385/17270] Time:0.234, Train Loss:1.0807050466537476\n",
      "Epoch 0[13386/17270] Time:0.237, Train Loss:0.2738666832447052\n",
      "Epoch 0[13387/17270] Time:0.223, Train Loss:0.5424721240997314\n",
      "Epoch 0[13388/17270] Time:0.234, Train Loss:0.2528071403503418\n",
      "Epoch 0[13389/17270] Time:0.252, Train Loss:0.394142210483551\n",
      "Epoch 0[13390/17270] Time:0.215, Train Loss:0.6652969121932983\n",
      "Epoch 0[13391/17270] Time:0.241, Train Loss:0.29995498061180115\n",
      "Epoch 0[13392/17270] Time:0.224, Train Loss:0.421358197927475\n",
      "Epoch 0[13393/17270] Time:0.234, Train Loss:0.37763330340385437\n",
      "Epoch 0[13394/17270] Time:0.225, Train Loss:1.4119415283203125\n",
      "Epoch 0[13395/17270] Time:0.243, Train Loss:0.4255542755126953\n",
      "Epoch 0[13396/17270] Time:0.233, Train Loss:0.4691501557826996\n",
      "Epoch 0[13397/17270] Time:0.239, Train Loss:0.7796729207038879\n",
      "Epoch 0[13398/17270] Time:0.238, Train Loss:0.47949275374412537\n",
      "Epoch 0[13399/17270] Time:0.226, Train Loss:0.2855566740036011\n",
      "Epoch 0[13400/17270] Time:0.231, Train Loss:0.3729778528213501\n",
      "Epoch 0[13401/17270] Time:0.247, Train Loss:0.4431926906108856\n",
      "Epoch 0[13402/17270] Time:0.232, Train Loss:0.39581573009490967\n",
      "Epoch 0[13403/17270] Time:0.236, Train Loss:0.7787227630615234\n",
      "Epoch 0[13404/17270] Time:0.232, Train Loss:0.438411682844162\n",
      "Epoch 0[13405/17270] Time:0.233, Train Loss:0.8740757703781128\n",
      "Epoch 0[13406/17270] Time:0.236, Train Loss:0.6022406220436096\n",
      "Epoch 0[13407/17270] Time:0.23, Train Loss:0.3817715644836426\n",
      "Epoch 0[13408/17270] Time:0.238, Train Loss:0.5867831707000732\n",
      "Epoch 0[13409/17270] Time:0.238, Train Loss:0.9626144766807556\n",
      "Epoch 0[13410/17270] Time:0.229, Train Loss:0.36907297372817993\n",
      "Epoch 0[13411/17270] Time:0.231, Train Loss:0.32810068130493164\n",
      "Epoch 0[13412/17270] Time:0.227, Train Loss:0.4862865209579468\n",
      "Epoch 0[13413/17270] Time:0.235, Train Loss:0.8555129766464233\n",
      "Epoch 0[13414/17270] Time:0.244, Train Loss:0.515654444694519\n",
      "Epoch 0[13415/17270] Time:0.231, Train Loss:0.8501969575881958\n",
      "Epoch 0[13416/17270] Time:0.236, Train Loss:0.3606164753437042\n",
      "Epoch 0[13417/17270] Time:0.24, Train Loss:0.9211950898170471\n",
      "Epoch 0[13418/17270] Time:0.237, Train Loss:0.453213095664978\n",
      "Epoch 0[13419/17270] Time:0.223, Train Loss:0.4345695674419403\n",
      "Epoch 0[13420/17270] Time:0.228, Train Loss:0.5601833462715149\n",
      "Epoch 0[13421/17270] Time:0.23, Train Loss:0.7654029726982117\n",
      "Epoch 0[13422/17270] Time:0.233, Train Loss:0.35917410254478455\n",
      "Epoch 0[13423/17270] Time:0.228, Train Loss:0.4804050326347351\n",
      "Epoch 0[13424/17270] Time:0.228, Train Loss:0.4510267376899719\n",
      "Epoch 0[13425/17270] Time:0.232, Train Loss:0.6519253253936768\n",
      "Epoch 0[13426/17270] Time:0.24, Train Loss:0.7701371312141418\n",
      "Epoch 0[13427/17270] Time:0.227, Train Loss:0.6664940118789673\n",
      "Epoch 0[13428/17270] Time:0.229, Train Loss:0.5838735103607178\n",
      "Epoch 0[13429/17270] Time:0.233, Train Loss:0.48288124799728394\n",
      "Epoch 0[13430/17270] Time:0.25, Train Loss:0.42096269130706787\n",
      "Epoch 0[13431/17270] Time:0.232, Train Loss:0.5492933988571167\n",
      "Epoch 0[13432/17270] Time:0.233, Train Loss:0.551301121711731\n",
      "Epoch 0[13433/17270] Time:0.231, Train Loss:0.6776004433631897\n",
      "Epoch 0[13434/17270] Time:0.233, Train Loss:0.9979152679443359\n",
      "Epoch 0[13435/17270] Time:0.236, Train Loss:0.5087520480155945\n",
      "Epoch 0[13436/17270] Time:0.23, Train Loss:0.8470497727394104\n",
      "Epoch 0[13437/17270] Time:0.228, Train Loss:0.6844494342803955\n",
      "Epoch 0[13438/17270] Time:0.233, Train Loss:0.402846097946167\n",
      "Epoch 0[13439/17270] Time:0.23, Train Loss:0.4825070798397064\n",
      "Epoch 0[13440/17270] Time:0.237, Train Loss:0.4027908444404602\n",
      "Epoch 0[13441/17270] Time:0.236, Train Loss:0.46430259943008423\n",
      "Epoch 0[13442/17270] Time:0.229, Train Loss:0.48875823616981506\n",
      "Epoch 0[13443/17270] Time:0.234, Train Loss:0.384288489818573\n",
      "Epoch 0[13444/17270] Time:0.228, Train Loss:0.7144550681114197\n",
      "Epoch 0[13445/17270] Time:0.23, Train Loss:0.9004636406898499\n",
      "Epoch 0[13446/17270] Time:0.228, Train Loss:0.4044535458087921\n",
      "Epoch 0[13447/17270] Time:0.236, Train Loss:0.7988280653953552\n",
      "Epoch 0[13448/17270] Time:0.236, Train Loss:0.513921856880188\n",
      "Epoch 0[13449/17270] Time:0.238, Train Loss:0.4616008996963501\n",
      "Epoch 0[13450/17270] Time:0.227, Train Loss:0.3798942267894745\n",
      "Epoch 0[13451/17270] Time:0.227, Train Loss:0.4611544907093048\n",
      "Epoch 0[13452/17270] Time:0.231, Train Loss:0.37979206442832947\n",
      "Epoch 0[13453/17270] Time:0.234, Train Loss:0.6233155727386475\n",
      "Epoch 0[13454/17270] Time:0.242, Train Loss:0.731678307056427\n",
      "Epoch 0[13455/17270] Time:0.232, Train Loss:0.4696423411369324\n",
      "Epoch 0[13456/17270] Time:0.22, Train Loss:0.796948254108429\n",
      "Epoch 0[13457/17270] Time:0.234, Train Loss:1.277409553527832\n",
      "Epoch 0[13458/17270] Time:0.237, Train Loss:0.5013977289199829\n",
      "Epoch 0[13459/17270] Time:0.238, Train Loss:0.3144734799861908\n",
      "Epoch 0[13460/17270] Time:0.227, Train Loss:0.2589550316333771\n",
      "Epoch 0[13461/17270] Time:0.233, Train Loss:0.6570991277694702\n",
      "Epoch 0[13462/17270] Time:0.234, Train Loss:1.0624077320098877\n",
      "Epoch 0[13463/17270] Time:0.236, Train Loss:1.0271657705307007\n",
      "Epoch 0[13464/17270] Time:0.238, Train Loss:0.4901195466518402\n",
      "Epoch 0[13465/17270] Time:0.228, Train Loss:0.46141716837882996\n",
      "Epoch 0[13466/17270] Time:0.24, Train Loss:0.2782244086265564\n",
      "Epoch 0[13467/17270] Time:0.232, Train Loss:0.5939639806747437\n",
      "Epoch 0[13468/17270] Time:0.232, Train Loss:0.2820097506046295\n",
      "Epoch 0[13469/17270] Time:0.235, Train Loss:0.6375527381896973\n",
      "Epoch 0[13470/17270] Time:0.224, Train Loss:0.5118588209152222\n",
      "Epoch 0[13471/17270] Time:0.237, Train Loss:0.5098823308944702\n",
      "Epoch 0[13472/17270] Time:0.242, Train Loss:0.5327864289283752\n",
      "Epoch 0[13473/17270] Time:0.231, Train Loss:0.40350350737571716\n",
      "Epoch 0[13474/17270] Time:0.226, Train Loss:0.960305392742157\n",
      "Epoch 0[13475/17270] Time:0.231, Train Loss:0.7694643139839172\n",
      "Epoch 0[13476/17270] Time:0.238, Train Loss:0.727800190448761\n",
      "Epoch 0[13477/17270] Time:0.232, Train Loss:1.1310020685195923\n",
      "Epoch 0[13478/17270] Time:0.226, Train Loss:0.9992817640304565\n",
      "Epoch 0[13479/17270] Time:0.239, Train Loss:0.4893791675567627\n",
      "Epoch 0[13480/17270] Time:0.236, Train Loss:0.7175096273422241\n",
      "Epoch 0[13481/17270] Time:0.234, Train Loss:0.7343027591705322\n",
      "Epoch 0[13482/17270] Time:0.237, Train Loss:0.9214915633201599\n",
      "Epoch 0[13483/17270] Time:0.23, Train Loss:0.43154793977737427\n",
      "Epoch 0[13484/17270] Time:0.23, Train Loss:0.5922220349311829\n",
      "Epoch 0[13485/17270] Time:0.238, Train Loss:1.0947084426879883\n",
      "Epoch 0[13486/17270] Time:0.238, Train Loss:0.4116705358028412\n",
      "Epoch 0[13487/17270] Time:0.24, Train Loss:0.47917744517326355\n",
      "Epoch 0[13488/17270] Time:0.233, Train Loss:0.5308612585067749\n",
      "Epoch 0[13489/17270] Time:0.231, Train Loss:0.6621915102005005\n",
      "Epoch 0[13490/17270] Time:0.232, Train Loss:0.4821818470954895\n",
      "Epoch 0[13491/17270] Time:0.238, Train Loss:0.48914834856987\n",
      "Epoch 0[13492/17270] Time:0.232, Train Loss:0.5887547731399536\n",
      "Epoch 0[13493/17270] Time:0.237, Train Loss:0.43350186944007874\n",
      "Epoch 0[13494/17270] Time:0.235, Train Loss:0.4015890955924988\n",
      "Epoch 0[13495/17270] Time:0.239, Train Loss:1.1016587018966675\n",
      "Epoch 0[13496/17270] Time:0.238, Train Loss:0.5778862833976746\n",
      "Epoch 0[13497/17270] Time:0.227, Train Loss:0.714451014995575\n",
      "Epoch 0[13498/17270] Time:0.247, Train Loss:0.713923454284668\n",
      "Epoch 0[13499/17270] Time:0.23, Train Loss:0.5382157564163208\n",
      "Epoch 0[13500/17270] Time:0.229, Train Loss:0.4825733006000519\n",
      "Epoch 0[13501/17270] Time:0.238, Train Loss:0.3627523183822632\n",
      "Epoch 0[13502/17270] Time:0.243, Train Loss:0.41097307205200195\n",
      "Epoch 0[13503/17270] Time:0.239, Train Loss:0.4165206551551819\n",
      "Epoch 0[13504/17270] Time:0.226, Train Loss:0.4737853705883026\n",
      "Epoch 0[13505/17270] Time:0.247, Train Loss:0.7261090874671936\n",
      "Epoch 0[13506/17270] Time:0.226, Train Loss:0.7940583229064941\n",
      "Epoch 0[13507/17270] Time:0.237, Train Loss:0.695924699306488\n",
      "Epoch 0[13508/17270] Time:0.236, Train Loss:0.42323237657546997\n",
      "Epoch 0[13509/17270] Time:0.237, Train Loss:0.45694535970687866\n",
      "Epoch 0[13510/17270] Time:0.237, Train Loss:0.6571151614189148\n",
      "Epoch 0[13511/17270] Time:0.236, Train Loss:0.7078975439071655\n",
      "Epoch 0[13512/17270] Time:0.23, Train Loss:1.2394299507141113\n",
      "Epoch 0[13513/17270] Time:0.229, Train Loss:0.45692041516304016\n",
      "Epoch 0[13514/17270] Time:0.234, Train Loss:0.5872475504875183\n",
      "Epoch 0[13515/17270] Time:0.242, Train Loss:0.8903728127479553\n",
      "Epoch 0[13516/17270] Time:0.229, Train Loss:0.2736656069755554\n",
      "Epoch 0[13517/17270] Time:0.238, Train Loss:0.3159375488758087\n",
      "Epoch 0[13518/17270] Time:0.238, Train Loss:0.4760919511318207\n",
      "Epoch 0[13519/17270] Time:0.238, Train Loss:0.5928050875663757\n",
      "Epoch 0[13520/17270] Time:0.239, Train Loss:0.6467528939247131\n",
      "Epoch 0[13521/17270] Time:0.245, Train Loss:0.6832457780838013\n",
      "Epoch 0[13522/17270] Time:0.238, Train Loss:0.2949385941028595\n",
      "Epoch 0[13523/17270] Time:0.235, Train Loss:0.45384761691093445\n",
      "Epoch 0[13524/17270] Time:0.233, Train Loss:0.7482835054397583\n",
      "Epoch 0[13525/17270] Time:0.239, Train Loss:0.3842644691467285\n",
      "Epoch 0[13526/17270] Time:0.238, Train Loss:0.7060840129852295\n",
      "Epoch 0[13527/17270] Time:0.231, Train Loss:0.6370954513549805\n",
      "Epoch 0[13528/17270] Time:0.222, Train Loss:0.3438816964626312\n",
      "Epoch 0[13529/17270] Time:0.23, Train Loss:0.8586646318435669\n",
      "Epoch 0[13530/17270] Time:0.239, Train Loss:0.5699477791786194\n",
      "Epoch 0[13531/17270] Time:0.235, Train Loss:0.4125708043575287\n",
      "Epoch 0[13532/17270] Time:0.236, Train Loss:0.4442192018032074\n",
      "Epoch 0[13533/17270] Time:0.233, Train Loss:1.462307333946228\n",
      "Epoch 0[13534/17270] Time:0.219, Train Loss:0.37359097599983215\n",
      "Epoch 0[13535/17270] Time:0.233, Train Loss:1.1410353183746338\n",
      "Epoch 0[13536/17270] Time:0.244, Train Loss:0.4404950439929962\n",
      "Epoch 0[13537/17270] Time:0.233, Train Loss:0.5400461554527283\n",
      "Epoch 0[13538/17270] Time:0.23, Train Loss:0.9457083940505981\n",
      "Epoch 0[13539/17270] Time:0.245, Train Loss:0.37277504801750183\n",
      "Epoch 0[13540/17270] Time:0.222, Train Loss:0.8190109133720398\n",
      "Epoch 0[13541/17270] Time:0.228, Train Loss:1.1672680377960205\n",
      "Epoch 0[13542/17270] Time:0.239, Train Loss:0.49632880091667175\n",
      "Epoch 0[13543/17270] Time:0.231, Train Loss:0.4963785707950592\n",
      "Epoch 0[13544/17270] Time:0.238, Train Loss:1.189931035041809\n",
      "Epoch 0[13545/17270] Time:0.231, Train Loss:1.089012861251831\n",
      "Epoch 0[13546/17270] Time:0.234, Train Loss:0.8655142188072205\n",
      "Epoch 0[13547/17270] Time:0.236, Train Loss:0.6297599077224731\n",
      "Epoch 0[13548/17270] Time:0.24, Train Loss:1.3375794887542725\n",
      "Epoch 0[13549/17270] Time:0.231, Train Loss:0.3888865113258362\n",
      "Epoch 0[13550/17270] Time:0.239, Train Loss:0.9763966798782349\n",
      "Epoch 0[13551/17270] Time:0.234, Train Loss:0.38937604427337646\n",
      "Epoch 0[13552/17270] Time:0.224, Train Loss:0.9238132834434509\n",
      "Epoch 0[13553/17270] Time:0.237, Train Loss:0.4380507171154022\n",
      "Epoch 0[13554/17270] Time:0.234, Train Loss:0.5434924960136414\n",
      "Epoch 0[13555/17270] Time:0.254, Train Loss:0.46893802285194397\n",
      "Epoch 0[13556/17270] Time:0.229, Train Loss:0.4857889413833618\n",
      "Epoch 0[13557/17270] Time:0.234, Train Loss:0.8427202105522156\n",
      "Epoch 0[13558/17270] Time:0.236, Train Loss:0.587074339389801\n",
      "Epoch 0[13559/17270] Time:0.235, Train Loss:0.6207965016365051\n",
      "Epoch 0[13560/17270] Time:0.228, Train Loss:0.5892158150672913\n",
      "Epoch 0[13561/17270] Time:0.233, Train Loss:0.5514586567878723\n",
      "Epoch 0[13562/17270] Time:0.237, Train Loss:0.8639358878135681\n",
      "Epoch 0[13563/17270] Time:0.252, Train Loss:0.5924287438392639\n",
      "Epoch 0[13564/17270] Time:0.224, Train Loss:0.5910722017288208\n",
      "Epoch 0[13565/17270] Time:0.237, Train Loss:0.5878022313117981\n",
      "Epoch 0[13566/17270] Time:0.256, Train Loss:0.3646549582481384\n",
      "Epoch 0[13567/17270] Time:0.225, Train Loss:0.611822783946991\n",
      "Epoch 0[13568/17270] Time:0.251, Train Loss:0.5220859050750732\n",
      "Epoch 0[13569/17270] Time:0.239, Train Loss:0.36271795630455017\n",
      "Epoch 0[13570/17270] Time:0.238, Train Loss:0.7144944071769714\n",
      "Epoch 0[13571/17270] Time:0.252, Train Loss:0.597430944442749\n",
      "Epoch 0[13572/17270] Time:0.235, Train Loss:0.40928003191947937\n",
      "Epoch 0[13573/17270] Time:0.228, Train Loss:0.29883819818496704\n",
      "Epoch 0[13574/17270] Time:0.248, Train Loss:0.5378838777542114\n",
      "Epoch 0[13575/17270] Time:0.239, Train Loss:0.4827648997306824\n",
      "Epoch 0[13576/17270] Time:0.233, Train Loss:0.8632332682609558\n",
      "Epoch 0[13577/17270] Time:0.227, Train Loss:0.6952947974205017\n",
      "Epoch 0[13578/17270] Time:0.229, Train Loss:0.7719092965126038\n",
      "Epoch 0[13579/17270] Time:0.234, Train Loss:0.5376096367835999\n",
      "Epoch 0[13580/17270] Time:0.239, Train Loss:0.580898106098175\n",
      "Epoch 0[13581/17270] Time:0.234, Train Loss:0.6935629844665527\n",
      "Epoch 0[13582/17270] Time:0.232, Train Loss:0.3563881516456604\n",
      "Epoch 0[13583/17270] Time:0.235, Train Loss:1.2958208322525024\n",
      "Epoch 0[13584/17270] Time:0.223, Train Loss:0.4414074122905731\n",
      "Epoch 0[13585/17270] Time:0.24, Train Loss:0.613446831703186\n",
      "Epoch 0[13586/17270] Time:0.232, Train Loss:0.8659653663635254\n",
      "Epoch 0[13587/17270] Time:0.232, Train Loss:0.5450217127799988\n",
      "Epoch 0[13588/17270] Time:0.238, Train Loss:0.5403201580047607\n",
      "Epoch 0[13589/17270] Time:0.234, Train Loss:0.38139352202415466\n",
      "Epoch 0[13590/17270] Time:0.235, Train Loss:1.0330716371536255\n",
      "Epoch 0[13591/17270] Time:0.238, Train Loss:1.0556166172027588\n",
      "Epoch 0[13592/17270] Time:0.248, Train Loss:0.5312472581863403\n",
      "Epoch 0[13593/17270] Time:0.23, Train Loss:0.776482105255127\n",
      "Epoch 0[13594/17270] Time:0.232, Train Loss:0.5830245018005371\n",
      "Epoch 0[13595/17270] Time:0.237, Train Loss:0.553006112575531\n",
      "Epoch 0[13596/17270] Time:0.234, Train Loss:0.5508368015289307\n",
      "Epoch 0[13597/17270] Time:0.233, Train Loss:0.5920854806900024\n",
      "Epoch 0[13598/17270] Time:0.232, Train Loss:1.7072440385818481\n",
      "Epoch 0[13599/17270] Time:0.242, Train Loss:0.3391741216182709\n",
      "Epoch 0[13600/17270] Time:0.232, Train Loss:0.47761186957359314\n",
      "Epoch 0[13601/17270] Time:0.233, Train Loss:0.44743791222572327\n",
      "Epoch 0[13602/17270] Time:0.226, Train Loss:0.43187791109085083\n",
      "Epoch 0[13603/17270] Time:0.235, Train Loss:1.039909839630127\n",
      "Epoch 0[13604/17270] Time:0.234, Train Loss:0.9036162495613098\n",
      "Epoch 0[13605/17270] Time:0.231, Train Loss:0.4771571457386017\n",
      "Epoch 0[13606/17270] Time:0.24, Train Loss:0.6814956665039062\n",
      "Epoch 0[13607/17270] Time:0.226, Train Loss:0.521105945110321\n",
      "Epoch 0[13608/17270] Time:0.232, Train Loss:0.4631238281726837\n",
      "Epoch 0[13609/17270] Time:0.224, Train Loss:0.3961764872074127\n",
      "Epoch 0[13610/17270] Time:0.247, Train Loss:0.5782616138458252\n",
      "Epoch 0[13611/17270] Time:0.234, Train Loss:0.6245989799499512\n",
      "Epoch 0[13612/17270] Time:0.23, Train Loss:0.5965083837509155\n",
      "Epoch 0[13613/17270] Time:0.237, Train Loss:0.39164552092552185\n",
      "Epoch 0[13614/17270] Time:0.23, Train Loss:0.5805217027664185\n",
      "Epoch 0[13615/17270] Time:0.233, Train Loss:0.5040865540504456\n",
      "Epoch 0[13616/17270] Time:0.236, Train Loss:0.5552569031715393\n",
      "Epoch 0[13617/17270] Time:0.229, Train Loss:0.6008798480033875\n",
      "Epoch 0[13618/17270] Time:0.23, Train Loss:0.9688611030578613\n",
      "Epoch 0[13619/17270] Time:0.237, Train Loss:0.3911115825176239\n",
      "Epoch 0[13620/17270] Time:0.243, Train Loss:0.2561955153942108\n",
      "Epoch 0[13621/17270] Time:0.234, Train Loss:0.8978158831596375\n",
      "Epoch 0[13622/17270] Time:0.236, Train Loss:0.3046669661998749\n",
      "Epoch 0[13623/17270] Time:0.225, Train Loss:0.4173010289669037\n",
      "Epoch 0[13624/17270] Time:0.233, Train Loss:0.6893447041511536\n",
      "Epoch 0[13625/17270] Time:0.241, Train Loss:0.533035933971405\n",
      "Epoch 0[13626/17270] Time:0.238, Train Loss:0.3644355535507202\n",
      "Epoch 0[13627/17270] Time:0.234, Train Loss:0.5487545132637024\n",
      "Epoch 0[13628/17270] Time:0.223, Train Loss:0.4189642369747162\n",
      "Epoch 0[13629/17270] Time:0.247, Train Loss:0.5088013410568237\n",
      "Epoch 0[13630/17270] Time:0.231, Train Loss:0.36235311627388\n",
      "Epoch 0[13631/17270] Time:0.23, Train Loss:0.7451834082603455\n",
      "Epoch 0[13632/17270] Time:0.238, Train Loss:0.46469566226005554\n",
      "Epoch 0[13633/17270] Time:0.231, Train Loss:0.38422641158103943\n",
      "Epoch 0[13634/17270] Time:0.228, Train Loss:0.6351420879364014\n",
      "Epoch 0[13635/17270] Time:0.238, Train Loss:0.28073397278785706\n",
      "Epoch 0[13636/17270] Time:0.239, Train Loss:0.6627518534660339\n",
      "Epoch 0[13637/17270] Time:0.235, Train Loss:0.48350098729133606\n",
      "Epoch 0[13638/17270] Time:0.233, Train Loss:0.4593006372451782\n",
      "Epoch 0[13639/17270] Time:0.23, Train Loss:1.1640851497650146\n",
      "Epoch 0[13640/17270] Time:0.237, Train Loss:0.5312201976776123\n",
      "Epoch 0[13641/17270] Time:0.239, Train Loss:0.43746811151504517\n",
      "Epoch 0[13642/17270] Time:0.223, Train Loss:0.6692075133323669\n",
      "Epoch 0[13643/17270] Time:0.23, Train Loss:0.4220324754714966\n",
      "Epoch 0[13644/17270] Time:0.236, Train Loss:0.6390072703361511\n",
      "Epoch 0[13645/17270] Time:0.231, Train Loss:0.6219139099121094\n",
      "Epoch 0[13646/17270] Time:0.237, Train Loss:0.380042165517807\n",
      "Epoch 0[13647/17270] Time:0.239, Train Loss:0.36557453870773315\n",
      "Epoch 0[13648/17270] Time:0.234, Train Loss:0.6656555533409119\n",
      "Epoch 0[13649/17270] Time:0.237, Train Loss:0.9640994668006897\n",
      "Epoch 0[13650/17270] Time:0.235, Train Loss:0.5422505736351013\n",
      "Epoch 0[13651/17270] Time:0.232, Train Loss:1.1705076694488525\n",
      "Epoch 0[13652/17270] Time:0.243, Train Loss:0.4960705637931824\n",
      "Epoch 0[13653/17270] Time:0.239, Train Loss:0.24524039030075073\n",
      "Epoch 0[13654/17270] Time:0.233, Train Loss:0.5299507975578308\n",
      "Epoch 0[13655/17270] Time:0.231, Train Loss:0.5127696990966797\n",
      "Epoch 0[13656/17270] Time:0.234, Train Loss:1.0583409070968628\n",
      "Epoch 0[13657/17270] Time:0.229, Train Loss:0.43229520320892334\n",
      "Epoch 0[13658/17270] Time:0.249, Train Loss:0.4198109805583954\n",
      "Epoch 0[13659/17270] Time:0.241, Train Loss:0.7242022156715393\n",
      "Epoch 0[13660/17270] Time:0.249, Train Loss:0.509583592414856\n",
      "Epoch 0[13661/17270] Time:0.221, Train Loss:0.36875924468040466\n",
      "Epoch 0[13662/17270] Time:0.243, Train Loss:0.46740010380744934\n",
      "Epoch 0[13663/17270] Time:0.244, Train Loss:0.7811693549156189\n",
      "Epoch 0[13664/17270] Time:0.239, Train Loss:0.38391000032424927\n",
      "Epoch 0[13665/17270] Time:0.238, Train Loss:0.5982968807220459\n",
      "Epoch 0[13666/17270] Time:0.223, Train Loss:0.35593104362487793\n",
      "Epoch 0[13667/17270] Time:0.246, Train Loss:1.046744704246521\n",
      "Epoch 0[13668/17270] Time:0.237, Train Loss:0.5647711157798767\n",
      "Epoch 0[13669/17270] Time:0.225, Train Loss:0.4686516523361206\n",
      "Epoch 0[13670/17270] Time:0.235, Train Loss:0.6241989135742188\n",
      "Epoch 0[13671/17270] Time:0.229, Train Loss:0.5926376581192017\n",
      "Epoch 0[13672/17270] Time:0.234, Train Loss:0.67864590883255\n",
      "Epoch 0[13673/17270] Time:0.23, Train Loss:0.6109514832496643\n",
      "Epoch 0[13674/17270] Time:0.231, Train Loss:0.4549129605293274\n",
      "Epoch 0[13675/17270] Time:0.237, Train Loss:0.8248303532600403\n",
      "Epoch 0[13676/17270] Time:0.233, Train Loss:0.3773491680622101\n",
      "Epoch 0[13677/17270] Time:0.237, Train Loss:0.6192104816436768\n",
      "Epoch 0[13678/17270] Time:0.237, Train Loss:0.5184928178787231\n",
      "Epoch 0[13679/17270] Time:0.238, Train Loss:0.6784523725509644\n",
      "Epoch 0[13680/17270] Time:0.231, Train Loss:0.5509065389633179\n",
      "Epoch 0[13681/17270] Time:0.231, Train Loss:0.5359534621238708\n",
      "Epoch 0[13682/17270] Time:0.234, Train Loss:0.6102468967437744\n",
      "Epoch 0[13683/17270] Time:0.233, Train Loss:0.7684646248817444\n",
      "Epoch 0[13684/17270] Time:0.231, Train Loss:0.3528733551502228\n",
      "Epoch 0[13685/17270] Time:0.242, Train Loss:0.38307347893714905\n",
      "Epoch 0[13686/17270] Time:0.224, Train Loss:0.4438527524471283\n",
      "Epoch 0[13687/17270] Time:0.246, Train Loss:0.32457271218299866\n",
      "Epoch 0[13688/17270] Time:0.224, Train Loss:0.6152214407920837\n",
      "Epoch 0[13689/17270] Time:0.232, Train Loss:0.4146413505077362\n",
      "Epoch 0[13690/17270] Time:0.234, Train Loss:1.2348231077194214\n",
      "Epoch 0[13691/17270] Time:0.245, Train Loss:0.5725536346435547\n",
      "Epoch 0[13692/17270] Time:0.227, Train Loss:0.31611189246177673\n",
      "Epoch 0[13693/17270] Time:0.249, Train Loss:0.4337628483772278\n",
      "Epoch 0[13694/17270] Time:0.255, Train Loss:0.2858458161354065\n",
      "Epoch 0[13695/17270] Time:0.235, Train Loss:0.7490151524543762\n",
      "Epoch 0[13696/17270] Time:0.228, Train Loss:0.28436821699142456\n",
      "Epoch 0[13697/17270] Time:0.219, Train Loss:0.5871944427490234\n",
      "Epoch 0[13698/17270] Time:0.231, Train Loss:0.36131247878074646\n",
      "Epoch 0[13699/17270] Time:0.23, Train Loss:1.1376866102218628\n",
      "Epoch 0[13700/17270] Time:0.238, Train Loss:0.39281460642814636\n",
      "Epoch 0[13701/17270] Time:0.246, Train Loss:0.2861025929450989\n",
      "Epoch 0[13702/17270] Time:0.235, Train Loss:0.35953283309936523\n",
      "Epoch 0[13703/17270] Time:0.233, Train Loss:0.32027795910835266\n",
      "Epoch 0[13704/17270] Time:0.243, Train Loss:0.43981727957725525\n",
      "Epoch 0[13705/17270] Time:0.233, Train Loss:0.6702859401702881\n",
      "Epoch 0[13706/17270] Time:0.241, Train Loss:0.5034006834030151\n",
      "Epoch 0[13707/17270] Time:0.222, Train Loss:0.5507436990737915\n",
      "Epoch 0[13708/17270] Time:0.244, Train Loss:0.9624208211898804\n",
      "Epoch 0[13709/17270] Time:0.234, Train Loss:0.5209869742393494\n",
      "Epoch 0[13710/17270] Time:0.238, Train Loss:0.4542871415615082\n",
      "Epoch 0[13711/17270] Time:0.232, Train Loss:0.42086291313171387\n",
      "Epoch 0[13712/17270] Time:0.228, Train Loss:0.6018954515457153\n",
      "Epoch 0[13713/17270] Time:0.23, Train Loss:0.5269079208374023\n",
      "Epoch 0[13714/17270] Time:0.235, Train Loss:0.20041604340076447\n",
      "Epoch 0[13715/17270] Time:0.229, Train Loss:0.4623830318450928\n",
      "Epoch 0[13716/17270] Time:0.227, Train Loss:0.6548581123352051\n",
      "Epoch 0[13717/17270] Time:0.228, Train Loss:0.6756716370582581\n",
      "Epoch 0[13718/17270] Time:0.237, Train Loss:0.8644152879714966\n",
      "Epoch 0[13719/17270] Time:0.235, Train Loss:0.47477951645851135\n",
      "Epoch 0[13720/17270] Time:0.24, Train Loss:0.7286763787269592\n",
      "Epoch 0[13721/17270] Time:0.231, Train Loss:0.4689972698688507\n",
      "Epoch 0[13722/17270] Time:0.24, Train Loss:0.6574943661689758\n",
      "Epoch 0[13723/17270] Time:0.226, Train Loss:1.2874723672866821\n",
      "Epoch 0[13724/17270] Time:0.234, Train Loss:0.8604750037193298\n",
      "Epoch 0[13725/17270] Time:0.233, Train Loss:0.5561394691467285\n",
      "Epoch 0[13726/17270] Time:0.237, Train Loss:0.6273840665817261\n",
      "Epoch 0[13727/17270] Time:0.22, Train Loss:0.2395797073841095\n",
      "Epoch 0[13728/17270] Time:0.236, Train Loss:0.38998162746429443\n",
      "Epoch 0[13729/17270] Time:0.245, Train Loss:0.3369772732257843\n",
      "Epoch 0[13730/17270] Time:0.232, Train Loss:0.5810019969940186\n",
      "Epoch 0[13731/17270] Time:0.226, Train Loss:0.5256448984146118\n",
      "Epoch 0[13732/17270] Time:0.233, Train Loss:0.26007577776908875\n",
      "Epoch 0[13733/17270] Time:0.24, Train Loss:0.4774937331676483\n",
      "Epoch 0[13734/17270] Time:0.23, Train Loss:0.44698870182037354\n",
      "Epoch 0[13735/17270] Time:0.233, Train Loss:0.3971004784107208\n",
      "Epoch 0[13736/17270] Time:0.242, Train Loss:0.3690342307090759\n",
      "Epoch 0[13737/17270] Time:0.233, Train Loss:0.3741028904914856\n",
      "Epoch 0[13738/17270] Time:0.236, Train Loss:0.3490898013114929\n",
      "Epoch 0[13739/17270] Time:0.248, Train Loss:0.5991150736808777\n",
      "Epoch 0[13740/17270] Time:0.234, Train Loss:0.31361904740333557\n",
      "Epoch 0[13741/17270] Time:0.245, Train Loss:0.39256444573402405\n",
      "Epoch 0[13742/17270] Time:0.229, Train Loss:0.45647135376930237\n",
      "Epoch 0[13743/17270] Time:0.239, Train Loss:0.3138536810874939\n",
      "Epoch 0[13744/17270] Time:0.251, Train Loss:0.6732575297355652\n",
      "Epoch 0[13745/17270] Time:0.241, Train Loss:0.29827433824539185\n",
      "Epoch 0[13746/17270] Time:0.239, Train Loss:0.45452725887298584\n",
      "Epoch 0[13747/17270] Time:0.239, Train Loss:0.3270505964756012\n",
      "Epoch 0[13748/17270] Time:0.232, Train Loss:0.804454505443573\n",
      "Epoch 0[13749/17270] Time:0.232, Train Loss:0.45007142424583435\n",
      "Epoch 0[13750/17270] Time:0.237, Train Loss:0.502560555934906\n",
      "Epoch 0[13751/17270] Time:0.229, Train Loss:0.3268420398235321\n",
      "Epoch 0[13752/17270] Time:0.244, Train Loss:0.5117616057395935\n",
      "Epoch 0[13753/17270] Time:0.236, Train Loss:0.7125625014305115\n",
      "Epoch 0[13754/17270] Time:0.242, Train Loss:0.616238534450531\n",
      "Epoch 0[13755/17270] Time:0.241, Train Loss:1.1470999717712402\n",
      "Epoch 0[13756/17270] Time:0.239, Train Loss:0.39997872710227966\n",
      "Epoch 0[13757/17270] Time:0.236, Train Loss:0.5705110430717468\n",
      "Epoch 0[13758/17270] Time:0.242, Train Loss:0.5744444131851196\n",
      "Epoch 0[13759/17270] Time:0.243, Train Loss:0.5023072361946106\n",
      "Epoch 0[13760/17270] Time:0.238, Train Loss:0.5519527792930603\n",
      "Epoch 0[13761/17270] Time:0.232, Train Loss:0.5833660960197449\n",
      "Epoch 0[13762/17270] Time:0.233, Train Loss:0.4414249360561371\n",
      "Epoch 0[13763/17270] Time:0.235, Train Loss:0.44462162256240845\n",
      "Epoch 0[13764/17270] Time:0.24, Train Loss:0.36679983139038086\n",
      "Epoch 0[13765/17270] Time:0.23, Train Loss:0.28905704617500305\n",
      "Epoch 0[13766/17270] Time:0.236, Train Loss:0.6369794011116028\n",
      "Epoch 0[13767/17270] Time:0.237, Train Loss:0.864632248878479\n",
      "Epoch 0[13768/17270] Time:0.239, Train Loss:0.3705345094203949\n",
      "Epoch 0[13769/17270] Time:0.23, Train Loss:0.45784083008766174\n",
      "Epoch 0[13770/17270] Time:0.241, Train Loss:1.3159724473953247\n",
      "Epoch 0[13771/17270] Time:0.225, Train Loss:0.4727790355682373\n",
      "Epoch 0[13772/17270] Time:0.234, Train Loss:0.45691296458244324\n",
      "Epoch 0[13773/17270] Time:0.23, Train Loss:0.5946314334869385\n",
      "Epoch 0[13774/17270] Time:0.238, Train Loss:0.48194265365600586\n",
      "Epoch 0[13775/17270] Time:0.24, Train Loss:0.9025659561157227\n",
      "Epoch 0[13776/17270] Time:0.231, Train Loss:0.4962342083454132\n",
      "Epoch 0[13777/17270] Time:0.236, Train Loss:0.39446842670440674\n",
      "Epoch 0[13778/17270] Time:0.237, Train Loss:0.5658850073814392\n",
      "Epoch 0[13779/17270] Time:0.242, Train Loss:0.8979780077934265\n",
      "Epoch 0[13780/17270] Time:0.237, Train Loss:0.5606606006622314\n",
      "Epoch 0[13781/17270] Time:0.23, Train Loss:0.4560656249523163\n",
      "Epoch 0[13782/17270] Time:0.238, Train Loss:0.627317488193512\n",
      "Epoch 0[13783/17270] Time:0.249, Train Loss:0.7971781492233276\n",
      "Epoch 0[13784/17270] Time:0.25, Train Loss:0.6967290043830872\n",
      "Epoch 0[13785/17270] Time:0.219, Train Loss:0.5643600821495056\n",
      "Epoch 0[13786/17270] Time:0.236, Train Loss:0.27167341113090515\n",
      "Epoch 0[13787/17270] Time:0.254, Train Loss:0.34766730666160583\n",
      "Epoch 0[13788/17270] Time:0.235, Train Loss:0.460452675819397\n",
      "Epoch 0[13789/17270] Time:0.239, Train Loss:0.4450821578502655\n",
      "Epoch 0[13790/17270] Time:0.237, Train Loss:1.3806430101394653\n",
      "Epoch 0[13791/17270] Time:0.224, Train Loss:0.6222593188285828\n",
      "Epoch 0[13792/17270] Time:0.24, Train Loss:0.6144075989723206\n",
      "Epoch 0[13793/17270] Time:0.228, Train Loss:0.499101459980011\n",
      "Epoch 0[13794/17270] Time:0.24, Train Loss:0.5541388392448425\n",
      "Epoch 0[13795/17270] Time:0.236, Train Loss:0.43563902378082275\n",
      "Epoch 0[13796/17270] Time:0.232, Train Loss:0.7083334922790527\n",
      "Epoch 0[13797/17270] Time:0.23, Train Loss:0.34091076254844666\n",
      "Epoch 0[13798/17270] Time:0.236, Train Loss:0.598232626914978\n",
      "Epoch 0[13799/17270] Time:0.235, Train Loss:0.6521852016448975\n",
      "Epoch 0[13800/17270] Time:0.238, Train Loss:0.4184790849685669\n",
      "Epoch 0[13801/17270] Time:0.227, Train Loss:0.44901788234710693\n",
      "Epoch 0[13802/17270] Time:0.231, Train Loss:0.6113697290420532\n",
      "Epoch 0[13803/17270] Time:0.226, Train Loss:0.40322914719581604\n",
      "Epoch 0[13804/17270] Time:0.233, Train Loss:0.7078073620796204\n",
      "Epoch 0[13805/17270] Time:0.225, Train Loss:1.3671488761901855\n",
      "Epoch 0[13806/17270] Time:0.238, Train Loss:0.455728143453598\n",
      "Epoch 0[13807/17270] Time:0.223, Train Loss:0.5218835473060608\n",
      "Epoch 0[13808/17270] Time:0.231, Train Loss:0.778045117855072\n",
      "Epoch 0[13809/17270] Time:0.243, Train Loss:0.5772522687911987\n",
      "Epoch 0[13810/17270] Time:0.237, Train Loss:0.5794712901115417\n",
      "Epoch 0[13811/17270] Time:0.226, Train Loss:0.43840092420578003\n",
      "Epoch 0[13812/17270] Time:0.228, Train Loss:0.4093247056007385\n",
      "Epoch 0[13813/17270] Time:0.23, Train Loss:0.6378949284553528\n",
      "Epoch 0[13814/17270] Time:0.23, Train Loss:0.8193677067756653\n",
      "Epoch 0[13815/17270] Time:0.236, Train Loss:0.9041370749473572\n",
      "Epoch 0[13816/17270] Time:0.23, Train Loss:0.7167527675628662\n",
      "Epoch 0[13817/17270] Time:0.238, Train Loss:0.7865130305290222\n",
      "Epoch 0[13818/17270] Time:0.239, Train Loss:0.45832815766334534\n",
      "Epoch 0[13819/17270] Time:0.231, Train Loss:0.4705604910850525\n",
      "Epoch 0[13820/17270] Time:0.236, Train Loss:0.33700650930404663\n",
      "Epoch 0[13821/17270] Time:0.234, Train Loss:0.6571166515350342\n",
      "Epoch 0[13822/17270] Time:0.237, Train Loss:1.0990630388259888\n",
      "Epoch 0[13823/17270] Time:0.239, Train Loss:0.45776858925819397\n",
      "Epoch 0[13824/17270] Time:0.234, Train Loss:0.5672733187675476\n",
      "Epoch 0[13825/17270] Time:0.231, Train Loss:0.48853281140327454\n",
      "Epoch 0[13826/17270] Time:0.226, Train Loss:0.42484527826309204\n",
      "Epoch 0[13827/17270] Time:0.236, Train Loss:0.5110726356506348\n",
      "Epoch 0[13828/17270] Time:0.237, Train Loss:0.7390938997268677\n",
      "Epoch 0[13829/17270] Time:0.229, Train Loss:0.30044111609458923\n",
      "Epoch 0[13830/17270] Time:0.229, Train Loss:0.4811784625053406\n",
      "Epoch 0[13831/17270] Time:0.23, Train Loss:0.7029168605804443\n",
      "Epoch 0[13832/17270] Time:0.229, Train Loss:0.49094778299331665\n",
      "Epoch 0[13833/17270] Time:0.234, Train Loss:0.5536236763000488\n",
      "Epoch 0[13834/17270] Time:0.231, Train Loss:0.5232118964195251\n",
      "Epoch 0[13835/17270] Time:0.23, Train Loss:0.7291216850280762\n",
      "Epoch 0[13836/17270] Time:0.245, Train Loss:0.6574727296829224\n",
      "Epoch 0[13837/17270] Time:0.22, Train Loss:1.3392225503921509\n",
      "Epoch 0[13838/17270] Time:0.238, Train Loss:0.4281139075756073\n",
      "Epoch 0[13839/17270] Time:0.239, Train Loss:0.47509926557540894\n",
      "Epoch 0[13840/17270] Time:0.236, Train Loss:0.6162595152854919\n",
      "Epoch 0[13841/17270] Time:0.233, Train Loss:0.30536264181137085\n",
      "Epoch 0[13842/17270] Time:0.236, Train Loss:0.6559476256370544\n",
      "Epoch 0[13843/17270] Time:0.231, Train Loss:0.4483729600906372\n",
      "Epoch 0[13844/17270] Time:0.246, Train Loss:0.6304843425750732\n",
      "Epoch 0[13845/17270] Time:0.222, Train Loss:0.9963947534561157\n",
      "Epoch 0[13846/17270] Time:0.246, Train Loss:0.6471293568611145\n",
      "Epoch 0[13847/17270] Time:0.226, Train Loss:0.33657610416412354\n",
      "Epoch 0[13848/17270] Time:0.234, Train Loss:0.5220473408699036\n",
      "Epoch 0[13849/17270] Time:0.234, Train Loss:0.7118578553199768\n",
      "Epoch 0[13850/17270] Time:0.244, Train Loss:0.4979756474494934\n",
      "Epoch 0[13851/17270] Time:0.24, Train Loss:0.7936062812805176\n",
      "Epoch 0[13852/17270] Time:0.232, Train Loss:0.46815288066864014\n",
      "Epoch 0[13853/17270] Time:0.24, Train Loss:0.3366887867450714\n",
      "Epoch 0[13854/17270] Time:0.237, Train Loss:0.7877031564712524\n",
      "Epoch 0[13855/17270] Time:0.226, Train Loss:0.2851365804672241\n",
      "Epoch 0[13856/17270] Time:0.243, Train Loss:0.5185244679450989\n",
      "Epoch 0[13857/17270] Time:0.232, Train Loss:0.6027482151985168\n",
      "Epoch 0[13858/17270] Time:0.239, Train Loss:0.5308540463447571\n",
      "Epoch 0[13859/17270] Time:0.225, Train Loss:1.4963138103485107\n",
      "Epoch 0[13860/17270] Time:0.228, Train Loss:0.39625084400177\n",
      "Epoch 0[13861/17270] Time:0.235, Train Loss:0.5462130308151245\n",
      "Epoch 0[13862/17270] Time:0.236, Train Loss:0.35624954104423523\n",
      "Epoch 0[13863/17270] Time:0.232, Train Loss:0.9982563853263855\n",
      "Epoch 0[13864/17270] Time:0.231, Train Loss:0.5508568286895752\n",
      "Epoch 0[13865/17270] Time:0.222, Train Loss:0.4971122443675995\n",
      "Epoch 0[13866/17270] Time:0.243, Train Loss:0.3670678734779358\n",
      "Epoch 0[13867/17270] Time:0.22, Train Loss:0.41137346625328064\n",
      "Epoch 0[13868/17270] Time:0.25, Train Loss:0.38832229375839233\n",
      "Epoch 0[13869/17270] Time:0.239, Train Loss:0.4593144953250885\n",
      "Epoch 0[13870/17270] Time:0.25, Train Loss:0.49417147040367126\n",
      "Epoch 0[13871/17270] Time:0.23, Train Loss:0.4960632026195526\n",
      "Epoch 0[13872/17270] Time:0.245, Train Loss:0.6776504516601562\n",
      "Epoch 0[13873/17270] Time:0.238, Train Loss:0.41590869426727295\n",
      "Epoch 0[13874/17270] Time:0.224, Train Loss:0.92759770154953\n",
      "Epoch 0[13875/17270] Time:0.242, Train Loss:0.4030899703502655\n",
      "Epoch 0[13876/17270] Time:0.242, Train Loss:0.7046927809715271\n",
      "Epoch 0[13877/17270] Time:0.24, Train Loss:0.7644484043121338\n",
      "Epoch 0[13878/17270] Time:0.237, Train Loss:0.3858031928539276\n",
      "Epoch 0[13879/17270] Time:0.225, Train Loss:0.4726546108722687\n",
      "Epoch 0[13880/17270] Time:0.234, Train Loss:0.9611561894416809\n",
      "Epoch 0[13881/17270] Time:0.23, Train Loss:0.7506401538848877\n",
      "Epoch 0[13882/17270] Time:0.237, Train Loss:0.6045394539833069\n",
      "Epoch 0[13883/17270] Time:0.221, Train Loss:0.5140969157218933\n",
      "Epoch 0[13884/17270] Time:0.232, Train Loss:0.440397173166275\n",
      "Epoch 0[13885/17270] Time:0.237, Train Loss:0.7097948789596558\n",
      "Epoch 0[13886/17270] Time:0.232, Train Loss:0.9715996384620667\n",
      "Epoch 0[13887/17270] Time:0.231, Train Loss:0.7835577130317688\n",
      "Epoch 0[13888/17270] Time:0.236, Train Loss:0.44203153252601624\n",
      "Epoch 0[13889/17270] Time:0.237, Train Loss:0.6145928502082825\n",
      "Epoch 0[13890/17270] Time:0.234, Train Loss:0.5077685713768005\n",
      "Epoch 0[13891/17270] Time:0.235, Train Loss:0.6755615472793579\n",
      "Epoch 0[13892/17270] Time:0.231, Train Loss:0.476510614156723\n",
      "Epoch 0[13893/17270] Time:0.233, Train Loss:0.5543774962425232\n",
      "Epoch 0[13894/17270] Time:0.233, Train Loss:0.5329440236091614\n",
      "Epoch 0[13895/17270] Time:0.238, Train Loss:0.6836534142494202\n",
      "Epoch 0[13896/17270] Time:0.236, Train Loss:0.4595235586166382\n",
      "Epoch 0[13897/17270] Time:0.232, Train Loss:0.7205959558486938\n",
      "Epoch 0[13898/17270] Time:0.24, Train Loss:0.5827602744102478\n",
      "Epoch 0[13899/17270] Time:0.235, Train Loss:0.7322320938110352\n",
      "Epoch 0[13900/17270] Time:0.233, Train Loss:0.20929716527462006\n",
      "Epoch 0[13901/17270] Time:0.234, Train Loss:0.796138346195221\n",
      "Epoch 0[13902/17270] Time:0.243, Train Loss:0.324042409658432\n",
      "Epoch 0[13903/17270] Time:0.237, Train Loss:0.6822603344917297\n",
      "Epoch 0[13904/17270] Time:0.233, Train Loss:0.5546938180923462\n",
      "Epoch 0[13905/17270] Time:0.239, Train Loss:1.065707802772522\n",
      "Epoch 0[13906/17270] Time:0.23, Train Loss:0.937552273273468\n",
      "Epoch 0[13907/17270] Time:0.24, Train Loss:0.7041828632354736\n",
      "Epoch 0[13908/17270] Time:0.222, Train Loss:0.3584720194339752\n",
      "Epoch 0[13909/17270] Time:0.236, Train Loss:0.5094271898269653\n",
      "Epoch 0[13910/17270] Time:0.223, Train Loss:0.5785850286483765\n",
      "Epoch 0[13911/17270] Time:0.242, Train Loss:0.5161515474319458\n",
      "Epoch 0[13912/17270] Time:0.232, Train Loss:0.3243577182292938\n",
      "Epoch 0[13913/17270] Time:0.232, Train Loss:0.3572932183742523\n",
      "Epoch 0[13914/17270] Time:0.243, Train Loss:0.28542274236679077\n",
      "Epoch 0[13915/17270] Time:0.228, Train Loss:0.5549724102020264\n",
      "Epoch 0[13916/17270] Time:0.243, Train Loss:0.5632917881011963\n",
      "Epoch 0[13917/17270] Time:0.239, Train Loss:0.796961784362793\n",
      "Epoch 0[13918/17270] Time:0.232, Train Loss:0.6722851991653442\n",
      "Epoch 0[13919/17270] Time:0.233, Train Loss:1.0914851427078247\n",
      "Epoch 0[13920/17270] Time:0.237, Train Loss:1.003077507019043\n",
      "Epoch 0[13921/17270] Time:0.227, Train Loss:0.675870954990387\n",
      "Epoch 0[13922/17270] Time:0.237, Train Loss:0.9389423727989197\n",
      "Epoch 0[13923/17270] Time:0.227, Train Loss:0.4520120620727539\n",
      "Epoch 0[13924/17270] Time:0.229, Train Loss:0.3199666142463684\n",
      "Epoch 0[13925/17270] Time:0.231, Train Loss:0.5592301487922668\n",
      "Epoch 0[13926/17270] Time:0.245, Train Loss:0.650590717792511\n",
      "Epoch 0[13927/17270] Time:0.223, Train Loss:0.3496963381767273\n",
      "Epoch 0[13928/17270] Time:0.245, Train Loss:0.45600831508636475\n",
      "Epoch 0[13929/17270] Time:0.224, Train Loss:0.9561006426811218\n",
      "Epoch 0[13930/17270] Time:0.242, Train Loss:0.34592777490615845\n",
      "Epoch 0[13931/17270] Time:0.24, Train Loss:1.2476747035980225\n",
      "Epoch 0[13932/17270] Time:0.224, Train Loss:0.437080055475235\n",
      "Epoch 0[13933/17270] Time:0.245, Train Loss:0.6115488409996033\n",
      "Epoch 0[13934/17270] Time:0.225, Train Loss:0.47334229946136475\n",
      "Epoch 0[13935/17270] Time:0.242, Train Loss:0.7372177243232727\n",
      "Epoch 0[13936/17270] Time:0.236, Train Loss:0.8906812071800232\n",
      "Epoch 0[13937/17270] Time:0.227, Train Loss:0.60896897315979\n",
      "Epoch 0[13938/17270] Time:0.225, Train Loss:0.5959873795509338\n",
      "Epoch 0[13939/17270] Time:0.235, Train Loss:0.6164803504943848\n",
      "Epoch 0[13940/17270] Time:0.257, Train Loss:0.4183763563632965\n",
      "Epoch 0[13941/17270] Time:0.244, Train Loss:0.5274699330329895\n",
      "Epoch 0[13942/17270] Time:0.226, Train Loss:0.599129319190979\n",
      "Epoch 0[13943/17270] Time:0.232, Train Loss:0.46291783452033997\n",
      "Epoch 0[13944/17270] Time:0.254, Train Loss:0.5594841837882996\n",
      "Epoch 0[13945/17270] Time:0.234, Train Loss:0.7134786248207092\n",
      "Epoch 0[13946/17270] Time:0.229, Train Loss:1.687882661819458\n",
      "Epoch 0[13947/17270] Time:0.236, Train Loss:0.4496394693851471\n",
      "Epoch 0[13948/17270] Time:0.23, Train Loss:1.121724009513855\n",
      "Epoch 0[13949/17270] Time:0.221, Train Loss:0.8574405312538147\n",
      "Epoch 0[13950/17270] Time:0.231, Train Loss:0.460281103849411\n",
      "Epoch 0[13951/17270] Time:0.232, Train Loss:0.45311152935028076\n",
      "Epoch 0[13952/17270] Time:0.229, Train Loss:0.752466082572937\n",
      "Epoch 0[13953/17270] Time:0.233, Train Loss:0.43272727727890015\n",
      "Epoch 0[13954/17270] Time:0.24, Train Loss:0.8836024403572083\n",
      "Epoch 0[13955/17270] Time:0.233, Train Loss:0.6913508772850037\n",
      "Epoch 0[13956/17270] Time:0.237, Train Loss:0.6619312763214111\n",
      "Epoch 0[13957/17270] Time:0.244, Train Loss:0.6356592178344727\n",
      "Epoch 0[13958/17270] Time:0.236, Train Loss:0.27924007177352905\n",
      "Epoch 0[13959/17270] Time:0.232, Train Loss:0.7232195138931274\n",
      "Epoch 0[13960/17270] Time:0.232, Train Loss:1.090592861175537\n",
      "Epoch 0[13961/17270] Time:0.236, Train Loss:0.3550775945186615\n",
      "Epoch 0[13962/17270] Time:0.232, Train Loss:1.1552373170852661\n",
      "Epoch 0[13963/17270] Time:0.234, Train Loss:0.3816673159599304\n",
      "Epoch 0[13964/17270] Time:0.237, Train Loss:0.41780605912208557\n",
      "Epoch 0[13965/17270] Time:0.236, Train Loss:0.5638535618782043\n",
      "Epoch 0[13966/17270] Time:0.231, Train Loss:1.1399909257888794\n",
      "Epoch 0[13967/17270] Time:0.234, Train Loss:0.6646031737327576\n",
      "Epoch 0[13968/17270] Time:0.231, Train Loss:0.7331663370132446\n",
      "Epoch 0[13969/17270] Time:0.232, Train Loss:0.7033256888389587\n",
      "Epoch 0[13970/17270] Time:0.237, Train Loss:1.0490120649337769\n",
      "Epoch 0[13971/17270] Time:0.232, Train Loss:0.7882550358772278\n",
      "Epoch 0[13972/17270] Time:0.231, Train Loss:0.4615173041820526\n",
      "Epoch 0[13973/17270] Time:0.232, Train Loss:0.5913553237915039\n",
      "Epoch 0[13974/17270] Time:0.238, Train Loss:0.5281531810760498\n",
      "Epoch 0[13975/17270] Time:0.229, Train Loss:0.6392214894294739\n",
      "Epoch 0[13976/17270] Time:0.235, Train Loss:0.4743713438510895\n",
      "Epoch 0[13977/17270] Time:0.255, Train Loss:0.469907283782959\n",
      "Epoch 0[13978/17270] Time:0.235, Train Loss:0.5327931642532349\n",
      "Epoch 0[13979/17270] Time:0.238, Train Loss:1.131080150604248\n",
      "Epoch 0[13980/17270] Time:0.222, Train Loss:0.9518775939941406\n",
      "Epoch 0[13981/17270] Time:0.245, Train Loss:0.5752604007720947\n",
      "Epoch 0[13982/17270] Time:0.245, Train Loss:1.1278607845306396\n",
      "Epoch 0[13983/17270] Time:0.237, Train Loss:0.4365878999233246\n",
      "Epoch 0[13984/17270] Time:0.235, Train Loss:0.5973227024078369\n",
      "Epoch 0[13985/17270] Time:0.245, Train Loss:0.9685733318328857\n",
      "Epoch 0[13986/17270] Time:0.24, Train Loss:0.6089726686477661\n",
      "Epoch 0[13987/17270] Time:0.23, Train Loss:0.34258487820625305\n",
      "Epoch 0[13988/17270] Time:0.247, Train Loss:0.6258285641670227\n",
      "Epoch 0[13989/17270] Time:0.23, Train Loss:0.3715808093547821\n",
      "Epoch 0[13990/17270] Time:0.232, Train Loss:0.432242751121521\n",
      "Epoch 0[13991/17270] Time:0.242, Train Loss:0.5689894556999207\n",
      "Epoch 0[13992/17270] Time:0.223, Train Loss:0.5232210159301758\n",
      "Epoch 0[13993/17270] Time:0.233, Train Loss:0.48687443137168884\n",
      "Epoch 0[13994/17270] Time:0.234, Train Loss:0.48139095306396484\n",
      "Epoch 0[13995/17270] Time:0.229, Train Loss:0.5011030435562134\n",
      "Epoch 0[13996/17270] Time:0.238, Train Loss:0.4148869216442108\n",
      "Epoch 0[13997/17270] Time:0.232, Train Loss:0.9151062965393066\n",
      "Epoch 0[13998/17270] Time:0.226, Train Loss:0.7096410989761353\n",
      "Epoch 0[13999/17270] Time:0.237, Train Loss:0.5620314478874207\n",
      "Epoch 0[14000/17270] Time:0.239, Train Loss:0.5412352681159973\n",
      "Epoch 0[14001/17270] Time:0.23, Train Loss:0.6430467367172241\n",
      "Epoch 0[14002/17270] Time:0.228, Train Loss:0.428053081035614\n",
      "Epoch 0[14003/17270] Time:0.237, Train Loss:0.5861608982086182\n",
      "Epoch 0[14004/17270] Time:0.238, Train Loss:0.5582908987998962\n",
      "Epoch 0[14005/17270] Time:0.237, Train Loss:0.5905882716178894\n",
      "Epoch 0[14006/17270] Time:0.236, Train Loss:0.5607118010520935\n",
      "Epoch 0[14007/17270] Time:0.231, Train Loss:0.8420124650001526\n",
      "Epoch 0[14008/17270] Time:0.23, Train Loss:0.4461143910884857\n",
      "Epoch 0[14009/17270] Time:0.237, Train Loss:0.4173535406589508\n",
      "Epoch 0[14010/17270] Time:0.238, Train Loss:0.5017330050468445\n",
      "Epoch 0[14011/17270] Time:0.235, Train Loss:0.6039066910743713\n",
      "Epoch 0[14012/17270] Time:0.238, Train Loss:0.43416330218315125\n",
      "Epoch 0[14013/17270] Time:0.228, Train Loss:0.4324907958507538\n",
      "Epoch 0[14014/17270] Time:0.231, Train Loss:0.33823421597480774\n",
      "Epoch 0[14015/17270] Time:0.228, Train Loss:0.7220752835273743\n",
      "Epoch 0[14016/17270] Time:0.227, Train Loss:0.7361547350883484\n",
      "Epoch 0[14017/17270] Time:0.237, Train Loss:0.5227245092391968\n",
      "Epoch 0[14018/17270] Time:0.246, Train Loss:0.742790699005127\n",
      "Epoch 0[14019/17270] Time:0.22, Train Loss:0.5310022234916687\n",
      "Epoch 0[14020/17270] Time:0.232, Train Loss:0.4133193790912628\n",
      "Epoch 0[14021/17270] Time:0.229, Train Loss:0.4075259864330292\n",
      "Epoch 0[14022/17270] Time:0.232, Train Loss:1.7596776485443115\n",
      "Epoch 0[14023/17270] Time:0.238, Train Loss:0.7286542654037476\n",
      "Epoch 0[14024/17270] Time:0.237, Train Loss:0.49060285091400146\n",
      "Epoch 0[14025/17270] Time:0.238, Train Loss:0.5130137801170349\n",
      "Epoch 0[14026/17270] Time:0.238, Train Loss:0.6261709332466125\n",
      "Epoch 0[14027/17270] Time:0.229, Train Loss:0.500304102897644\n",
      "Epoch 0[14028/17270] Time:0.23, Train Loss:0.6140444278717041\n",
      "Epoch 0[14029/17270] Time:0.231, Train Loss:0.5262349843978882\n",
      "Epoch 0[14030/17270] Time:0.237, Train Loss:0.7245404124259949\n",
      "Epoch 0[14031/17270] Time:0.241, Train Loss:0.7716611623764038\n",
      "Epoch 0[14032/17270] Time:0.232, Train Loss:0.6815797686576843\n",
      "Epoch 0[14033/17270] Time:0.23, Train Loss:0.4338974058628082\n",
      "Epoch 0[14034/17270] Time:0.246, Train Loss:0.8135979771614075\n",
      "Epoch 0[14035/17270] Time:0.23, Train Loss:0.4177023768424988\n",
      "Epoch 0[14036/17270] Time:0.247, Train Loss:0.5750620365142822\n",
      "Epoch 0[14037/17270] Time:0.243, Train Loss:0.46298375725746155\n",
      "Epoch 0[14038/17270] Time:0.238, Train Loss:0.778156042098999\n",
      "Epoch 0[14039/17270] Time:0.228, Train Loss:0.815024733543396\n",
      "Epoch 0[14040/17270] Time:0.25, Train Loss:0.7671865820884705\n",
      "Epoch 0[14041/17270] Time:0.237, Train Loss:0.49629291892051697\n",
      "Epoch 0[14042/17270] Time:0.223, Train Loss:0.5178589820861816\n",
      "Epoch 0[14043/17270] Time:0.238, Train Loss:0.41427627205848694\n",
      "Epoch 0[14044/17270] Time:0.244, Train Loss:0.4153321385383606\n",
      "Epoch 0[14045/17270] Time:0.236, Train Loss:0.31538981199264526\n",
      "Epoch 0[14046/17270] Time:0.238, Train Loss:0.45596787333488464\n",
      "Epoch 0[14047/17270] Time:0.222, Train Loss:1.419012188911438\n",
      "Epoch 0[14048/17270] Time:0.248, Train Loss:0.5952121615409851\n",
      "Epoch 0[14049/17270] Time:0.237, Train Loss:0.7155418395996094\n",
      "Epoch 0[14050/17270] Time:0.237, Train Loss:0.49276453256607056\n",
      "Epoch 0[14051/17270] Time:0.234, Train Loss:0.3900848925113678\n",
      "Epoch 0[14052/17270] Time:0.248, Train Loss:0.5682793259620667\n",
      "Epoch 0[14053/17270] Time:0.222, Train Loss:0.4848567843437195\n",
      "Epoch 0[14054/17270] Time:0.24, Train Loss:0.6319181323051453\n",
      "Epoch 0[14055/17270] Time:0.235, Train Loss:0.9512177109718323\n",
      "Epoch 0[14056/17270] Time:0.223, Train Loss:0.7123422622680664\n",
      "Epoch 0[14057/17270] Time:0.231, Train Loss:0.3761693835258484\n",
      "Epoch 0[14058/17270] Time:0.228, Train Loss:0.49136725068092346\n",
      "Epoch 0[14059/17270] Time:0.24, Train Loss:0.4769308567047119\n",
      "Epoch 0[14060/17270] Time:0.227, Train Loss:0.39385345578193665\n",
      "Epoch 0[14061/17270] Time:0.236, Train Loss:0.5096789598464966\n",
      "Epoch 0[14062/17270] Time:0.231, Train Loss:0.2430037260055542\n",
      "Epoch 0[14063/17270] Time:0.234, Train Loss:0.4739927053451538\n",
      "Epoch 0[14064/17270] Time:0.231, Train Loss:0.2736757695674896\n",
      "Epoch 0[14065/17270] Time:0.253, Train Loss:0.5149293541908264\n",
      "Epoch 0[14066/17270] Time:0.227, Train Loss:0.8028402924537659\n",
      "Epoch 0[14067/17270] Time:0.231, Train Loss:0.3746592402458191\n",
      "Epoch 0[14068/17270] Time:0.228, Train Loss:0.4190227687358856\n",
      "Epoch 0[14069/17270] Time:0.239, Train Loss:0.45408502221107483\n",
      "Epoch 0[14070/17270] Time:0.238, Train Loss:0.9925255179405212\n",
      "Epoch 0[14071/17270] Time:0.233, Train Loss:0.9604004621505737\n",
      "Epoch 0[14072/17270] Time:0.237, Train Loss:0.5481659173965454\n",
      "Epoch 0[14073/17270] Time:0.24, Train Loss:0.6267930269241333\n",
      "Epoch 0[14074/17270] Time:0.238, Train Loss:0.5459380149841309\n",
      "Epoch 0[14075/17270] Time:0.232, Train Loss:0.38889145851135254\n",
      "Epoch 0[14076/17270] Time:0.228, Train Loss:0.23288366198539734\n",
      "Epoch 0[14077/17270] Time:0.23, Train Loss:0.5576136708259583\n",
      "Epoch 0[14078/17270] Time:0.23, Train Loss:0.4934525787830353\n",
      "Epoch 0[14079/17270] Time:0.231, Train Loss:0.5289271473884583\n",
      "Epoch 0[14080/17270] Time:0.239, Train Loss:0.30163174867630005\n",
      "Epoch 0[14081/17270] Time:0.229, Train Loss:0.7439637780189514\n",
      "Epoch 0[14082/17270] Time:0.231, Train Loss:1.0927834510803223\n",
      "Epoch 0[14083/17270] Time:0.235, Train Loss:0.6694865226745605\n",
      "Epoch 0[14084/17270] Time:0.236, Train Loss:0.41375574469566345\n",
      "Epoch 0[14085/17270] Time:0.239, Train Loss:0.2750084102153778\n",
      "Epoch 0[14086/17270] Time:0.231, Train Loss:0.3694409728050232\n",
      "Epoch 0[14087/17270] Time:0.234, Train Loss:0.4596105217933655\n",
      "Epoch 0[14088/17270] Time:0.237, Train Loss:0.4772410988807678\n",
      "Epoch 0[14089/17270] Time:0.24, Train Loss:0.7008417248725891\n",
      "Epoch 0[14090/17270] Time:0.238, Train Loss:0.48480406403541565\n",
      "Epoch 0[14091/17270] Time:0.235, Train Loss:0.4113825559616089\n",
      "Epoch 0[14092/17270] Time:0.223, Train Loss:0.28155800700187683\n",
      "Epoch 0[14093/17270] Time:0.234, Train Loss:1.1645816564559937\n",
      "Epoch 0[14094/17270] Time:0.233, Train Loss:0.3745138943195343\n",
      "Epoch 0[14095/17270] Time:0.233, Train Loss:0.8959189057350159\n",
      "Epoch 0[14096/17270] Time:0.234, Train Loss:0.8578321933746338\n",
      "Epoch 0[14097/17270] Time:0.239, Train Loss:0.472000390291214\n",
      "Epoch 0[14098/17270] Time:0.241, Train Loss:0.7911812663078308\n",
      "Epoch 0[14099/17270] Time:0.238, Train Loss:0.5347474217414856\n",
      "Epoch 0[14100/17270] Time:0.247, Train Loss:0.8531433343887329\n",
      "Epoch 0[14101/17270] Time:0.249, Train Loss:0.8682792782783508\n",
      "Epoch 0[14102/17270] Time:0.235, Train Loss:0.5183357000350952\n",
      "Epoch 0[14103/17270] Time:0.23, Train Loss:0.7869585752487183\n",
      "Epoch 0[14104/17270] Time:0.23, Train Loss:0.8809734582901001\n",
      "Epoch 0[14105/17270] Time:0.232, Train Loss:0.8512635827064514\n",
      "Epoch 0[14106/17270] Time:0.235, Train Loss:0.7623242735862732\n",
      "Epoch 0[14107/17270] Time:0.235, Train Loss:0.551732063293457\n",
      "Epoch 0[14108/17270] Time:0.235, Train Loss:0.629499614238739\n",
      "Epoch 0[14109/17270] Time:0.238, Train Loss:0.7240205407142639\n",
      "Epoch 0[14110/17270] Time:0.231, Train Loss:0.5735492706298828\n",
      "Epoch 0[14111/17270] Time:0.237, Train Loss:0.510566234588623\n",
      "Epoch 0[14112/17270] Time:0.238, Train Loss:0.5400282144546509\n",
      "Epoch 0[14113/17270] Time:0.235, Train Loss:0.8090393543243408\n",
      "Epoch 0[14114/17270] Time:0.231, Train Loss:0.38531988859176636\n",
      "Epoch 0[14115/17270] Time:0.239, Train Loss:0.3612084686756134\n",
      "Epoch 0[14116/17270] Time:0.237, Train Loss:0.4829601049423218\n",
      "Epoch 0[14117/17270] Time:0.231, Train Loss:0.44132721424102783\n",
      "Epoch 0[14118/17270] Time:0.232, Train Loss:0.6098282337188721\n",
      "Epoch 0[14119/17270] Time:0.229, Train Loss:0.3099666237831116\n",
      "Epoch 0[14120/17270] Time:0.239, Train Loss:0.5907740592956543\n",
      "Epoch 0[14121/17270] Time:0.255, Train Loss:0.49366870522499084\n",
      "Epoch 0[14122/17270] Time:0.237, Train Loss:0.5977627635002136\n",
      "Epoch 0[14123/17270] Time:0.22, Train Loss:0.591850221157074\n",
      "Epoch 0[14124/17270] Time:0.226, Train Loss:0.548098623752594\n",
      "Epoch 0[14125/17270] Time:0.237, Train Loss:0.4276285469532013\n",
      "Epoch 0[14126/17270] Time:0.237, Train Loss:0.45299264788627625\n",
      "Epoch 0[14127/17270] Time:0.23, Train Loss:0.7297284603118896\n",
      "Epoch 0[14128/17270] Time:0.238, Train Loss:0.5993832349777222\n",
      "Epoch 0[14129/17270] Time:0.242, Train Loss:0.391134649515152\n",
      "Epoch 0[14130/17270] Time:0.247, Train Loss:0.68675696849823\n",
      "Epoch 0[14131/17270] Time:0.225, Train Loss:0.6024029850959778\n",
      "Epoch 0[14132/17270] Time:0.242, Train Loss:1.07113778591156\n",
      "Epoch 0[14133/17270] Time:0.251, Train Loss:0.47697755694389343\n",
      "Epoch 0[14134/17270] Time:0.221, Train Loss:0.48983699083328247\n",
      "Epoch 0[14135/17270] Time:0.238, Train Loss:0.8850012421607971\n",
      "Epoch 0[14136/17270] Time:0.236, Train Loss:0.42829516530036926\n",
      "Epoch 0[14137/17270] Time:0.236, Train Loss:0.3332218527793884\n",
      "Epoch 0[14138/17270] Time:0.228, Train Loss:0.8975695371627808\n",
      "Epoch 0[14139/17270] Time:0.235, Train Loss:0.7478348016738892\n",
      "Epoch 0[14140/17270] Time:0.24, Train Loss:0.8586775064468384\n",
      "Epoch 0[14141/17270] Time:0.231, Train Loss:0.7713186740875244\n",
      "Epoch 0[14142/17270] Time:0.227, Train Loss:0.455748587846756\n",
      "Epoch 0[14143/17270] Time:0.24, Train Loss:0.6372781991958618\n",
      "Epoch 0[14144/17270] Time:0.235, Train Loss:0.33556851744651794\n",
      "Epoch 0[14145/17270] Time:0.233, Train Loss:1.0309433937072754\n",
      "Epoch 0[14146/17270] Time:0.238, Train Loss:0.5964609980583191\n",
      "Epoch 0[14147/17270] Time:0.232, Train Loss:0.7977925539016724\n",
      "Epoch 0[14148/17270] Time:0.233, Train Loss:0.5280364155769348\n",
      "Epoch 0[14149/17270] Time:0.233, Train Loss:0.47394439578056335\n",
      "Epoch 0[14150/17270] Time:0.224, Train Loss:0.3600691854953766\n",
      "Epoch 0[14151/17270] Time:0.232, Train Loss:0.44230103492736816\n",
      "Epoch 0[14152/17270] Time:0.246, Train Loss:0.48301926255226135\n",
      "Epoch 0[14153/17270] Time:0.241, Train Loss:0.4776916801929474\n",
      "Epoch 0[14154/17270] Time:0.23, Train Loss:0.6452943086624146\n",
      "Epoch 0[14155/17270] Time:0.229, Train Loss:0.5362999439239502\n",
      "Epoch 0[14156/17270] Time:0.231, Train Loss:0.46899017691612244\n",
      "Epoch 0[14157/17270] Time:0.242, Train Loss:0.9295680522918701\n",
      "Epoch 0[14158/17270] Time:0.225, Train Loss:0.6140672564506531\n",
      "Epoch 0[14159/17270] Time:0.268, Train Loss:0.5318537950515747\n",
      "Epoch 0[14160/17270] Time:0.226, Train Loss:0.37224477529525757\n",
      "Epoch 0[14161/17270] Time:0.235, Train Loss:1.34432053565979\n",
      "Epoch 0[14162/17270] Time:0.235, Train Loss:0.5308616161346436\n",
      "Epoch 0[14163/17270] Time:0.239, Train Loss:0.5879920125007629\n",
      "Epoch 0[14164/17270] Time:0.234, Train Loss:0.6789547801017761\n",
      "Epoch 0[14165/17270] Time:0.234, Train Loss:0.38604703545570374\n",
      "Epoch 0[14166/17270] Time:0.237, Train Loss:0.48043254017829895\n",
      "Epoch 0[14167/17270] Time:0.234, Train Loss:0.5813098549842834\n",
      "Epoch 0[14168/17270] Time:0.232, Train Loss:0.47507214546203613\n",
      "Epoch 0[14169/17270] Time:0.235, Train Loss:0.45365285873413086\n",
      "Epoch 0[14170/17270] Time:0.235, Train Loss:0.6344914436340332\n",
      "Epoch 0[14171/17270] Time:0.23, Train Loss:0.4807857573032379\n",
      "Epoch 0[14172/17270] Time:0.237, Train Loss:0.4298146069049835\n",
      "Epoch 0[14173/17270] Time:0.237, Train Loss:0.37038952112197876\n",
      "Epoch 0[14174/17270] Time:0.229, Train Loss:0.5651922225952148\n",
      "Epoch 0[14175/17270] Time:0.241, Train Loss:0.3704187273979187\n",
      "Epoch 0[14176/17270] Time:0.245, Train Loss:0.37464186549186707\n",
      "Epoch 0[14177/17270] Time:0.225, Train Loss:0.5000405311584473\n",
      "Epoch 0[14178/17270] Time:0.231, Train Loss:0.5493703484535217\n",
      "Epoch 0[14179/17270] Time:0.226, Train Loss:0.599265992641449\n",
      "Epoch 0[14180/17270] Time:0.237, Train Loss:0.5790458917617798\n",
      "Epoch 0[14181/17270] Time:0.237, Train Loss:0.45403027534484863\n",
      "Epoch 0[14182/17270] Time:0.235, Train Loss:1.4981449842453003\n",
      "Epoch 0[14183/17270] Time:0.237, Train Loss:1.58185875415802\n",
      "Epoch 0[14184/17270] Time:0.236, Train Loss:0.5590164661407471\n",
      "Epoch 0[14185/17270] Time:0.231, Train Loss:1.3801356554031372\n",
      "Epoch 0[14186/17270] Time:0.23, Train Loss:0.4505481421947479\n",
      "Epoch 0[14187/17270] Time:0.234, Train Loss:0.43808263540267944\n",
      "Epoch 0[14188/17270] Time:0.232, Train Loss:0.42056843638420105\n",
      "Epoch 0[14189/17270] Time:0.233, Train Loss:0.5337221026420593\n",
      "Epoch 0[14190/17270] Time:0.235, Train Loss:0.5166969895362854\n",
      "Epoch 0[14191/17270] Time:0.238, Train Loss:0.5649245977401733\n",
      "Epoch 0[14192/17270] Time:0.238, Train Loss:0.5352250933647156\n",
      "Epoch 0[14193/17270] Time:0.235, Train Loss:0.47799691557884216\n",
      "Epoch 0[14194/17270] Time:0.239, Train Loss:0.954334020614624\n",
      "Epoch 0[14195/17270] Time:0.218, Train Loss:0.5115278363227844\n",
      "Epoch 0[14196/17270] Time:0.26, Train Loss:0.9591758847236633\n",
      "Epoch 0[14197/17270] Time:0.219, Train Loss:0.3756183385848999\n",
      "Epoch 0[14198/17270] Time:0.248, Train Loss:0.5917878746986389\n",
      "Epoch 0[14199/17270] Time:0.232, Train Loss:0.5686163306236267\n",
      "Epoch 0[14200/17270] Time:0.224, Train Loss:0.6520345211029053\n",
      "Epoch 0[14201/17270] Time:0.241, Train Loss:0.44739794731140137\n",
      "Epoch 0[14202/17270] Time:0.242, Train Loss:0.622431755065918\n",
      "Epoch 0[14203/17270] Time:0.233, Train Loss:0.504349410533905\n",
      "Epoch 0[14204/17270] Time:0.225, Train Loss:0.5022789239883423\n",
      "Epoch 0[14205/17270] Time:0.24, Train Loss:0.4682813584804535\n",
      "Epoch 0[14206/17270] Time:0.241, Train Loss:0.7454400062561035\n",
      "Epoch 0[14207/17270] Time:0.238, Train Loss:1.2264950275421143\n",
      "Epoch 0[14208/17270] Time:0.233, Train Loss:0.6001759171485901\n",
      "Epoch 0[14209/17270] Time:0.222, Train Loss:0.6154335141181946\n",
      "Epoch 0[14210/17270] Time:0.238, Train Loss:0.7272893190383911\n",
      "Epoch 0[14211/17270] Time:0.232, Train Loss:0.5944100618362427\n",
      "Epoch 0[14212/17270] Time:0.244, Train Loss:0.41103360056877136\n",
      "Epoch 0[14213/17270] Time:0.239, Train Loss:1.2043898105621338\n",
      "Epoch 0[14214/17270] Time:0.239, Train Loss:0.3834454417228699\n",
      "Epoch 0[14215/17270] Time:0.23, Train Loss:0.41712486743927\n",
      "Epoch 0[14216/17270] Time:0.237, Train Loss:0.8298448324203491\n",
      "Epoch 0[14217/17270] Time:0.238, Train Loss:0.5713518857955933\n",
      "Epoch 0[14218/17270] Time:0.227, Train Loss:0.8802067637443542\n",
      "Epoch 0[14219/17270] Time:0.232, Train Loss:0.37536633014678955\n",
      "Epoch 0[14220/17270] Time:0.246, Train Loss:0.31100448966026306\n",
      "Epoch 0[14221/17270] Time:0.234, Train Loss:0.5000758767127991\n",
      "Epoch 0[14222/17270] Time:0.224, Train Loss:0.30263465642929077\n",
      "Epoch 0[14223/17270] Time:0.245, Train Loss:0.8827905058860779\n",
      "Epoch 0[14224/17270] Time:0.231, Train Loss:0.4677591919898987\n",
      "Epoch 0[14225/17270] Time:0.233, Train Loss:0.7276664972305298\n",
      "Epoch 0[14226/17270] Time:0.221, Train Loss:0.5976690053939819\n",
      "Epoch 0[14227/17270] Time:0.228, Train Loss:0.7029674053192139\n",
      "Epoch 0[14228/17270] Time:0.229, Train Loss:0.3827863037586212\n",
      "Epoch 0[14229/17270] Time:0.254, Train Loss:0.7159460186958313\n",
      "Epoch 0[14230/17270] Time:0.227, Train Loss:1.1763492822647095\n",
      "Epoch 0[14231/17270] Time:0.233, Train Loss:0.7509279251098633\n",
      "Epoch 0[14232/17270] Time:0.227, Train Loss:0.615087628364563\n",
      "Epoch 0[14233/17270] Time:0.238, Train Loss:0.7966758608818054\n",
      "Epoch 0[14234/17270] Time:0.231, Train Loss:0.31006330251693726\n",
      "Epoch 0[14235/17270] Time:0.244, Train Loss:0.7782502770423889\n",
      "Epoch 0[14236/17270] Time:0.24, Train Loss:0.6263659596443176\n",
      "Epoch 0[14237/17270] Time:0.237, Train Loss:0.4915260374546051\n",
      "Epoch 0[14238/17270] Time:0.228, Train Loss:0.39506325125694275\n",
      "Epoch 0[14239/17270] Time:0.231, Train Loss:0.5519299507141113\n",
      "Epoch 0[14240/17270] Time:0.231, Train Loss:0.33946532011032104\n",
      "Epoch 0[14241/17270] Time:0.233, Train Loss:0.42795324325561523\n",
      "Epoch 0[14242/17270] Time:0.239, Train Loss:0.6580806374549866\n",
      "Epoch 0[14243/17270] Time:0.237, Train Loss:0.38292887806892395\n",
      "Epoch 0[14244/17270] Time:0.238, Train Loss:0.7111992239952087\n",
      "Epoch 0[14245/17270] Time:0.251, Train Loss:0.8235407471656799\n",
      "Epoch 0[14246/17270] Time:0.238, Train Loss:0.5882827043533325\n",
      "Epoch 0[14247/17270] Time:0.231, Train Loss:0.5240886211395264\n",
      "Epoch 0[14248/17270] Time:0.234, Train Loss:0.5686066746711731\n",
      "Epoch 0[14249/17270] Time:0.248, Train Loss:0.5176517963409424\n",
      "Epoch 0[14250/17270] Time:0.224, Train Loss:0.48794791102409363\n",
      "Epoch 0[14251/17270] Time:0.24, Train Loss:0.6060959100723267\n",
      "Epoch 0[14252/17270] Time:0.225, Train Loss:1.0093470811843872\n",
      "Epoch 0[14253/17270] Time:0.235, Train Loss:0.636023759841919\n",
      "Epoch 0[14254/17270] Time:0.239, Train Loss:0.5139330625534058\n",
      "Epoch 0[14255/17270] Time:0.228, Train Loss:0.6516610980033875\n",
      "Epoch 0[14256/17270] Time:0.239, Train Loss:0.5072610974311829\n",
      "Epoch 0[14257/17270] Time:0.231, Train Loss:0.38371047377586365\n",
      "Epoch 0[14258/17270] Time:0.234, Train Loss:0.3938002586364746\n",
      "Epoch 0[14259/17270] Time:0.235, Train Loss:0.2930307686328888\n",
      "Epoch 0[14260/17270] Time:0.238, Train Loss:0.4688296914100647\n",
      "Epoch 0[14261/17270] Time:0.236, Train Loss:0.6240524053573608\n",
      "Epoch 0[14262/17270] Time:0.238, Train Loss:0.8956219553947449\n",
      "Epoch 0[14263/17270] Time:0.24, Train Loss:0.6056996583938599\n",
      "Epoch 0[14264/17270] Time:0.233, Train Loss:0.3795434534549713\n",
      "Epoch 0[14265/17270] Time:0.238, Train Loss:0.514891505241394\n",
      "Epoch 0[14266/17270] Time:0.249, Train Loss:0.5674530863761902\n",
      "Epoch 0[14267/17270] Time:0.227, Train Loss:0.4426462948322296\n",
      "Epoch 0[14268/17270] Time:0.237, Train Loss:0.4233965277671814\n",
      "Epoch 0[14269/17270] Time:0.236, Train Loss:0.3156719207763672\n",
      "Epoch 0[14270/17270] Time:0.234, Train Loss:0.3945316672325134\n",
      "Epoch 0[14271/17270] Time:0.233, Train Loss:0.4524785876274109\n",
      "Epoch 0[14272/17270] Time:0.235, Train Loss:1.0056759119033813\n",
      "Epoch 0[14273/17270] Time:0.231, Train Loss:0.32287198305130005\n",
      "Epoch 0[14274/17270] Time:0.234, Train Loss:0.5828337669372559\n",
      "Epoch 0[14275/17270] Time:0.234, Train Loss:0.7547341585159302\n",
      "Epoch 0[14276/17270] Time:0.233, Train Loss:0.5552173256874084\n",
      "Epoch 0[14277/17270] Time:0.238, Train Loss:1.4290952682495117\n",
      "Epoch 0[14278/17270] Time:0.242, Train Loss:0.5582137703895569\n",
      "Epoch 0[14279/17270] Time:0.24, Train Loss:0.5504363179206848\n",
      "Epoch 0[14280/17270] Time:0.238, Train Loss:0.3893808424472809\n",
      "Epoch 0[14281/17270] Time:0.235, Train Loss:0.463565468788147\n",
      "Epoch 0[14282/17270] Time:0.231, Train Loss:1.0888575315475464\n",
      "Epoch 0[14283/17270] Time:0.232, Train Loss:0.5594912767410278\n",
      "Epoch 0[14284/17270] Time:0.229, Train Loss:0.7072274684906006\n",
      "Epoch 0[14285/17270] Time:0.24, Train Loss:0.3731476366519928\n",
      "Epoch 0[14286/17270] Time:0.231, Train Loss:0.35533398389816284\n",
      "Epoch 0[14287/17270] Time:0.232, Train Loss:0.5763016939163208\n",
      "Epoch 0[14288/17270] Time:0.239, Train Loss:0.7920160889625549\n",
      "Epoch 0[14289/17270] Time:0.24, Train Loss:0.3741045892238617\n",
      "Epoch 0[14290/17270] Time:0.239, Train Loss:0.3826015293598175\n",
      "Epoch 0[14291/17270] Time:0.238, Train Loss:0.3120066225528717\n",
      "Epoch 0[14292/17270] Time:0.234, Train Loss:0.29541999101638794\n",
      "Epoch 0[14293/17270] Time:0.234, Train Loss:1.0342133045196533\n",
      "Epoch 0[14294/17270] Time:0.224, Train Loss:0.34722551703453064\n",
      "Epoch 0[14295/17270] Time:0.23, Train Loss:0.2841792404651642\n",
      "Epoch 0[14296/17270] Time:0.239, Train Loss:0.5736428499221802\n",
      "Epoch 0[14297/17270] Time:0.239, Train Loss:0.5452447533607483\n",
      "Epoch 0[14298/17270] Time:0.227, Train Loss:0.5896539688110352\n",
      "Epoch 0[14299/17270] Time:0.242, Train Loss:0.8717612624168396\n",
      "Epoch 0[14300/17270] Time:0.234, Train Loss:0.44054049253463745\n",
      "Epoch 0[14301/17270] Time:0.233, Train Loss:0.35542845726013184\n",
      "Epoch 0[14302/17270] Time:0.236, Train Loss:0.47721928358078003\n",
      "Epoch 0[14303/17270] Time:0.222, Train Loss:0.8474546074867249\n",
      "Epoch 0[14304/17270] Time:0.237, Train Loss:0.3457314670085907\n",
      "Epoch 0[14305/17270] Time:0.241, Train Loss:0.3547947406768799\n",
      "Epoch 0[14306/17270] Time:0.235, Train Loss:0.9007447361946106\n",
      "Epoch 0[14307/17270] Time:0.237, Train Loss:0.4555087983608246\n",
      "Epoch 0[14308/17270] Time:0.236, Train Loss:0.6249560713768005\n",
      "Epoch 0[14309/17270] Time:0.233, Train Loss:0.2362634688615799\n",
      "Epoch 0[14310/17270] Time:0.232, Train Loss:0.20453622937202454\n",
      "Epoch 0[14311/17270] Time:0.233, Train Loss:1.0293056964874268\n",
      "Epoch 0[14312/17270] Time:0.231, Train Loss:0.5459519028663635\n",
      "Epoch 0[14313/17270] Time:0.237, Train Loss:0.2510796785354614\n",
      "Epoch 0[14314/17270] Time:0.232, Train Loss:0.561850905418396\n",
      "Epoch 0[14315/17270] Time:0.233, Train Loss:0.40996137261390686\n",
      "Epoch 0[14316/17270] Time:0.232, Train Loss:0.5883448719978333\n",
      "Epoch 0[14317/17270] Time:0.235, Train Loss:0.8474256992340088\n",
      "Epoch 0[14318/17270] Time:0.234, Train Loss:0.3355427384376526\n",
      "Epoch 0[14319/17270] Time:0.233, Train Loss:0.3433862328529358\n",
      "Epoch 0[14320/17270] Time:0.238, Train Loss:0.5634539127349854\n",
      "Epoch 0[14321/17270] Time:0.233, Train Loss:0.5549559593200684\n",
      "Epoch 0[14322/17270] Time:0.233, Train Loss:0.7443107962608337\n",
      "Epoch 0[14323/17270] Time:0.233, Train Loss:0.8200305104255676\n",
      "Epoch 0[14324/17270] Time:0.233, Train Loss:0.5409629344940186\n",
      "Epoch 0[14325/17270] Time:0.233, Train Loss:0.27900874614715576\n",
      "Epoch 0[14326/17270] Time:0.232, Train Loss:0.6074265837669373\n",
      "Epoch 0[14327/17270] Time:0.233, Train Loss:0.5542652606964111\n",
      "Epoch 0[14328/17270] Time:0.233, Train Loss:0.6397971510887146\n",
      "Epoch 0[14329/17270] Time:0.235, Train Loss:0.5206015110015869\n",
      "Epoch 0[14330/17270] Time:0.237, Train Loss:0.5879552960395813\n",
      "Epoch 0[14331/17270] Time:0.236, Train Loss:1.1974730491638184\n",
      "Epoch 0[14332/17270] Time:0.239, Train Loss:0.3858683407306671\n",
      "Epoch 0[14333/17270] Time:0.237, Train Loss:0.5765868425369263\n",
      "Epoch 0[14334/17270] Time:0.228, Train Loss:0.5070257186889648\n",
      "Epoch 0[14335/17270] Time:0.228, Train Loss:0.5049276351928711\n",
      "Epoch 0[14336/17270] Time:0.23, Train Loss:0.3427644968032837\n",
      "Epoch 0[14337/17270] Time:0.233, Train Loss:0.47081446647644043\n",
      "Epoch 0[14338/17270] Time:0.235, Train Loss:0.43129095435142517\n",
      "Epoch 0[14339/17270] Time:0.234, Train Loss:0.7226735949516296\n",
      "Epoch 0[14340/17270] Time:0.236, Train Loss:0.4138348400592804\n",
      "Epoch 0[14341/17270] Time:0.232, Train Loss:0.40061917901039124\n",
      "Epoch 0[14342/17270] Time:0.236, Train Loss:0.7054864168167114\n",
      "Epoch 0[14343/17270] Time:0.235, Train Loss:0.34930795431137085\n",
      "Epoch 0[14344/17270] Time:0.236, Train Loss:0.5067627429962158\n",
      "Epoch 0[14345/17270] Time:0.234, Train Loss:0.7293282151222229\n",
      "Epoch 0[14346/17270] Time:0.232, Train Loss:0.4650236964225769\n",
      "Epoch 0[14347/17270] Time:0.249, Train Loss:0.6635449528694153\n",
      "Epoch 0[14348/17270] Time:0.24, Train Loss:0.7225373983383179\n",
      "Epoch 0[14349/17270] Time:0.246, Train Loss:0.30347734689712524\n",
      "Epoch 0[14350/17270] Time:0.237, Train Loss:0.7467374801635742\n",
      "Epoch 0[14351/17270] Time:0.225, Train Loss:0.42501702904701233\n",
      "Epoch 0[14352/17270] Time:0.24, Train Loss:0.5550309419631958\n",
      "Epoch 0[14353/17270] Time:0.236, Train Loss:0.6614261269569397\n",
      "Epoch 0[14354/17270] Time:0.229, Train Loss:0.5308868885040283\n",
      "Epoch 0[14355/17270] Time:0.233, Train Loss:0.7436022758483887\n",
      "Epoch 0[14356/17270] Time:0.23, Train Loss:0.3751540184020996\n",
      "Epoch 0[14357/17270] Time:0.246, Train Loss:0.4953039586544037\n",
      "Epoch 0[14358/17270] Time:0.226, Train Loss:0.5804261565208435\n",
      "Epoch 0[14359/17270] Time:0.233, Train Loss:0.7029753923416138\n",
      "Epoch 0[14360/17270] Time:0.24, Train Loss:0.5044540762901306\n",
      "Epoch 0[14361/17270] Time:0.238, Train Loss:0.36239495873451233\n",
      "Epoch 0[14362/17270] Time:0.239, Train Loss:0.6285199522972107\n",
      "Epoch 0[14363/17270] Time:0.223, Train Loss:0.6690843105316162\n",
      "Epoch 0[14364/17270] Time:0.234, Train Loss:0.4213907718658447\n",
      "Epoch 0[14365/17270] Time:0.23, Train Loss:0.30943048000335693\n",
      "Epoch 0[14366/17270] Time:0.235, Train Loss:0.33304107189178467\n",
      "Epoch 0[14367/17270] Time:0.228, Train Loss:0.5932379364967346\n",
      "Epoch 0[14368/17270] Time:0.235, Train Loss:0.4643174111843109\n",
      "Epoch 0[14369/17270] Time:0.237, Train Loss:0.5100821852684021\n",
      "Epoch 0[14370/17270] Time:0.236, Train Loss:0.9499302506446838\n",
      "Epoch 0[14371/17270] Time:0.234, Train Loss:0.9863244295120239\n",
      "Epoch 0[14372/17270] Time:0.233, Train Loss:0.23390550911426544\n",
      "Epoch 0[14373/17270] Time:0.235, Train Loss:0.5433787107467651\n",
      "Epoch 0[14374/17270] Time:0.237, Train Loss:0.3936814069747925\n",
      "Epoch 0[14375/17270] Time:0.233, Train Loss:0.5290923714637756\n",
      "Epoch 0[14376/17270] Time:0.225, Train Loss:0.3882426917552948\n",
      "Epoch 0[14377/17270] Time:0.244, Train Loss:0.42127105593681335\n",
      "Epoch 0[14378/17270] Time:0.236, Train Loss:0.3968087136745453\n",
      "Epoch 0[14379/17270] Time:0.234, Train Loss:0.8837950229644775\n",
      "Epoch 0[14380/17270] Time:0.249, Train Loss:0.4514494240283966\n",
      "Epoch 0[14381/17270] Time:0.226, Train Loss:0.5161747336387634\n",
      "Epoch 0[14382/17270] Time:0.232, Train Loss:0.8905765414237976\n",
      "Epoch 0[14383/17270] Time:0.231, Train Loss:0.6408013105392456\n",
      "Epoch 0[14384/17270] Time:0.227, Train Loss:0.6819725632667542\n",
      "Epoch 0[14385/17270] Time:0.233, Train Loss:0.6499707698822021\n",
      "Epoch 0[14386/17270] Time:0.226, Train Loss:0.4101782739162445\n",
      "Epoch 0[14387/17270] Time:0.227, Train Loss:0.5782771706581116\n",
      "Epoch 0[14388/17270] Time:0.236, Train Loss:0.5647215247154236\n",
      "Epoch 0[14389/17270] Time:0.232, Train Loss:0.5410174131393433\n",
      "Epoch 0[14390/17270] Time:0.229, Train Loss:0.5181756615638733\n",
      "Epoch 0[14391/17270] Time:0.226, Train Loss:0.5186168551445007\n",
      "Epoch 0[14392/17270] Time:0.231, Train Loss:0.2749797999858856\n",
      "Epoch 0[14393/17270] Time:0.236, Train Loss:0.5240774154663086\n",
      "Epoch 0[14394/17270] Time:0.236, Train Loss:0.23964954912662506\n",
      "Epoch 0[14395/17270] Time:0.229, Train Loss:0.5676460266113281\n",
      "Epoch 0[14396/17270] Time:0.229, Train Loss:0.3872043490409851\n",
      "Epoch 0[14397/17270] Time:0.229, Train Loss:0.6349990367889404\n",
      "Epoch 0[14398/17270] Time:0.23, Train Loss:0.7885035872459412\n",
      "Epoch 0[14399/17270] Time:0.228, Train Loss:0.6027609705924988\n",
      "Epoch 0[14400/17270] Time:0.227, Train Loss:0.5778470635414124\n",
      "Epoch 0[14401/17270] Time:0.237, Train Loss:0.6304521560668945\n",
      "Epoch 0[14402/17270] Time:0.236, Train Loss:0.5922145843505859\n",
      "Epoch 0[14403/17270] Time:0.235, Train Loss:0.8500835299491882\n",
      "Epoch 0[14404/17270] Time:0.231, Train Loss:0.6061254143714905\n",
      "Epoch 0[14405/17270] Time:0.241, Train Loss:0.5071061849594116\n",
      "Epoch 0[14406/17270] Time:0.225, Train Loss:0.7889305949211121\n",
      "Epoch 0[14407/17270] Time:0.239, Train Loss:0.6023208498954773\n",
      "Epoch 0[14408/17270] Time:0.233, Train Loss:0.5691130757331848\n",
      "Epoch 0[14409/17270] Time:0.243, Train Loss:0.5728493332862854\n",
      "Epoch 0[14410/17270] Time:0.233, Train Loss:0.3073022663593292\n",
      "Epoch 0[14411/17270] Time:0.237, Train Loss:0.8703266382217407\n",
      "Epoch 0[14412/17270] Time:0.232, Train Loss:0.5838714241981506\n",
      "Epoch 0[14413/17270] Time:0.23, Train Loss:0.6204269528388977\n",
      "Epoch 0[14414/17270] Time:0.239, Train Loss:0.5678701996803284\n",
      "Epoch 0[14415/17270] Time:0.242, Train Loss:0.4097650647163391\n",
      "Epoch 0[14416/17270] Time:0.236, Train Loss:0.8648266792297363\n",
      "Epoch 0[14417/17270] Time:0.245, Train Loss:1.5622756481170654\n",
      "Epoch 0[14418/17270] Time:0.235, Train Loss:0.4663155674934387\n",
      "Epoch 0[14419/17270] Time:0.237, Train Loss:1.0966463088989258\n",
      "Epoch 0[14420/17270] Time:0.236, Train Loss:0.7327103614807129\n",
      "Epoch 0[14421/17270] Time:0.237, Train Loss:0.3953479826450348\n",
      "Epoch 0[14422/17270] Time:0.228, Train Loss:0.8926764726638794\n",
      "Epoch 0[14423/17270] Time:0.233, Train Loss:0.3698968291282654\n",
      "Epoch 0[14424/17270] Time:0.236, Train Loss:0.45976006984710693\n",
      "Epoch 0[14425/17270] Time:0.234, Train Loss:0.46841925382614136\n",
      "Epoch 0[14426/17270] Time:0.237, Train Loss:0.43196696043014526\n",
      "Epoch 0[14427/17270] Time:0.23, Train Loss:0.5677096843719482\n",
      "Epoch 0[14428/17270] Time:0.228, Train Loss:0.46307623386383057\n",
      "Epoch 0[14429/17270] Time:0.228, Train Loss:0.7115819454193115\n",
      "Epoch 0[14430/17270] Time:0.227, Train Loss:0.2888203561306\n",
      "Epoch 0[14431/17270] Time:0.237, Train Loss:0.32750996947288513\n",
      "Epoch 0[14432/17270] Time:0.23, Train Loss:0.4755855202674866\n",
      "Epoch 0[14433/17270] Time:0.23, Train Loss:0.6826861500740051\n",
      "Epoch 0[14434/17270] Time:0.232, Train Loss:0.697957456111908\n",
      "Epoch 0[14435/17270] Time:0.231, Train Loss:0.3873254954814911\n",
      "Epoch 0[14436/17270] Time:0.242, Train Loss:0.45228704810142517\n",
      "Epoch 0[14437/17270] Time:0.233, Train Loss:0.5260440111160278\n",
      "Epoch 0[14438/17270] Time:0.232, Train Loss:0.29461362957954407\n",
      "Epoch 0[14439/17270] Time:0.227, Train Loss:0.8170757293701172\n",
      "Epoch 0[14440/17270] Time:0.228, Train Loss:0.4774829149246216\n",
      "Epoch 0[14441/17270] Time:0.23, Train Loss:0.31285905838012695\n",
      "Epoch 0[14442/17270] Time:0.234, Train Loss:0.6725891828536987\n",
      "Epoch 0[14443/17270] Time:0.224, Train Loss:0.6061993837356567\n",
      "Epoch 0[14444/17270] Time:0.232, Train Loss:0.9755811095237732\n",
      "Epoch 0[14445/17270] Time:0.236, Train Loss:0.5774460434913635\n",
      "Epoch 0[14446/17270] Time:0.237, Train Loss:0.7105482816696167\n",
      "Epoch 0[14447/17270] Time:0.239, Train Loss:0.2282637655735016\n",
      "Epoch 0[14448/17270] Time:0.228, Train Loss:0.6280310750007629\n",
      "Epoch 0[14449/17270] Time:0.23, Train Loss:0.3858349919319153\n",
      "Epoch 0[14450/17270] Time:0.239, Train Loss:0.42989447712898254\n",
      "Epoch 0[14451/17270] Time:0.226, Train Loss:1.2801141738891602\n",
      "Epoch 0[14452/17270] Time:0.23, Train Loss:0.7864730358123779\n",
      "Epoch 0[14453/17270] Time:0.227, Train Loss:0.545161247253418\n",
      "Epoch 0[14454/17270] Time:0.235, Train Loss:0.4314267039299011\n",
      "Epoch 0[14455/17270] Time:0.235, Train Loss:0.47915521264076233\n",
      "Epoch 0[14456/17270] Time:0.238, Train Loss:0.5636609792709351\n",
      "Epoch 0[14457/17270] Time:0.225, Train Loss:0.3007338345050812\n",
      "Epoch 0[14458/17270] Time:0.237, Train Loss:1.2074238061904907\n",
      "Epoch 0[14459/17270] Time:0.23, Train Loss:0.48127859830856323\n",
      "Epoch 0[14460/17270] Time:0.239, Train Loss:0.4788786768913269\n",
      "Epoch 0[14461/17270] Time:0.229, Train Loss:1.2222000360488892\n",
      "Epoch 0[14462/17270] Time:0.26, Train Loss:1.2016839981079102\n",
      "Epoch 0[14463/17270] Time:0.231, Train Loss:0.5428755283355713\n",
      "Epoch 0[14464/17270] Time:0.232, Train Loss:1.5346492528915405\n",
      "Epoch 0[14465/17270] Time:0.24, Train Loss:1.0087212324142456\n",
      "Epoch 0[14466/17270] Time:0.236, Train Loss:0.446517676115036\n",
      "Epoch 0[14467/17270] Time:0.222, Train Loss:0.4699300229549408\n",
      "Epoch 0[14468/17270] Time:0.238, Train Loss:0.529654860496521\n",
      "Epoch 0[14469/17270] Time:0.236, Train Loss:0.5503877401351929\n",
      "Epoch 0[14470/17270] Time:0.225, Train Loss:0.816512942314148\n",
      "Epoch 0[14471/17270] Time:0.245, Train Loss:0.6014978885650635\n",
      "Epoch 0[14472/17270] Time:0.234, Train Loss:0.5922926068305969\n",
      "Epoch 0[14473/17270] Time:0.231, Train Loss:0.532575786113739\n",
      "Epoch 0[14474/17270] Time:0.233, Train Loss:0.4905627369880676\n",
      "Epoch 0[14475/17270] Time:0.227, Train Loss:0.7144178748130798\n",
      "Epoch 0[14476/17270] Time:0.239, Train Loss:0.5745059847831726\n",
      "Epoch 0[14477/17270] Time:0.256, Train Loss:0.47625240683555603\n",
      "Epoch 0[14478/17270] Time:0.233, Train Loss:0.5677482485771179\n",
      "Epoch 0[14479/17270] Time:0.22, Train Loss:0.5311619639396667\n",
      "Epoch 0[14480/17270] Time:0.25, Train Loss:0.5819583535194397\n",
      "Epoch 0[14481/17270] Time:0.221, Train Loss:0.5897688269615173\n",
      "Epoch 0[14482/17270] Time:0.235, Train Loss:0.48317044973373413\n",
      "Epoch 0[14483/17270] Time:0.247, Train Loss:0.6446148753166199\n",
      "Epoch 0[14484/17270] Time:0.244, Train Loss:0.6233774423599243\n",
      "Epoch 0[14485/17270] Time:0.239, Train Loss:0.4766843020915985\n",
      "Epoch 0[14486/17270] Time:0.227, Train Loss:0.30420318245887756\n",
      "Epoch 0[14487/17270] Time:0.231, Train Loss:0.3069506287574768\n",
      "Epoch 0[14488/17270] Time:0.237, Train Loss:0.364917129278183\n",
      "Epoch 0[14489/17270] Time:0.235, Train Loss:0.3413711488246918\n",
      "Epoch 0[14490/17270] Time:0.238, Train Loss:0.7372359037399292\n",
      "Epoch 0[14491/17270] Time:0.229, Train Loss:0.361483633518219\n",
      "Epoch 0[14492/17270] Time:0.238, Train Loss:0.9509515762329102\n",
      "Epoch 0[14493/17270] Time:0.238, Train Loss:0.3104252219200134\n",
      "Epoch 0[14494/17270] Time:0.229, Train Loss:0.30422547459602356\n",
      "Epoch 0[14495/17270] Time:0.245, Train Loss:0.38139140605926514\n",
      "Epoch 0[14496/17270] Time:0.24, Train Loss:0.4893099069595337\n",
      "Epoch 0[14497/17270] Time:0.242, Train Loss:0.3197389841079712\n",
      "Epoch 0[14498/17270] Time:0.225, Train Loss:0.6183705925941467\n",
      "Epoch 0[14499/17270] Time:0.244, Train Loss:1.0116236209869385\n",
      "Epoch 0[14500/17270] Time:0.239, Train Loss:0.48925307393074036\n",
      "Epoch 0[14501/17270] Time:0.234, Train Loss:0.38444483280181885\n",
      "Epoch 0[14502/17270] Time:0.238, Train Loss:0.6826854944229126\n",
      "Epoch 0[14503/17270] Time:0.234, Train Loss:0.6851851344108582\n",
      "Epoch 0[14504/17270] Time:0.224, Train Loss:0.5007474422454834\n",
      "Epoch 0[14505/17270] Time:0.23, Train Loss:0.3841754198074341\n",
      "Epoch 0[14506/17270] Time:0.228, Train Loss:0.29057785868644714\n",
      "Epoch 0[14507/17270] Time:0.231, Train Loss:0.2815087139606476\n",
      "Epoch 0[14508/17270] Time:0.23, Train Loss:0.4721309542655945\n",
      "Epoch 0[14509/17270] Time:0.226, Train Loss:0.4680228531360626\n",
      "Epoch 0[14510/17270] Time:0.227, Train Loss:0.47902053594589233\n",
      "Epoch 0[14511/17270] Time:0.234, Train Loss:0.5327997803688049\n",
      "Epoch 0[14512/17270] Time:0.241, Train Loss:0.8225852251052856\n",
      "Epoch 0[14513/17270] Time:0.24, Train Loss:0.2636370062828064\n",
      "Epoch 0[14514/17270] Time:0.242, Train Loss:0.271865576505661\n",
      "Epoch 0[14515/17270] Time:0.217, Train Loss:0.39607685804367065\n",
      "Epoch 0[14516/17270] Time:0.242, Train Loss:1.0152928829193115\n",
      "Epoch 0[14517/17270] Time:0.25, Train Loss:0.45223212242126465\n",
      "Epoch 0[14518/17270] Time:0.234, Train Loss:0.5737335085868835\n",
      "Epoch 0[14519/17270] Time:0.245, Train Loss:0.7275331020355225\n",
      "Epoch 0[14520/17270] Time:0.236, Train Loss:1.3498774766921997\n",
      "Epoch 0[14521/17270] Time:0.236, Train Loss:0.6397552490234375\n",
      "Epoch 0[14522/17270] Time:0.232, Train Loss:0.5514352321624756\n",
      "Epoch 0[14523/17270] Time:0.233, Train Loss:0.4661882221698761\n",
      "Epoch 0[14524/17270] Time:0.23, Train Loss:0.35008347034454346\n",
      "Epoch 0[14525/17270] Time:0.238, Train Loss:0.4351362884044647\n",
      "Epoch 0[14526/17270] Time:0.229, Train Loss:0.4024113416671753\n",
      "Epoch 0[14527/17270] Time:0.231, Train Loss:0.32323676347732544\n",
      "Epoch 0[14528/17270] Time:0.231, Train Loss:0.4096492528915405\n",
      "Epoch 0[14529/17270] Time:0.236, Train Loss:0.7301271557807922\n",
      "Epoch 0[14530/17270] Time:0.241, Train Loss:0.4366065561771393\n",
      "Epoch 0[14531/17270] Time:0.225, Train Loss:0.6420518755912781\n",
      "Epoch 0[14532/17270] Time:0.233, Train Loss:0.650294840335846\n",
      "Epoch 0[14533/17270] Time:0.231, Train Loss:0.4272043704986572\n",
      "Epoch 0[14534/17270] Time:0.241, Train Loss:0.4457266628742218\n",
      "Epoch 0[14535/17270] Time:0.228, Train Loss:0.4157712459564209\n",
      "Epoch 0[14536/17270] Time:0.228, Train Loss:0.45387372374534607\n",
      "Epoch 0[14537/17270] Time:0.231, Train Loss:0.37378624081611633\n",
      "Epoch 0[14538/17270] Time:0.231, Train Loss:0.7634534239768982\n",
      "Epoch 0[14539/17270] Time:0.239, Train Loss:0.38178715109825134\n",
      "Epoch 0[14540/17270] Time:0.232, Train Loss:0.48658227920532227\n",
      "Epoch 0[14541/17270] Time:0.227, Train Loss:0.4526901841163635\n",
      "Epoch 0[14542/17270] Time:0.229, Train Loss:0.7923173904418945\n",
      "Epoch 0[14543/17270] Time:0.227, Train Loss:0.32449230551719666\n",
      "Epoch 0[14544/17270] Time:0.229, Train Loss:0.6723507046699524\n",
      "Epoch 0[14545/17270] Time:0.234, Train Loss:0.2081361562013626\n",
      "Epoch 0[14546/17270] Time:0.233, Train Loss:0.3028079569339752\n",
      "Epoch 0[14547/17270] Time:0.236, Train Loss:0.35992133617401123\n",
      "Epoch 0[14548/17270] Time:0.225, Train Loss:0.35391730070114136\n",
      "Epoch 0[14549/17270] Time:0.226, Train Loss:0.5275523662567139\n",
      "Epoch 0[14550/17270] Time:0.23, Train Loss:0.47778692841529846\n",
      "Epoch 0[14551/17270] Time:0.238, Train Loss:0.5280891060829163\n",
      "Epoch 0[14552/17270] Time:0.237, Train Loss:0.5497121214866638\n",
      "Epoch 0[14553/17270] Time:0.23, Train Loss:0.5192773938179016\n",
      "Epoch 0[14554/17270] Time:0.235, Train Loss:0.471153199672699\n",
      "Epoch 0[14555/17270] Time:0.238, Train Loss:1.0731782913208008\n",
      "Epoch 0[14556/17270] Time:0.229, Train Loss:0.3863038718700409\n",
      "Epoch 0[14557/17270] Time:0.233, Train Loss:0.3117039203643799\n",
      "Epoch 0[14558/17270] Time:0.231, Train Loss:0.40229883790016174\n",
      "Epoch 0[14559/17270] Time:0.234, Train Loss:0.575717568397522\n",
      "Epoch 0[14560/17270] Time:0.232, Train Loss:0.6167254447937012\n",
      "Epoch 0[14561/17270] Time:0.235, Train Loss:0.6109297871589661\n",
      "Epoch 0[14562/17270] Time:0.237, Train Loss:0.7313964366912842\n",
      "Epoch 0[14563/17270] Time:0.23, Train Loss:0.4670160710811615\n",
      "Epoch 0[14564/17270] Time:0.228, Train Loss:0.4566374123096466\n",
      "Epoch 0[14565/17270] Time:0.228, Train Loss:0.2989921569824219\n",
      "Epoch 0[14566/17270] Time:0.237, Train Loss:1.3550300598144531\n",
      "Epoch 0[14567/17270] Time:0.238, Train Loss:0.6825956702232361\n",
      "Epoch 0[14568/17270] Time:0.235, Train Loss:0.49323564767837524\n",
      "Epoch 0[14569/17270] Time:0.231, Train Loss:0.5472748875617981\n",
      "Epoch 0[14570/17270] Time:0.236, Train Loss:0.6005101203918457\n",
      "Epoch 0[14571/17270] Time:0.229, Train Loss:1.014874815940857\n",
      "Epoch 0[14572/17270] Time:0.23, Train Loss:0.7122681736946106\n",
      "Epoch 0[14573/17270] Time:0.231, Train Loss:0.8419474363327026\n",
      "Epoch 0[14574/17270] Time:0.232, Train Loss:0.5192745923995972\n",
      "Epoch 0[14575/17270] Time:0.24, Train Loss:0.34805768728256226\n",
      "Epoch 0[14576/17270] Time:0.23, Train Loss:0.5102193355560303\n",
      "Epoch 0[14577/17270] Time:0.233, Train Loss:0.18670062720775604\n",
      "Epoch 0[14578/17270] Time:0.222, Train Loss:0.32596978545188904\n",
      "Epoch 0[14579/17270] Time:0.251, Train Loss:0.376632422208786\n",
      "Epoch 0[14580/17270] Time:0.232, Train Loss:0.7937309741973877\n",
      "Epoch 0[14581/17270] Time:0.249, Train Loss:0.2791828513145447\n",
      "Epoch 0[14582/17270] Time:0.23, Train Loss:0.6275327801704407\n",
      "Epoch 0[14583/17270] Time:0.221, Train Loss:0.8582915663719177\n",
      "Epoch 0[14584/17270] Time:0.238, Train Loss:0.5280546545982361\n",
      "Epoch 0[14585/17270] Time:0.226, Train Loss:0.5187949538230896\n",
      "Epoch 0[14586/17270] Time:0.242, Train Loss:0.35980328917503357\n",
      "Epoch 0[14587/17270] Time:0.238, Train Loss:0.86376953125\n",
      "Epoch 0[14588/17270] Time:0.23, Train Loss:0.6467199325561523\n",
      "Epoch 0[14589/17270] Time:0.231, Train Loss:0.8602300882339478\n",
      "Epoch 0[14590/17270] Time:0.245, Train Loss:0.6109127998352051\n",
      "Epoch 0[14591/17270] Time:0.241, Train Loss:0.3898361623287201\n",
      "Epoch 0[14592/17270] Time:0.234, Train Loss:0.4755622148513794\n",
      "Epoch 0[14593/17270] Time:0.224, Train Loss:0.502698540687561\n",
      "Epoch 0[14594/17270] Time:0.245, Train Loss:0.868161141872406\n",
      "Epoch 0[14595/17270] Time:0.237, Train Loss:1.04672372341156\n",
      "Epoch 0[14596/17270] Time:0.226, Train Loss:0.3166097104549408\n",
      "Epoch 0[14597/17270] Time:0.239, Train Loss:0.692646324634552\n",
      "Epoch 0[14598/17270] Time:0.235, Train Loss:0.6642082929611206\n",
      "Epoch 0[14599/17270] Time:0.236, Train Loss:0.6855261921882629\n",
      "Epoch 0[14600/17270] Time:0.238, Train Loss:0.7799600958824158\n",
      "Epoch 0[14601/17270] Time:0.235, Train Loss:0.6323977112770081\n",
      "Epoch 0[14602/17270] Time:0.226, Train Loss:0.3841036260128021\n",
      "Epoch 0[14603/17270] Time:0.243, Train Loss:0.3822150230407715\n",
      "Epoch 0[14604/17270] Time:0.236, Train Loss:1.1351423263549805\n",
      "Epoch 0[14605/17270] Time:0.233, Train Loss:1.120065689086914\n",
      "Epoch 0[14606/17270] Time:0.231, Train Loss:0.5538960099220276\n",
      "Epoch 0[14607/17270] Time:0.24, Train Loss:0.5605000853538513\n",
      "Epoch 0[14608/17270] Time:0.238, Train Loss:0.38139814138412476\n",
      "Epoch 0[14609/17270] Time:0.247, Train Loss:0.4211277961730957\n",
      "Epoch 0[14610/17270] Time:0.242, Train Loss:0.5236607789993286\n",
      "Epoch 0[14611/17270] Time:0.234, Train Loss:0.32535791397094727\n",
      "Epoch 0[14612/17270] Time:0.241, Train Loss:0.5341755151748657\n",
      "Epoch 0[14613/17270] Time:0.243, Train Loss:0.6223078966140747\n",
      "Epoch 0[14614/17270] Time:0.242, Train Loss:1.270851492881775\n",
      "Epoch 0[14615/17270] Time:0.241, Train Loss:0.49791496992111206\n",
      "Epoch 0[14616/17270] Time:0.238, Train Loss:0.4203985333442688\n",
      "Epoch 0[14617/17270] Time:0.234, Train Loss:0.5147264003753662\n",
      "Epoch 0[14618/17270] Time:0.24, Train Loss:0.5324628949165344\n",
      "Epoch 0[14619/17270] Time:0.235, Train Loss:0.7306169867515564\n",
      "Epoch 0[14620/17270] Time:0.248, Train Loss:0.6264654994010925\n",
      "Epoch 0[14621/17270] Time:0.251, Train Loss:0.3662121295928955\n",
      "Epoch 0[14622/17270] Time:0.232, Train Loss:0.24324826896190643\n",
      "Epoch 0[14623/17270] Time:0.238, Train Loss:0.47740912437438965\n",
      "Epoch 0[14624/17270] Time:0.239, Train Loss:0.5323469042778015\n",
      "Epoch 0[14625/17270] Time:0.231, Train Loss:0.7880975008010864\n",
      "Epoch 0[14626/17270] Time:0.237, Train Loss:0.32655319571495056\n",
      "Epoch 0[14627/17270] Time:0.235, Train Loss:0.51649010181427\n",
      "Epoch 0[14628/17270] Time:0.249, Train Loss:0.592953622341156\n",
      "Epoch 0[14629/17270] Time:0.235, Train Loss:0.7058829665184021\n",
      "Epoch 0[14630/17270] Time:0.238, Train Loss:0.5436678528785706\n",
      "Epoch 0[14631/17270] Time:0.243, Train Loss:0.5062215924263\n",
      "Epoch 0[14632/17270] Time:0.238, Train Loss:0.34756720066070557\n",
      "Epoch 0[14633/17270] Time:0.245, Train Loss:0.6361313462257385\n",
      "Epoch 0[14634/17270] Time:0.238, Train Loss:0.5123881101608276\n",
      "Epoch 0[14635/17270] Time:0.238, Train Loss:0.7886823415756226\n",
      "Epoch 0[14636/17270] Time:0.234, Train Loss:0.35598310828208923\n",
      "Epoch 0[14637/17270] Time:0.234, Train Loss:0.5202154517173767\n",
      "Epoch 0[14638/17270] Time:0.231, Train Loss:0.6820712089538574\n",
      "Epoch 0[14639/17270] Time:0.23, Train Loss:0.4417521059513092\n",
      "Epoch 0[14640/17270] Time:0.228, Train Loss:0.5966129899024963\n",
      "Epoch 0[14641/17270] Time:0.232, Train Loss:0.2831403911113739\n",
      "Epoch 0[14642/17270] Time:0.248, Train Loss:0.470781534910202\n",
      "Epoch 0[14643/17270] Time:0.234, Train Loss:0.7227469682693481\n",
      "Epoch 0[14644/17270] Time:0.235, Train Loss:0.32274410128593445\n",
      "Epoch 0[14645/17270] Time:0.245, Train Loss:0.33086928725242615\n",
      "Epoch 0[14646/17270] Time:0.228, Train Loss:0.5674952268600464\n",
      "Epoch 0[14647/17270] Time:0.231, Train Loss:0.8613766431808472\n",
      "Epoch 0[14648/17270] Time:0.231, Train Loss:0.7210522890090942\n",
      "Epoch 0[14649/17270] Time:0.234, Train Loss:0.5179452300071716\n",
      "Epoch 0[14650/17270] Time:0.24, Train Loss:0.34173592925071716\n",
      "Epoch 0[14651/17270] Time:0.234, Train Loss:1.0549116134643555\n",
      "Epoch 0[14652/17270] Time:0.228, Train Loss:0.3492680788040161\n",
      "Epoch 0[14653/17270] Time:0.237, Train Loss:0.40965813398361206\n",
      "Epoch 0[14654/17270] Time:0.232, Train Loss:0.2760666310787201\n",
      "Epoch 0[14655/17270] Time:0.24, Train Loss:0.5589740872383118\n",
      "Epoch 0[14656/17270] Time:0.227, Train Loss:0.4340769946575165\n",
      "Epoch 0[14657/17270] Time:0.237, Train Loss:0.8030915856361389\n",
      "Epoch 0[14658/17270] Time:0.235, Train Loss:0.75650954246521\n",
      "Epoch 0[14659/17270] Time:0.233, Train Loss:0.49959975481033325\n",
      "Epoch 0[14660/17270] Time:0.224, Train Loss:0.45569097995758057\n",
      "Epoch 0[14661/17270] Time:0.238, Train Loss:0.5391227602958679\n",
      "Epoch 0[14662/17270] Time:0.225, Train Loss:0.7512569427490234\n",
      "Epoch 0[14663/17270] Time:0.241, Train Loss:0.4380180835723877\n",
      "Epoch 0[14664/17270] Time:0.236, Train Loss:0.2392314225435257\n",
      "Epoch 0[14665/17270] Time:0.232, Train Loss:0.34946954250335693\n",
      "Epoch 0[14666/17270] Time:0.229, Train Loss:0.27193546295166016\n",
      "Epoch 0[14667/17270] Time:0.229, Train Loss:0.39536815881729126\n",
      "Epoch 0[14668/17270] Time:0.228, Train Loss:0.25018879771232605\n",
      "Epoch 0[14669/17270] Time:0.23, Train Loss:0.3428456485271454\n",
      "Epoch 0[14670/17270] Time:0.234, Train Loss:1.245809555053711\n",
      "Epoch 0[14671/17270] Time:0.236, Train Loss:0.3431193232536316\n",
      "Epoch 0[14672/17270] Time:0.228, Train Loss:0.7656254768371582\n",
      "Epoch 0[14673/17270] Time:0.238, Train Loss:0.7044360637664795\n",
      "Epoch 0[14674/17270] Time:0.235, Train Loss:0.35179051756858826\n",
      "Epoch 0[14675/17270] Time:0.246, Train Loss:0.6071799397468567\n",
      "Epoch 0[14676/17270] Time:0.231, Train Loss:0.5090329051017761\n",
      "Epoch 0[14677/17270] Time:0.239, Train Loss:0.5569970607757568\n",
      "Epoch 0[14678/17270] Time:0.232, Train Loss:0.3914947807788849\n",
      "Epoch 0[14679/17270] Time:0.23, Train Loss:0.6076949238777161\n",
      "Epoch 0[14680/17270] Time:0.238, Train Loss:0.4233959913253784\n",
      "Epoch 0[14681/17270] Time:0.231, Train Loss:0.4242675304412842\n",
      "Epoch 0[14682/17270] Time:0.234, Train Loss:0.5355834364891052\n",
      "Epoch 0[14683/17270] Time:0.234, Train Loss:0.49986472725868225\n",
      "Epoch 0[14684/17270] Time:0.224, Train Loss:1.5554274320602417\n",
      "Epoch 0[14685/17270] Time:0.243, Train Loss:0.4624500274658203\n",
      "Epoch 0[14686/17270] Time:0.238, Train Loss:0.28698888421058655\n",
      "Epoch 0[14687/17270] Time:0.222, Train Loss:0.4196259081363678\n",
      "Epoch 0[14688/17270] Time:0.233, Train Loss:0.47099119424819946\n",
      "Epoch 0[14689/17270] Time:0.246, Train Loss:0.9995133280754089\n",
      "Epoch 0[14690/17270] Time:0.238, Train Loss:0.5428391098976135\n",
      "Epoch 0[14691/17270] Time:0.228, Train Loss:0.6251975297927856\n",
      "Epoch 0[14692/17270] Time:0.238, Train Loss:0.6659911274909973\n",
      "Epoch 0[14693/17270] Time:0.226, Train Loss:0.31240949034690857\n",
      "Epoch 0[14694/17270] Time:0.242, Train Loss:0.5112687945365906\n",
      "Epoch 0[14695/17270] Time:0.233, Train Loss:1.4576923847198486\n",
      "Epoch 0[14696/17270] Time:0.222, Train Loss:0.4053368866443634\n",
      "Epoch 0[14697/17270] Time:0.241, Train Loss:0.4986705780029297\n",
      "Epoch 0[14698/17270] Time:0.235, Train Loss:0.5230624675750732\n",
      "Epoch 0[14699/17270] Time:0.232, Train Loss:0.3070574700832367\n",
      "Epoch 0[14700/17270] Time:0.234, Train Loss:0.6052794456481934\n",
      "Epoch 0[14701/17270] Time:0.247, Train Loss:0.4283809959888458\n",
      "Epoch 0[14702/17270] Time:0.236, Train Loss:0.2337084859609604\n",
      "Epoch 0[14703/17270] Time:0.236, Train Loss:0.5802825093269348\n",
      "Epoch 0[14704/17270] Time:0.241, Train Loss:0.28518787026405334\n",
      "Epoch 0[14705/17270] Time:0.242, Train Loss:0.5186105966567993\n",
      "Epoch 0[14706/17270] Time:0.242, Train Loss:0.41472336649894714\n",
      "Epoch 0[14707/17270] Time:0.243, Train Loss:0.5218966603279114\n",
      "Epoch 0[14708/17270] Time:0.235, Train Loss:0.5255835056304932\n",
      "Epoch 0[14709/17270] Time:0.238, Train Loss:0.46896499395370483\n",
      "Epoch 0[14710/17270] Time:0.234, Train Loss:0.2894830107688904\n",
      "Epoch 0[14711/17270] Time:0.232, Train Loss:0.7608224749565125\n",
      "Epoch 0[14712/17270] Time:0.245, Train Loss:0.3216756284236908\n",
      "Epoch 0[14713/17270] Time:0.227, Train Loss:0.5944421291351318\n",
      "Epoch 0[14714/17270] Time:0.241, Train Loss:0.4947161078453064\n",
      "Epoch 0[14715/17270] Time:0.246, Train Loss:0.4427715539932251\n",
      "Epoch 0[14716/17270] Time:0.229, Train Loss:0.3456898331642151\n",
      "Epoch 0[14717/17270] Time:0.232, Train Loss:0.5763052701950073\n",
      "Epoch 0[14718/17270] Time:0.232, Train Loss:0.5133730173110962\n",
      "Epoch 0[14719/17270] Time:0.245, Train Loss:0.2594608962535858\n",
      "Epoch 0[14720/17270] Time:0.23, Train Loss:0.3025974631309509\n",
      "Epoch 0[14721/17270] Time:0.232, Train Loss:0.5519642233848572\n",
      "Epoch 0[14722/17270] Time:0.235, Train Loss:0.35648444294929504\n",
      "Epoch 0[14723/17270] Time:0.225, Train Loss:1.1618132591247559\n",
      "Epoch 0[14724/17270] Time:0.246, Train Loss:0.1863517314195633\n",
      "Epoch 0[14725/17270] Time:0.233, Train Loss:0.42577412724494934\n",
      "Epoch 0[14726/17270] Time:0.235, Train Loss:0.8322093486785889\n",
      "Epoch 0[14727/17270] Time:0.233, Train Loss:0.35664257407188416\n",
      "Epoch 0[14728/17270] Time:0.219, Train Loss:0.5647116303443909\n",
      "Epoch 0[14729/17270] Time:0.247, Train Loss:0.3622564375400543\n",
      "Epoch 0[14730/17270] Time:0.23, Train Loss:0.8344948887825012\n",
      "Epoch 0[14731/17270] Time:0.238, Train Loss:0.5230423212051392\n",
      "Epoch 0[14732/17270] Time:0.233, Train Loss:0.4206836521625519\n",
      "Epoch 0[14733/17270] Time:0.225, Train Loss:0.3069893717765808\n",
      "Epoch 0[14734/17270] Time:0.235, Train Loss:0.44013309478759766\n",
      "Epoch 0[14735/17270] Time:0.243, Train Loss:0.37680530548095703\n",
      "Epoch 0[14736/17270] Time:0.228, Train Loss:0.42652565240859985\n",
      "Epoch 0[14737/17270] Time:0.23, Train Loss:0.6553098559379578\n",
      "Epoch 0[14738/17270] Time:0.231, Train Loss:0.5153657793998718\n",
      "Epoch 0[14739/17270] Time:0.231, Train Loss:0.46839791536331177\n",
      "Epoch 0[14740/17270] Time:0.231, Train Loss:0.4449106752872467\n",
      "Epoch 0[14741/17270] Time:0.243, Train Loss:0.4300203025341034\n",
      "Epoch 0[14742/17270] Time:0.222, Train Loss:1.157989740371704\n",
      "Epoch 0[14743/17270] Time:0.235, Train Loss:0.3820343613624573\n",
      "Epoch 0[14744/17270] Time:0.24, Train Loss:0.5471351146697998\n",
      "Epoch 0[14745/17270] Time:0.244, Train Loss:0.4301297962665558\n",
      "Epoch 0[14746/17270] Time:0.239, Train Loss:0.4450385570526123\n",
      "Epoch 0[14747/17270] Time:0.227, Train Loss:0.27743980288505554\n",
      "Epoch 0[14748/17270] Time:0.242, Train Loss:0.262652188539505\n",
      "Epoch 0[14749/17270] Time:0.231, Train Loss:0.5886335968971252\n",
      "Epoch 0[14750/17270] Time:0.226, Train Loss:1.2359256744384766\n",
      "Epoch 0[14751/17270] Time:0.23, Train Loss:0.28965672850608826\n",
      "Epoch 0[14752/17270] Time:0.25, Train Loss:0.22603213787078857\n",
      "Epoch 0[14753/17270] Time:0.229, Train Loss:0.5317284464836121\n",
      "Epoch 0[14754/17270] Time:0.247, Train Loss:0.7022236585617065\n",
      "Epoch 0[14755/17270] Time:0.239, Train Loss:0.9945504069328308\n",
      "Epoch 0[14756/17270] Time:0.223, Train Loss:0.6033185720443726\n",
      "Epoch 0[14757/17270] Time:0.239, Train Loss:0.32871943712234497\n",
      "Epoch 0[14758/17270] Time:0.248, Train Loss:0.8302090764045715\n",
      "Epoch 0[14759/17270] Time:0.245, Train Loss:2.049496650695801\n",
      "Epoch 0[14760/17270] Time:0.228, Train Loss:0.5629265308380127\n",
      "Epoch 0[14761/17270] Time:0.237, Train Loss:0.49347954988479614\n",
      "Epoch 0[14762/17270] Time:0.245, Train Loss:0.5672198534011841\n",
      "Epoch 0[14763/17270] Time:0.239, Train Loss:0.6705283522605896\n",
      "Epoch 0[14764/17270] Time:0.238, Train Loss:0.30690494179725647\n",
      "Epoch 0[14765/17270] Time:0.238, Train Loss:0.6831813454627991\n",
      "Epoch 0[14766/17270] Time:0.238, Train Loss:0.4409765303134918\n",
      "Epoch 0[14767/17270] Time:0.236, Train Loss:0.5154366493225098\n",
      "Epoch 0[14768/17270] Time:0.237, Train Loss:0.3295738101005554\n",
      "Epoch 0[14769/17270] Time:0.237, Train Loss:0.551483690738678\n",
      "Epoch 0[14770/17270] Time:0.223, Train Loss:0.9040343761444092\n",
      "Epoch 0[14771/17270] Time:0.229, Train Loss:0.7287270426750183\n",
      "Epoch 0[14772/17270] Time:0.234, Train Loss:0.2967131435871124\n",
      "Epoch 0[14773/17270] Time:0.239, Train Loss:0.41034144163131714\n",
      "Epoch 0[14774/17270] Time:0.233, Train Loss:0.4623663127422333\n",
      "Epoch 0[14775/17270] Time:0.25, Train Loss:0.44678595662117004\n",
      "Epoch 0[14776/17270] Time:0.237, Train Loss:0.38881146907806396\n",
      "Epoch 0[14777/17270] Time:0.223, Train Loss:0.5883752703666687\n",
      "Epoch 0[14778/17270] Time:0.247, Train Loss:0.4268934428691864\n",
      "Epoch 0[14779/17270] Time:0.225, Train Loss:0.4597528278827667\n",
      "Epoch 0[14780/17270] Time:0.245, Train Loss:0.38593924045562744\n",
      "Epoch 0[14781/17270] Time:0.235, Train Loss:0.3129621148109436\n",
      "Epoch 0[14782/17270] Time:0.234, Train Loss:0.3706102669239044\n",
      "Epoch 0[14783/17270] Time:0.232, Train Loss:0.8042342066764832\n",
      "Epoch 0[14784/17270] Time:0.238, Train Loss:0.4428522288799286\n",
      "Epoch 0[14785/17270] Time:0.228, Train Loss:0.4799826741218567\n",
      "Epoch 0[14786/17270] Time:0.234, Train Loss:1.0180637836456299\n",
      "Epoch 0[14787/17270] Time:0.25, Train Loss:1.0456209182739258\n",
      "Epoch 0[14788/17270] Time:0.237, Train Loss:0.30088022351264954\n",
      "Epoch 0[14789/17270] Time:0.237, Train Loss:0.5916006565093994\n",
      "Epoch 0[14790/17270] Time:0.235, Train Loss:0.5166308283805847\n",
      "Epoch 0[14791/17270] Time:0.231, Train Loss:0.5496495962142944\n",
      "Epoch 0[14792/17270] Time:0.235, Train Loss:0.6221969723701477\n",
      "Epoch 0[14793/17270] Time:0.236, Train Loss:0.501461386680603\n",
      "Epoch 0[14794/17270] Time:0.241, Train Loss:0.4388718605041504\n",
      "Epoch 0[14795/17270] Time:0.235, Train Loss:0.4186926484107971\n",
      "Epoch 0[14796/17270] Time:0.251, Train Loss:0.5027855038642883\n",
      "Epoch 0[14797/17270] Time:0.24, Train Loss:0.7633994817733765\n",
      "Epoch 0[14798/17270] Time:0.239, Train Loss:1.0582702159881592\n",
      "Epoch 0[14799/17270] Time:0.232, Train Loss:0.6968147158622742\n",
      "Epoch 0[14800/17270] Time:0.237, Train Loss:0.9431219696998596\n",
      "Epoch 0[14801/17270] Time:0.23, Train Loss:0.4909478724002838\n",
      "Epoch 0[14802/17270] Time:0.226, Train Loss:0.7567914128303528\n",
      "Epoch 0[14803/17270] Time:0.236, Train Loss:0.4594781994819641\n",
      "Epoch 0[14804/17270] Time:0.236, Train Loss:0.2911154627799988\n",
      "Epoch 0[14805/17270] Time:0.226, Train Loss:0.7368274927139282\n",
      "Epoch 0[14806/17270] Time:0.231, Train Loss:0.45247188210487366\n",
      "Epoch 0[14807/17270] Time:0.23, Train Loss:0.7556439638137817\n",
      "Epoch 0[14808/17270] Time:0.226, Train Loss:0.6574081778526306\n",
      "Epoch 0[14809/17270] Time:0.239, Train Loss:0.4536343216896057\n",
      "Epoch 0[14810/17270] Time:0.229, Train Loss:0.6406168341636658\n",
      "Epoch 0[14811/17270] Time:0.233, Train Loss:0.9350935816764832\n",
      "Epoch 0[14812/17270] Time:0.231, Train Loss:0.5741864442825317\n",
      "Epoch 0[14813/17270] Time:0.237, Train Loss:0.35999587178230286\n",
      "Epoch 0[14814/17270] Time:0.229, Train Loss:0.3424348533153534\n",
      "Epoch 0[14815/17270] Time:0.232, Train Loss:0.5624284148216248\n",
      "Epoch 0[14816/17270] Time:0.229, Train Loss:0.4323827028274536\n",
      "Epoch 0[14817/17270] Time:0.229, Train Loss:0.549925684928894\n",
      "Epoch 0[14818/17270] Time:0.23, Train Loss:0.5907050371170044\n",
      "Epoch 0[14819/17270] Time:0.231, Train Loss:0.7450495958328247\n",
      "Epoch 0[14820/17270] Time:0.234, Train Loss:1.266559362411499\n",
      "Epoch 0[14821/17270] Time:0.232, Train Loss:0.2355111986398697\n",
      "Epoch 0[14822/17270] Time:0.245, Train Loss:0.4928569197654724\n",
      "Epoch 0[14823/17270] Time:0.238, Train Loss:0.4068014919757843\n",
      "Epoch 0[14824/17270] Time:0.237, Train Loss:0.36883801221847534\n",
      "Epoch 0[14825/17270] Time:0.226, Train Loss:0.19946546852588654\n",
      "Epoch 0[14826/17270] Time:0.232, Train Loss:0.47101327776908875\n",
      "Epoch 0[14827/17270] Time:0.23, Train Loss:0.29213786125183105\n",
      "Epoch 0[14828/17270] Time:0.232, Train Loss:0.25739237666130066\n",
      "Epoch 0[14829/17270] Time:0.236, Train Loss:0.4541604518890381\n",
      "Epoch 0[14830/17270] Time:0.232, Train Loss:0.9334716200828552\n",
      "Epoch 0[14831/17270] Time:0.226, Train Loss:0.3702901005744934\n",
      "Epoch 0[14832/17270] Time:0.235, Train Loss:0.46713897585868835\n",
      "Epoch 0[14833/17270] Time:0.237, Train Loss:0.4919494390487671\n",
      "Epoch 0[14834/17270] Time:0.227, Train Loss:0.8961540460586548\n",
      "Epoch 0[14835/17270] Time:0.226, Train Loss:0.49429357051849365\n",
      "Epoch 0[14836/17270] Time:0.235, Train Loss:0.3881241977214813\n",
      "Epoch 0[14837/17270] Time:0.233, Train Loss:0.5732917189598083\n",
      "Epoch 0[14838/17270] Time:0.235, Train Loss:0.48952507972717285\n",
      "Epoch 0[14839/17270] Time:0.227, Train Loss:0.5449528694152832\n",
      "Epoch 0[14840/17270] Time:0.233, Train Loss:0.5958839058876038\n",
      "Epoch 0[14841/17270] Time:0.234, Train Loss:0.5258639454841614\n",
      "Epoch 0[14842/17270] Time:0.233, Train Loss:0.27151936292648315\n",
      "Epoch 0[14843/17270] Time:0.239, Train Loss:0.8230063915252686\n",
      "Epoch 0[14844/17270] Time:0.228, Train Loss:0.4834301769733429\n",
      "Epoch 0[14845/17270] Time:0.225, Train Loss:0.510913074016571\n",
      "Epoch 0[14846/17270] Time:0.228, Train Loss:0.3615081012248993\n",
      "Epoch 0[14847/17270] Time:0.232, Train Loss:0.7381938695907593\n",
      "Epoch 0[14848/17270] Time:0.229, Train Loss:0.3600248098373413\n",
      "Epoch 0[14849/17270] Time:0.235, Train Loss:0.6949334740638733\n",
      "Epoch 0[14850/17270] Time:0.235, Train Loss:0.6076322793960571\n",
      "Epoch 0[14851/17270] Time:0.232, Train Loss:0.32459279894828796\n",
      "Epoch 0[14852/17270] Time:0.237, Train Loss:1.932181477546692\n",
      "Epoch 0[14853/17270] Time:0.236, Train Loss:1.0346143245697021\n",
      "Epoch 0[14854/17270] Time:0.231, Train Loss:0.25142544507980347\n",
      "Epoch 0[14855/17270] Time:0.229, Train Loss:0.37112703919410706\n",
      "Epoch 0[14856/17270] Time:0.237, Train Loss:0.39765405654907227\n",
      "Epoch 0[14857/17270] Time:0.237, Train Loss:0.8067678809165955\n",
      "Epoch 0[14858/17270] Time:0.24, Train Loss:0.4126558005809784\n",
      "Epoch 0[14859/17270] Time:0.226, Train Loss:0.37319105863571167\n",
      "Epoch 0[14860/17270] Time:0.231, Train Loss:1.3417434692382812\n",
      "Epoch 0[14861/17270] Time:0.232, Train Loss:1.0720549821853638\n",
      "Epoch 0[14862/17270] Time:0.221, Train Loss:0.6107437610626221\n",
      "Epoch 0[14863/17270] Time:0.229, Train Loss:1.0669512748718262\n",
      "Epoch 0[14864/17270] Time:0.229, Train Loss:0.6074854731559753\n",
      "Epoch 0[14865/17270] Time:0.233, Train Loss:1.1252470016479492\n",
      "Epoch 0[14866/17270] Time:0.241, Train Loss:0.6705857515335083\n",
      "Epoch 0[14867/17270] Time:0.231, Train Loss:0.4488410949707031\n",
      "Epoch 0[14868/17270] Time:0.25, Train Loss:0.510085940361023\n",
      "Epoch 0[14869/17270] Time:0.233, Train Loss:1.3520296812057495\n",
      "Epoch 0[14870/17270] Time:0.233, Train Loss:1.2329219579696655\n",
      "Epoch 0[14871/17270] Time:0.222, Train Loss:0.8580876588821411\n",
      "Epoch 0[14872/17270] Time:0.241, Train Loss:0.48247450590133667\n",
      "Epoch 0[14873/17270] Time:0.224, Train Loss:0.34577199816703796\n",
      "Epoch 0[14874/17270] Time:0.239, Train Loss:0.78533536195755\n",
      "Epoch 0[14875/17270] Time:0.229, Train Loss:0.5158953070640564\n",
      "Epoch 0[14876/17270] Time:0.238, Train Loss:0.6470232009887695\n",
      "Epoch 0[14877/17270] Time:0.244, Train Loss:0.8329383730888367\n",
      "Epoch 0[14878/17270] Time:0.244, Train Loss:0.48044919967651367\n",
      "Epoch 0[14879/17270] Time:0.231, Train Loss:0.6027196645736694\n",
      "Epoch 0[14880/17270] Time:0.235, Train Loss:0.7390941381454468\n",
      "Epoch 0[14881/17270] Time:0.235, Train Loss:1.1008961200714111\n",
      "Epoch 0[14882/17270] Time:0.238, Train Loss:0.44964101910591125\n",
      "Epoch 0[14883/17270] Time:0.239, Train Loss:0.7357207536697388\n",
      "Epoch 0[14884/17270] Time:0.221, Train Loss:0.6009575128555298\n",
      "Epoch 0[14885/17270] Time:0.248, Train Loss:0.3397456109523773\n",
      "Epoch 0[14886/17270] Time:0.233, Train Loss:0.38435232639312744\n",
      "Epoch 0[14887/17270] Time:0.226, Train Loss:0.3205624520778656\n",
      "Epoch 0[14888/17270] Time:0.228, Train Loss:0.40113064646720886\n",
      "Epoch 0[14889/17270] Time:0.235, Train Loss:0.7896162867546082\n",
      "Epoch 0[14890/17270] Time:0.234, Train Loss:0.4944402873516083\n",
      "Epoch 0[14891/17270] Time:0.23, Train Loss:0.5480098128318787\n",
      "Epoch 0[14892/17270] Time:0.231, Train Loss:0.7129329442977905\n",
      "Epoch 0[14893/17270] Time:0.244, Train Loss:0.6627258062362671\n",
      "Epoch 0[14894/17270] Time:0.231, Train Loss:0.9399504065513611\n",
      "Epoch 0[14895/17270] Time:0.246, Train Loss:0.7417741417884827\n",
      "Epoch 0[14896/17270] Time:0.245, Train Loss:0.6654046177864075\n",
      "Epoch 0[14897/17270] Time:0.223, Train Loss:0.3449195325374603\n",
      "Epoch 0[14898/17270] Time:0.245, Train Loss:0.46785667538642883\n",
      "Epoch 0[14899/17270] Time:0.231, Train Loss:0.5032373666763306\n",
      "Epoch 0[14900/17270] Time:0.245, Train Loss:0.6261110305786133\n",
      "Epoch 0[14901/17270] Time:0.239, Train Loss:0.3661983907222748\n",
      "Epoch 0[14902/17270] Time:0.237, Train Loss:0.8609504699707031\n",
      "Epoch 0[14903/17270] Time:0.233, Train Loss:0.4844636023044586\n",
      "Epoch 0[14904/17270] Time:0.241, Train Loss:0.6959623098373413\n",
      "Epoch 0[14905/17270] Time:0.233, Train Loss:0.40753984451293945\n",
      "Epoch 0[14906/17270] Time:0.227, Train Loss:0.4529942274093628\n",
      "Epoch 0[14907/17270] Time:0.231, Train Loss:0.5067061185836792\n",
      "Epoch 0[14908/17270] Time:0.246, Train Loss:0.37661290168762207\n",
      "Epoch 0[14909/17270] Time:0.231, Train Loss:0.6494324803352356\n",
      "Epoch 0[14910/17270] Time:0.227, Train Loss:0.34539005160331726\n",
      "Epoch 0[14911/17270] Time:0.236, Train Loss:0.7752241492271423\n",
      "Epoch 0[14912/17270] Time:0.243, Train Loss:1.0686490535736084\n",
      "Epoch 0[14913/17270] Time:0.223, Train Loss:0.39320939779281616\n",
      "Epoch 0[14914/17270] Time:0.243, Train Loss:0.7332907319068909\n",
      "Epoch 0[14915/17270] Time:0.226, Train Loss:0.3219069540500641\n",
      "Epoch 0[14916/17270] Time:0.24, Train Loss:1.0345118045806885\n",
      "Epoch 0[14917/17270] Time:0.242, Train Loss:0.4985146224498749\n",
      "Epoch 0[14918/17270] Time:0.236, Train Loss:0.4432511031627655\n",
      "Epoch 0[14919/17270] Time:0.233, Train Loss:0.5102689266204834\n",
      "Epoch 0[14920/17270] Time:0.244, Train Loss:0.5515174269676208\n",
      "Epoch 0[14921/17270] Time:0.224, Train Loss:0.6769139170646667\n",
      "Epoch 0[14922/17270] Time:0.241, Train Loss:0.5528855919837952\n",
      "Epoch 0[14923/17270] Time:0.227, Train Loss:0.9015524983406067\n",
      "Epoch 0[14924/17270] Time:0.237, Train Loss:0.4838884174823761\n",
      "Epoch 0[14925/17270] Time:0.223, Train Loss:0.3828980326652527\n",
      "Epoch 0[14926/17270] Time:0.238, Train Loss:0.3484843373298645\n",
      "Epoch 0[14927/17270] Time:0.248, Train Loss:0.583756685256958\n",
      "Epoch 0[14928/17270] Time:0.232, Train Loss:0.4468939006328583\n",
      "Epoch 0[14929/17270] Time:0.235, Train Loss:0.627047598361969\n",
      "Epoch 0[14930/17270] Time:0.233, Train Loss:0.49671223759651184\n",
      "Epoch 0[14931/17270] Time:0.232, Train Loss:0.5997365117073059\n",
      "Epoch 0[14932/17270] Time:0.233, Train Loss:0.36808329820632935\n",
      "Epoch 0[14933/17270] Time:0.237, Train Loss:0.5428518056869507\n",
      "Epoch 0[14934/17270] Time:0.23, Train Loss:0.6248137354850769\n",
      "Epoch 0[14935/17270] Time:0.236, Train Loss:1.0399466753005981\n",
      "Epoch 0[14936/17270] Time:0.24, Train Loss:0.9019411206245422\n",
      "Epoch 0[14937/17270] Time:0.243, Train Loss:0.4376254379749298\n",
      "Epoch 0[14938/17270] Time:0.223, Train Loss:1.027570366859436\n",
      "Epoch 0[14939/17270] Time:0.246, Train Loss:0.5238360166549683\n",
      "Epoch 0[14940/17270] Time:0.235, Train Loss:0.4340507686138153\n",
      "Epoch 0[14941/17270] Time:0.231, Train Loss:0.896293044090271\n",
      "Epoch 0[14942/17270] Time:0.235, Train Loss:0.30040115118026733\n",
      "Epoch 0[14943/17270] Time:0.229, Train Loss:0.883387565612793\n",
      "Epoch 0[14944/17270] Time:0.235, Train Loss:0.6178188920021057\n",
      "Epoch 0[14945/17270] Time:0.236, Train Loss:0.3819107115268707\n",
      "Epoch 0[14946/17270] Time:0.236, Train Loss:0.505342960357666\n",
      "Epoch 0[14947/17270] Time:0.244, Train Loss:0.7697075605392456\n",
      "Epoch 0[14948/17270] Time:0.236, Train Loss:0.603249192237854\n",
      "Epoch 0[14949/17270] Time:0.236, Train Loss:0.4809379577636719\n",
      "Epoch 0[14950/17270] Time:0.234, Train Loss:0.6462041139602661\n",
      "Epoch 0[14951/17270] Time:0.228, Train Loss:0.4562452733516693\n",
      "Epoch 0[14952/17270] Time:0.237, Train Loss:0.384391725063324\n",
      "Epoch 0[14953/17270] Time:0.229, Train Loss:0.5442438721656799\n",
      "Epoch 0[14954/17270] Time:0.239, Train Loss:0.3560919463634491\n",
      "Epoch 0[14955/17270] Time:0.224, Train Loss:0.37571465969085693\n",
      "Epoch 0[14956/17270] Time:0.232, Train Loss:0.8650357723236084\n",
      "Epoch 0[14957/17270] Time:0.246, Train Loss:0.3690570592880249\n",
      "Epoch 0[14958/17270] Time:0.234, Train Loss:0.27906155586242676\n",
      "Epoch 0[14959/17270] Time:0.242, Train Loss:1.0314828157424927\n",
      "Epoch 0[14960/17270] Time:0.236, Train Loss:0.5638303160667419\n",
      "Epoch 0[14961/17270] Time:0.232, Train Loss:0.6356109976768494\n",
      "Epoch 0[14962/17270] Time:0.232, Train Loss:0.3228701949119568\n",
      "Epoch 0[14963/17270] Time:0.236, Train Loss:0.35841408371925354\n",
      "Epoch 0[14964/17270] Time:0.236, Train Loss:0.6422167420387268\n",
      "Epoch 0[14965/17270] Time:0.23, Train Loss:0.34527289867401123\n",
      "Epoch 0[14966/17270] Time:0.235, Train Loss:0.5380517840385437\n",
      "Epoch 0[14967/17270] Time:0.238, Train Loss:0.6158873438835144\n",
      "Epoch 0[14968/17270] Time:0.229, Train Loss:0.5641459226608276\n",
      "Epoch 0[14969/17270] Time:0.228, Train Loss:0.31554269790649414\n",
      "Epoch 0[14970/17270] Time:0.238, Train Loss:0.3741185665130615\n",
      "Epoch 0[14971/17270] Time:0.249, Train Loss:0.506679892539978\n",
      "Epoch 0[14972/17270] Time:0.23, Train Loss:0.5700677633285522\n",
      "Epoch 0[14973/17270] Time:0.238, Train Loss:0.3210792541503906\n",
      "Epoch 0[14974/17270] Time:0.236, Train Loss:0.3626553416252136\n",
      "Epoch 0[14975/17270] Time:0.228, Train Loss:0.519551694393158\n",
      "Epoch 0[14976/17270] Time:0.237, Train Loss:0.5296082496643066\n",
      "Epoch 0[14977/17270] Time:0.231, Train Loss:0.2381434291601181\n",
      "Epoch 0[14978/17270] Time:0.241, Train Loss:0.5795921683311462\n",
      "Epoch 0[14979/17270] Time:0.226, Train Loss:0.25833573937416077\n",
      "Epoch 0[14980/17270] Time:0.229, Train Loss:0.7373837828636169\n",
      "Epoch 0[14981/17270] Time:0.232, Train Loss:0.6166239976882935\n",
      "Epoch 0[14982/17270] Time:0.237, Train Loss:0.7502021193504333\n",
      "Epoch 0[14983/17270] Time:0.239, Train Loss:0.716903805732727\n",
      "Epoch 0[14984/17270] Time:0.234, Train Loss:0.44373902678489685\n",
      "Epoch 0[14985/17270] Time:0.232, Train Loss:0.3502078056335449\n",
      "Epoch 0[14986/17270] Time:0.239, Train Loss:0.43592724204063416\n",
      "Epoch 0[14987/17270] Time:0.228, Train Loss:0.5155158042907715\n",
      "Epoch 0[14988/17270] Time:0.245, Train Loss:0.6921646595001221\n",
      "Epoch 0[14989/17270] Time:0.228, Train Loss:0.45295432209968567\n",
      "Epoch 0[14990/17270] Time:0.227, Train Loss:0.4519760310649872\n",
      "Epoch 0[14991/17270] Time:0.229, Train Loss:0.3933444023132324\n",
      "Epoch 0[14992/17270] Time:0.232, Train Loss:0.796790599822998\n",
      "Epoch 0[14993/17270] Time:0.23, Train Loss:0.383660227060318\n",
      "Epoch 0[14994/17270] Time:0.243, Train Loss:0.4132198393344879\n",
      "Epoch 0[14995/17270] Time:0.231, Train Loss:0.8707524538040161\n",
      "Epoch 0[14996/17270] Time:0.231, Train Loss:0.4476707875728607\n",
      "Epoch 0[14997/17270] Time:0.228, Train Loss:0.5447552800178528\n",
      "Epoch 0[14998/17270] Time:0.235, Train Loss:0.5801082849502563\n",
      "Epoch 0[14999/17270] Time:0.245, Train Loss:0.42667555809020996\n",
      "Epoch 0[15000/17270] Time:0.236, Train Loss:0.5339125394821167\n",
      "Epoch 0[15001/17270] Time:0.233, Train Loss:0.5863000154495239\n",
      "Epoch 0[15002/17270] Time:0.232, Train Loss:1.1578847169876099\n",
      "Epoch 0[15003/17270] Time:0.247, Train Loss:0.9997893571853638\n",
      "Epoch 0[15004/17270] Time:0.235, Train Loss:0.4753029942512512\n",
      "Epoch 0[15005/17270] Time:0.238, Train Loss:0.46852290630340576\n",
      "Epoch 0[15006/17270] Time:0.232, Train Loss:0.27797073125839233\n",
      "Epoch 0[15007/17270] Time:0.249, Train Loss:0.505771279335022\n",
      "Epoch 0[15008/17270] Time:0.234, Train Loss:0.2894078493118286\n",
      "Epoch 0[15009/17270] Time:0.225, Train Loss:0.7955126166343689\n",
      "Epoch 0[15010/17270] Time:0.238, Train Loss:0.41179704666137695\n",
      "Epoch 0[15011/17270] Time:0.23, Train Loss:0.2545619308948517\n",
      "Epoch 0[15012/17270] Time:0.234, Train Loss:0.2756223976612091\n",
      "Epoch 0[15013/17270] Time:0.237, Train Loss:0.5699259638786316\n",
      "Epoch 0[15014/17270] Time:0.228, Train Loss:0.6770200729370117\n",
      "Epoch 0[15015/17270] Time:0.226, Train Loss:0.3602179288864136\n",
      "Epoch 0[15016/17270] Time:0.237, Train Loss:0.6467675566673279\n",
      "Epoch 0[15017/17270] Time:0.239, Train Loss:0.6188549995422363\n",
      "Epoch 0[15018/17270] Time:0.237, Train Loss:0.6407294869422913\n",
      "Epoch 0[15019/17270] Time:0.238, Train Loss:0.3313845694065094\n",
      "Epoch 0[15020/17270] Time:0.236, Train Loss:0.33426469564437866\n",
      "Epoch 0[15021/17270] Time:0.234, Train Loss:0.6613194942474365\n",
      "Epoch 0[15022/17270] Time:0.235, Train Loss:0.6031693816184998\n",
      "Epoch 0[15023/17270] Time:0.249, Train Loss:0.8362838625907898\n",
      "Epoch 0[15024/17270] Time:0.221, Train Loss:0.3969714343547821\n",
      "Epoch 0[15025/17270] Time:0.236, Train Loss:0.6825881600379944\n",
      "Epoch 0[15026/17270] Time:0.238, Train Loss:1.150708556175232\n",
      "Epoch 0[15027/17270] Time:0.229, Train Loss:0.5309752225875854\n",
      "Epoch 0[15028/17270] Time:0.232, Train Loss:0.2983246445655823\n",
      "Epoch 0[15029/17270] Time:0.229, Train Loss:0.41507062315940857\n",
      "Epoch 0[15030/17270] Time:0.236, Train Loss:1.0777342319488525\n",
      "Epoch 0[15031/17270] Time:0.237, Train Loss:0.37386006116867065\n",
      "Epoch 0[15032/17270] Time:0.24, Train Loss:0.7005409598350525\n",
      "Epoch 0[15033/17270] Time:0.231, Train Loss:0.7354309558868408\n",
      "Epoch 0[15034/17270] Time:0.237, Train Loss:0.5043283104896545\n",
      "Epoch 0[15035/17270] Time:0.231, Train Loss:0.47791945934295654\n",
      "Epoch 0[15036/17270] Time:0.236, Train Loss:0.5100142359733582\n",
      "Epoch 0[15037/17270] Time:0.239, Train Loss:0.5445516705513\n",
      "Epoch 0[15038/17270] Time:0.242, Train Loss:0.7632879614830017\n",
      "Epoch 0[15039/17270] Time:0.233, Train Loss:0.4004170596599579\n",
      "Epoch 0[15040/17270] Time:0.231, Train Loss:0.6081065535545349\n",
      "Epoch 0[15041/17270] Time:0.236, Train Loss:0.5692468881607056\n",
      "Epoch 0[15042/17270] Time:0.238, Train Loss:0.30984771251678467\n",
      "Epoch 0[15043/17270] Time:0.23, Train Loss:0.7422738671302795\n",
      "Epoch 0[15044/17270] Time:0.236, Train Loss:0.24486719071865082\n",
      "Epoch 0[15045/17270] Time:0.235, Train Loss:0.5263076424598694\n",
      "Epoch 0[15046/17270] Time:0.238, Train Loss:0.578424870967865\n",
      "Epoch 0[15047/17270] Time:0.238, Train Loss:0.5678059458732605\n",
      "Epoch 0[15048/17270] Time:0.237, Train Loss:1.3158366680145264\n",
      "Epoch 0[15049/17270] Time:0.226, Train Loss:0.26875704526901245\n",
      "Epoch 0[15050/17270] Time:0.231, Train Loss:0.6323127746582031\n",
      "Epoch 0[15051/17270] Time:0.23, Train Loss:0.711238443851471\n",
      "Epoch 0[15052/17270] Time:0.233, Train Loss:0.38059914112091064\n",
      "Epoch 0[15053/17270] Time:0.224, Train Loss:0.38258999586105347\n",
      "Epoch 0[15054/17270] Time:0.227, Train Loss:0.30804917216300964\n",
      "Epoch 0[15055/17270] Time:0.239, Train Loss:0.442566841840744\n",
      "Epoch 0[15056/17270] Time:0.231, Train Loss:0.5225726366043091\n",
      "Epoch 0[15057/17270] Time:0.238, Train Loss:0.32364365458488464\n",
      "Epoch 0[15058/17270] Time:0.234, Train Loss:0.3869723677635193\n",
      "Epoch 0[15059/17270] Time:0.242, Train Loss:0.5397425293922424\n",
      "Epoch 0[15060/17270] Time:0.23, Train Loss:0.5212741494178772\n",
      "Epoch 0[15061/17270] Time:0.227, Train Loss:0.5275185108184814\n",
      "Epoch 0[15062/17270] Time:0.237, Train Loss:0.3195362091064453\n",
      "Epoch 0[15063/17270] Time:0.246, Train Loss:0.4261105954647064\n",
      "Epoch 0[15064/17270] Time:0.23, Train Loss:0.44327545166015625\n",
      "Epoch 0[15065/17270] Time:0.239, Train Loss:0.367866575717926\n",
      "Epoch 0[15066/17270] Time:0.234, Train Loss:0.7697737216949463\n",
      "Epoch 0[15067/17270] Time:0.229, Train Loss:0.5269326567649841\n",
      "Epoch 0[15068/17270] Time:0.233, Train Loss:0.3666086196899414\n",
      "Epoch 0[15069/17270] Time:0.234, Train Loss:1.1787558794021606\n",
      "Epoch 0[15070/17270] Time:0.232, Train Loss:0.8379849195480347\n",
      "Epoch 0[15071/17270] Time:0.232, Train Loss:0.3596021234989166\n",
      "Epoch 0[15072/17270] Time:0.234, Train Loss:0.4671865701675415\n",
      "Epoch 0[15073/17270] Time:0.238, Train Loss:0.42728477716445923\n",
      "Epoch 0[15074/17270] Time:0.238, Train Loss:0.7473068237304688\n",
      "Epoch 0[15075/17270] Time:0.234, Train Loss:0.5138683319091797\n",
      "Epoch 0[15076/17270] Time:0.236, Train Loss:0.6053720712661743\n",
      "Epoch 0[15077/17270] Time:0.227, Train Loss:0.5406745076179504\n",
      "Epoch 0[15078/17270] Time:0.236, Train Loss:0.37053075432777405\n",
      "Epoch 0[15079/17270] Time:0.219, Train Loss:0.7367076873779297\n",
      "Epoch 0[15080/17270] Time:0.226, Train Loss:0.5517507195472717\n",
      "Epoch 0[15081/17270] Time:0.24, Train Loss:0.41741207242012024\n",
      "Epoch 0[15082/17270] Time:0.237, Train Loss:0.3177705407142639\n",
      "Epoch 0[15083/17270] Time:0.225, Train Loss:0.7252141833305359\n",
      "Epoch 0[15084/17270] Time:0.229, Train Loss:0.571353554725647\n",
      "Epoch 0[15085/17270] Time:0.231, Train Loss:0.6351834535598755\n",
      "Epoch 0[15086/17270] Time:0.238, Train Loss:0.5008877515792847\n",
      "Epoch 0[15087/17270] Time:0.237, Train Loss:1.24238920211792\n",
      "Epoch 0[15088/17270] Time:0.255, Train Loss:0.5910353064537048\n",
      "Epoch 0[15089/17270] Time:0.217, Train Loss:0.310850590467453\n",
      "Epoch 0[15090/17270] Time:0.234, Train Loss:0.5886449813842773\n",
      "Epoch 0[15091/17270] Time:0.23, Train Loss:0.48117488622665405\n",
      "Epoch 0[15092/17270] Time:0.238, Train Loss:0.4701003432273865\n",
      "Epoch 0[15093/17270] Time:0.253, Train Loss:0.32939180731773376\n",
      "Epoch 0[15094/17270] Time:0.23, Train Loss:1.3292714357376099\n",
      "Epoch 0[15095/17270] Time:0.234, Train Loss:0.5198977589607239\n",
      "Epoch 0[15096/17270] Time:0.228, Train Loss:0.9366311430931091\n",
      "Epoch 0[15097/17270] Time:0.238, Train Loss:0.6662201285362244\n",
      "Epoch 0[15098/17270] Time:0.234, Train Loss:0.8672303557395935\n",
      "Epoch 0[15099/17270] Time:0.238, Train Loss:0.4529373049736023\n",
      "Epoch 0[15100/17270] Time:0.245, Train Loss:0.4104160666465759\n",
      "Epoch 0[15101/17270] Time:0.237, Train Loss:0.4604054093360901\n",
      "Epoch 0[15102/17270] Time:0.237, Train Loss:0.4456735849380493\n",
      "Epoch 0[15103/17270] Time:0.234, Train Loss:1.4433331489562988\n",
      "Epoch 0[15104/17270] Time:0.235, Train Loss:0.3000767230987549\n",
      "Epoch 0[15105/17270] Time:0.225, Train Loss:0.9819322824478149\n",
      "Epoch 0[15106/17270] Time:0.243, Train Loss:0.49964913725852966\n",
      "Epoch 0[15107/17270] Time:0.241, Train Loss:0.42488619685173035\n",
      "Epoch 0[15108/17270] Time:0.234, Train Loss:0.698152482509613\n",
      "Epoch 0[15109/17270] Time:0.225, Train Loss:1.0005524158477783\n",
      "Epoch 0[15110/17270] Time:0.237, Train Loss:0.4822978377342224\n",
      "Epoch 0[15111/17270] Time:0.232, Train Loss:0.533207356929779\n",
      "Epoch 0[15112/17270] Time:0.223, Train Loss:0.41645240783691406\n",
      "Epoch 0[15113/17270] Time:0.235, Train Loss:0.7754458785057068\n",
      "Epoch 0[15114/17270] Time:0.237, Train Loss:0.6082373261451721\n",
      "Epoch 0[15115/17270] Time:0.237, Train Loss:1.0145269632339478\n",
      "Epoch 0[15116/17270] Time:0.232, Train Loss:0.6567909717559814\n",
      "Epoch 0[15117/17270] Time:0.236, Train Loss:0.5565137267112732\n",
      "Epoch 0[15118/17270] Time:0.234, Train Loss:0.829643189907074\n",
      "Epoch 0[15119/17270] Time:0.238, Train Loss:0.8928687572479248\n",
      "Epoch 0[15120/17270] Time:0.233, Train Loss:0.7767117023468018\n",
      "Epoch 0[15121/17270] Time:0.235, Train Loss:0.7896151542663574\n",
      "Epoch 0[15122/17270] Time:0.237, Train Loss:0.578206479549408\n",
      "Epoch 0[15123/17270] Time:0.233, Train Loss:0.7579792141914368\n",
      "Epoch 0[15124/17270] Time:0.236, Train Loss:0.6221317052841187\n",
      "Epoch 0[15125/17270] Time:0.219, Train Loss:0.5886077880859375\n",
      "Epoch 0[15126/17270] Time:0.231, Train Loss:0.7382999062538147\n",
      "Epoch 0[15127/17270] Time:0.238, Train Loss:0.7066971063613892\n",
      "Epoch 0[15128/17270] Time:0.236, Train Loss:0.5955803394317627\n",
      "Epoch 0[15129/17270] Time:0.23, Train Loss:0.9340826272964478\n",
      "Epoch 0[15130/17270] Time:0.235, Train Loss:0.6288844347000122\n",
      "Epoch 0[15131/17270] Time:0.237, Train Loss:0.7479055523872375\n",
      "Epoch 0[15132/17270] Time:0.236, Train Loss:0.48371776938438416\n",
      "Epoch 0[15133/17270] Time:0.236, Train Loss:0.6062894463539124\n",
      "Epoch 0[15134/17270] Time:0.247, Train Loss:0.65711510181427\n",
      "Epoch 0[15135/17270] Time:0.245, Train Loss:0.7487725615501404\n",
      "Epoch 0[15136/17270] Time:0.224, Train Loss:0.45282143354415894\n",
      "Epoch 0[15137/17270] Time:0.235, Train Loss:0.6417155861854553\n",
      "Epoch 0[15138/17270] Time:0.263, Train Loss:0.5817477107048035\n",
      "Epoch 0[15139/17270] Time:0.242, Train Loss:0.6459415555000305\n",
      "Epoch 0[15140/17270] Time:0.231, Train Loss:0.7277918457984924\n",
      "Epoch 0[15141/17270] Time:0.23, Train Loss:0.5878199934959412\n",
      "Epoch 0[15142/17270] Time:0.239, Train Loss:0.4636591970920563\n",
      "Epoch 0[15143/17270] Time:0.228, Train Loss:0.837109386920929\n",
      "Epoch 0[15144/17270] Time:0.236, Train Loss:0.806257426738739\n",
      "Epoch 0[15145/17270] Time:0.228, Train Loss:0.3450380265712738\n",
      "Epoch 0[15146/17270] Time:0.229, Train Loss:0.4374935030937195\n",
      "Epoch 0[15147/17270] Time:0.236, Train Loss:0.542086660861969\n",
      "Epoch 0[15148/17270] Time:0.224, Train Loss:0.5942381620407104\n",
      "Epoch 0[15149/17270] Time:0.244, Train Loss:1.0843887329101562\n",
      "Epoch 0[15150/17270] Time:0.239, Train Loss:0.42731934785842896\n",
      "Epoch 0[15151/17270] Time:0.238, Train Loss:0.9290249943733215\n",
      "Epoch 0[15152/17270] Time:0.235, Train Loss:0.5448780655860901\n",
      "Epoch 0[15153/17270] Time:0.234, Train Loss:0.6449476480484009\n",
      "Epoch 0[15154/17270] Time:0.233, Train Loss:0.48704230785369873\n",
      "Epoch 0[15155/17270] Time:0.238, Train Loss:0.4544227719306946\n",
      "Epoch 0[15156/17270] Time:0.233, Train Loss:0.5419443845748901\n",
      "Epoch 0[15157/17270] Time:0.234, Train Loss:0.505857527256012\n",
      "Epoch 0[15158/17270] Time:0.234, Train Loss:0.4907985329627991\n",
      "Epoch 0[15159/17270] Time:0.23, Train Loss:0.5353622436523438\n",
      "Epoch 0[15160/17270] Time:0.229, Train Loss:0.39230507612228394\n",
      "Epoch 0[15161/17270] Time:0.235, Train Loss:1.0480901002883911\n",
      "Epoch 0[15162/17270] Time:0.231, Train Loss:0.4825878143310547\n",
      "Epoch 0[15163/17270] Time:0.232, Train Loss:0.43410319089889526\n",
      "Epoch 0[15164/17270] Time:0.24, Train Loss:0.717016339302063\n",
      "Epoch 0[15165/17270] Time:0.236, Train Loss:0.4116803705692291\n",
      "Epoch 0[15166/17270] Time:0.237, Train Loss:0.44667744636535645\n",
      "Epoch 0[15167/17270] Time:0.229, Train Loss:0.6492301225662231\n",
      "Epoch 0[15168/17270] Time:0.244, Train Loss:0.698726236820221\n",
      "Epoch 0[15169/17270] Time:0.233, Train Loss:0.8855977058410645\n",
      "Epoch 0[15170/17270] Time:0.236, Train Loss:0.44780606031417847\n",
      "Epoch 0[15171/17270] Time:0.231, Train Loss:0.9759398698806763\n",
      "Epoch 0[15172/17270] Time:0.233, Train Loss:0.4616641700267792\n",
      "Epoch 0[15173/17270] Time:0.222, Train Loss:0.23080262541770935\n",
      "Epoch 0[15174/17270] Time:0.236, Train Loss:0.9595682621002197\n",
      "Epoch 0[15175/17270] Time:0.239, Train Loss:1.3165735006332397\n",
      "Epoch 0[15176/17270] Time:0.236, Train Loss:0.4656406044960022\n",
      "Epoch 0[15177/17270] Time:0.234, Train Loss:0.5560311079025269\n",
      "Epoch 0[15178/17270] Time:0.232, Train Loss:0.5217207074165344\n",
      "Epoch 0[15179/17270] Time:0.234, Train Loss:0.4502871632575989\n",
      "Epoch 0[15180/17270] Time:0.224, Train Loss:0.9989367127418518\n",
      "Epoch 0[15181/17270] Time:0.241, Train Loss:0.43455398082733154\n",
      "Epoch 0[15182/17270] Time:0.231, Train Loss:0.6339295506477356\n",
      "Epoch 0[15183/17270] Time:0.227, Train Loss:0.847855806350708\n",
      "Epoch 0[15184/17270] Time:0.235, Train Loss:0.33954834938049316\n",
      "Epoch 0[15185/17270] Time:0.234, Train Loss:0.21420195698738098\n",
      "Epoch 0[15186/17270] Time:0.235, Train Loss:0.469441294670105\n",
      "Epoch 0[15187/17270] Time:0.223, Train Loss:0.9534153342247009\n",
      "Epoch 0[15188/17270] Time:0.23, Train Loss:0.3704948425292969\n",
      "Epoch 0[15189/17270] Time:0.243, Train Loss:0.4607129991054535\n",
      "Epoch 0[15190/17270] Time:0.228, Train Loss:0.3251924514770508\n",
      "Epoch 0[15191/17270] Time:0.232, Train Loss:0.5535804033279419\n",
      "Epoch 0[15192/17270] Time:0.236, Train Loss:0.9951225519180298\n",
      "Epoch 0[15193/17270] Time:0.227, Train Loss:0.521013081073761\n",
      "Epoch 0[15194/17270] Time:0.233, Train Loss:0.42956849932670593\n",
      "Epoch 0[15195/17270] Time:0.229, Train Loss:0.40861955285072327\n",
      "Epoch 0[15196/17270] Time:0.231, Train Loss:0.40879958868026733\n",
      "Epoch 0[15197/17270] Time:0.232, Train Loss:0.4178485870361328\n",
      "Epoch 0[15198/17270] Time:0.237, Train Loss:0.7925919890403748\n",
      "Epoch 0[15199/17270] Time:0.237, Train Loss:0.48157837986946106\n",
      "Epoch 0[15200/17270] Time:0.237, Train Loss:1.1630150079727173\n",
      "Epoch 0[15201/17270] Time:0.223, Train Loss:0.7188446521759033\n",
      "Epoch 0[15202/17270] Time:0.231, Train Loss:0.48733198642730713\n",
      "Epoch 0[15203/17270] Time:0.232, Train Loss:0.7062285542488098\n",
      "Epoch 0[15204/17270] Time:0.231, Train Loss:0.554496169090271\n",
      "Epoch 0[15205/17270] Time:0.237, Train Loss:0.4204654395580292\n",
      "Epoch 0[15206/17270] Time:0.236, Train Loss:0.41049647331237793\n",
      "Epoch 0[15207/17270] Time:0.235, Train Loss:0.47244468331336975\n",
      "Epoch 0[15208/17270] Time:0.237, Train Loss:0.47231703996658325\n",
      "Epoch 0[15209/17270] Time:0.22, Train Loss:1.2348440885543823\n",
      "Epoch 0[15210/17270] Time:0.239, Train Loss:0.7853381037712097\n",
      "Epoch 0[15211/17270] Time:0.229, Train Loss:0.5008026361465454\n",
      "Epoch 0[15212/17270] Time:0.227, Train Loss:0.5540315508842468\n",
      "Epoch 0[15213/17270] Time:0.236, Train Loss:0.8660595417022705\n",
      "Epoch 0[15214/17270] Time:0.237, Train Loss:0.43261218070983887\n",
      "Epoch 0[15215/17270] Time:0.231, Train Loss:0.9140623807907104\n",
      "Epoch 0[15216/17270] Time:0.226, Train Loss:0.8148571848869324\n",
      "Epoch 0[15217/17270] Time:0.239, Train Loss:0.5684565305709839\n",
      "Epoch 0[15218/17270] Time:0.23, Train Loss:0.9433376789093018\n",
      "Epoch 0[15219/17270] Time:0.229, Train Loss:0.5881479978561401\n",
      "Epoch 0[15220/17270] Time:0.237, Train Loss:0.6064612865447998\n",
      "Epoch 0[15221/17270] Time:0.229, Train Loss:1.4195151329040527\n",
      "Epoch 0[15222/17270] Time:0.233, Train Loss:0.7722181081771851\n",
      "Epoch 0[15223/17270] Time:0.228, Train Loss:0.7730680704116821\n",
      "Epoch 0[15224/17270] Time:0.235, Train Loss:0.3994145393371582\n",
      "Epoch 0[15225/17270] Time:0.237, Train Loss:0.44748470187187195\n",
      "Epoch 0[15226/17270] Time:0.236, Train Loss:0.8194400668144226\n",
      "Epoch 0[15227/17270] Time:0.239, Train Loss:0.9121668934822083\n",
      "Epoch 0[15228/17270] Time:0.225, Train Loss:0.44558918476104736\n",
      "Epoch 0[15229/17270] Time:0.23, Train Loss:0.8816776871681213\n",
      "Epoch 0[15230/17270] Time:0.241, Train Loss:0.5306240916252136\n",
      "Epoch 0[15231/17270] Time:0.234, Train Loss:0.4310677945613861\n",
      "Epoch 0[15232/17270] Time:0.231, Train Loss:0.49050936102867126\n",
      "Epoch 0[15233/17270] Time:0.232, Train Loss:0.345022052526474\n",
      "Epoch 0[15234/17270] Time:0.246, Train Loss:0.7179039716720581\n",
      "Epoch 0[15235/17270] Time:0.233, Train Loss:0.6909936666488647\n",
      "Epoch 0[15236/17270] Time:0.232, Train Loss:0.9128120541572571\n",
      "Epoch 0[15237/17270] Time:0.23, Train Loss:0.3809436857700348\n",
      "Epoch 0[15238/17270] Time:0.235, Train Loss:0.6675336956977844\n",
      "Epoch 0[15239/17270] Time:0.238, Train Loss:0.3745049238204956\n",
      "Epoch 0[15240/17270] Time:0.229, Train Loss:0.8946491479873657\n",
      "Epoch 0[15241/17270] Time:0.228, Train Loss:0.5191662907600403\n",
      "Epoch 0[15242/17270] Time:0.227, Train Loss:0.3253290355205536\n",
      "Epoch 0[15243/17270] Time:0.232, Train Loss:0.5698474049568176\n",
      "Epoch 0[15244/17270] Time:0.229, Train Loss:0.618472158908844\n",
      "Epoch 0[15245/17270] Time:0.234, Train Loss:0.363261342048645\n",
      "Epoch 0[15246/17270] Time:0.234, Train Loss:0.5865246653556824\n",
      "Epoch 0[15247/17270] Time:0.236, Train Loss:0.9582706093788147\n",
      "Epoch 0[15248/17270] Time:0.235, Train Loss:0.5000023245811462\n",
      "Epoch 0[15249/17270] Time:0.229, Train Loss:0.4541836678981781\n",
      "Epoch 0[15250/17270] Time:0.232, Train Loss:0.5536338686943054\n",
      "Epoch 0[15251/17270] Time:0.23, Train Loss:0.6075893640518188\n",
      "Epoch 0[15252/17270] Time:0.229, Train Loss:0.6895827054977417\n",
      "Epoch 0[15253/17270] Time:0.237, Train Loss:0.7483832240104675\n",
      "Epoch 0[15254/17270] Time:0.236, Train Loss:0.5367124676704407\n",
      "Epoch 0[15255/17270] Time:0.235, Train Loss:0.48675984144210815\n",
      "Epoch 0[15256/17270] Time:0.237, Train Loss:0.3135831356048584\n",
      "Epoch 0[15257/17270] Time:0.233, Train Loss:0.6420938372612\n",
      "Epoch 0[15258/17270] Time:0.231, Train Loss:0.4250245988368988\n",
      "Epoch 0[15259/17270] Time:0.239, Train Loss:0.5005298256874084\n",
      "Epoch 0[15260/17270] Time:0.242, Train Loss:0.9559609293937683\n",
      "Epoch 0[15261/17270] Time:0.22, Train Loss:0.8422216773033142\n",
      "Epoch 0[15262/17270] Time:0.229, Train Loss:0.3906872868537903\n",
      "Epoch 0[15263/17270] Time:0.226, Train Loss:0.6762393712997437\n",
      "Epoch 0[15264/17270] Time:0.238, Train Loss:0.3624669909477234\n",
      "Epoch 0[15265/17270] Time:0.229, Train Loss:0.30777814984321594\n",
      "Epoch 0[15266/17270] Time:0.233, Train Loss:0.3039480745792389\n",
      "Epoch 0[15267/17270] Time:0.235, Train Loss:0.39472609758377075\n",
      "Epoch 0[15268/17270] Time:0.226, Train Loss:0.39472025632858276\n",
      "Epoch 0[15269/17270] Time:0.231, Train Loss:0.3770899176597595\n",
      "Epoch 0[15270/17270] Time:0.233, Train Loss:0.5706692934036255\n",
      "Epoch 0[15271/17270] Time:0.248, Train Loss:0.4836742877960205\n",
      "Epoch 0[15272/17270] Time:0.232, Train Loss:0.6269577145576477\n",
      "Epoch 0[15273/17270] Time:0.232, Train Loss:0.5337207913398743\n",
      "Epoch 0[15274/17270] Time:0.233, Train Loss:0.5840431451797485\n",
      "Epoch 0[15275/17270] Time:0.232, Train Loss:0.4424653649330139\n",
      "Epoch 0[15276/17270] Time:0.238, Train Loss:0.5445857644081116\n",
      "Epoch 0[15277/17270] Time:0.227, Train Loss:0.3204799294471741\n",
      "Epoch 0[15278/17270] Time:0.226, Train Loss:0.30177903175354004\n",
      "Epoch 0[15279/17270] Time:0.238, Train Loss:1.1153584718704224\n",
      "Epoch 0[15280/17270] Time:0.238, Train Loss:0.7483925819396973\n",
      "Epoch 0[15281/17270] Time:0.236, Train Loss:0.29592767357826233\n",
      "Epoch 0[15282/17270] Time:0.228, Train Loss:0.6695539951324463\n",
      "Epoch 0[15283/17270] Time:0.232, Train Loss:0.40100255608558655\n",
      "Epoch 0[15284/17270] Time:0.229, Train Loss:0.36705854535102844\n",
      "Epoch 0[15285/17270] Time:0.23, Train Loss:0.3590656816959381\n",
      "Epoch 0[15286/17270] Time:0.238, Train Loss:0.796832799911499\n",
      "Epoch 0[15287/17270] Time:0.229, Train Loss:0.6474878787994385\n",
      "Epoch 0[15288/17270] Time:0.235, Train Loss:0.6289615035057068\n",
      "Epoch 0[15289/17270] Time:0.232, Train Loss:0.5131344795227051\n",
      "Epoch 0[15290/17270] Time:0.23, Train Loss:0.6414036750793457\n",
      "Epoch 0[15291/17270] Time:0.238, Train Loss:0.6233089566230774\n",
      "Epoch 0[15292/17270] Time:0.23, Train Loss:0.7715991735458374\n",
      "Epoch 0[15293/17270] Time:0.229, Train Loss:0.412768691778183\n",
      "Epoch 0[15294/17270] Time:0.229, Train Loss:0.8155857920646667\n",
      "Epoch 0[15295/17270] Time:0.231, Train Loss:0.30417051911354065\n",
      "Epoch 0[15296/17270] Time:0.236, Train Loss:0.4538954794406891\n",
      "Epoch 0[15297/17270] Time:0.224, Train Loss:0.2524200975894928\n",
      "Epoch 0[15298/17270] Time:0.228, Train Loss:0.28793883323669434\n",
      "Epoch 0[15299/17270] Time:0.249, Train Loss:0.2524605095386505\n",
      "Epoch 0[15300/17270] Time:0.224, Train Loss:0.2852419912815094\n",
      "Epoch 0[15301/17270] Time:0.245, Train Loss:0.5345225930213928\n",
      "Epoch 0[15302/17270] Time:0.237, Train Loss:0.7212817668914795\n",
      "Epoch 0[15303/17270] Time:0.238, Train Loss:0.3252169191837311\n",
      "Epoch 0[15304/17270] Time:0.235, Train Loss:0.3136803209781647\n",
      "Epoch 0[15305/17270] Time:0.246, Train Loss:0.27209770679473877\n",
      "Epoch 0[15306/17270] Time:0.237, Train Loss:0.7908521890640259\n",
      "Epoch 0[15307/17270] Time:0.241, Train Loss:0.4129502773284912\n",
      "Epoch 0[15308/17270] Time:0.223, Train Loss:0.26046115159988403\n",
      "Epoch 0[15309/17270] Time:0.233, Train Loss:0.4841550290584564\n",
      "Epoch 0[15310/17270] Time:0.229, Train Loss:0.3803030252456665\n",
      "Epoch 0[15311/17270] Time:0.243, Train Loss:0.2919027507305145\n",
      "Epoch 0[15312/17270] Time:0.233, Train Loss:0.2749214470386505\n",
      "Epoch 0[15313/17270] Time:0.233, Train Loss:0.42305588722229004\n",
      "Epoch 0[15314/17270] Time:0.234, Train Loss:0.4844062328338623\n",
      "Epoch 0[15315/17270] Time:0.233, Train Loss:0.489075630903244\n",
      "Epoch 0[15316/17270] Time:0.233, Train Loss:0.4857851564884186\n",
      "Epoch 0[15317/17270] Time:0.236, Train Loss:0.5072208642959595\n",
      "Epoch 0[15318/17270] Time:0.231, Train Loss:0.42018818855285645\n",
      "Epoch 0[15319/17270] Time:0.236, Train Loss:0.25521519780158997\n",
      "Epoch 0[15320/17270] Time:0.23, Train Loss:0.5189145803451538\n",
      "Epoch 0[15321/17270] Time:0.237, Train Loss:0.6014348268508911\n",
      "Epoch 0[15322/17270] Time:0.229, Train Loss:0.609285831451416\n",
      "Epoch 0[15323/17270] Time:0.237, Train Loss:0.43919798731803894\n",
      "Epoch 0[15324/17270] Time:0.238, Train Loss:0.5094122886657715\n",
      "Epoch 0[15325/17270] Time:0.228, Train Loss:0.322974294424057\n",
      "Epoch 0[15326/17270] Time:0.231, Train Loss:0.9225946068763733\n",
      "Epoch 0[15327/17270] Time:0.232, Train Loss:0.4167329668998718\n",
      "Epoch 0[15328/17270] Time:0.238, Train Loss:0.4560251533985138\n",
      "Epoch 0[15329/17270] Time:0.23, Train Loss:0.4300352931022644\n",
      "Epoch 0[15330/17270] Time:0.231, Train Loss:0.5573092699050903\n",
      "Epoch 0[15331/17270] Time:0.229, Train Loss:0.5502102971076965\n",
      "Epoch 0[15332/17270] Time:0.229, Train Loss:0.32180044054985046\n",
      "Epoch 0[15333/17270] Time:0.229, Train Loss:0.31628450751304626\n",
      "Epoch 0[15334/17270] Time:0.237, Train Loss:1.0073637962341309\n",
      "Epoch 0[15335/17270] Time:0.235, Train Loss:0.848141610622406\n",
      "Epoch 0[15336/17270] Time:0.236, Train Loss:0.617148220539093\n",
      "Epoch 0[15337/17270] Time:0.232, Train Loss:0.9682409167289734\n",
      "Epoch 0[15338/17270] Time:0.232, Train Loss:0.6404727697372437\n",
      "Epoch 0[15339/17270] Time:0.237, Train Loss:0.5674554705619812\n",
      "Epoch 0[15340/17270] Time:0.23, Train Loss:0.5538408160209656\n",
      "Epoch 0[15341/17270] Time:0.23, Train Loss:0.2340415120124817\n",
      "Epoch 0[15342/17270] Time:0.229, Train Loss:0.42846640944480896\n",
      "Epoch 0[15343/17270] Time:0.234, Train Loss:0.4127674698829651\n",
      "Epoch 0[15344/17270] Time:0.238, Train Loss:0.5541641712188721\n",
      "Epoch 0[15345/17270] Time:0.23, Train Loss:0.9629508256912231\n",
      "Epoch 0[15346/17270] Time:0.232, Train Loss:0.4868137538433075\n",
      "Epoch 0[15347/17270] Time:0.228, Train Loss:0.2586241066455841\n",
      "Epoch 0[15348/17270] Time:0.234, Train Loss:0.8684391975402832\n",
      "Epoch 0[15349/17270] Time:0.232, Train Loss:0.36391642689704895\n",
      "Epoch 0[15350/17270] Time:0.229, Train Loss:0.4061771631240845\n",
      "Epoch 0[15351/17270] Time:0.228, Train Loss:0.3396877646446228\n",
      "Epoch 0[15352/17270] Time:0.242, Train Loss:0.3894474506378174\n",
      "Epoch 0[15353/17270] Time:0.23, Train Loss:0.5209421515464783\n",
      "Epoch 0[15354/17270] Time:0.231, Train Loss:0.6289516091346741\n",
      "Epoch 0[15355/17270] Time:0.232, Train Loss:0.4827027916908264\n",
      "Epoch 0[15356/17270] Time:0.237, Train Loss:0.39852458238601685\n",
      "Epoch 0[15357/17270] Time:0.232, Train Loss:0.6002541184425354\n",
      "Epoch 0[15358/17270] Time:0.232, Train Loss:1.3622573614120483\n",
      "Epoch 0[15359/17270] Time:0.237, Train Loss:1.1142150163650513\n",
      "Epoch 0[15360/17270] Time:0.228, Train Loss:0.30729860067367554\n",
      "Epoch 0[15361/17270] Time:0.239, Train Loss:0.5556254982948303\n",
      "Epoch 0[15362/17270] Time:0.227, Train Loss:0.4598957598209381\n",
      "Epoch 0[15363/17270] Time:0.226, Train Loss:0.7985550165176392\n",
      "Epoch 0[15364/17270] Time:0.243, Train Loss:0.7838844060897827\n",
      "Epoch 0[15365/17270] Time:0.222, Train Loss:0.5435189604759216\n",
      "Epoch 0[15366/17270] Time:0.246, Train Loss:1.1996616125106812\n",
      "Epoch 0[15367/17270] Time:0.235, Train Loss:0.5816323757171631\n",
      "Epoch 0[15368/17270] Time:0.219, Train Loss:1.0915247201919556\n",
      "Epoch 0[15369/17270] Time:0.23, Train Loss:0.3250787854194641\n",
      "Epoch 0[15370/17270] Time:0.227, Train Loss:0.4243912100791931\n",
      "Epoch 0[15371/17270] Time:0.228, Train Loss:0.4436330497264862\n",
      "Epoch 0[15372/17270] Time:0.228, Train Loss:0.42503610253334045\n",
      "Epoch 0[15373/17270] Time:0.231, Train Loss:0.5569356083869934\n",
      "Epoch 0[15374/17270] Time:0.239, Train Loss:0.8448708653450012\n",
      "Epoch 0[15375/17270] Time:0.234, Train Loss:0.5380531549453735\n",
      "Epoch 0[15376/17270] Time:0.234, Train Loss:0.36334940791130066\n",
      "Epoch 0[15377/17270] Time:0.229, Train Loss:0.5608211159706116\n",
      "Epoch 0[15378/17270] Time:0.237, Train Loss:0.4224582612514496\n",
      "Epoch 0[15379/17270] Time:0.237, Train Loss:0.9833155274391174\n",
      "Epoch 0[15380/17270] Time:0.235, Train Loss:0.7669252753257751\n",
      "Epoch 0[15381/17270] Time:0.23, Train Loss:0.6718851327896118\n",
      "Epoch 0[15382/17270] Time:0.237, Train Loss:0.6233884692192078\n",
      "Epoch 0[15383/17270] Time:0.236, Train Loss:0.7083547115325928\n",
      "Epoch 0[15384/17270] Time:0.237, Train Loss:0.9066676497459412\n",
      "Epoch 0[15385/17270] Time:0.23, Train Loss:0.5432193279266357\n",
      "Epoch 0[15386/17270] Time:0.261, Train Loss:0.6030614972114563\n",
      "Epoch 0[15387/17270] Time:0.23, Train Loss:0.37982961535453796\n",
      "Epoch 0[15388/17270] Time:0.232, Train Loss:0.3911580443382263\n",
      "Epoch 0[15389/17270] Time:0.224, Train Loss:0.5819162130355835\n",
      "Epoch 0[15390/17270] Time:0.234, Train Loss:0.8729780912399292\n",
      "Epoch 0[15391/17270] Time:0.239, Train Loss:0.4788852334022522\n",
      "Epoch 0[15392/17270] Time:0.245, Train Loss:0.6369205117225647\n",
      "Epoch 0[15393/17270] Time:0.237, Train Loss:0.4179401695728302\n",
      "Epoch 0[15394/17270] Time:0.239, Train Loss:0.38510096073150635\n",
      "Epoch 0[15395/17270] Time:0.227, Train Loss:0.534815788269043\n",
      "Epoch 0[15396/17270] Time:0.228, Train Loss:1.1328176259994507\n",
      "Epoch 0[15397/17270] Time:0.239, Train Loss:0.5283604860305786\n",
      "Epoch 0[15398/17270] Time:0.238, Train Loss:0.6533679962158203\n",
      "Epoch 0[15399/17270] Time:0.238, Train Loss:0.44974201917648315\n",
      "Epoch 0[15400/17270] Time:0.229, Train Loss:0.6167169809341431\n",
      "Epoch 0[15401/17270] Time:0.236, Train Loss:0.7266558408737183\n",
      "Epoch 0[15402/17270] Time:0.239, Train Loss:0.7782754302024841\n",
      "Epoch 0[15403/17270] Time:0.236, Train Loss:0.44651395082473755\n",
      "Epoch 0[15404/17270] Time:0.235, Train Loss:0.5949581265449524\n",
      "Epoch 0[15405/17270] Time:0.24, Train Loss:0.4905269145965576\n",
      "Epoch 0[15406/17270] Time:0.238, Train Loss:0.5876045823097229\n",
      "Epoch 0[15407/17270] Time:0.247, Train Loss:0.6462964415550232\n",
      "Epoch 0[15408/17270] Time:0.232, Train Loss:0.6852638721466064\n",
      "Epoch 0[15409/17270] Time:0.229, Train Loss:0.4577580392360687\n",
      "Epoch 0[15410/17270] Time:0.231, Train Loss:0.5116939544677734\n",
      "Epoch 0[15411/17270] Time:0.233, Train Loss:0.5008480548858643\n",
      "Epoch 0[15412/17270] Time:0.234, Train Loss:0.6760035157203674\n",
      "Epoch 0[15413/17270] Time:0.232, Train Loss:0.694740891456604\n",
      "Epoch 0[15414/17270] Time:0.236, Train Loss:0.4607390761375427\n",
      "Epoch 0[15415/17270] Time:0.232, Train Loss:0.5178874135017395\n",
      "Epoch 0[15416/17270] Time:0.225, Train Loss:0.5237175226211548\n",
      "Epoch 0[15417/17270] Time:0.226, Train Loss:0.5121723413467407\n",
      "Epoch 0[15418/17270] Time:0.236, Train Loss:0.43049851059913635\n",
      "Epoch 0[15419/17270] Time:0.248, Train Loss:0.6662067770957947\n",
      "Epoch 0[15420/17270] Time:0.241, Train Loss:0.774092435836792\n",
      "Epoch 0[15421/17270] Time:0.235, Train Loss:0.37400200963020325\n",
      "Epoch 0[15422/17270] Time:0.234, Train Loss:0.3630010187625885\n",
      "Epoch 0[15423/17270] Time:0.236, Train Loss:0.8009939789772034\n",
      "Epoch 0[15424/17270] Time:0.236, Train Loss:0.6252883672714233\n",
      "Epoch 0[15425/17270] Time:0.231, Train Loss:0.41800767183303833\n",
      "Epoch 0[15426/17270] Time:0.234, Train Loss:0.7244828939437866\n",
      "Epoch 0[15427/17270] Time:0.228, Train Loss:0.5693799257278442\n",
      "Epoch 0[15428/17270] Time:0.229, Train Loss:0.5305948257446289\n",
      "Epoch 0[15429/17270] Time:0.234, Train Loss:0.2629445195198059\n",
      "Epoch 0[15430/17270] Time:0.242, Train Loss:0.9070252776145935\n",
      "Epoch 0[15431/17270] Time:0.238, Train Loss:0.5974729061126709\n",
      "Epoch 0[15432/17270] Time:0.236, Train Loss:0.3854433000087738\n",
      "Epoch 0[15433/17270] Time:0.234, Train Loss:0.2874111235141754\n",
      "Epoch 0[15434/17270] Time:0.242, Train Loss:0.2775025963783264\n",
      "Epoch 0[15435/17270] Time:0.247, Train Loss:0.49126744270324707\n",
      "Epoch 0[15436/17270] Time:0.236, Train Loss:0.6482534408569336\n",
      "Epoch 0[15437/17270] Time:0.235, Train Loss:0.46182116866111755\n",
      "Epoch 0[15438/17270] Time:0.233, Train Loss:0.3757862448692322\n",
      "Epoch 0[15439/17270] Time:0.23, Train Loss:0.40862900018692017\n",
      "Epoch 0[15440/17270] Time:0.232, Train Loss:0.5659070014953613\n",
      "Epoch 0[15441/17270] Time:0.233, Train Loss:0.6945719122886658\n",
      "Epoch 0[15442/17270] Time:0.233, Train Loss:0.3717816174030304\n",
      "Epoch 0[15443/17270] Time:0.235, Train Loss:0.4908493757247925\n",
      "Epoch 0[15444/17270] Time:0.229, Train Loss:0.6704736351966858\n",
      "Epoch 0[15445/17270] Time:0.239, Train Loss:0.9481006264686584\n",
      "Epoch 0[15446/17270] Time:0.235, Train Loss:0.5689581632614136\n",
      "Epoch 0[15447/17270] Time:0.232, Train Loss:0.594160258769989\n",
      "Epoch 0[15448/17270] Time:0.232, Train Loss:0.6104733347892761\n",
      "Epoch 0[15449/17270] Time:0.229, Train Loss:0.44004717469215393\n",
      "Epoch 0[15450/17270] Time:0.236, Train Loss:0.44176727533340454\n",
      "Epoch 0[15451/17270] Time:0.231, Train Loss:0.507107138633728\n",
      "Epoch 0[15452/17270] Time:0.231, Train Loss:0.4150455892086029\n",
      "Epoch 0[15453/17270] Time:0.232, Train Loss:0.5205704569816589\n",
      "Epoch 0[15454/17270] Time:0.233, Train Loss:0.680902361869812\n",
      "Epoch 0[15455/17270] Time:0.234, Train Loss:0.528982400894165\n",
      "Epoch 0[15456/17270] Time:0.236, Train Loss:0.3438425362110138\n",
      "Epoch 0[15457/17270] Time:0.232, Train Loss:0.5601480007171631\n",
      "Epoch 0[15458/17270] Time:0.231, Train Loss:0.37665554881095886\n",
      "Epoch 0[15459/17270] Time:0.239, Train Loss:0.5803961157798767\n",
      "Epoch 0[15460/17270] Time:0.235, Train Loss:1.0204321146011353\n",
      "Epoch 0[15461/17270] Time:0.235, Train Loss:0.7385044097900391\n",
      "Epoch 0[15462/17270] Time:0.226, Train Loss:0.6115239858627319\n",
      "Epoch 0[15463/17270] Time:0.231, Train Loss:0.2567812502384186\n",
      "Epoch 0[15464/17270] Time:0.232, Train Loss:0.39539045095443726\n",
      "Epoch 0[15465/17270] Time:0.231, Train Loss:0.6144505739212036\n",
      "Epoch 0[15466/17270] Time:0.232, Train Loss:0.7033647298812866\n",
      "Epoch 0[15467/17270] Time:0.229, Train Loss:1.7836276292800903\n",
      "Epoch 0[15468/17270] Time:0.252, Train Loss:1.0777462720870972\n",
      "Epoch 0[15469/17270] Time:0.231, Train Loss:0.5003378391265869\n",
      "Epoch 0[15470/17270] Time:0.231, Train Loss:0.4589335024356842\n",
      "Epoch 0[15471/17270] Time:0.241, Train Loss:0.596114993095398\n",
      "Epoch 0[15472/17270] Time:0.233, Train Loss:0.4391081929206848\n",
      "Epoch 0[15473/17270] Time:0.235, Train Loss:0.51163649559021\n",
      "Epoch 0[15474/17270] Time:0.242, Train Loss:1.1625332832336426\n",
      "Epoch 0[15475/17270] Time:0.234, Train Loss:0.4425603449344635\n",
      "Epoch 0[15476/17270] Time:0.232, Train Loss:0.6849504113197327\n",
      "Epoch 0[15477/17270] Time:0.231, Train Loss:0.7712340950965881\n",
      "Epoch 0[15478/17270] Time:0.23, Train Loss:0.37591543793678284\n",
      "Epoch 0[15479/17270] Time:0.226, Train Loss:0.604419469833374\n",
      "Epoch 0[15480/17270] Time:0.232, Train Loss:0.39759090542793274\n",
      "Epoch 0[15481/17270] Time:0.233, Train Loss:0.3380565643310547\n",
      "Epoch 0[15482/17270] Time:0.229, Train Loss:1.0830014944076538\n",
      "Epoch 0[15483/17270] Time:0.236, Train Loss:0.46147432923316956\n",
      "Epoch 0[15484/17270] Time:0.23, Train Loss:0.6715248227119446\n",
      "Epoch 0[15485/17270] Time:0.226, Train Loss:1.3068941831588745\n",
      "Epoch 0[15486/17270] Time:0.238, Train Loss:0.31799831986427307\n",
      "Epoch 0[15487/17270] Time:0.232, Train Loss:0.45886096358299255\n",
      "Epoch 0[15488/17270] Time:0.235, Train Loss:0.24329547584056854\n",
      "Epoch 0[15489/17270] Time:0.239, Train Loss:0.543951690196991\n",
      "Epoch 0[15490/17270] Time:0.229, Train Loss:0.441699355840683\n",
      "Epoch 0[15491/17270] Time:0.234, Train Loss:0.5155184864997864\n",
      "Epoch 0[15492/17270] Time:0.237, Train Loss:0.560415506362915\n",
      "Epoch 0[15493/17270] Time:0.239, Train Loss:0.35528817772865295\n",
      "Epoch 0[15494/17270] Time:0.241, Train Loss:0.8856849670410156\n",
      "Epoch 0[15495/17270] Time:0.235, Train Loss:1.0590932369232178\n",
      "Epoch 0[15496/17270] Time:0.238, Train Loss:0.7764578461647034\n",
      "Epoch 0[15497/17270] Time:0.239, Train Loss:0.9218516945838928\n",
      "Epoch 0[15498/17270] Time:0.235, Train Loss:0.6668159365653992\n",
      "Epoch 0[15499/17270] Time:0.231, Train Loss:0.42338043451309204\n",
      "Epoch 0[15500/17270] Time:0.224, Train Loss:0.6490269899368286\n",
      "Epoch 0[15501/17270] Time:0.241, Train Loss:0.7213590741157532\n",
      "Epoch 0[15502/17270] Time:0.234, Train Loss:0.6582260727882385\n",
      "Epoch 0[15503/17270] Time:0.227, Train Loss:0.692261278629303\n",
      "Epoch 0[15504/17270] Time:0.238, Train Loss:0.6163617968559265\n",
      "Epoch 0[15505/17270] Time:0.242, Train Loss:0.9579092860221863\n",
      "Epoch 0[15506/17270] Time:0.245, Train Loss:0.43691661953926086\n",
      "Epoch 0[15507/17270] Time:0.236, Train Loss:0.5423449873924255\n",
      "Epoch 0[15508/17270] Time:0.232, Train Loss:0.29798170924186707\n",
      "Epoch 0[15509/17270] Time:0.245, Train Loss:0.6003938913345337\n",
      "Epoch 0[15510/17270] Time:0.247, Train Loss:0.3934132158756256\n",
      "Epoch 0[15511/17270] Time:0.225, Train Loss:0.8536573052406311\n",
      "Epoch 0[15512/17270] Time:0.238, Train Loss:0.34001466631889343\n",
      "Epoch 0[15513/17270] Time:0.237, Train Loss:0.6584029197692871\n",
      "Epoch 0[15514/17270] Time:0.265, Train Loss:0.3304787278175354\n",
      "Epoch 0[15515/17270] Time:0.233, Train Loss:0.5983706712722778\n",
      "Epoch 0[15516/17270] Time:0.225, Train Loss:0.5755763649940491\n",
      "Epoch 0[15517/17270] Time:0.234, Train Loss:0.6408258676528931\n",
      "Epoch 0[15518/17270] Time:0.234, Train Loss:0.6492326855659485\n",
      "Epoch 0[15519/17270] Time:0.233, Train Loss:0.5885267853736877\n",
      "Epoch 0[15520/17270] Time:0.223, Train Loss:0.5444919466972351\n",
      "Epoch 0[15521/17270] Time:0.241, Train Loss:0.7071763873100281\n",
      "Epoch 0[15522/17270] Time:0.222, Train Loss:0.4001852869987488\n",
      "Epoch 0[15523/17270] Time:0.224, Train Loss:0.4340384304523468\n",
      "Epoch 0[15524/17270] Time:0.244, Train Loss:1.0571811199188232\n",
      "Epoch 0[15525/17270] Time:0.23, Train Loss:0.4602828621864319\n",
      "Epoch 0[15526/17270] Time:0.233, Train Loss:0.568747341632843\n",
      "Epoch 0[15527/17270] Time:0.232, Train Loss:0.33222460746765137\n",
      "Epoch 0[15528/17270] Time:0.222, Train Loss:0.5751875638961792\n",
      "Epoch 0[15529/17270] Time:0.233, Train Loss:1.1167778968811035\n",
      "Epoch 0[15530/17270] Time:0.235, Train Loss:0.4785565733909607\n",
      "Epoch 0[15531/17270] Time:0.239, Train Loss:0.3198792338371277\n",
      "Epoch 0[15532/17270] Time:0.24, Train Loss:0.5212978720664978\n",
      "Epoch 0[15533/17270] Time:0.233, Train Loss:0.44250571727752686\n",
      "Epoch 0[15534/17270] Time:0.243, Train Loss:0.44471022486686707\n",
      "Epoch 0[15535/17270] Time:0.233, Train Loss:0.3908548355102539\n",
      "Epoch 0[15536/17270] Time:0.241, Train Loss:0.41753754019737244\n",
      "Epoch 0[15537/17270] Time:0.236, Train Loss:0.4182432293891907\n",
      "Epoch 0[15538/17270] Time:0.228, Train Loss:0.37728846073150635\n",
      "Epoch 0[15539/17270] Time:0.238, Train Loss:0.6791055202484131\n",
      "Epoch 0[15540/17270] Time:0.234, Train Loss:1.2906746864318848\n",
      "Epoch 0[15541/17270] Time:0.235, Train Loss:0.5647234320640564\n",
      "Epoch 0[15542/17270] Time:0.237, Train Loss:0.4408077001571655\n",
      "Epoch 0[15543/17270] Time:0.243, Train Loss:0.43896323442459106\n",
      "Epoch 0[15544/17270] Time:0.239, Train Loss:0.3874375820159912\n",
      "Epoch 0[15545/17270] Time:0.239, Train Loss:0.5707201957702637\n",
      "Epoch 0[15546/17270] Time:0.236, Train Loss:0.478792279958725\n",
      "Epoch 0[15547/17270] Time:0.235, Train Loss:0.4756583869457245\n",
      "Epoch 0[15548/17270] Time:0.233, Train Loss:0.3636826276779175\n",
      "Epoch 0[15549/17270] Time:0.23, Train Loss:0.5348426699638367\n",
      "Epoch 0[15550/17270] Time:0.24, Train Loss:0.5922712087631226\n",
      "Epoch 0[15551/17270] Time:0.234, Train Loss:0.34702378511428833\n",
      "Epoch 0[15552/17270] Time:0.236, Train Loss:0.5589306950569153\n",
      "Epoch 0[15553/17270] Time:0.237, Train Loss:0.4226497709751129\n",
      "Epoch 0[15554/17270] Time:0.239, Train Loss:0.6724345088005066\n",
      "Epoch 0[15555/17270] Time:0.233, Train Loss:0.7327401638031006\n",
      "Epoch 0[15556/17270] Time:0.225, Train Loss:0.36648088693618774\n",
      "Epoch 0[15557/17270] Time:0.245, Train Loss:0.30444595217704773\n",
      "Epoch 0[15558/17270] Time:0.233, Train Loss:0.39750564098358154\n",
      "Epoch 0[15559/17270] Time:0.222, Train Loss:0.8703439235687256\n",
      "Epoch 0[15560/17270] Time:0.23, Train Loss:1.839932918548584\n",
      "Epoch 0[15561/17270] Time:0.243, Train Loss:0.7316154837608337\n",
      "Epoch 0[15562/17270] Time:0.237, Train Loss:0.4500465393066406\n",
      "Epoch 0[15563/17270] Time:0.239, Train Loss:0.9159550666809082\n",
      "Epoch 0[15564/17270] Time:0.233, Train Loss:0.42773377895355225\n",
      "Epoch 0[15565/17270] Time:0.23, Train Loss:0.5380546450614929\n",
      "Epoch 0[15566/17270] Time:0.234, Train Loss:0.9008676409721375\n",
      "Epoch 0[15567/17270] Time:0.229, Train Loss:0.4852060079574585\n",
      "Epoch 0[15568/17270] Time:0.223, Train Loss:0.6898460984230042\n",
      "Epoch 0[15569/17270] Time:0.239, Train Loss:0.6880819797515869\n",
      "Epoch 0[15570/17270] Time:0.229, Train Loss:0.3954721987247467\n",
      "Epoch 0[15571/17270] Time:0.241, Train Loss:0.41788190603256226\n",
      "Epoch 0[15572/17270] Time:0.226, Train Loss:0.7104389667510986\n",
      "Epoch 0[15573/17270] Time:0.232, Train Loss:0.34043920040130615\n",
      "Epoch 0[15574/17270] Time:0.229, Train Loss:0.4815088212490082\n",
      "Epoch 0[15575/17270] Time:0.236, Train Loss:0.5715370178222656\n",
      "Epoch 0[15576/17270] Time:0.239, Train Loss:0.5094332695007324\n",
      "Epoch 0[15577/17270] Time:0.23, Train Loss:0.5244706273078918\n",
      "Epoch 0[15578/17270] Time:0.239, Train Loss:0.5783525705337524\n",
      "Epoch 0[15579/17270] Time:0.235, Train Loss:0.48069024085998535\n",
      "Epoch 0[15580/17270] Time:0.246, Train Loss:0.7522684335708618\n",
      "Epoch 0[15581/17270] Time:0.246, Train Loss:0.5262136459350586\n",
      "Epoch 0[15582/17270] Time:0.238, Train Loss:0.5725718140602112\n",
      "Epoch 0[15583/17270] Time:0.23, Train Loss:0.46713709831237793\n",
      "Epoch 0[15584/17270] Time:0.242, Train Loss:0.7931675314903259\n",
      "Epoch 0[15585/17270] Time:0.238, Train Loss:0.5288211703300476\n",
      "Epoch 0[15586/17270] Time:0.225, Train Loss:0.46227216720581055\n",
      "Epoch 0[15587/17270] Time:0.244, Train Loss:0.4218629002571106\n",
      "Epoch 0[15588/17270] Time:0.238, Train Loss:0.4145625829696655\n",
      "Epoch 0[15589/17270] Time:0.238, Train Loss:1.3401408195495605\n",
      "Epoch 0[15590/17270] Time:0.231, Train Loss:0.45237037539482117\n",
      "Epoch 0[15591/17270] Time:0.237, Train Loss:0.5931530594825745\n",
      "Epoch 0[15592/17270] Time:0.248, Train Loss:0.4919809401035309\n",
      "Epoch 0[15593/17270] Time:0.233, Train Loss:0.5705384016036987\n",
      "Epoch 0[15594/17270] Time:0.233, Train Loss:0.3628791868686676\n",
      "Epoch 0[15595/17270] Time:0.235, Train Loss:0.24963940680027008\n",
      "Epoch 0[15596/17270] Time:0.223, Train Loss:0.7369605898857117\n",
      "Epoch 0[15597/17270] Time:0.226, Train Loss:0.6857910752296448\n",
      "Epoch 0[15598/17270] Time:0.246, Train Loss:0.48708412051200867\n",
      "Epoch 0[15599/17270] Time:0.228, Train Loss:0.43474069237709045\n",
      "Epoch 0[15600/17270] Time:0.221, Train Loss:1.1323198080062866\n",
      "Epoch 0[15601/17270] Time:0.241, Train Loss:0.5092599391937256\n",
      "Epoch 0[15602/17270] Time:0.238, Train Loss:0.5196643471717834\n",
      "Epoch 0[15603/17270] Time:0.238, Train Loss:0.3495601713657379\n",
      "Epoch 0[15604/17270] Time:0.238, Train Loss:0.2851039171218872\n",
      "Epoch 0[15605/17270] Time:0.241, Train Loss:0.33125248551368713\n",
      "Epoch 0[15606/17270] Time:0.235, Train Loss:0.6822327971458435\n",
      "Epoch 0[15607/17270] Time:0.236, Train Loss:0.45105862617492676\n",
      "Epoch 0[15608/17270] Time:0.238, Train Loss:0.6330239176750183\n",
      "Epoch 0[15609/17270] Time:0.231, Train Loss:0.6696169376373291\n",
      "Epoch 0[15610/17270] Time:0.229, Train Loss:0.5044503211975098\n",
      "Epoch 0[15611/17270] Time:0.238, Train Loss:0.6062263250350952\n",
      "Epoch 0[15612/17270] Time:0.237, Train Loss:0.46749699115753174\n",
      "Epoch 0[15613/17270] Time:0.234, Train Loss:0.517290472984314\n",
      "Epoch 0[15614/17270] Time:0.227, Train Loss:0.6651220321655273\n",
      "Epoch 0[15615/17270] Time:0.231, Train Loss:0.5479910373687744\n",
      "Epoch 0[15616/17270] Time:0.241, Train Loss:0.312288373708725\n",
      "Epoch 0[15617/17270] Time:0.235, Train Loss:1.165277361869812\n",
      "Epoch 0[15618/17270] Time:0.23, Train Loss:0.37322530150413513\n",
      "Epoch 0[15619/17270] Time:0.248, Train Loss:0.6010425090789795\n",
      "Epoch 0[15620/17270] Time:0.238, Train Loss:0.3635055720806122\n",
      "Epoch 0[15621/17270] Time:0.233, Train Loss:0.6717442274093628\n",
      "Epoch 0[15622/17270] Time:0.235, Train Loss:0.491832971572876\n",
      "Epoch 0[15623/17270] Time:0.238, Train Loss:0.9130786061286926\n",
      "Epoch 0[15624/17270] Time:0.235, Train Loss:0.37475982308387756\n",
      "Epoch 0[15625/17270] Time:0.238, Train Loss:0.46563130617141724\n",
      "Epoch 0[15626/17270] Time:0.241, Train Loss:0.7686594724655151\n",
      "Epoch 0[15627/17270] Time:0.22, Train Loss:0.4946567714214325\n",
      "Epoch 0[15628/17270] Time:0.237, Train Loss:0.7620376944541931\n",
      "Epoch 0[15629/17270] Time:0.231, Train Loss:0.2926606833934784\n",
      "Epoch 0[15630/17270] Time:0.237, Train Loss:0.33401116728782654\n",
      "Epoch 0[15631/17270] Time:0.222, Train Loss:0.48774635791778564\n",
      "Epoch 0[15632/17270] Time:0.252, Train Loss:0.35433316230773926\n",
      "Epoch 0[15633/17270] Time:0.226, Train Loss:0.3744393289089203\n",
      "Epoch 0[15634/17270] Time:0.231, Train Loss:1.5795533657073975\n",
      "Epoch 0[15635/17270] Time:0.245, Train Loss:0.4433203637599945\n",
      "Epoch 0[15636/17270] Time:0.23, Train Loss:0.45731469988822937\n",
      "Epoch 0[15637/17270] Time:0.225, Train Loss:0.516323983669281\n",
      "Epoch 0[15638/17270] Time:0.237, Train Loss:0.6068421006202698\n",
      "Epoch 0[15639/17270] Time:0.235, Train Loss:0.295006662607193\n",
      "Epoch 0[15640/17270] Time:0.238, Train Loss:0.6281710863113403\n",
      "Epoch 0[15641/17270] Time:0.234, Train Loss:0.31855618953704834\n",
      "Epoch 0[15642/17270] Time:0.238, Train Loss:0.48965826630592346\n",
      "Epoch 0[15643/17270] Time:0.231, Train Loss:1.033074975013733\n",
      "Epoch 0[15644/17270] Time:0.238, Train Loss:0.5463882684707642\n",
      "Epoch 0[15645/17270] Time:0.229, Train Loss:0.7565125823020935\n",
      "Epoch 0[15646/17270] Time:0.238, Train Loss:0.5238857269287109\n",
      "Epoch 0[15647/17270] Time:0.236, Train Loss:0.6082316637039185\n",
      "Epoch 0[15648/17270] Time:0.229, Train Loss:0.3153627812862396\n",
      "Epoch 0[15649/17270] Time:0.232, Train Loss:0.4817921817302704\n",
      "Epoch 0[15650/17270] Time:0.231, Train Loss:1.2759795188903809\n",
      "Epoch 0[15651/17270] Time:0.234, Train Loss:0.3893018364906311\n",
      "Epoch 0[15652/17270] Time:0.228, Train Loss:0.7161539196968079\n",
      "Epoch 0[15653/17270] Time:0.226, Train Loss:0.455151230096817\n",
      "Epoch 0[15654/17270] Time:0.234, Train Loss:0.49746155738830566\n",
      "Epoch 0[15655/17270] Time:0.23, Train Loss:0.32505112886428833\n",
      "Epoch 0[15656/17270] Time:0.23, Train Loss:0.9977385997772217\n",
      "Epoch 0[15657/17270] Time:0.233, Train Loss:0.26972949504852295\n",
      "Epoch 0[15658/17270] Time:0.244, Train Loss:0.5693563222885132\n",
      "Epoch 0[15659/17270] Time:0.234, Train Loss:0.43920761346817017\n",
      "Epoch 0[15660/17270] Time:0.227, Train Loss:0.47790566086769104\n",
      "Epoch 0[15661/17270] Time:0.232, Train Loss:0.38903382420539856\n",
      "Epoch 0[15662/17270] Time:0.229, Train Loss:0.3746403157711029\n",
      "Epoch 0[15663/17270] Time:0.237, Train Loss:0.3686855733394623\n",
      "Epoch 0[15664/17270] Time:0.238, Train Loss:0.9866867661476135\n",
      "Epoch 0[15665/17270] Time:0.244, Train Loss:0.5623985528945923\n",
      "Epoch 0[15666/17270] Time:0.219, Train Loss:0.7582882046699524\n",
      "Epoch 0[15667/17270] Time:0.231, Train Loss:0.3385428786277771\n",
      "Epoch 0[15668/17270] Time:0.237, Train Loss:0.5962123274803162\n",
      "Epoch 0[15669/17270] Time:0.234, Train Loss:0.44415441155433655\n",
      "Epoch 0[15670/17270] Time:0.238, Train Loss:0.6527655124664307\n",
      "Epoch 0[15671/17270] Time:0.229, Train Loss:0.337515264749527\n",
      "Epoch 0[15672/17270] Time:0.236, Train Loss:0.4434431195259094\n",
      "Epoch 0[15673/17270] Time:0.247, Train Loss:1.4170478582382202\n",
      "Epoch 0[15674/17270] Time:0.236, Train Loss:0.9512193202972412\n",
      "Epoch 0[15675/17270] Time:0.232, Train Loss:0.7597509026527405\n",
      "Epoch 0[15676/17270] Time:0.231, Train Loss:0.7333648204803467\n",
      "Epoch 0[15677/17270] Time:0.233, Train Loss:0.8630782961845398\n",
      "Epoch 0[15678/17270] Time:0.251, Train Loss:0.7130489349365234\n",
      "Epoch 0[15679/17270] Time:0.223, Train Loss:0.6669626235961914\n",
      "Epoch 0[15680/17270] Time:0.232, Train Loss:0.5743576288223267\n",
      "Epoch 0[15681/17270] Time:0.24, Train Loss:1.0174450874328613\n",
      "Epoch 0[15682/17270] Time:0.232, Train Loss:0.5642911195755005\n",
      "Epoch 0[15683/17270] Time:0.225, Train Loss:0.44380274415016174\n",
      "Epoch 0[15684/17270] Time:0.246, Train Loss:0.5835812091827393\n",
      "Epoch 0[15685/17270] Time:0.232, Train Loss:0.36746135354042053\n",
      "Epoch 0[15686/17270] Time:0.234, Train Loss:0.5393712520599365\n",
      "Epoch 0[15687/17270] Time:0.234, Train Loss:0.4079733192920685\n",
      "Epoch 0[15688/17270] Time:0.229, Train Loss:0.5732098817825317\n",
      "Epoch 0[15689/17270] Time:0.235, Train Loss:0.9793737530708313\n",
      "Epoch 0[15690/17270] Time:0.233, Train Loss:0.3810020387172699\n",
      "Epoch 0[15691/17270] Time:0.231, Train Loss:1.054013967514038\n",
      "Epoch 0[15692/17270] Time:0.236, Train Loss:0.3642413020133972\n",
      "Epoch 0[15693/17270] Time:0.232, Train Loss:0.5296437740325928\n",
      "Epoch 0[15694/17270] Time:0.222, Train Loss:0.4089222848415375\n",
      "Epoch 0[15695/17270] Time:0.231, Train Loss:1.1971409320831299\n",
      "Epoch 0[15696/17270] Time:0.236, Train Loss:0.5537106990814209\n",
      "Epoch 0[15697/17270] Time:0.217, Train Loss:0.5179955959320068\n",
      "Epoch 0[15698/17270] Time:0.231, Train Loss:0.3797227740287781\n",
      "Epoch 0[15699/17270] Time:0.239, Train Loss:0.8755376935005188\n",
      "Epoch 0[15700/17270] Time:0.233, Train Loss:0.6727752685546875\n",
      "Epoch 0[15701/17270] Time:0.231, Train Loss:0.7459731101989746\n",
      "Epoch 0[15702/17270] Time:0.237, Train Loss:0.3321726322174072\n",
      "Epoch 0[15703/17270] Time:0.244, Train Loss:0.6149925589561462\n",
      "Epoch 0[15704/17270] Time:0.234, Train Loss:0.715196430683136\n",
      "Epoch 0[15705/17270] Time:0.235, Train Loss:0.5682345032691956\n",
      "Epoch 0[15706/17270] Time:0.232, Train Loss:0.3855738043785095\n",
      "Epoch 0[15707/17270] Time:0.234, Train Loss:0.5080615282058716\n",
      "Epoch 0[15708/17270] Time:0.234, Train Loss:0.49604782462120056\n",
      "Epoch 0[15709/17270] Time:0.239, Train Loss:0.586276650428772\n",
      "Epoch 0[15710/17270] Time:0.231, Train Loss:0.44765907526016235\n",
      "Epoch 0[15711/17270] Time:0.25, Train Loss:0.8310683965682983\n",
      "Epoch 0[15712/17270] Time:0.242, Train Loss:1.1947193145751953\n",
      "Epoch 0[15713/17270] Time:0.222, Train Loss:0.3830438256263733\n",
      "Epoch 0[15714/17270] Time:0.234, Train Loss:0.5454469323158264\n",
      "Epoch 0[15715/17270] Time:0.229, Train Loss:0.7231453061103821\n",
      "Epoch 0[15716/17270] Time:0.236, Train Loss:0.37574678659439087\n",
      "Epoch 0[15717/17270] Time:0.227, Train Loss:1.1323736906051636\n",
      "Epoch 0[15718/17270] Time:0.243, Train Loss:0.5210645198822021\n",
      "Epoch 0[15719/17270] Time:0.249, Train Loss:0.45752575993537903\n",
      "Epoch 0[15720/17270] Time:0.226, Train Loss:0.7664849758148193\n",
      "Epoch 0[15721/17270] Time:0.232, Train Loss:0.5562623143196106\n",
      "Epoch 0[15722/17270] Time:0.228, Train Loss:0.7704026103019714\n",
      "Epoch 0[15723/17270] Time:0.234, Train Loss:0.4719087481498718\n",
      "Epoch 0[15724/17270] Time:0.233, Train Loss:0.28604158759117126\n",
      "Epoch 0[15725/17270] Time:0.233, Train Loss:0.3938990533351898\n",
      "Epoch 0[15726/17270] Time:0.237, Train Loss:0.5062600374221802\n",
      "Epoch 0[15727/17270] Time:0.235, Train Loss:0.29666051268577576\n",
      "Epoch 0[15728/17270] Time:0.237, Train Loss:0.6032332181930542\n",
      "Epoch 0[15729/17270] Time:0.249, Train Loss:0.26539474725723267\n",
      "Epoch 0[15730/17270] Time:0.234, Train Loss:0.45219776034355164\n",
      "Epoch 0[15731/17270] Time:0.235, Train Loss:0.18571385741233826\n",
      "Epoch 0[15732/17270] Time:0.236, Train Loss:0.6206812858581543\n",
      "Epoch 0[15733/17270] Time:0.237, Train Loss:0.4266122579574585\n",
      "Epoch 0[15734/17270] Time:0.238, Train Loss:0.991016149520874\n",
      "Epoch 0[15735/17270] Time:0.226, Train Loss:0.3779204785823822\n",
      "Epoch 0[15736/17270] Time:0.245, Train Loss:0.5283118486404419\n",
      "Epoch 0[15737/17270] Time:0.224, Train Loss:1.185373306274414\n",
      "Epoch 0[15738/17270] Time:0.229, Train Loss:0.39578574895858765\n",
      "Epoch 0[15739/17270] Time:0.233, Train Loss:0.4419080913066864\n",
      "Epoch 0[15740/17270] Time:0.234, Train Loss:0.6502536535263062\n",
      "Epoch 0[15741/17270] Time:0.224, Train Loss:0.3395646810531616\n",
      "Epoch 0[15742/17270] Time:0.232, Train Loss:0.648978590965271\n",
      "Epoch 0[15743/17270] Time:0.241, Train Loss:0.4073723554611206\n",
      "Epoch 0[15744/17270] Time:0.238, Train Loss:0.5600908398628235\n",
      "Epoch 0[15745/17270] Time:0.233, Train Loss:0.5620201230049133\n",
      "Epoch 0[15746/17270] Time:0.223, Train Loss:0.461696058511734\n",
      "Epoch 0[15747/17270] Time:0.242, Train Loss:0.4440515339374542\n",
      "Epoch 0[15748/17270] Time:0.241, Train Loss:0.5601047873497009\n",
      "Epoch 0[15749/17270] Time:0.242, Train Loss:0.6653243899345398\n",
      "Epoch 0[15750/17270] Time:0.217, Train Loss:0.5935502052307129\n",
      "Epoch 0[15751/17270] Time:0.239, Train Loss:0.41853150725364685\n",
      "Epoch 0[15752/17270] Time:0.238, Train Loss:0.5262840986251831\n",
      "Epoch 0[15753/17270] Time:0.226, Train Loss:0.297027051448822\n",
      "Epoch 0[15754/17270] Time:0.244, Train Loss:1.0502345561981201\n",
      "Epoch 0[15755/17270] Time:0.225, Train Loss:0.5957229733467102\n",
      "Epoch 0[15756/17270] Time:0.244, Train Loss:0.3019059896469116\n",
      "Epoch 0[15757/17270] Time:0.234, Train Loss:0.4082411527633667\n",
      "Epoch 0[15758/17270] Time:0.227, Train Loss:0.5540154576301575\n",
      "Epoch 0[15759/17270] Time:0.239, Train Loss:0.689691960811615\n",
      "Epoch 0[15760/17270] Time:0.228, Train Loss:0.383306086063385\n",
      "Epoch 0[15761/17270] Time:0.237, Train Loss:0.7115126848220825\n",
      "Epoch 0[15762/17270] Time:0.244, Train Loss:0.4892120063304901\n",
      "Epoch 0[15763/17270] Time:0.239, Train Loss:0.527337372303009\n",
      "Epoch 0[15764/17270] Time:0.244, Train Loss:0.47297072410583496\n",
      "Epoch 0[15765/17270] Time:0.237, Train Loss:0.8562873005867004\n",
      "Epoch 0[15766/17270] Time:0.235, Train Loss:0.4627307951450348\n",
      "Epoch 0[15767/17270] Time:0.244, Train Loss:0.386601984500885\n",
      "Epoch 0[15768/17270] Time:0.226, Train Loss:0.8971806764602661\n",
      "Epoch 0[15769/17270] Time:0.232, Train Loss:0.39284926652908325\n",
      "Epoch 0[15770/17270] Time:0.242, Train Loss:0.32587653398513794\n",
      "Epoch 0[15771/17270] Time:0.238, Train Loss:0.5030933022499084\n",
      "Epoch 0[15772/17270] Time:0.233, Train Loss:0.9656890630722046\n",
      "Epoch 0[15773/17270] Time:0.224, Train Loss:0.7273629307746887\n",
      "Epoch 0[15774/17270] Time:0.245, Train Loss:1.462311863899231\n",
      "Epoch 0[15775/17270] Time:0.234, Train Loss:0.49422088265419006\n",
      "Epoch 0[15776/17270] Time:0.233, Train Loss:0.46335068345069885\n",
      "Epoch 0[15777/17270] Time:0.225, Train Loss:0.46949687600135803\n",
      "Epoch 0[15778/17270] Time:0.241, Train Loss:0.27954766154289246\n",
      "Epoch 0[15779/17270] Time:0.229, Train Loss:0.40776318311691284\n",
      "Epoch 0[15780/17270] Time:0.229, Train Loss:0.3614748418331146\n",
      "Epoch 0[15781/17270] Time:0.227, Train Loss:0.5108746886253357\n",
      "Epoch 0[15782/17270] Time:0.231, Train Loss:0.7240732312202454\n",
      "Epoch 0[15783/17270] Time:0.244, Train Loss:0.38449135422706604\n",
      "Epoch 0[15784/17270] Time:0.221, Train Loss:0.3981899321079254\n",
      "Epoch 0[15785/17270] Time:0.237, Train Loss:0.6220067143440247\n",
      "Epoch 0[15786/17270] Time:0.229, Train Loss:0.3970072567462921\n",
      "Epoch 0[15787/17270] Time:0.231, Train Loss:0.48832249641418457\n",
      "Epoch 0[15788/17270] Time:0.232, Train Loss:0.48546066880226135\n",
      "Epoch 0[15789/17270] Time:0.238, Train Loss:0.8543960452079773\n",
      "Epoch 0[15790/17270] Time:0.225, Train Loss:0.5880699157714844\n",
      "Epoch 0[15791/17270] Time:0.23, Train Loss:0.5671668648719788\n",
      "Epoch 0[15792/17270] Time:0.246, Train Loss:0.5148109793663025\n",
      "Epoch 0[15793/17270] Time:0.236, Train Loss:0.5460165143013\n",
      "Epoch 0[15794/17270] Time:0.228, Train Loss:0.4278596341609955\n",
      "Epoch 0[15795/17270] Time:0.235, Train Loss:0.3735724091529846\n",
      "Epoch 0[15796/17270] Time:0.245, Train Loss:0.4586111307144165\n",
      "Epoch 0[15797/17270] Time:0.225, Train Loss:0.36721599102020264\n",
      "Epoch 0[15798/17270] Time:0.236, Train Loss:0.27746930718421936\n",
      "Epoch 0[15799/17270] Time:0.234, Train Loss:0.355021208524704\n",
      "Epoch 0[15800/17270] Time:0.238, Train Loss:0.8745173811912537\n",
      "Epoch 0[15801/17270] Time:0.242, Train Loss:0.48561909794807434\n",
      "Epoch 0[15802/17270] Time:0.231, Train Loss:0.5629695057868958\n",
      "Epoch 0[15803/17270] Time:0.242, Train Loss:0.26318660378456116\n",
      "Epoch 0[15804/17270] Time:0.227, Train Loss:0.4183276891708374\n",
      "Epoch 0[15805/17270] Time:0.232, Train Loss:0.4550949037075043\n",
      "Epoch 0[15806/17270] Time:0.223, Train Loss:0.5590076446533203\n",
      "Epoch 0[15807/17270] Time:0.244, Train Loss:0.3603459596633911\n",
      "Epoch 0[15808/17270] Time:0.222, Train Loss:0.5030773878097534\n",
      "Epoch 0[15809/17270] Time:0.235, Train Loss:0.49742263555526733\n",
      "Epoch 0[15810/17270] Time:0.231, Train Loss:0.4227478504180908\n",
      "Epoch 0[15811/17270] Time:0.236, Train Loss:0.5039198994636536\n",
      "Epoch 0[15812/17270] Time:0.224, Train Loss:0.48089414834976196\n",
      "Epoch 0[15813/17270] Time:0.244, Train Loss:0.2603570818901062\n",
      "Epoch 0[15814/17270] Time:0.224, Train Loss:0.3057325482368469\n",
      "Epoch 0[15815/17270] Time:0.244, Train Loss:0.5837640762329102\n",
      "Epoch 0[15816/17270] Time:0.239, Train Loss:0.8136347532272339\n",
      "Epoch 0[15817/17270] Time:0.237, Train Loss:0.4585529863834381\n",
      "Epoch 0[15818/17270] Time:0.238, Train Loss:0.7065716981887817\n",
      "Epoch 0[15819/17270] Time:0.228, Train Loss:0.8101500868797302\n",
      "Epoch 0[15820/17270] Time:0.244, Train Loss:0.3520616590976715\n",
      "Epoch 0[15821/17270] Time:0.234, Train Loss:0.3647511601448059\n",
      "Epoch 0[15822/17270] Time:0.227, Train Loss:0.5495301485061646\n",
      "Epoch 0[15823/17270] Time:0.231, Train Loss:0.4873010218143463\n",
      "Epoch 0[15824/17270] Time:0.246, Train Loss:0.8725129961967468\n",
      "Epoch 0[15825/17270] Time:0.237, Train Loss:0.5533944368362427\n",
      "Epoch 0[15826/17270] Time:0.238, Train Loss:0.4346906244754791\n",
      "Epoch 0[15827/17270] Time:0.233, Train Loss:0.41606682538986206\n",
      "Epoch 0[15828/17270] Time:0.237, Train Loss:0.6208057403564453\n",
      "Epoch 0[15829/17270] Time:0.226, Train Loss:0.39429354667663574\n",
      "Epoch 0[15830/17270] Time:0.235, Train Loss:0.5845066905021667\n",
      "Epoch 0[15831/17270] Time:0.243, Train Loss:0.5057506561279297\n",
      "Epoch 0[15832/17270] Time:0.243, Train Loss:0.8938658833503723\n",
      "Epoch 0[15833/17270] Time:0.243, Train Loss:0.31127724051475525\n",
      "Epoch 0[15834/17270] Time:0.229, Train Loss:0.48759475350379944\n",
      "Epoch 0[15835/17270] Time:0.236, Train Loss:0.8738725185394287\n",
      "Epoch 0[15836/17270] Time:0.225, Train Loss:0.5063799619674683\n",
      "Epoch 0[15837/17270] Time:0.227, Train Loss:1.1920748949050903\n",
      "Epoch 0[15838/17270] Time:0.238, Train Loss:0.398093581199646\n",
      "Epoch 0[15839/17270] Time:0.238, Train Loss:0.34910863637924194\n",
      "Epoch 0[15840/17270] Time:0.23, Train Loss:0.3667701482772827\n",
      "Epoch 0[15841/17270] Time:0.233, Train Loss:0.5269205570220947\n",
      "Epoch 0[15842/17270] Time:0.236, Train Loss:0.6716087460517883\n",
      "Epoch 0[15843/17270] Time:0.231, Train Loss:0.44282349944114685\n",
      "Epoch 0[15844/17270] Time:0.231, Train Loss:0.28929391503334045\n",
      "Epoch 0[15845/17270] Time:0.226, Train Loss:0.8003861308097839\n",
      "Epoch 0[15846/17270] Time:0.229, Train Loss:0.5613571405410767\n",
      "Epoch 0[15847/17270] Time:0.247, Train Loss:0.4344203770160675\n",
      "Epoch 0[15848/17270] Time:0.233, Train Loss:0.7287590503692627\n",
      "Epoch 0[15849/17270] Time:0.235, Train Loss:1.212784767150879\n",
      "Epoch 0[15850/17270] Time:0.231, Train Loss:0.4653922915458679\n",
      "Epoch 0[15851/17270] Time:0.227, Train Loss:0.7603267431259155\n",
      "Epoch 0[15852/17270] Time:0.243, Train Loss:0.5971102118492126\n",
      "Epoch 0[15853/17270] Time:0.243, Train Loss:0.4727334678173065\n",
      "Epoch 0[15854/17270] Time:0.232, Train Loss:0.6833056211471558\n",
      "Epoch 0[15855/17270] Time:0.229, Train Loss:0.7255762815475464\n",
      "Epoch 0[15856/17270] Time:0.244, Train Loss:1.359354853630066\n",
      "Epoch 0[15857/17270] Time:0.238, Train Loss:0.8816202282905579\n",
      "Epoch 0[15858/17270] Time:0.225, Train Loss:0.4997009336948395\n",
      "Epoch 0[15859/17270] Time:0.23, Train Loss:0.18377839028835297\n",
      "Epoch 0[15860/17270] Time:0.24, Train Loss:0.3736475110054016\n",
      "Epoch 0[15861/17270] Time:0.236, Train Loss:0.702168345451355\n",
      "Epoch 0[15862/17270] Time:0.247, Train Loss:0.33736827969551086\n",
      "Epoch 0[15863/17270] Time:0.247, Train Loss:0.5045619606971741\n",
      "Epoch 0[15864/17270] Time:0.231, Train Loss:1.0203139781951904\n",
      "Epoch 0[15865/17270] Time:0.233, Train Loss:1.304243803024292\n",
      "Epoch 0[15866/17270] Time:0.234, Train Loss:0.35922935605049133\n",
      "Epoch 0[15867/17270] Time:0.244, Train Loss:0.9093062281608582\n",
      "Epoch 0[15868/17270] Time:0.233, Train Loss:0.46224531531333923\n",
      "Epoch 0[15869/17270] Time:0.227, Train Loss:0.34064018726348877\n",
      "Epoch 0[15870/17270] Time:0.24, Train Loss:0.28028586506843567\n",
      "Epoch 0[15871/17270] Time:0.248, Train Loss:0.5028204321861267\n",
      "Epoch 0[15872/17270] Time:0.234, Train Loss:0.5317423939704895\n",
      "Epoch 0[15873/17270] Time:0.233, Train Loss:0.46500617265701294\n",
      "Epoch 0[15874/17270] Time:0.233, Train Loss:1.1786315441131592\n",
      "Epoch 0[15875/17270] Time:0.238, Train Loss:0.39859873056411743\n",
      "Epoch 0[15876/17270] Time:0.23, Train Loss:1.1377477645874023\n",
      "Epoch 0[15877/17270] Time:0.23, Train Loss:1.046252965927124\n",
      "Epoch 0[15878/17270] Time:0.24, Train Loss:0.9776459336280823\n",
      "Epoch 0[15879/17270] Time:0.232, Train Loss:0.6556164026260376\n",
      "Epoch 0[15880/17270] Time:0.232, Train Loss:0.3934438228607178\n",
      "Epoch 0[15881/17270] Time:0.242, Train Loss:0.3095903992652893\n",
      "Epoch 0[15882/17270] Time:0.228, Train Loss:0.32876089215278625\n",
      "Epoch 0[15883/17270] Time:0.249, Train Loss:0.5239571332931519\n",
      "Epoch 0[15884/17270] Time:0.244, Train Loss:0.44633451104164124\n",
      "Epoch 0[15885/17270] Time:0.238, Train Loss:0.4616634249687195\n",
      "Epoch 0[15886/17270] Time:0.223, Train Loss:0.3948274254798889\n",
      "Epoch 0[15887/17270] Time:0.226, Train Loss:0.6499974727630615\n",
      "Epoch 0[15888/17270] Time:0.237, Train Loss:0.3386882543563843\n",
      "Epoch 0[15889/17270] Time:0.239, Train Loss:0.40976741909980774\n",
      "Epoch 0[15890/17270] Time:0.236, Train Loss:0.44035592675209045\n",
      "Epoch 0[15891/17270] Time:0.233, Train Loss:1.2350165843963623\n",
      "Epoch 0[15892/17270] Time:0.228, Train Loss:0.5163730382919312\n",
      "Epoch 0[15893/17270] Time:0.232, Train Loss:0.45435458421707153\n",
      "Epoch 0[15894/17270] Time:0.236, Train Loss:0.5496984720230103\n",
      "Epoch 0[15895/17270] Time:0.242, Train Loss:0.3657435476779938\n",
      "Epoch 0[15896/17270] Time:0.231, Train Loss:0.4937843680381775\n",
      "Epoch 0[15897/17270] Time:0.233, Train Loss:0.5359533429145813\n",
      "Epoch 0[15898/17270] Time:0.234, Train Loss:1.0298411846160889\n",
      "Epoch 0[15899/17270] Time:0.23, Train Loss:0.6367604732513428\n",
      "Epoch 0[15900/17270] Time:0.223, Train Loss:0.471183180809021\n",
      "Epoch 0[15901/17270] Time:0.245, Train Loss:0.3332662880420685\n",
      "Epoch 0[15902/17270] Time:0.239, Train Loss:0.4238694906234741\n",
      "Epoch 0[15903/17270] Time:0.224, Train Loss:0.2787613868713379\n",
      "Epoch 0[15904/17270] Time:0.236, Train Loss:0.5922166705131531\n",
      "Epoch 0[15905/17270] Time:0.223, Train Loss:0.36704686284065247\n",
      "Epoch 0[15906/17270] Time:0.23, Train Loss:0.6900132298469543\n",
      "Epoch 0[15907/17270] Time:0.236, Train Loss:0.4078754186630249\n",
      "Epoch 0[15908/17270] Time:0.227, Train Loss:0.38774728775024414\n",
      "Epoch 0[15909/17270] Time:0.235, Train Loss:0.7544823884963989\n",
      "Epoch 0[15910/17270] Time:0.239, Train Loss:0.5699179172515869\n",
      "Epoch 0[15911/17270] Time:0.233, Train Loss:0.556003987789154\n",
      "Epoch 0[15912/17270] Time:0.23, Train Loss:0.3587839901447296\n",
      "Epoch 0[15913/17270] Time:0.233, Train Loss:0.5441253781318665\n",
      "Epoch 0[15914/17270] Time:0.237, Train Loss:0.3756125569343567\n",
      "Epoch 0[15915/17270] Time:0.23, Train Loss:0.3541354835033417\n",
      "Epoch 0[15916/17270] Time:0.237, Train Loss:0.5500226020812988\n",
      "Epoch 0[15917/17270] Time:0.23, Train Loss:0.36927807331085205\n",
      "Epoch 0[15918/17270] Time:0.24, Train Loss:0.23303620517253876\n",
      "Epoch 0[15919/17270] Time:0.239, Train Loss:0.638248860836029\n",
      "Epoch 0[15920/17270] Time:0.231, Train Loss:0.586152970790863\n",
      "Epoch 0[15921/17270] Time:0.235, Train Loss:0.5326986908912659\n",
      "Epoch 0[15922/17270] Time:0.23, Train Loss:0.7792432308197021\n",
      "Epoch 0[15923/17270] Time:0.237, Train Loss:0.7781462073326111\n",
      "Epoch 0[15924/17270] Time:0.233, Train Loss:1.3281190395355225\n",
      "Epoch 0[15925/17270] Time:0.236, Train Loss:0.3724181354045868\n",
      "Epoch 0[15926/17270] Time:0.236, Train Loss:0.4750206470489502\n",
      "Epoch 0[15927/17270] Time:0.236, Train Loss:0.61371910572052\n",
      "Epoch 0[15928/17270] Time:0.238, Train Loss:0.4684518575668335\n",
      "Epoch 0[15929/17270] Time:0.229, Train Loss:0.5557342767715454\n",
      "Epoch 0[15930/17270] Time:0.24, Train Loss:0.38278594613075256\n",
      "Epoch 0[15931/17270] Time:0.237, Train Loss:0.3658536672592163\n",
      "Epoch 0[15932/17270] Time:0.234, Train Loss:0.7392833828926086\n",
      "Epoch 0[15933/17270] Time:0.235, Train Loss:0.5048421025276184\n",
      "Epoch 0[15934/17270] Time:0.225, Train Loss:0.5686324834823608\n",
      "Epoch 0[15935/17270] Time:0.238, Train Loss:0.428772896528244\n",
      "Epoch 0[15936/17270] Time:0.225, Train Loss:1.0264759063720703\n",
      "Epoch 0[15937/17270] Time:0.24, Train Loss:0.30800655484199524\n",
      "Epoch 0[15938/17270] Time:0.228, Train Loss:0.3893056809902191\n",
      "Epoch 0[15939/17270] Time:0.236, Train Loss:0.5556158423423767\n",
      "Epoch 0[15940/17270] Time:0.231, Train Loss:0.5261892676353455\n",
      "Epoch 0[15941/17270] Time:0.236, Train Loss:0.4282214343547821\n",
      "Epoch 0[15942/17270] Time:0.237, Train Loss:0.8281604647636414\n",
      "Epoch 0[15943/17270] Time:0.233, Train Loss:0.39666667580604553\n",
      "Epoch 0[15944/17270] Time:0.227, Train Loss:0.9469698071479797\n",
      "Epoch 0[15945/17270] Time:0.228, Train Loss:0.4399269223213196\n",
      "Epoch 0[15946/17270] Time:0.229, Train Loss:0.5019238591194153\n",
      "Epoch 0[15947/17270] Time:0.241, Train Loss:1.3854355812072754\n",
      "Epoch 0[15948/17270] Time:0.231, Train Loss:0.3927914500236511\n",
      "Epoch 0[15949/17270] Time:0.238, Train Loss:0.6550814509391785\n",
      "Epoch 0[15950/17270] Time:0.24, Train Loss:0.44587934017181396\n",
      "Epoch 0[15951/17270] Time:0.237, Train Loss:0.376530259847641\n",
      "Epoch 0[15952/17270] Time:0.231, Train Loss:1.1324684619903564\n",
      "Epoch 0[15953/17270] Time:0.23, Train Loss:0.7155482172966003\n",
      "Epoch 0[15954/17270] Time:0.245, Train Loss:0.3707740902900696\n",
      "Epoch 0[15955/17270] Time:0.238, Train Loss:0.6071065664291382\n",
      "Epoch 0[15956/17270] Time:0.231, Train Loss:0.9123049378395081\n",
      "Epoch 0[15957/17270] Time:0.238, Train Loss:0.44568702578544617\n",
      "Epoch 0[15958/17270] Time:0.228, Train Loss:0.6134950518608093\n",
      "Epoch 0[15959/17270] Time:0.228, Train Loss:1.3542944192886353\n",
      "Epoch 0[15960/17270] Time:0.227, Train Loss:1.1002072095870972\n",
      "Epoch 0[15961/17270] Time:0.25, Train Loss:0.40041661262512207\n",
      "Epoch 0[15962/17270] Time:0.223, Train Loss:0.4160061776638031\n",
      "Epoch 0[15963/17270] Time:0.239, Train Loss:0.538789689540863\n",
      "Epoch 0[15964/17270] Time:0.249, Train Loss:0.615024209022522\n",
      "Epoch 0[15965/17270] Time:0.233, Train Loss:0.6559295058250427\n",
      "Epoch 0[15966/17270] Time:0.234, Train Loss:0.44503915309906006\n",
      "Epoch 0[15967/17270] Time:0.229, Train Loss:0.6704131960868835\n",
      "Epoch 0[15968/17270] Time:0.242, Train Loss:0.42306286096572876\n",
      "Epoch 0[15969/17270] Time:0.236, Train Loss:0.5386691093444824\n",
      "Epoch 0[15970/17270] Time:0.223, Train Loss:0.46794945001602173\n",
      "Epoch 0[15971/17270] Time:0.247, Train Loss:0.3892875909805298\n",
      "Epoch 0[15972/17270] Time:0.233, Train Loss:1.6276785135269165\n",
      "Epoch 0[15973/17270] Time:0.233, Train Loss:0.4177767336368561\n",
      "Epoch 0[15974/17270] Time:0.245, Train Loss:0.5652440786361694\n",
      "Epoch 0[15975/17270] Time:0.232, Train Loss:0.6712976694107056\n",
      "Epoch 0[15976/17270] Time:0.233, Train Loss:0.624101459980011\n",
      "Epoch 0[15977/17270] Time:0.226, Train Loss:0.45987534523010254\n",
      "Epoch 0[15978/17270] Time:0.232, Train Loss:0.3902372419834137\n",
      "Epoch 0[15979/17270] Time:0.25, Train Loss:0.5770208239555359\n",
      "Epoch 0[15980/17270] Time:0.229, Train Loss:0.4159209430217743\n",
      "Epoch 0[15981/17270] Time:0.237, Train Loss:0.9589369893074036\n",
      "Epoch 0[15982/17270] Time:0.225, Train Loss:0.864795446395874\n",
      "Epoch 0[15983/17270] Time:0.236, Train Loss:0.45022526383399963\n",
      "Epoch 0[15984/17270] Time:0.243, Train Loss:0.40753722190856934\n",
      "Epoch 0[15985/17270] Time:0.225, Train Loss:0.40930604934692383\n",
      "Epoch 0[15986/17270] Time:0.251, Train Loss:0.39448270201683044\n",
      "Epoch 0[15987/17270] Time:0.233, Train Loss:0.6994187235832214\n",
      "Epoch 0[15988/17270] Time:0.229, Train Loss:0.3112190365791321\n",
      "Epoch 0[15989/17270] Time:0.239, Train Loss:0.49997037649154663\n",
      "Epoch 0[15990/17270] Time:0.229, Train Loss:0.9190935492515564\n",
      "Epoch 0[15991/17270] Time:0.244, Train Loss:0.5182593464851379\n",
      "Epoch 0[15992/17270] Time:0.236, Train Loss:0.7100077867507935\n",
      "Epoch 0[15993/17270] Time:0.234, Train Loss:0.5560954213142395\n",
      "Epoch 0[15994/17270] Time:0.241, Train Loss:0.28975000977516174\n",
      "Epoch 0[15995/17270] Time:0.237, Train Loss:0.3269111216068268\n",
      "Epoch 0[15996/17270] Time:0.238, Train Loss:0.5768058896064758\n",
      "Epoch 0[15997/17270] Time:0.232, Train Loss:0.5315277576446533\n",
      "Epoch 0[15998/17270] Time:0.246, Train Loss:0.46988770365715027\n",
      "Epoch 0[15999/17270] Time:0.234, Train Loss:0.6436200737953186\n",
      "Epoch 0[16000/17270] Time:0.233, Train Loss:0.34915709495544434\n",
      "Epoch 0[16001/17270] Time:0.252, Train Loss:0.3404315412044525\n",
      "Epoch 0[16002/17270] Time:0.241, Train Loss:0.3829434812068939\n",
      "Epoch 0[16003/17270] Time:0.231, Train Loss:0.7866390347480774\n",
      "Epoch 0[16004/17270] Time:0.25, Train Loss:0.5459679365158081\n",
      "Epoch 0[16005/17270] Time:0.233, Train Loss:1.3091050386428833\n",
      "Epoch 0[16006/17270] Time:0.24, Train Loss:0.5092278122901917\n",
      "Epoch 0[16007/17270] Time:0.253, Train Loss:0.7892423272132874\n",
      "Epoch 0[16008/17270] Time:0.234, Train Loss:0.3269132077693939\n",
      "Epoch 0[16009/17270] Time:0.23, Train Loss:0.47264304757118225\n",
      "Epoch 0[16010/17270] Time:0.242, Train Loss:0.39134007692337036\n",
      "Epoch 0[16011/17270] Time:0.236, Train Loss:0.5427125692367554\n",
      "Epoch 0[16012/17270] Time:0.226, Train Loss:0.5259363651275635\n",
      "Epoch 0[16013/17270] Time:0.24, Train Loss:0.4571138620376587\n",
      "Epoch 0[16014/17270] Time:0.227, Train Loss:0.8140478730201721\n",
      "Epoch 0[16015/17270] Time:0.238, Train Loss:0.289876788854599\n",
      "Epoch 0[16016/17270] Time:0.234, Train Loss:0.38082242012023926\n",
      "Epoch 0[16017/17270] Time:0.234, Train Loss:0.5222784876823425\n",
      "Epoch 0[16018/17270] Time:0.222, Train Loss:0.573563277721405\n",
      "Epoch 0[16019/17270] Time:0.247, Train Loss:0.5888175964355469\n",
      "Epoch 0[16020/17270] Time:0.238, Train Loss:0.5908088088035583\n",
      "Epoch 0[16021/17270] Time:0.225, Train Loss:0.405679851770401\n",
      "Epoch 0[16022/17270] Time:0.236, Train Loss:1.3063174486160278\n",
      "Epoch 0[16023/17270] Time:0.224, Train Loss:0.45477959513664246\n",
      "Epoch 0[16024/17270] Time:0.24, Train Loss:0.3087785840034485\n",
      "Epoch 0[16025/17270] Time:0.242, Train Loss:0.7595820426940918\n",
      "Epoch 0[16026/17270] Time:0.223, Train Loss:0.8313155770301819\n",
      "Epoch 0[16027/17270] Time:0.242, Train Loss:0.3245879113674164\n",
      "Epoch 0[16028/17270] Time:0.229, Train Loss:1.026158094406128\n",
      "Epoch 0[16029/17270] Time:0.241, Train Loss:0.3995630443096161\n",
      "Epoch 0[16030/17270] Time:0.241, Train Loss:0.5848576426506042\n",
      "Epoch 0[16031/17270] Time:0.235, Train Loss:0.4223819077014923\n",
      "Epoch 0[16032/17270] Time:0.236, Train Loss:0.5106402635574341\n",
      "Epoch 0[16033/17270] Time:0.232, Train Loss:0.7949286103248596\n",
      "Epoch 0[16034/17270] Time:0.233, Train Loss:0.37179407477378845\n",
      "Epoch 0[16035/17270] Time:0.236, Train Loss:0.3486059010028839\n",
      "Epoch 0[16036/17270] Time:0.246, Train Loss:1.233106017112732\n",
      "Epoch 0[16037/17270] Time:0.241, Train Loss:0.3432910740375519\n",
      "Epoch 0[16038/17270] Time:0.235, Train Loss:0.5103187561035156\n",
      "Epoch 0[16039/17270] Time:0.234, Train Loss:0.4041876792907715\n",
      "Epoch 0[16040/17270] Time:0.238, Train Loss:0.6067032217979431\n",
      "Epoch 0[16041/17270] Time:0.238, Train Loss:1.2375848293304443\n",
      "Epoch 0[16042/17270] Time:0.234, Train Loss:0.5608161091804504\n",
      "Epoch 0[16043/17270] Time:0.24, Train Loss:0.5841744542121887\n",
      "Epoch 0[16044/17270] Time:0.241, Train Loss:0.7837748527526855\n",
      "Epoch 0[16045/17270] Time:0.227, Train Loss:0.5158310532569885\n",
      "Epoch 0[16046/17270] Time:0.238, Train Loss:0.4162593185901642\n",
      "Epoch 0[16047/17270] Time:0.236, Train Loss:0.41435080766677856\n",
      "Epoch 0[16048/17270] Time:0.237, Train Loss:0.2827342748641968\n",
      "Epoch 0[16049/17270] Time:0.227, Train Loss:0.6091960072517395\n",
      "Epoch 0[16050/17270] Time:0.238, Train Loss:0.5284161567687988\n",
      "Epoch 0[16051/17270] Time:0.231, Train Loss:0.6763940453529358\n",
      "Epoch 0[16052/17270] Time:0.234, Train Loss:0.7323918342590332\n",
      "Epoch 0[16053/17270] Time:0.229, Train Loss:0.9909485578536987\n",
      "Epoch 0[16054/17270] Time:0.244, Train Loss:0.3941475749015808\n",
      "Epoch 0[16055/17270] Time:0.237, Train Loss:0.760413408279419\n",
      "Epoch 0[16056/17270] Time:0.241, Train Loss:0.6320461630821228\n",
      "Epoch 0[16057/17270] Time:0.237, Train Loss:0.34287166595458984\n",
      "Epoch 0[16058/17270] Time:0.232, Train Loss:0.40527740120887756\n",
      "Epoch 0[16059/17270] Time:0.225, Train Loss:0.5367178916931152\n",
      "Epoch 0[16060/17270] Time:0.238, Train Loss:0.6795623898506165\n",
      "Epoch 0[16061/17270] Time:0.241, Train Loss:0.6136465668678284\n",
      "Epoch 0[16062/17270] Time:0.229, Train Loss:0.31376326084136963\n",
      "Epoch 0[16063/17270] Time:0.232, Train Loss:0.621569037437439\n",
      "Epoch 0[16064/17270] Time:0.223, Train Loss:0.5980110168457031\n",
      "Epoch 0[16065/17270] Time:0.228, Train Loss:0.34411582350730896\n",
      "Epoch 0[16066/17270] Time:0.229, Train Loss:0.43134668469429016\n",
      "Epoch 0[16067/17270] Time:0.228, Train Loss:0.8492448925971985\n",
      "Epoch 0[16068/17270] Time:0.243, Train Loss:0.46853727102279663\n",
      "Epoch 0[16069/17270] Time:0.23, Train Loss:0.6660823822021484\n",
      "Epoch 0[16070/17270] Time:0.244, Train Loss:0.7030206918716431\n",
      "Epoch 0[16071/17270] Time:0.233, Train Loss:0.49602457880973816\n",
      "Epoch 0[16072/17270] Time:0.236, Train Loss:0.3908090889453888\n",
      "Epoch 0[16073/17270] Time:0.225, Train Loss:0.3647930920124054\n",
      "Epoch 0[16074/17270] Time:0.239, Train Loss:1.2616429328918457\n",
      "Epoch 0[16075/17270] Time:0.246, Train Loss:0.48550277948379517\n",
      "Epoch 0[16076/17270] Time:0.234, Train Loss:0.6559451222419739\n",
      "Epoch 0[16077/17270] Time:0.247, Train Loss:0.2818998694419861\n",
      "Epoch 0[16078/17270] Time:0.245, Train Loss:0.6824366450309753\n",
      "Epoch 0[16079/17270] Time:0.239, Train Loss:0.2609357237815857\n",
      "Epoch 0[16080/17270] Time:0.22, Train Loss:0.78257817029953\n",
      "Epoch 0[16081/17270] Time:0.235, Train Loss:0.5919331908226013\n",
      "Epoch 0[16082/17270] Time:0.252, Train Loss:0.2830897569656372\n",
      "Epoch 0[16083/17270] Time:0.235, Train Loss:0.5259352922439575\n",
      "Epoch 0[16084/17270] Time:0.247, Train Loss:0.32549676299095154\n",
      "Epoch 0[16085/17270] Time:0.231, Train Loss:0.519365131855011\n",
      "Epoch 0[16086/17270] Time:0.241, Train Loss:1.4165611267089844\n",
      "Epoch 0[16087/17270] Time:0.233, Train Loss:0.592263400554657\n",
      "Epoch 0[16088/17270] Time:0.23, Train Loss:0.5094731450080872\n",
      "Epoch 0[16089/17270] Time:0.246, Train Loss:0.5269104838371277\n",
      "Epoch 0[16090/17270] Time:0.244, Train Loss:0.602291464805603\n",
      "Epoch 0[16091/17270] Time:0.239, Train Loss:0.5741912722587585\n",
      "Epoch 0[16092/17270] Time:0.232, Train Loss:0.553058385848999\n",
      "Epoch 0[16093/17270] Time:0.231, Train Loss:0.41574010252952576\n",
      "Epoch 0[16094/17270] Time:0.243, Train Loss:0.7188295125961304\n",
      "Epoch 0[16095/17270] Time:0.241, Train Loss:0.43430182337760925\n",
      "Epoch 0[16096/17270] Time:0.237, Train Loss:0.8661243915557861\n",
      "Epoch 0[16097/17270] Time:0.241, Train Loss:0.36963415145874023\n",
      "Epoch 0[16098/17270] Time:0.233, Train Loss:0.6198885440826416\n",
      "Epoch 0[16099/17270] Time:0.237, Train Loss:0.4801919460296631\n",
      "Epoch 0[16100/17270] Time:0.239, Train Loss:0.3732762932777405\n",
      "Epoch 0[16101/17270] Time:0.248, Train Loss:0.7453469634056091\n",
      "Epoch 0[16102/17270] Time:0.233, Train Loss:0.29442450404167175\n",
      "Epoch 0[16103/17270] Time:0.245, Train Loss:0.42004600167274475\n",
      "Epoch 0[16104/17270] Time:0.245, Train Loss:0.36965757608413696\n",
      "Epoch 0[16105/17270] Time:0.226, Train Loss:0.7198517918586731\n",
      "Epoch 0[16106/17270] Time:0.237, Train Loss:0.8051832914352417\n",
      "Epoch 0[16107/17270] Time:0.238, Train Loss:0.8722159266471863\n",
      "Epoch 0[16108/17270] Time:0.236, Train Loss:0.6402963995933533\n",
      "Epoch 0[16109/17270] Time:0.241, Train Loss:0.7587822675704956\n",
      "Epoch 0[16110/17270] Time:0.243, Train Loss:1.0899063348770142\n",
      "Epoch 0[16111/17270] Time:0.24, Train Loss:0.7056527137756348\n",
      "Epoch 0[16112/17270] Time:0.232, Train Loss:0.432036429643631\n",
      "Epoch 0[16113/17270] Time:0.245, Train Loss:0.48241961002349854\n",
      "Epoch 0[16114/17270] Time:0.24, Train Loss:0.3236345946788788\n",
      "Epoch 0[16115/17270] Time:0.248, Train Loss:0.707490861415863\n",
      "Epoch 0[16116/17270] Time:0.24, Train Loss:0.6586278676986694\n",
      "Epoch 0[16117/17270] Time:0.23, Train Loss:0.285002201795578\n",
      "Epoch 0[16118/17270] Time:0.231, Train Loss:0.3261527121067047\n",
      "Epoch 0[16119/17270] Time:0.248, Train Loss:0.7039462924003601\n",
      "Epoch 0[16120/17270] Time:0.245, Train Loss:0.8159424066543579\n",
      "Epoch 0[16121/17270] Time:0.244, Train Loss:0.6947373747825623\n",
      "Epoch 0[16122/17270] Time:0.236, Train Loss:0.5036978721618652\n",
      "Epoch 0[16123/17270] Time:0.23, Train Loss:0.5515007972717285\n",
      "Epoch 0[16124/17270] Time:0.236, Train Loss:0.3101891279220581\n",
      "Epoch 0[16125/17270] Time:0.231, Train Loss:0.4078112840652466\n",
      "Epoch 0[16126/17270] Time:0.238, Train Loss:0.6349598169326782\n",
      "Epoch 0[16127/17270] Time:0.243, Train Loss:0.5623683333396912\n",
      "Epoch 0[16128/17270] Time:0.235, Train Loss:0.5266703963279724\n",
      "Epoch 0[16129/17270] Time:0.229, Train Loss:0.5596515536308289\n",
      "Epoch 0[16130/17270] Time:0.238, Train Loss:0.45498356223106384\n",
      "Epoch 0[16131/17270] Time:0.233, Train Loss:0.39977145195007324\n",
      "Epoch 0[16132/17270] Time:0.231, Train Loss:0.5100055932998657\n",
      "Epoch 0[16133/17270] Time:0.235, Train Loss:0.5249131917953491\n",
      "Epoch 0[16134/17270] Time:0.227, Train Loss:0.5422491431236267\n",
      "Epoch 0[16135/17270] Time:0.226, Train Loss:0.40232083201408386\n",
      "Epoch 0[16136/17270] Time:0.239, Train Loss:1.0046740770339966\n",
      "Epoch 0[16137/17270] Time:0.235, Train Loss:0.7114848494529724\n",
      "Epoch 0[16138/17270] Time:0.227, Train Loss:0.4719792306423187\n",
      "Epoch 0[16139/17270] Time:0.229, Train Loss:0.41356733441352844\n",
      "Epoch 0[16140/17270] Time:0.228, Train Loss:0.5924454927444458\n",
      "Epoch 0[16141/17270] Time:0.233, Train Loss:1.3896827697753906\n",
      "Epoch 0[16142/17270] Time:0.233, Train Loss:0.43320751190185547\n",
      "Epoch 0[16143/17270] Time:0.233, Train Loss:0.9742645621299744\n",
      "Epoch 0[16144/17270] Time:0.229, Train Loss:0.5002621412277222\n",
      "Epoch 0[16145/17270] Time:0.235, Train Loss:0.5437623858451843\n",
      "Epoch 0[16146/17270] Time:0.233, Train Loss:0.3501878082752228\n",
      "Epoch 0[16147/17270] Time:0.232, Train Loss:0.4371486008167267\n",
      "Epoch 0[16148/17270] Time:0.232, Train Loss:0.38787391781806946\n",
      "Epoch 0[16149/17270] Time:0.234, Train Loss:0.6348375678062439\n",
      "Epoch 0[16150/17270] Time:0.239, Train Loss:0.8450207114219666\n",
      "Epoch 0[16151/17270] Time:0.246, Train Loss:0.3970475196838379\n",
      "Epoch 0[16152/17270] Time:0.234, Train Loss:0.5180692076683044\n",
      "Epoch 0[16153/17270] Time:0.227, Train Loss:0.6179513335227966\n",
      "Epoch 0[16154/17270] Time:0.246, Train Loss:0.4493426978588104\n",
      "Epoch 0[16155/17270] Time:0.228, Train Loss:0.4221932291984558\n",
      "Epoch 0[16156/17270] Time:0.244, Train Loss:0.3324684798717499\n",
      "Epoch 0[16157/17270] Time:0.232, Train Loss:0.3607015013694763\n",
      "Epoch 0[16158/17270] Time:0.233, Train Loss:0.32804131507873535\n",
      "Epoch 0[16159/17270] Time:0.229, Train Loss:0.8188279867172241\n",
      "Epoch 0[16160/17270] Time:0.239, Train Loss:0.4812380373477936\n",
      "Epoch 0[16161/17270] Time:0.233, Train Loss:0.8033791184425354\n",
      "Epoch 0[16162/17270] Time:0.238, Train Loss:2.3320441246032715\n",
      "Epoch 0[16163/17270] Time:0.228, Train Loss:0.4647333323955536\n",
      "Epoch 0[16164/17270] Time:0.238, Train Loss:0.39861318469047546\n",
      "Epoch 0[16165/17270] Time:0.236, Train Loss:0.5827628970146179\n",
      "Epoch 0[16166/17270] Time:0.23, Train Loss:0.4013490080833435\n",
      "Epoch 0[16167/17270] Time:0.227, Train Loss:0.4370585083961487\n",
      "Epoch 0[16168/17270] Time:0.236, Train Loss:0.3510550558567047\n",
      "Epoch 0[16169/17270] Time:0.233, Train Loss:0.8031346797943115\n",
      "Epoch 0[16170/17270] Time:0.241, Train Loss:1.0507127046585083\n",
      "Epoch 0[16171/17270] Time:0.226, Train Loss:0.5606352686882019\n",
      "Epoch 0[16172/17270] Time:0.229, Train Loss:0.6963409185409546\n",
      "Epoch 0[16173/17270] Time:0.247, Train Loss:0.22480961680412292\n",
      "Epoch 0[16174/17270] Time:0.239, Train Loss:0.652548611164093\n",
      "Epoch 0[16175/17270] Time:0.231, Train Loss:0.48202455043792725\n",
      "Epoch 0[16176/17270] Time:0.223, Train Loss:0.7079785466194153\n",
      "Epoch 0[16177/17270] Time:0.248, Train Loss:0.36808714270591736\n",
      "Epoch 0[16178/17270] Time:0.234, Train Loss:0.5071516633033752\n",
      "Epoch 0[16179/17270] Time:0.239, Train Loss:0.6625375151634216\n",
      "Epoch 0[16180/17270] Time:0.232, Train Loss:0.6650731563568115\n",
      "Epoch 0[16181/17270] Time:0.221, Train Loss:0.37522488832473755\n",
      "Epoch 0[16182/17270] Time:0.245, Train Loss:0.35760417580604553\n",
      "Epoch 0[16183/17270] Time:0.235, Train Loss:0.8960368633270264\n",
      "Epoch 0[16184/17270] Time:0.241, Train Loss:0.6421549916267395\n",
      "Epoch 0[16185/17270] Time:0.238, Train Loss:0.7723207473754883\n",
      "Epoch 0[16186/17270] Time:0.236, Train Loss:0.5875054001808167\n",
      "Epoch 0[16187/17270] Time:0.237, Train Loss:0.4600238502025604\n",
      "Epoch 0[16188/17270] Time:0.218, Train Loss:0.7319867014884949\n",
      "Epoch 0[16189/17270] Time:0.233, Train Loss:0.2658523619174957\n",
      "Epoch 0[16190/17270] Time:0.244, Train Loss:0.49805736541748047\n",
      "Epoch 0[16191/17270] Time:0.226, Train Loss:0.4296126365661621\n",
      "Epoch 0[16192/17270] Time:0.248, Train Loss:0.3331764340400696\n",
      "Epoch 0[16193/17270] Time:0.236, Train Loss:0.4368777871131897\n",
      "Epoch 0[16194/17270] Time:0.239, Train Loss:0.3606938421726227\n",
      "Epoch 0[16195/17270] Time:0.246, Train Loss:0.5297408103942871\n",
      "Epoch 0[16196/17270] Time:0.23, Train Loss:1.263332486152649\n",
      "Epoch 0[16197/17270] Time:0.239, Train Loss:0.41580280661582947\n",
      "Epoch 0[16198/17270] Time:0.241, Train Loss:0.6916369199752808\n",
      "Epoch 0[16199/17270] Time:0.224, Train Loss:0.5562179088592529\n",
      "Epoch 0[16200/17270] Time:0.246, Train Loss:0.5554993152618408\n",
      "Epoch 0[16201/17270] Time:0.227, Train Loss:0.4987342953681946\n",
      "Epoch 0[16202/17270] Time:0.243, Train Loss:0.8090183734893799\n",
      "Epoch 0[16203/17270] Time:0.255, Train Loss:0.3099886476993561\n",
      "Epoch 0[16204/17270] Time:0.232, Train Loss:0.47899529337882996\n",
      "Epoch 0[16205/17270] Time:0.228, Train Loss:0.3938494026660919\n",
      "Epoch 0[16206/17270] Time:0.234, Train Loss:0.3317354619503021\n",
      "Epoch 0[16207/17270] Time:0.228, Train Loss:0.6673638820648193\n",
      "Epoch 0[16208/17270] Time:0.231, Train Loss:0.4911854565143585\n",
      "Epoch 0[16209/17270] Time:0.233, Train Loss:0.2505071759223938\n",
      "Epoch 0[16210/17270] Time:0.235, Train Loss:0.5269564390182495\n",
      "Epoch 0[16211/17270] Time:0.234, Train Loss:1.0725210905075073\n",
      "Epoch 0[16212/17270] Time:0.231, Train Loss:0.48119232058525085\n",
      "Epoch 0[16213/17270] Time:0.227, Train Loss:0.4883832335472107\n",
      "Epoch 0[16214/17270] Time:0.228, Train Loss:1.3954472541809082\n",
      "Epoch 0[16215/17270] Time:0.229, Train Loss:1.1955938339233398\n",
      "Epoch 0[16216/17270] Time:0.231, Train Loss:0.7145617604255676\n",
      "Epoch 0[16217/17270] Time:0.236, Train Loss:0.5589825510978699\n",
      "Epoch 0[16218/17270] Time:0.227, Train Loss:1.1990805864334106\n",
      "Epoch 0[16219/17270] Time:0.224, Train Loss:0.5728732943534851\n",
      "Epoch 0[16220/17270] Time:0.236, Train Loss:0.590160608291626\n",
      "Epoch 0[16221/17270] Time:0.237, Train Loss:0.35603851079940796\n",
      "Epoch 0[16222/17270] Time:0.237, Train Loss:1.0476734638214111\n",
      "Epoch 0[16223/17270] Time:0.229, Train Loss:0.29255375266075134\n",
      "Epoch 0[16224/17270] Time:0.229, Train Loss:0.5781846642494202\n",
      "Epoch 0[16225/17270] Time:0.23, Train Loss:0.4551568031311035\n",
      "Epoch 0[16226/17270] Time:0.231, Train Loss:0.238172248005867\n",
      "Epoch 0[16227/17270] Time:0.227, Train Loss:0.44172173738479614\n",
      "Epoch 0[16228/17270] Time:0.237, Train Loss:0.6341265439987183\n",
      "Epoch 0[16229/17270] Time:0.237, Train Loss:0.48294633626937866\n",
      "Epoch 0[16230/17270] Time:0.24, Train Loss:0.8012143969535828\n",
      "Epoch 0[16231/17270] Time:0.23, Train Loss:0.8629421591758728\n",
      "Epoch 0[16232/17270] Time:0.232, Train Loss:0.46265140175819397\n",
      "Epoch 0[16233/17270] Time:0.235, Train Loss:1.3153640031814575\n",
      "Epoch 0[16234/17270] Time:0.23, Train Loss:0.9269129633903503\n",
      "Epoch 0[16235/17270] Time:0.228, Train Loss:0.705866277217865\n",
      "Epoch 0[16236/17270] Time:0.239, Train Loss:0.6700947284698486\n",
      "Epoch 0[16237/17270] Time:0.237, Train Loss:0.5040076971054077\n",
      "Epoch 0[16238/17270] Time:0.23, Train Loss:0.5211502313613892\n",
      "Epoch 0[16239/17270] Time:0.241, Train Loss:0.662519097328186\n",
      "Epoch 0[16240/17270] Time:0.226, Train Loss:0.3245350122451782\n",
      "Epoch 0[16241/17270] Time:0.231, Train Loss:1.0978972911834717\n",
      "Epoch 0[16242/17270] Time:0.232, Train Loss:0.4768296778202057\n",
      "Epoch 0[16243/17270] Time:0.229, Train Loss:0.5420392155647278\n",
      "Epoch 0[16244/17270] Time:0.239, Train Loss:0.6075282692909241\n",
      "Epoch 0[16245/17270] Time:0.224, Train Loss:0.3684818744659424\n",
      "Epoch 0[16246/17270] Time:0.246, Train Loss:0.47599005699157715\n",
      "Epoch 0[16247/17270] Time:0.223, Train Loss:0.5158756375312805\n",
      "Epoch 0[16248/17270] Time:0.243, Train Loss:0.5270092487335205\n",
      "Epoch 0[16249/17270] Time:0.23, Train Loss:0.4936785399913788\n",
      "Epoch 0[16250/17270] Time:0.242, Train Loss:0.3710139989852905\n",
      "Epoch 0[16251/17270] Time:0.239, Train Loss:0.3014720678329468\n",
      "Epoch 0[16252/17270] Time:0.233, Train Loss:0.36142048239707947\n",
      "Epoch 0[16253/17270] Time:0.233, Train Loss:0.5981160402297974\n",
      "Epoch 0[16254/17270] Time:0.255, Train Loss:0.39080438017845154\n",
      "Epoch 0[16255/17270] Time:0.235, Train Loss:0.48330560326576233\n",
      "Epoch 0[16256/17270] Time:0.231, Train Loss:0.3037390410900116\n",
      "Epoch 0[16257/17270] Time:0.227, Train Loss:0.45946937799453735\n",
      "Epoch 0[16258/17270] Time:0.235, Train Loss:0.4305098056793213\n",
      "Epoch 0[16259/17270] Time:0.231, Train Loss:0.5086492300033569\n",
      "Epoch 0[16260/17270] Time:0.228, Train Loss:0.40532898902893066\n",
      "Epoch 0[16261/17270] Time:0.23, Train Loss:0.5966429114341736\n",
      "Epoch 0[16262/17270] Time:0.238, Train Loss:0.3598964810371399\n",
      "Epoch 0[16263/17270] Time:0.23, Train Loss:0.8487135171890259\n",
      "Epoch 0[16264/17270] Time:0.236, Train Loss:0.7999781966209412\n",
      "Epoch 0[16265/17270] Time:0.239, Train Loss:0.9420886039733887\n",
      "Epoch 0[16266/17270] Time:0.228, Train Loss:0.7131037712097168\n",
      "Epoch 0[16267/17270] Time:0.228, Train Loss:0.3683968782424927\n",
      "Epoch 0[16268/17270] Time:0.231, Train Loss:0.5227866768836975\n",
      "Epoch 0[16269/17270] Time:0.231, Train Loss:0.23709040880203247\n",
      "Epoch 0[16270/17270] Time:0.23, Train Loss:0.5119322538375854\n",
      "Epoch 0[16271/17270] Time:0.238, Train Loss:0.25619789958000183\n",
      "Epoch 0[16272/17270] Time:0.229, Train Loss:1.0200791358947754\n",
      "Epoch 0[16273/17270] Time:0.236, Train Loss:0.5385850071907043\n",
      "Epoch 0[16274/17270] Time:0.232, Train Loss:0.512593686580658\n",
      "Epoch 0[16275/17270] Time:0.231, Train Loss:0.7302159070968628\n",
      "Epoch 0[16276/17270] Time:0.227, Train Loss:0.6999626159667969\n",
      "Epoch 0[16277/17270] Time:0.236, Train Loss:0.6383699774742126\n",
      "Epoch 0[16278/17270] Time:0.23, Train Loss:0.7536482810974121\n",
      "Epoch 0[16279/17270] Time:0.236, Train Loss:0.4175138473510742\n",
      "Epoch 0[16280/17270] Time:0.237, Train Loss:0.5113145112991333\n",
      "Epoch 0[16281/17270] Time:0.231, Train Loss:0.3618232011795044\n",
      "Epoch 0[16282/17270] Time:0.23, Train Loss:0.7913885116577148\n",
      "Epoch 0[16283/17270] Time:0.238, Train Loss:1.156967043876648\n",
      "Epoch 0[16284/17270] Time:0.233, Train Loss:0.7290927171707153\n",
      "Epoch 0[16285/17270] Time:0.229, Train Loss:0.5657444000244141\n",
      "Epoch 0[16286/17270] Time:0.232, Train Loss:0.55113685131073\n",
      "Epoch 0[16287/17270] Time:0.236, Train Loss:0.37165185809135437\n",
      "Epoch 0[16288/17270] Time:0.247, Train Loss:0.6464205384254456\n",
      "Epoch 0[16289/17270] Time:0.223, Train Loss:1.0543782711029053\n",
      "Epoch 0[16290/17270] Time:0.235, Train Loss:0.3435462415218353\n",
      "Epoch 0[16291/17270] Time:0.24, Train Loss:0.7909713387489319\n",
      "Epoch 0[16292/17270] Time:0.228, Train Loss:0.5128248333930969\n",
      "Epoch 0[16293/17270] Time:0.248, Train Loss:0.41352570056915283\n",
      "Epoch 0[16294/17270] Time:0.244, Train Loss:0.38854125142097473\n",
      "Epoch 0[16295/17270] Time:0.222, Train Loss:0.3943065404891968\n",
      "Epoch 0[16296/17270] Time:0.233, Train Loss:0.8364694118499756\n",
      "Epoch 0[16297/17270] Time:0.229, Train Loss:0.842331051826477\n",
      "Epoch 0[16298/17270] Time:0.237, Train Loss:0.44557833671569824\n",
      "Epoch 0[16299/17270] Time:0.229, Train Loss:0.538817822933197\n",
      "Epoch 0[16300/17270] Time:0.233, Train Loss:0.2624512314796448\n",
      "Epoch 0[16301/17270] Time:0.231, Train Loss:0.7028552889823914\n",
      "Epoch 0[16302/17270] Time:0.235, Train Loss:1.1796363592147827\n",
      "Epoch 0[16303/17270] Time:0.238, Train Loss:0.43195879459381104\n",
      "Epoch 0[16304/17270] Time:0.23, Train Loss:0.3634897768497467\n",
      "Epoch 0[16305/17270] Time:0.229, Train Loss:0.6257023811340332\n",
      "Epoch 0[16306/17270] Time:0.259, Train Loss:1.1041779518127441\n",
      "Epoch 0[16307/17270] Time:0.224, Train Loss:0.7359961867332458\n",
      "Epoch 0[16308/17270] Time:0.237, Train Loss:0.5411728024482727\n",
      "Epoch 0[16309/17270] Time:0.238, Train Loss:0.49389728903770447\n",
      "Epoch 0[16310/17270] Time:0.231, Train Loss:0.3933469355106354\n",
      "Epoch 0[16311/17270] Time:0.227, Train Loss:0.6418946981430054\n",
      "Epoch 0[16312/17270] Time:0.237, Train Loss:0.6328475475311279\n",
      "Epoch 0[16313/17270] Time:0.231, Train Loss:0.4455636441707611\n",
      "Epoch 0[16314/17270] Time:0.237, Train Loss:0.6364179849624634\n",
      "Epoch 0[16315/17270] Time:0.231, Train Loss:0.690386176109314\n",
      "Epoch 0[16316/17270] Time:0.228, Train Loss:0.4844520092010498\n",
      "Epoch 0[16317/17270] Time:0.239, Train Loss:0.44468119740486145\n",
      "Epoch 0[16318/17270] Time:0.231, Train Loss:0.29972779750823975\n",
      "Epoch 0[16319/17270] Time:0.238, Train Loss:0.6622743010520935\n",
      "Epoch 0[16320/17270] Time:0.239, Train Loss:0.8832770586013794\n",
      "Epoch 0[16321/17270] Time:0.23, Train Loss:0.3645084500312805\n",
      "Epoch 0[16322/17270] Time:0.229, Train Loss:0.39507535099983215\n",
      "Epoch 0[16323/17270] Time:0.241, Train Loss:0.4136219918727875\n",
      "Epoch 0[16324/17270] Time:0.23, Train Loss:0.3719162344932556\n",
      "Epoch 0[16325/17270] Time:0.246, Train Loss:0.32819217443466187\n",
      "Epoch 0[16326/17270] Time:0.243, Train Loss:0.4746931195259094\n",
      "Epoch 0[16327/17270] Time:0.243, Train Loss:0.5841059684753418\n",
      "Epoch 0[16328/17270] Time:0.233, Train Loss:0.7378169298171997\n",
      "Epoch 0[16329/17270] Time:0.237, Train Loss:0.6986768841743469\n",
      "Epoch 0[16330/17270] Time:0.226, Train Loss:0.39220526814460754\n",
      "Epoch 0[16331/17270] Time:0.232, Train Loss:0.6638885140419006\n",
      "Epoch 0[16332/17270] Time:0.238, Train Loss:0.405965656042099\n",
      "Epoch 0[16333/17270] Time:0.238, Train Loss:0.6508327126502991\n",
      "Epoch 0[16334/17270] Time:0.241, Train Loss:0.9089832901954651\n",
      "Epoch 0[16335/17270] Time:0.226, Train Loss:0.6018980741500854\n",
      "Epoch 0[16336/17270] Time:0.223, Train Loss:0.5857758522033691\n",
      "Epoch 0[16337/17270] Time:0.229, Train Loss:0.8893000483512878\n",
      "Epoch 0[16338/17270] Time:0.24, Train Loss:0.3449712097644806\n",
      "Epoch 0[16339/17270] Time:0.237, Train Loss:0.358163058757782\n",
      "Epoch 0[16340/17270] Time:0.234, Train Loss:0.64959716796875\n",
      "Epoch 0[16341/17270] Time:0.235, Train Loss:0.32494255900382996\n",
      "Epoch 0[16342/17270] Time:0.236, Train Loss:0.6229296326637268\n",
      "Epoch 0[16343/17270] Time:0.254, Train Loss:0.47575777769088745\n",
      "Epoch 0[16344/17270] Time:0.236, Train Loss:0.5156346559524536\n",
      "Epoch 0[16345/17270] Time:0.242, Train Loss:0.5054017901420593\n",
      "Epoch 0[16346/17270] Time:0.22, Train Loss:0.5850516557693481\n",
      "Epoch 0[16347/17270] Time:0.234, Train Loss:0.5688725113868713\n",
      "Epoch 0[16348/17270] Time:0.234, Train Loss:0.9790689945220947\n",
      "Epoch 0[16349/17270] Time:0.229, Train Loss:0.7808437943458557\n",
      "Epoch 0[16350/17270] Time:0.236, Train Loss:0.5872485637664795\n",
      "Epoch 0[16351/17270] Time:0.234, Train Loss:1.912761926651001\n",
      "Epoch 0[16352/17270] Time:0.24, Train Loss:0.3377377986907959\n",
      "Epoch 0[16353/17270] Time:0.226, Train Loss:0.8801507949829102\n",
      "Epoch 0[16354/17270] Time:0.238, Train Loss:1.034224033355713\n",
      "Epoch 0[16355/17270] Time:0.251, Train Loss:0.5939537882804871\n",
      "Epoch 0[16356/17270] Time:0.226, Train Loss:0.5269615650177002\n",
      "Epoch 0[16357/17270] Time:0.232, Train Loss:0.5922849178314209\n",
      "Epoch 0[16358/17270] Time:0.239, Train Loss:0.4213832914829254\n",
      "Epoch 0[16359/17270] Time:0.234, Train Loss:0.41909486055374146\n",
      "Epoch 0[16360/17270] Time:0.23, Train Loss:0.49154356122016907\n",
      "Epoch 0[16361/17270] Time:0.231, Train Loss:0.5361940264701843\n",
      "Epoch 0[16362/17270] Time:0.232, Train Loss:0.3777969181537628\n",
      "Epoch 0[16363/17270] Time:0.232, Train Loss:0.3329307734966278\n",
      "Epoch 0[16364/17270] Time:0.24, Train Loss:0.4111573398113251\n",
      "Epoch 0[16365/17270] Time:0.233, Train Loss:0.5254008173942566\n",
      "Epoch 0[16366/17270] Time:0.231, Train Loss:0.6515374779701233\n",
      "Epoch 0[16367/17270] Time:0.233, Train Loss:0.8923028707504272\n",
      "Epoch 0[16368/17270] Time:0.236, Train Loss:0.6651127338409424\n",
      "Epoch 0[16369/17270] Time:0.241, Train Loss:0.2563392221927643\n",
      "Epoch 0[16370/17270] Time:0.23, Train Loss:0.5410647392272949\n",
      "Epoch 0[16371/17270] Time:0.239, Train Loss:0.8743653893470764\n",
      "Epoch 0[16372/17270] Time:0.232, Train Loss:0.45740780234336853\n",
      "Epoch 0[16373/17270] Time:0.232, Train Loss:0.5134891271591187\n",
      "Epoch 0[16374/17270] Time:0.231, Train Loss:0.4588044285774231\n",
      "Epoch 0[16375/17270] Time:0.239, Train Loss:0.729524552822113\n",
      "Epoch 0[16376/17270] Time:0.242, Train Loss:0.8884761333465576\n",
      "Epoch 0[16377/17270] Time:0.228, Train Loss:0.5747097134590149\n",
      "Epoch 0[16378/17270] Time:0.236, Train Loss:0.38013339042663574\n",
      "Epoch 0[16379/17270] Time:0.231, Train Loss:0.6273762583732605\n",
      "Epoch 0[16380/17270] Time:0.245, Train Loss:0.5813441872596741\n",
      "Epoch 0[16381/17270] Time:0.233, Train Loss:1.0148617029190063\n",
      "Epoch 0[16382/17270] Time:0.228, Train Loss:1.270098328590393\n",
      "Epoch 0[16383/17270] Time:0.237, Train Loss:0.44627392292022705\n",
      "Epoch 0[16384/17270] Time:0.239, Train Loss:0.8845713138580322\n",
      "Epoch 0[16385/17270] Time:0.225, Train Loss:0.4635084867477417\n",
      "Epoch 0[16386/17270] Time:0.241, Train Loss:1.6549572944641113\n",
      "Epoch 0[16387/17270] Time:0.24, Train Loss:0.6286343932151794\n",
      "Epoch 0[16388/17270] Time:0.234, Train Loss:0.5335611701011658\n",
      "Epoch 0[16389/17270] Time:0.221, Train Loss:0.5439121127128601\n",
      "Epoch 0[16390/17270] Time:0.244, Train Loss:0.5582553148269653\n",
      "Epoch 0[16391/17270] Time:0.238, Train Loss:0.5697048902511597\n",
      "Epoch 0[16392/17270] Time:0.23, Train Loss:0.4490243196487427\n",
      "Epoch 0[16393/17270] Time:0.235, Train Loss:0.532485842704773\n",
      "Epoch 0[16394/17270] Time:0.236, Train Loss:0.5823791027069092\n",
      "Epoch 0[16395/17270] Time:0.236, Train Loss:0.7676869630813599\n",
      "Epoch 0[16396/17270] Time:0.237, Train Loss:1.007331132888794\n",
      "Epoch 0[16397/17270] Time:0.241, Train Loss:0.4072127938270569\n",
      "Epoch 0[16398/17270] Time:0.239, Train Loss:0.7941684126853943\n",
      "Epoch 0[16399/17270] Time:0.246, Train Loss:0.3382943272590637\n",
      "Epoch 0[16400/17270] Time:0.239, Train Loss:0.34175437688827515\n",
      "Epoch 0[16401/17270] Time:0.246, Train Loss:0.6200327277183533\n",
      "Epoch 0[16402/17270] Time:0.237, Train Loss:0.5118037462234497\n",
      "Epoch 0[16403/17270] Time:0.237, Train Loss:0.6732022166252136\n",
      "Epoch 0[16404/17270] Time:0.243, Train Loss:0.39503785967826843\n",
      "Epoch 0[16405/17270] Time:0.232, Train Loss:0.4976888597011566\n",
      "Epoch 0[16406/17270] Time:0.232, Train Loss:0.6107243299484253\n",
      "Epoch 0[16407/17270] Time:0.241, Train Loss:0.4304214417934418\n",
      "Epoch 0[16408/17270] Time:0.241, Train Loss:0.4697497487068176\n",
      "Epoch 0[16409/17270] Time:0.236, Train Loss:0.4432026147842407\n",
      "Epoch 0[16410/17270] Time:0.233, Train Loss:0.746279239654541\n",
      "Epoch 0[16411/17270] Time:0.233, Train Loss:0.5404430031776428\n",
      "Epoch 0[16412/17270] Time:0.241, Train Loss:0.591538667678833\n",
      "Epoch 0[16413/17270] Time:0.234, Train Loss:0.4028789699077606\n",
      "Epoch 0[16414/17270] Time:0.234, Train Loss:1.0780678987503052\n",
      "Epoch 0[16415/17270] Time:0.224, Train Loss:0.5838460922241211\n",
      "Epoch 0[16416/17270] Time:0.252, Train Loss:0.5346993207931519\n",
      "Epoch 0[16417/17270] Time:0.221, Train Loss:0.9911713004112244\n",
      "Epoch 0[16418/17270] Time:0.229, Train Loss:0.43633508682250977\n",
      "Epoch 0[16419/17270] Time:0.234, Train Loss:0.5467243194580078\n",
      "Epoch 0[16420/17270] Time:0.229, Train Loss:0.40354400873184204\n",
      "Epoch 0[16421/17270] Time:0.239, Train Loss:0.4965730309486389\n",
      "Epoch 0[16422/17270] Time:0.229, Train Loss:0.4664805233478546\n",
      "Epoch 0[16423/17270] Time:0.238, Train Loss:0.3386214077472687\n",
      "Epoch 0[16424/17270] Time:0.23, Train Loss:0.851928174495697\n",
      "Epoch 0[16425/17270] Time:0.231, Train Loss:0.63331139087677\n",
      "Epoch 0[16426/17270] Time:0.238, Train Loss:0.39667317271232605\n",
      "Epoch 0[16427/17270] Time:0.23, Train Loss:0.44973254203796387\n",
      "Epoch 0[16428/17270] Time:0.228, Train Loss:0.7760270237922668\n",
      "Epoch 0[16429/17270] Time:0.237, Train Loss:0.2734609544277191\n",
      "Epoch 0[16430/17270] Time:0.236, Train Loss:0.5000985860824585\n",
      "Epoch 0[16431/17270] Time:0.226, Train Loss:0.5925084948539734\n",
      "Epoch 0[16432/17270] Time:0.239, Train Loss:0.5089138150215149\n",
      "Epoch 0[16433/17270] Time:0.23, Train Loss:0.6967020630836487\n",
      "Epoch 0[16434/17270] Time:0.231, Train Loss:0.3083055913448334\n",
      "Epoch 0[16435/17270] Time:0.229, Train Loss:0.4919102191925049\n",
      "Epoch 0[16436/17270] Time:0.231, Train Loss:0.35506558418273926\n",
      "Epoch 0[16437/17270] Time:0.23, Train Loss:0.37421315908432007\n",
      "Epoch 0[16438/17270] Time:0.238, Train Loss:0.28828296065330505\n",
      "Epoch 0[16439/17270] Time:0.228, Train Loss:0.3831218481063843\n",
      "Epoch 0[16440/17270] Time:0.231, Train Loss:0.46706998348236084\n",
      "Epoch 0[16441/17270] Time:0.238, Train Loss:0.421269953250885\n",
      "Epoch 0[16442/17270] Time:0.237, Train Loss:0.4209246337413788\n",
      "Epoch 0[16443/17270] Time:0.237, Train Loss:0.3953559994697571\n",
      "Epoch 0[16444/17270] Time:0.226, Train Loss:0.5405395030975342\n",
      "Epoch 0[16445/17270] Time:0.228, Train Loss:0.6397521495819092\n",
      "Epoch 0[16446/17270] Time:0.239, Train Loss:0.5262744426727295\n",
      "Epoch 0[16447/17270] Time:0.231, Train Loss:0.3894386887550354\n",
      "Epoch 0[16448/17270] Time:0.231, Train Loss:0.3439485430717468\n",
      "Epoch 0[16449/17270] Time:0.233, Train Loss:0.3754009008407593\n",
      "Epoch 0[16450/17270] Time:0.242, Train Loss:0.4068276584148407\n",
      "Epoch 0[16451/17270] Time:0.23, Train Loss:0.36979368329048157\n",
      "Epoch 0[16452/17270] Time:0.236, Train Loss:0.3049262762069702\n",
      "Epoch 0[16453/17270] Time:0.233, Train Loss:0.2926689684391022\n",
      "Epoch 0[16454/17270] Time:0.232, Train Loss:0.36552056670188904\n",
      "Epoch 0[16455/17270] Time:0.235, Train Loss:0.4848633408546448\n",
      "Epoch 0[16456/17270] Time:0.233, Train Loss:0.9750370979309082\n",
      "Epoch 0[16457/17270] Time:0.232, Train Loss:0.42881229519844055\n",
      "Epoch 0[16458/17270] Time:0.247, Train Loss:0.9772613048553467\n",
      "Epoch 0[16459/17270] Time:0.236, Train Loss:0.6439445614814758\n",
      "Epoch 0[16460/17270] Time:0.234, Train Loss:0.8248915076255798\n",
      "Epoch 0[16461/17270] Time:0.238, Train Loss:0.3518328070640564\n",
      "Epoch 0[16462/17270] Time:0.232, Train Loss:1.0878446102142334\n",
      "Epoch 0[16463/17270] Time:0.234, Train Loss:0.4771096110343933\n",
      "Epoch 0[16464/17270] Time:0.238, Train Loss:0.6290381550788879\n",
      "Epoch 0[16465/17270] Time:0.229, Train Loss:0.44445639848709106\n",
      "Epoch 0[16466/17270] Time:0.239, Train Loss:0.21924585103988647\n",
      "Epoch 0[16467/17270] Time:0.237, Train Loss:0.5032810568809509\n",
      "Epoch 0[16468/17270] Time:0.238, Train Loss:0.34496140480041504\n",
      "Epoch 0[16469/17270] Time:0.237, Train Loss:0.5619789958000183\n",
      "Epoch 0[16470/17270] Time:0.237, Train Loss:0.4554210901260376\n",
      "Epoch 0[16471/17270] Time:0.238, Train Loss:0.2834256589412689\n",
      "Epoch 0[16472/17270] Time:0.231, Train Loss:0.5692017674446106\n",
      "Epoch 0[16473/17270] Time:0.235, Train Loss:0.2438226193189621\n",
      "Epoch 0[16474/17270] Time:0.234, Train Loss:0.9108296632766724\n",
      "Epoch 0[16475/17270] Time:0.23, Train Loss:0.48018649220466614\n",
      "Epoch 0[16476/17270] Time:0.233, Train Loss:0.41715770959854126\n",
      "Epoch 0[16477/17270] Time:0.23, Train Loss:0.3836023807525635\n",
      "Epoch 0[16478/17270] Time:0.235, Train Loss:0.6695652008056641\n",
      "Epoch 0[16479/17270] Time:0.238, Train Loss:0.24914339184761047\n",
      "Epoch 0[16480/17270] Time:0.238, Train Loss:0.5104009509086609\n",
      "Epoch 0[16481/17270] Time:0.238, Train Loss:0.41027846932411194\n",
      "Epoch 0[16482/17270] Time:0.236, Train Loss:0.5357652902603149\n",
      "Epoch 0[16483/17270] Time:0.245, Train Loss:0.47876670956611633\n",
      "Epoch 0[16484/17270] Time:0.225, Train Loss:0.39345040917396545\n",
      "Epoch 0[16485/17270] Time:0.23, Train Loss:0.5360304713249207\n",
      "Epoch 0[16486/17270] Time:0.24, Train Loss:0.7445966601371765\n",
      "Epoch 0[16487/17270] Time:0.232, Train Loss:0.5196517705917358\n",
      "Epoch 0[16488/17270] Time:0.231, Train Loss:0.8446781635284424\n",
      "Epoch 0[16489/17270] Time:0.231, Train Loss:0.3148324489593506\n",
      "Epoch 0[16490/17270] Time:0.229, Train Loss:0.25949496030807495\n",
      "Epoch 0[16491/17270] Time:0.238, Train Loss:0.5553203225135803\n",
      "Epoch 0[16492/17270] Time:0.231, Train Loss:0.5821598172187805\n",
      "Epoch 0[16493/17270] Time:0.23, Train Loss:0.4109431207180023\n",
      "Epoch 0[16494/17270] Time:0.231, Train Loss:1.188037633895874\n",
      "Epoch 0[16495/17270] Time:0.236, Train Loss:0.4393572509288788\n",
      "Epoch 0[16496/17270] Time:0.237, Train Loss:0.7249836921691895\n",
      "Epoch 0[16497/17270] Time:0.232, Train Loss:0.47300857305526733\n",
      "Epoch 0[16498/17270] Time:0.246, Train Loss:0.35422930121421814\n",
      "Epoch 0[16499/17270] Time:0.248, Train Loss:0.6057012677192688\n",
      "Epoch 0[16500/17270] Time:0.237, Train Loss:0.5540368556976318\n",
      "Epoch 0[16501/17270] Time:0.237, Train Loss:0.5567135810852051\n",
      "Epoch 0[16502/17270] Time:0.236, Train Loss:0.8723151683807373\n",
      "Epoch 0[16503/17270] Time:0.242, Train Loss:0.5322023034095764\n",
      "Epoch 0[16504/17270] Time:0.247, Train Loss:0.2803901135921478\n",
      "Epoch 0[16505/17270] Time:0.232, Train Loss:0.49937382340431213\n",
      "Epoch 0[16506/17270] Time:0.234, Train Loss:0.5171009302139282\n",
      "Epoch 0[16507/17270] Time:0.223, Train Loss:0.419234037399292\n",
      "Epoch 0[16508/17270] Time:0.23, Train Loss:0.4882775843143463\n",
      "Epoch 0[16509/17270] Time:0.238, Train Loss:0.7461310625076294\n",
      "Epoch 0[16510/17270] Time:0.23, Train Loss:0.3839632570743561\n",
      "Epoch 0[16511/17270] Time:0.228, Train Loss:0.23618604242801666\n",
      "Epoch 0[16512/17270] Time:0.229, Train Loss:0.4004920721054077\n",
      "Epoch 0[16513/17270] Time:0.227, Train Loss:0.541258692741394\n",
      "Epoch 0[16514/17270] Time:0.229, Train Loss:0.3762352466583252\n",
      "Epoch 0[16515/17270] Time:0.228, Train Loss:0.39980843663215637\n",
      "Epoch 0[16516/17270] Time:0.23, Train Loss:0.7975671291351318\n",
      "Epoch 0[16517/17270] Time:0.232, Train Loss:1.146118402481079\n",
      "Epoch 0[16518/17270] Time:0.24, Train Loss:0.43186134099960327\n",
      "Epoch 0[16519/17270] Time:0.23, Train Loss:0.39390984177589417\n",
      "Epoch 0[16520/17270] Time:0.228, Train Loss:0.36315691471099854\n",
      "Epoch 0[16521/17270] Time:0.234, Train Loss:0.5874232053756714\n",
      "Epoch 0[16522/17270] Time:0.229, Train Loss:0.5517493486404419\n",
      "Epoch 0[16523/17270] Time:0.237, Train Loss:0.7362451553344727\n",
      "Epoch 0[16524/17270] Time:0.235, Train Loss:0.33696573972702026\n",
      "Epoch 0[16525/17270] Time:0.231, Train Loss:0.23534566164016724\n",
      "Epoch 0[16526/17270] Time:0.24, Train Loss:0.3380776345729828\n",
      "Epoch 0[16527/17270] Time:0.227, Train Loss:0.4716636538505554\n",
      "Epoch 0[16528/17270] Time:0.236, Train Loss:0.6434326767921448\n",
      "Epoch 0[16529/17270] Time:0.235, Train Loss:1.0598816871643066\n",
      "Epoch 0[16530/17270] Time:0.23, Train Loss:0.360142320394516\n",
      "Epoch 0[16531/17270] Time:0.24, Train Loss:0.4900577962398529\n",
      "Epoch 0[16532/17270] Time:0.236, Train Loss:0.6566203236579895\n",
      "Epoch 0[16533/17270] Time:0.235, Train Loss:0.3009321689605713\n",
      "Epoch 0[16534/17270] Time:0.238, Train Loss:0.35964271426200867\n",
      "Epoch 0[16535/17270] Time:0.23, Train Loss:0.5331348776817322\n",
      "Epoch 0[16536/17270] Time:0.23, Train Loss:0.3757289946079254\n",
      "Epoch 0[16537/17270] Time:0.237, Train Loss:0.7597822546958923\n",
      "Epoch 0[16538/17270] Time:0.235, Train Loss:0.4658486843109131\n",
      "Epoch 0[16539/17270] Time:0.24, Train Loss:0.47421833872795105\n",
      "Epoch 0[16540/17270] Time:0.231, Train Loss:0.21537090837955475\n",
      "Epoch 0[16541/17270] Time:0.235, Train Loss:0.23558512330055237\n",
      "Epoch 0[16542/17270] Time:0.232, Train Loss:1.4181965589523315\n",
      "Epoch 0[16543/17270] Time:0.238, Train Loss:0.5306420922279358\n",
      "Epoch 0[16544/17270] Time:0.234, Train Loss:0.5547602772712708\n",
      "Epoch 0[16545/17270] Time:0.221, Train Loss:0.33346158266067505\n",
      "Epoch 0[16546/17270] Time:0.244, Train Loss:0.6109933257102966\n",
      "Epoch 0[16547/17270] Time:0.226, Train Loss:0.3353390693664551\n",
      "Epoch 0[16548/17270] Time:0.249, Train Loss:1.0143742561340332\n",
      "Epoch 0[16549/17270] Time:0.241, Train Loss:0.38212764263153076\n",
      "Epoch 0[16550/17270] Time:0.235, Train Loss:0.5649939179420471\n",
      "Epoch 0[16551/17270] Time:0.232, Train Loss:1.111390471458435\n",
      "Epoch 0[16552/17270] Time:0.233, Train Loss:0.5724359154701233\n",
      "Epoch 0[16553/17270] Time:0.237, Train Loss:0.36835259199142456\n",
      "Epoch 0[16554/17270] Time:0.231, Train Loss:0.5584969520568848\n",
      "Epoch 0[16555/17270] Time:0.237, Train Loss:0.9382570385932922\n",
      "Epoch 0[16556/17270] Time:0.225, Train Loss:0.9219733476638794\n",
      "Epoch 0[16557/17270] Time:0.24, Train Loss:0.5510610938072205\n",
      "Epoch 0[16558/17270] Time:0.24, Train Loss:0.46624717116355896\n",
      "Epoch 0[16559/17270] Time:0.239, Train Loss:0.6851319670677185\n",
      "Epoch 0[16560/17270] Time:0.232, Train Loss:0.7318673133850098\n",
      "Epoch 0[16561/17270] Time:0.224, Train Loss:0.7143471837043762\n",
      "Epoch 0[16562/17270] Time:0.238, Train Loss:0.47663646936416626\n",
      "Epoch 0[16563/17270] Time:0.237, Train Loss:0.2359054535627365\n",
      "Epoch 0[16564/17270] Time:0.238, Train Loss:0.6628245115280151\n",
      "Epoch 0[16565/17270] Time:0.235, Train Loss:0.5904514193534851\n",
      "Epoch 0[16566/17270] Time:0.239, Train Loss:0.5132966041564941\n",
      "Epoch 0[16567/17270] Time:0.227, Train Loss:0.8287719488143921\n",
      "Epoch 0[16568/17270] Time:0.233, Train Loss:0.40208885073661804\n",
      "Epoch 0[16569/17270] Time:0.241, Train Loss:0.4872075319290161\n",
      "Epoch 0[16570/17270] Time:0.243, Train Loss:0.3860027492046356\n",
      "Epoch 0[16571/17270] Time:0.234, Train Loss:0.2413148134946823\n",
      "Epoch 0[16572/17270] Time:0.25, Train Loss:0.23436763882637024\n",
      "Epoch 0[16573/17270] Time:0.236, Train Loss:0.5940060615539551\n",
      "Epoch 0[16574/17270] Time:0.231, Train Loss:0.4147874116897583\n",
      "Epoch 0[16575/17270] Time:0.232, Train Loss:0.7863379120826721\n",
      "Epoch 0[16576/17270] Time:0.235, Train Loss:0.45100927352905273\n",
      "Epoch 0[16577/17270] Time:0.25, Train Loss:1.156135082244873\n",
      "Epoch 0[16578/17270] Time:0.245, Train Loss:0.5279707312583923\n",
      "Epoch 0[16579/17270] Time:0.227, Train Loss:0.47046568989753723\n",
      "Epoch 0[16580/17270] Time:0.233, Train Loss:0.6999694108963013\n",
      "Epoch 0[16581/17270] Time:0.235, Train Loss:0.37520405650138855\n",
      "Epoch 0[16582/17270] Time:0.224, Train Loss:0.6524322032928467\n",
      "Epoch 0[16583/17270] Time:0.229, Train Loss:0.3634456396102905\n",
      "Epoch 0[16584/17270] Time:0.236, Train Loss:0.48481976985931396\n",
      "Epoch 0[16585/17270] Time:0.23, Train Loss:0.4995785653591156\n",
      "Epoch 0[16586/17270] Time:0.232, Train Loss:0.8566699624061584\n",
      "Epoch 0[16587/17270] Time:0.236, Train Loss:0.5819970965385437\n",
      "Epoch 0[16588/17270] Time:0.231, Train Loss:0.440041720867157\n",
      "Epoch 0[16589/17270] Time:0.23, Train Loss:0.46585580706596375\n",
      "Epoch 0[16590/17270] Time:0.234, Train Loss:0.7604193091392517\n",
      "Epoch 0[16591/17270] Time:0.223, Train Loss:1.1270596981048584\n",
      "Epoch 0[16592/17270] Time:0.233, Train Loss:0.5306232571601868\n",
      "Epoch 0[16593/17270] Time:0.231, Train Loss:0.3120209872722626\n",
      "Epoch 0[16594/17270] Time:0.24, Train Loss:0.5201355218887329\n",
      "Epoch 0[16595/17270] Time:0.23, Train Loss:0.44606801867485046\n",
      "Epoch 0[16596/17270] Time:0.231, Train Loss:0.9662078022956848\n",
      "Epoch 0[16597/17270] Time:0.235, Train Loss:0.3243089020252228\n",
      "Epoch 0[16598/17270] Time:0.238, Train Loss:0.44685450196266174\n",
      "Epoch 0[16599/17270] Time:0.232, Train Loss:0.4330522119998932\n",
      "Epoch 0[16600/17270] Time:0.234, Train Loss:0.33163803815841675\n",
      "Epoch 0[16601/17270] Time:0.236, Train Loss:0.8715432286262512\n",
      "Epoch 0[16602/17270] Time:0.228, Train Loss:0.40125685930252075\n",
      "Epoch 0[16603/17270] Time:0.229, Train Loss:0.5091314315795898\n",
      "Epoch 0[16604/17270] Time:0.23, Train Loss:0.38536661863327026\n",
      "Epoch 0[16605/17270] Time:0.226, Train Loss:0.6265853047370911\n",
      "Epoch 0[16606/17270] Time:0.236, Train Loss:0.6494957804679871\n",
      "Epoch 0[16607/17270] Time:0.233, Train Loss:0.743672251701355\n",
      "Epoch 0[16608/17270] Time:0.232, Train Loss:0.5644235610961914\n",
      "Epoch 0[16609/17270] Time:0.232, Train Loss:1.0710912942886353\n",
      "Epoch 0[16610/17270] Time:0.232, Train Loss:0.4707068204879761\n",
      "Epoch 0[16611/17270] Time:0.244, Train Loss:0.34882622957229614\n",
      "Epoch 0[16612/17270] Time:0.236, Train Loss:0.5491721034049988\n",
      "Epoch 0[16613/17270] Time:0.23, Train Loss:0.7919275760650635\n",
      "Epoch 0[16614/17270] Time:0.244, Train Loss:0.8956453204154968\n",
      "Epoch 0[16615/17270] Time:0.235, Train Loss:0.541834831237793\n",
      "Epoch 0[16616/17270] Time:0.237, Train Loss:0.345497190952301\n",
      "Epoch 0[16617/17270] Time:0.234, Train Loss:0.5347779393196106\n",
      "Epoch 0[16618/17270] Time:0.231, Train Loss:0.49333322048187256\n",
      "Epoch 0[16619/17270] Time:0.231, Train Loss:0.5528777837753296\n",
      "Epoch 0[16620/17270] Time:0.239, Train Loss:1.0693782567977905\n",
      "Epoch 0[16621/17270] Time:0.235, Train Loss:0.43115469813346863\n",
      "Epoch 0[16622/17270] Time:0.226, Train Loss:0.7096284627914429\n",
      "Epoch 0[16623/17270] Time:0.24, Train Loss:0.8097856640815735\n",
      "Epoch 0[16624/17270] Time:0.236, Train Loss:0.9010391235351562\n",
      "Epoch 0[16625/17270] Time:0.23, Train Loss:0.4036228358745575\n",
      "Epoch 0[16626/17270] Time:0.237, Train Loss:0.5046563744544983\n",
      "Epoch 0[16627/17270] Time:0.228, Train Loss:0.37080252170562744\n",
      "Epoch 0[16628/17270] Time:0.235, Train Loss:0.7482513785362244\n",
      "Epoch 0[16629/17270] Time:0.22, Train Loss:0.4246135652065277\n",
      "Epoch 0[16630/17270] Time:0.239, Train Loss:0.41783592104911804\n",
      "Epoch 0[16631/17270] Time:0.235, Train Loss:0.5154240727424622\n",
      "Epoch 0[16632/17270] Time:0.228, Train Loss:0.5647031664848328\n",
      "Epoch 0[16633/17270] Time:0.228, Train Loss:0.6592122912406921\n",
      "Epoch 0[16634/17270] Time:0.227, Train Loss:0.5049313306808472\n",
      "Epoch 0[16635/17270] Time:0.234, Train Loss:0.6320602297782898\n",
      "Epoch 0[16636/17270] Time:0.23, Train Loss:0.9160926342010498\n",
      "Epoch 0[16637/17270] Time:0.237, Train Loss:0.530731737613678\n",
      "Epoch 0[16638/17270] Time:0.237, Train Loss:0.775943398475647\n",
      "Epoch 0[16639/17270] Time:0.228, Train Loss:0.36029255390167236\n",
      "Epoch 0[16640/17270] Time:0.231, Train Loss:0.8314289450645447\n",
      "Epoch 0[16641/17270] Time:0.24, Train Loss:0.6080244183540344\n",
      "Epoch 0[16642/17270] Time:0.232, Train Loss:0.42972737550735474\n",
      "Epoch 0[16643/17270] Time:0.233, Train Loss:0.27553296089172363\n",
      "Epoch 0[16644/17270] Time:0.229, Train Loss:0.465691477060318\n",
      "Epoch 0[16645/17270] Time:0.23, Train Loss:0.39002910256385803\n",
      "Epoch 0[16646/17270] Time:0.229, Train Loss:0.23489734530448914\n",
      "Epoch 0[16647/17270] Time:0.238, Train Loss:1.6709092855453491\n",
      "Epoch 0[16648/17270] Time:0.238, Train Loss:0.679669201374054\n",
      "Epoch 0[16649/17270] Time:0.236, Train Loss:0.23923444747924805\n",
      "Epoch 0[16650/17270] Time:0.251, Train Loss:0.5872801542282104\n",
      "Epoch 0[16651/17270] Time:0.228, Train Loss:0.34488680958747864\n",
      "Epoch 0[16652/17270] Time:0.234, Train Loss:0.978310227394104\n",
      "Epoch 0[16653/17270] Time:0.23, Train Loss:0.4180317521095276\n",
      "Epoch 0[16654/17270] Time:0.236, Train Loss:0.7760236859321594\n",
      "Epoch 0[16655/17270] Time:0.239, Train Loss:0.5993219614028931\n",
      "Epoch 0[16656/17270] Time:0.231, Train Loss:0.4851020872592926\n",
      "Epoch 0[16657/17270] Time:0.237, Train Loss:0.6373952031135559\n",
      "Epoch 0[16658/17270] Time:0.228, Train Loss:0.558653712272644\n",
      "Epoch 0[16659/17270] Time:0.232, Train Loss:0.41373583674430847\n",
      "Epoch 0[16660/17270] Time:0.23, Train Loss:0.6372334957122803\n",
      "Epoch 0[16661/17270] Time:0.237, Train Loss:0.4115021228790283\n",
      "Epoch 0[16662/17270] Time:0.238, Train Loss:0.8189089298248291\n",
      "Epoch 0[16663/17270] Time:0.234, Train Loss:0.5012370347976685\n",
      "Epoch 0[16664/17270] Time:0.241, Train Loss:0.473626047372818\n",
      "Epoch 0[16665/17270] Time:0.232, Train Loss:0.37026074528694153\n",
      "Epoch 0[16666/17270] Time:0.227, Train Loss:0.48432785272598267\n",
      "Epoch 0[16667/17270] Time:0.228, Train Loss:0.4276250898838043\n",
      "Epoch 0[16668/17270] Time:0.245, Train Loss:1.2934333086013794\n",
      "Epoch 0[16669/17270] Time:0.231, Train Loss:0.7482463717460632\n",
      "Epoch 0[16670/17270] Time:0.238, Train Loss:0.7207380533218384\n",
      "Epoch 0[16671/17270] Time:0.234, Train Loss:0.34832125902175903\n",
      "Epoch 0[16672/17270] Time:0.229, Train Loss:1.0712296962738037\n",
      "Epoch 0[16673/17270] Time:0.235, Train Loss:0.5192369818687439\n",
      "Epoch 0[16674/17270] Time:0.237, Train Loss:0.49505677819252014\n",
      "Epoch 0[16675/17270] Time:0.225, Train Loss:0.37632471323013306\n",
      "Epoch 0[16676/17270] Time:0.233, Train Loss:0.42819732427597046\n",
      "Epoch 0[16677/17270] Time:0.237, Train Loss:0.36781466007232666\n",
      "Epoch 0[16678/17270] Time:0.235, Train Loss:0.40000802278518677\n",
      "Epoch 0[16679/17270] Time:0.229, Train Loss:0.4363548755645752\n",
      "Epoch 0[16680/17270] Time:0.238, Train Loss:0.55979323387146\n",
      "Epoch 0[16681/17270] Time:0.232, Train Loss:0.687736988067627\n",
      "Epoch 0[16682/17270] Time:0.233, Train Loss:0.5304080247879028\n",
      "Epoch 0[16683/17270] Time:0.25, Train Loss:0.5864593386650085\n",
      "Epoch 0[16684/17270] Time:0.229, Train Loss:0.44543737173080444\n",
      "Epoch 0[16685/17270] Time:0.232, Train Loss:0.6335591673851013\n",
      "Epoch 0[16686/17270] Time:0.239, Train Loss:0.3092000484466553\n",
      "Epoch 0[16687/17270] Time:0.246, Train Loss:0.7027064561843872\n",
      "Epoch 0[16688/17270] Time:0.235, Train Loss:0.5803981423377991\n",
      "Epoch 0[16689/17270] Time:0.24, Train Loss:0.5486425161361694\n",
      "Epoch 0[16690/17270] Time:0.236, Train Loss:1.011600136756897\n",
      "Epoch 0[16691/17270] Time:0.238, Train Loss:0.5206556916236877\n",
      "Epoch 0[16692/17270] Time:0.236, Train Loss:0.3274056315422058\n",
      "Epoch 0[16693/17270] Time:0.227, Train Loss:0.3928075432777405\n",
      "Epoch 0[16694/17270] Time:0.231, Train Loss:0.5267854928970337\n",
      "Epoch 0[16695/17270] Time:0.228, Train Loss:0.5346685647964478\n",
      "Epoch 0[16696/17270] Time:0.227, Train Loss:0.6726734638214111\n",
      "Epoch 0[16697/17270] Time:0.237, Train Loss:0.46344926953315735\n",
      "Epoch 0[16698/17270] Time:0.238, Train Loss:0.4526122212409973\n",
      "Epoch 0[16699/17270] Time:0.229, Train Loss:0.3530671298503876\n",
      "Epoch 0[16700/17270] Time:0.237, Train Loss:0.43260982632637024\n",
      "Epoch 0[16701/17270] Time:0.235, Train Loss:0.323725163936615\n",
      "Epoch 0[16702/17270] Time:0.229, Train Loss:0.3241824507713318\n",
      "Epoch 0[16703/17270] Time:0.235, Train Loss:0.3445587456226349\n",
      "Epoch 0[16704/17270] Time:0.237, Train Loss:0.4923577904701233\n",
      "Epoch 0[16705/17270] Time:0.24, Train Loss:0.3964955806732178\n",
      "Epoch 0[16706/17270] Time:0.227, Train Loss:0.34998267889022827\n",
      "Epoch 0[16707/17270] Time:0.237, Train Loss:0.3769495189189911\n",
      "Epoch 0[16708/17270] Time:0.239, Train Loss:0.6316013336181641\n",
      "Epoch 0[16709/17270] Time:0.225, Train Loss:0.8051720857620239\n",
      "Epoch 0[16710/17270] Time:0.237, Train Loss:0.5985087156295776\n",
      "Epoch 0[16711/17270] Time:0.238, Train Loss:0.46175381541252136\n",
      "Epoch 0[16712/17270] Time:0.242, Train Loss:0.5074515342712402\n",
      "Epoch 0[16713/17270] Time:0.23, Train Loss:0.370724081993103\n",
      "Epoch 0[16714/17270] Time:0.23, Train Loss:0.732046902179718\n",
      "Epoch 0[16715/17270] Time:0.236, Train Loss:0.5806974768638611\n",
      "Epoch 0[16716/17270] Time:0.235, Train Loss:1.085559606552124\n",
      "Epoch 0[16717/17270] Time:0.231, Train Loss:0.4801836907863617\n",
      "Epoch 0[16718/17270] Time:0.229, Train Loss:0.6860675811767578\n",
      "Epoch 0[16719/17270] Time:0.231, Train Loss:0.39702045917510986\n",
      "Epoch 0[16720/17270] Time:0.234, Train Loss:0.6799657940864563\n",
      "Epoch 0[16721/17270] Time:0.23, Train Loss:1.0430059432983398\n",
      "Epoch 0[16722/17270] Time:0.231, Train Loss:0.9541559219360352\n",
      "Epoch 0[16723/17270] Time:0.232, Train Loss:0.5483436584472656\n",
      "Epoch 0[16724/17270] Time:0.231, Train Loss:0.7742920517921448\n",
      "Epoch 0[16725/17270] Time:0.236, Train Loss:0.784127950668335\n",
      "Epoch 0[16726/17270] Time:0.233, Train Loss:0.4728326201438904\n",
      "Epoch 0[16727/17270] Time:0.23, Train Loss:0.7692465782165527\n",
      "Epoch 0[16728/17270] Time:0.239, Train Loss:0.5585812926292419\n",
      "Epoch 0[16729/17270] Time:0.23, Train Loss:0.3305823802947998\n",
      "Epoch 0[16730/17270] Time:0.23, Train Loss:1.141223430633545\n",
      "Epoch 0[16731/17270] Time:0.236, Train Loss:0.35153013467788696\n",
      "Epoch 0[16732/17270] Time:0.238, Train Loss:0.517662763595581\n",
      "Epoch 0[16733/17270] Time:0.238, Train Loss:0.6382299065589905\n",
      "Epoch 0[16734/17270] Time:0.23, Train Loss:0.6366016268730164\n",
      "Epoch 0[16735/17270] Time:0.237, Train Loss:0.3995171785354614\n",
      "Epoch 0[16736/17270] Time:0.237, Train Loss:0.4394476115703583\n",
      "Epoch 0[16737/17270] Time:0.237, Train Loss:0.41198480129241943\n",
      "Epoch 0[16738/17270] Time:0.242, Train Loss:0.5062422752380371\n",
      "Epoch 0[16739/17270] Time:0.229, Train Loss:0.5503462553024292\n",
      "Epoch 0[16740/17270] Time:0.234, Train Loss:1.3097028732299805\n",
      "Epoch 0[16741/17270] Time:0.244, Train Loss:0.4106273353099823\n",
      "Epoch 0[16742/17270] Time:0.237, Train Loss:0.581818163394928\n",
      "Epoch 0[16743/17270] Time:0.243, Train Loss:0.6665616035461426\n",
      "Epoch 0[16744/17270] Time:0.233, Train Loss:0.3419896066188812\n",
      "Epoch 0[16745/17270] Time:0.254, Train Loss:1.0838598012924194\n",
      "Epoch 0[16746/17270] Time:0.229, Train Loss:0.49874821305274963\n",
      "Epoch 0[16747/17270] Time:0.234, Train Loss:0.5773313641548157\n",
      "Epoch 0[16748/17270] Time:0.238, Train Loss:0.6115990281105042\n",
      "Epoch 0[16749/17270] Time:0.248, Train Loss:0.45767050981521606\n",
      "Epoch 0[16750/17270] Time:0.237, Train Loss:0.6145280599594116\n",
      "Epoch 0[16751/17270] Time:0.24, Train Loss:0.27006280422210693\n",
      "Epoch 0[16752/17270] Time:0.249, Train Loss:0.4553009569644928\n",
      "Epoch 0[16753/17270] Time:0.245, Train Loss:0.719633162021637\n",
      "Epoch 0[16754/17270] Time:0.237, Train Loss:0.5540732145309448\n",
      "Epoch 0[16755/17270] Time:0.244, Train Loss:0.5194178223609924\n",
      "Epoch 0[16756/17270] Time:0.242, Train Loss:0.5142587423324585\n",
      "Epoch 0[16757/17270] Time:0.236, Train Loss:0.6718596816062927\n",
      "Epoch 0[16758/17270] Time:0.237, Train Loss:0.36332327127456665\n",
      "Epoch 0[16759/17270] Time:0.227, Train Loss:0.8419170379638672\n",
      "Epoch 0[16760/17270] Time:0.244, Train Loss:0.5647642016410828\n",
      "Epoch 0[16761/17270] Time:0.243, Train Loss:0.6380499601364136\n",
      "Epoch 0[16762/17270] Time:0.237, Train Loss:0.392629474401474\n",
      "Epoch 0[16763/17270] Time:0.234, Train Loss:0.3828883469104767\n",
      "Epoch 0[16764/17270] Time:0.233, Train Loss:0.3773614168167114\n",
      "Epoch 0[16765/17270] Time:0.247, Train Loss:0.623941957950592\n",
      "Epoch 0[16766/17270] Time:0.229, Train Loss:0.4110412299633026\n",
      "Epoch 0[16767/17270] Time:0.228, Train Loss:0.3714892864227295\n",
      "Epoch 0[16768/17270] Time:0.227, Train Loss:0.5322171449661255\n",
      "Epoch 0[16769/17270] Time:0.228, Train Loss:0.3970770835876465\n",
      "Epoch 0[16770/17270] Time:0.231, Train Loss:0.6119506359100342\n",
      "Epoch 0[16771/17270] Time:0.226, Train Loss:0.5975173115730286\n",
      "Epoch 0[16772/17270] Time:0.238, Train Loss:1.0360198020935059\n",
      "Epoch 0[16773/17270] Time:0.237, Train Loss:0.5654723644256592\n",
      "Epoch 0[16774/17270] Time:0.227, Train Loss:0.3318144679069519\n",
      "Epoch 0[16775/17270] Time:0.241, Train Loss:0.417327880859375\n",
      "Epoch 0[16776/17270] Time:0.229, Train Loss:0.3777408301830292\n",
      "Epoch 0[16777/17270] Time:0.228, Train Loss:0.6381803154945374\n",
      "Epoch 0[16778/17270] Time:0.228, Train Loss:0.7676052451133728\n",
      "Epoch 0[16779/17270] Time:0.229, Train Loss:0.899933397769928\n",
      "Epoch 0[16780/17270] Time:0.228, Train Loss:0.4935609996318817\n",
      "Epoch 0[16781/17270] Time:0.23, Train Loss:0.7058598399162292\n",
      "Epoch 0[16782/17270] Time:0.23, Train Loss:0.45454034209251404\n",
      "Epoch 0[16783/17270] Time:0.247, Train Loss:0.4034254848957062\n",
      "Epoch 0[16784/17270] Time:0.231, Train Loss:0.8800138831138611\n",
      "Epoch 0[16785/17270] Time:0.236, Train Loss:0.42777156829833984\n",
      "Epoch 0[16786/17270] Time:0.239, Train Loss:1.00053870677948\n",
      "Epoch 0[16787/17270] Time:0.249, Train Loss:0.544684112071991\n",
      "Epoch 0[16788/17270] Time:0.255, Train Loss:0.4346162974834442\n",
      "Epoch 0[16789/17270] Time:0.24, Train Loss:0.6587678790092468\n",
      "Epoch 0[16790/17270] Time:0.235, Train Loss:0.37244391441345215\n",
      "Epoch 0[16791/17270] Time:0.248, Train Loss:0.36908015608787537\n",
      "Epoch 0[16792/17270] Time:0.23, Train Loss:0.4902550280094147\n",
      "Epoch 0[16793/17270] Time:0.236, Train Loss:0.4891943335533142\n",
      "Epoch 0[16794/17270] Time:0.229, Train Loss:0.47044017910957336\n",
      "Epoch 0[16795/17270] Time:0.233, Train Loss:0.5593245029449463\n",
      "Epoch 0[16796/17270] Time:0.237, Train Loss:1.3832993507385254\n",
      "Epoch 0[16797/17270] Time:0.235, Train Loss:0.2619403600692749\n",
      "Epoch 0[16798/17270] Time:0.238, Train Loss:0.498317152261734\n",
      "Epoch 0[16799/17270] Time:0.226, Train Loss:0.6471786499023438\n",
      "Epoch 0[16800/17270] Time:0.23, Train Loss:0.7320470809936523\n",
      "Epoch 0[16801/17270] Time:0.234, Train Loss:0.29041317105293274\n",
      "Epoch 0[16802/17270] Time:0.229, Train Loss:0.47982773184776306\n",
      "Epoch 0[16803/17270] Time:0.228, Train Loss:0.43832170963287354\n",
      "Epoch 0[16804/17270] Time:0.235, Train Loss:0.4087699353694916\n",
      "Epoch 0[16805/17270] Time:0.237, Train Loss:0.39619120955467224\n",
      "Epoch 0[16806/17270] Time:0.236, Train Loss:0.43837451934814453\n",
      "Epoch 0[16807/17270] Time:0.23, Train Loss:0.6267644762992859\n",
      "Epoch 0[16808/17270] Time:0.238, Train Loss:0.36266326904296875\n",
      "Epoch 0[16809/17270] Time:0.228, Train Loss:0.4312877357006073\n",
      "Epoch 0[16810/17270] Time:0.228, Train Loss:0.35995665192604065\n",
      "Epoch 0[16811/17270] Time:0.233, Train Loss:0.24453681707382202\n",
      "Epoch 0[16812/17270] Time:0.229, Train Loss:0.47361230850219727\n",
      "Epoch 0[16813/17270] Time:0.226, Train Loss:0.6144081354141235\n",
      "Epoch 0[16814/17270] Time:0.231, Train Loss:0.6093147397041321\n",
      "Epoch 0[16815/17270] Time:0.237, Train Loss:0.3041517734527588\n",
      "Epoch 0[16816/17270] Time:0.236, Train Loss:0.4637410640716553\n",
      "Epoch 0[16817/17270] Time:0.237, Train Loss:0.915503740310669\n",
      "Epoch 0[16818/17270] Time:0.237, Train Loss:0.8838077783584595\n",
      "Epoch 0[16819/17270] Time:0.228, Train Loss:0.642400324344635\n",
      "Epoch 0[16820/17270] Time:0.246, Train Loss:0.5986030697822571\n",
      "Epoch 0[16821/17270] Time:0.224, Train Loss:0.34096041321754456\n",
      "Epoch 0[16822/17270] Time:0.238, Train Loss:0.6182963252067566\n",
      "Epoch 0[16823/17270] Time:0.223, Train Loss:0.33781981468200684\n",
      "Epoch 0[16824/17270] Time:0.228, Train Loss:0.36757388710975647\n",
      "Epoch 0[16825/17270] Time:0.239, Train Loss:0.4038122892379761\n",
      "Epoch 0[16826/17270] Time:0.237, Train Loss:0.4822361469268799\n",
      "Epoch 0[16827/17270] Time:0.233, Train Loss:0.3919651210308075\n",
      "Epoch 0[16828/17270] Time:0.237, Train Loss:0.7260274291038513\n",
      "Epoch 0[16829/17270] Time:0.229, Train Loss:0.4538499414920807\n",
      "Epoch 0[16830/17270] Time:0.236, Train Loss:0.7498688697814941\n",
      "Epoch 0[16831/17270] Time:0.231, Train Loss:0.3174697756767273\n",
      "Epoch 0[16832/17270] Time:0.23, Train Loss:0.6263370513916016\n",
      "Epoch 0[16833/17270] Time:0.229, Train Loss:0.8348270058631897\n",
      "Epoch 0[16834/17270] Time:0.235, Train Loss:0.5003751516342163\n",
      "Epoch 0[16835/17270] Time:0.249, Train Loss:0.4786706566810608\n",
      "Epoch 0[16836/17270] Time:0.237, Train Loss:0.7863225936889648\n",
      "Epoch 0[16837/17270] Time:0.236, Train Loss:0.4999309480190277\n",
      "Epoch 0[16838/17270] Time:0.23, Train Loss:0.2210521548986435\n",
      "Epoch 0[16839/17270] Time:0.238, Train Loss:0.5435969829559326\n",
      "Epoch 0[16840/17270] Time:0.239, Train Loss:0.47679269313812256\n",
      "Epoch 0[16841/17270] Time:0.229, Train Loss:0.7672445774078369\n",
      "Epoch 0[16842/17270] Time:0.228, Train Loss:0.5295142531394958\n",
      "Epoch 0[16843/17270] Time:0.231, Train Loss:0.49743667244911194\n",
      "Epoch 0[16844/17270] Time:0.229, Train Loss:0.26395660638809204\n",
      "Epoch 0[16845/17270] Time:0.237, Train Loss:0.4375789165496826\n",
      "Epoch 0[16846/17270] Time:0.246, Train Loss:0.904773473739624\n",
      "Epoch 0[16847/17270] Time:0.236, Train Loss:0.33127743005752563\n",
      "Epoch 0[16848/17270] Time:0.235, Train Loss:0.41947823762893677\n",
      "Epoch 0[16849/17270] Time:0.254, Train Loss:0.266532301902771\n",
      "Epoch 0[16850/17270] Time:0.234, Train Loss:0.36848413944244385\n",
      "Epoch 0[16851/17270] Time:0.232, Train Loss:0.4839492440223694\n",
      "Epoch 0[16852/17270] Time:0.245, Train Loss:0.7124758362770081\n",
      "Epoch 0[16853/17270] Time:0.237, Train Loss:0.2500201165676117\n",
      "Epoch 0[16854/17270] Time:0.233, Train Loss:0.47296831011772156\n",
      "Epoch 0[16855/17270] Time:0.224, Train Loss:0.6589467525482178\n",
      "Epoch 0[16856/17270] Time:0.232, Train Loss:0.31700533628463745\n",
      "Epoch 0[16857/17270] Time:0.227, Train Loss:0.42524659633636475\n",
      "Epoch 0[16858/17270] Time:0.229, Train Loss:0.37953680753707886\n",
      "Epoch 0[16859/17270] Time:0.236, Train Loss:0.5588533878326416\n",
      "Epoch 0[16860/17270] Time:0.237, Train Loss:0.44564369320869446\n",
      "Epoch 0[16861/17270] Time:0.237, Train Loss:0.36355704069137573\n",
      "Epoch 0[16862/17270] Time:0.237, Train Loss:0.5224015116691589\n",
      "Epoch 0[16863/17270] Time:0.229, Train Loss:0.24355624616146088\n",
      "Epoch 0[16864/17270] Time:0.239, Train Loss:0.4071556627750397\n",
      "Epoch 0[16865/17270] Time:0.239, Train Loss:0.6260009407997131\n",
      "Epoch 0[16866/17270] Time:0.231, Train Loss:0.4448810815811157\n",
      "Epoch 0[16867/17270] Time:0.23, Train Loss:0.8543113470077515\n",
      "Epoch 0[16868/17270] Time:0.233, Train Loss:0.617550253868103\n",
      "Epoch 0[16869/17270] Time:0.229, Train Loss:0.5374410152435303\n",
      "Epoch 0[16870/17270] Time:0.231, Train Loss:0.4296434819698334\n",
      "Epoch 0[16871/17270] Time:0.231, Train Loss:0.42256563901901245\n",
      "Epoch 0[16872/17270] Time:0.233, Train Loss:0.43329137563705444\n",
      "Epoch 0[16873/17270] Time:0.237, Train Loss:0.40680956840515137\n",
      "Epoch 0[16874/17270] Time:0.231, Train Loss:0.3149300217628479\n",
      "Epoch 0[16875/17270] Time:0.238, Train Loss:0.32917651534080505\n",
      "Epoch 0[16876/17270] Time:0.229, Train Loss:0.5433735847473145\n",
      "Epoch 0[16877/17270] Time:0.228, Train Loss:0.3891104459762573\n",
      "Epoch 0[16878/17270] Time:0.229, Train Loss:0.33019110560417175\n",
      "Epoch 0[16879/17270] Time:0.228, Train Loss:1.246847152709961\n",
      "Epoch 0[16880/17270] Time:0.24, Train Loss:0.23464782536029816\n",
      "Epoch 0[16881/17270] Time:0.23, Train Loss:0.26189467310905457\n",
      "Epoch 0[16882/17270] Time:0.237, Train Loss:0.5920735001564026\n",
      "Epoch 0[16883/17270] Time:0.236, Train Loss:0.3762701153755188\n",
      "Epoch 0[16884/17270] Time:0.237, Train Loss:0.43521976470947266\n",
      "Epoch 0[16885/17270] Time:0.232, Train Loss:1.1084262132644653\n",
      "Epoch 0[16886/17270] Time:0.229, Train Loss:0.5381750464439392\n",
      "Epoch 0[16887/17270] Time:0.231, Train Loss:0.40630725026130676\n",
      "Epoch 0[16888/17270] Time:0.23, Train Loss:0.3408460021018982\n",
      "Epoch 0[16889/17270] Time:0.235, Train Loss:0.5195090770721436\n",
      "Epoch 0[16890/17270] Time:0.238, Train Loss:0.7309826016426086\n",
      "Epoch 0[16891/17270] Time:0.23, Train Loss:0.3956841826438904\n",
      "Epoch 0[16892/17270] Time:0.229, Train Loss:0.5816190242767334\n",
      "Epoch 0[16893/17270] Time:0.237, Train Loss:0.9643886089324951\n",
      "Epoch 0[16894/17270] Time:0.236, Train Loss:0.3547191321849823\n",
      "Epoch 0[16895/17270] Time:0.232, Train Loss:0.8529753684997559\n",
      "Epoch 0[16896/17270] Time:0.23, Train Loss:0.6547854542732239\n",
      "Epoch 0[16897/17270] Time:0.227, Train Loss:0.776731014251709\n",
      "Epoch 0[16898/17270] Time:0.237, Train Loss:0.43885648250579834\n",
      "Epoch 0[16899/17270] Time:0.233, Train Loss:0.672354519367218\n",
      "Epoch 0[16900/17270] Time:0.239, Train Loss:0.4507010877132416\n",
      "Epoch 0[16901/17270] Time:0.229, Train Loss:0.6578803658485413\n",
      "Epoch 0[16902/17270] Time:0.245, Train Loss:0.34155625104904175\n",
      "Epoch 0[16903/17270] Time:0.237, Train Loss:0.5000524520874023\n",
      "Epoch 0[16904/17270] Time:0.23, Train Loss:0.34680962562561035\n",
      "Epoch 0[16905/17270] Time:0.238, Train Loss:0.4170411229133606\n",
      "Epoch 0[16906/17270] Time:0.23, Train Loss:0.7547066807746887\n",
      "Epoch 0[16907/17270] Time:0.236, Train Loss:0.390708327293396\n",
      "Epoch 0[16908/17270] Time:0.237, Train Loss:0.3590845763683319\n",
      "Epoch 0[16909/17270] Time:0.229, Train Loss:0.838188886642456\n",
      "Epoch 0[16910/17270] Time:0.244, Train Loss:0.34296679496765137\n",
      "Epoch 0[16911/17270] Time:0.224, Train Loss:0.771093487739563\n",
      "Epoch 0[16912/17270] Time:0.251, Train Loss:0.21938969194889069\n",
      "Epoch 0[16913/17270] Time:0.232, Train Loss:0.5634338855743408\n",
      "Epoch 0[16914/17270] Time:0.234, Train Loss:0.5624440908432007\n",
      "Epoch 0[16915/17270] Time:0.243, Train Loss:0.3179483115673065\n",
      "Epoch 0[16916/17270] Time:0.23, Train Loss:0.7181124091148376\n",
      "Epoch 0[16917/17270] Time:0.234, Train Loss:0.47483885288238525\n",
      "Epoch 0[16918/17270] Time:0.234, Train Loss:0.43896591663360596\n",
      "Epoch 0[16919/17270] Time:0.243, Train Loss:0.5735213756561279\n",
      "Epoch 0[16920/17270] Time:0.238, Train Loss:0.5665479898452759\n",
      "Epoch 0[16921/17270] Time:0.231, Train Loss:0.7486295104026794\n",
      "Epoch 0[16922/17270] Time:0.231, Train Loss:0.2797844111919403\n",
      "Epoch 0[16923/17270] Time:0.236, Train Loss:0.507636308670044\n",
      "Epoch 0[16924/17270] Time:0.24, Train Loss:0.5667517185211182\n",
      "Epoch 0[16925/17270] Time:0.243, Train Loss:1.3048046827316284\n",
      "Epoch 0[16926/17270] Time:0.232, Train Loss:0.5868673324584961\n",
      "Epoch 0[16927/17270] Time:0.243, Train Loss:0.617843508720398\n",
      "Epoch 0[16928/17270] Time:0.225, Train Loss:1.2232617139816284\n",
      "Epoch 0[16929/17270] Time:0.239, Train Loss:1.0526154041290283\n",
      "Epoch 0[16930/17270] Time:0.233, Train Loss:0.662284255027771\n",
      "Epoch 0[16931/17270] Time:0.231, Train Loss:1.1185444593429565\n",
      "Epoch 0[16932/17270] Time:0.232, Train Loss:0.6647216081619263\n",
      "Epoch 0[16933/17270] Time:0.237, Train Loss:0.6086934804916382\n",
      "Epoch 0[16934/17270] Time:0.222, Train Loss:0.8224253058433533\n",
      "Epoch 0[16935/17270] Time:0.24, Train Loss:0.3970852494239807\n",
      "Epoch 0[16936/17270] Time:0.227, Train Loss:0.4285874366760254\n",
      "Epoch 0[16937/17270] Time:0.23, Train Loss:0.4196261465549469\n",
      "Epoch 0[16938/17270] Time:0.24, Train Loss:0.48685917258262634\n",
      "Epoch 0[16939/17270] Time:0.236, Train Loss:1.4111257791519165\n",
      "Epoch 0[16940/17270] Time:0.236, Train Loss:0.8025102615356445\n",
      "Epoch 0[16941/17270] Time:0.229, Train Loss:0.43262991309165955\n",
      "Epoch 0[16942/17270] Time:0.23, Train Loss:0.5732767581939697\n",
      "Epoch 0[16943/17270] Time:0.236, Train Loss:0.36948180198669434\n",
      "Epoch 0[16944/17270] Time:0.23, Train Loss:0.6455234289169312\n",
      "Epoch 0[16945/17270] Time:0.232, Train Loss:0.8513320088386536\n",
      "Epoch 0[16946/17270] Time:0.238, Train Loss:0.7675930261611938\n",
      "Epoch 0[16947/17270] Time:0.242, Train Loss:0.7726696133613586\n",
      "Epoch 0[16948/17270] Time:0.23, Train Loss:0.5688437819480896\n",
      "Epoch 0[16949/17270] Time:0.239, Train Loss:0.5451456308364868\n",
      "Epoch 0[16950/17270] Time:0.221, Train Loss:0.5241733193397522\n",
      "Epoch 0[16951/17270] Time:0.24, Train Loss:0.8794620037078857\n",
      "Epoch 0[16952/17270] Time:0.231, Train Loss:0.5236005187034607\n",
      "Epoch 0[16953/17270] Time:0.244, Train Loss:0.6106878519058228\n",
      "Epoch 0[16954/17270] Time:0.227, Train Loss:0.8023543953895569\n",
      "Epoch 0[16955/17270] Time:0.232, Train Loss:0.4748842418193817\n",
      "Epoch 0[16956/17270] Time:0.225, Train Loss:0.5311025977134705\n",
      "Epoch 0[16957/17270] Time:0.238, Train Loss:0.3783208131790161\n",
      "Epoch 0[16958/17270] Time:0.244, Train Loss:1.4206981658935547\n",
      "Epoch 0[16959/17270] Time:0.233, Train Loss:0.648861289024353\n",
      "Epoch 0[16960/17270] Time:0.235, Train Loss:0.48273831605911255\n",
      "Epoch 0[16961/17270] Time:0.223, Train Loss:0.5876137614250183\n",
      "Epoch 0[16962/17270] Time:0.247, Train Loss:0.5067070722579956\n",
      "Epoch 0[16963/17270] Time:0.233, Train Loss:0.4684402644634247\n",
      "Epoch 0[16964/17270] Time:0.233, Train Loss:0.7632559537887573\n",
      "Epoch 0[16965/17270] Time:0.233, Train Loss:0.5695264339447021\n",
      "Epoch 0[16966/17270] Time:0.232, Train Loss:0.8663890361785889\n",
      "Epoch 0[16967/17270] Time:0.225, Train Loss:0.7696052193641663\n",
      "Epoch 0[16968/17270] Time:0.239, Train Loss:0.5742124319076538\n",
      "Epoch 0[16969/17270] Time:0.232, Train Loss:0.5115347504615784\n",
      "Epoch 0[16970/17270] Time:0.241, Train Loss:0.30768588185310364\n",
      "Epoch 0[16971/17270] Time:0.232, Train Loss:0.3206644356250763\n",
      "Epoch 0[16972/17270] Time:0.231, Train Loss:0.6963183283805847\n",
      "Epoch 0[16973/17270] Time:0.239, Train Loss:0.6417331099510193\n",
      "Epoch 0[16974/17270] Time:0.237, Train Loss:0.8169386386871338\n",
      "Epoch 0[16975/17270] Time:0.246, Train Loss:1.2042921781539917\n",
      "Epoch 0[16976/17270] Time:0.231, Train Loss:0.4166349768638611\n",
      "Epoch 0[16977/17270] Time:0.233, Train Loss:0.49265894293785095\n",
      "Epoch 0[16978/17270] Time:0.236, Train Loss:0.5625576376914978\n",
      "Epoch 0[16979/17270] Time:0.231, Train Loss:0.6697453260421753\n",
      "Epoch 0[16980/17270] Time:0.229, Train Loss:1.1730799674987793\n",
      "Epoch 0[16981/17270] Time:0.229, Train Loss:0.36619117856025696\n",
      "Epoch 0[16982/17270] Time:0.227, Train Loss:0.5226479172706604\n",
      "Epoch 0[16983/17270] Time:0.237, Train Loss:0.4900093972682953\n",
      "Epoch 0[16984/17270] Time:0.236, Train Loss:0.6769177317619324\n",
      "Epoch 0[16985/17270] Time:0.231, Train Loss:0.5075094103813171\n",
      "Epoch 0[16986/17270] Time:0.23, Train Loss:0.4656945765018463\n",
      "Epoch 0[16987/17270] Time:0.233, Train Loss:0.44614502787590027\n",
      "Epoch 0[16988/17270] Time:0.234, Train Loss:0.4347493052482605\n",
      "Epoch 0[16989/17270] Time:0.235, Train Loss:0.4270048439502716\n",
      "Epoch 0[16990/17270] Time:0.237, Train Loss:0.528374969959259\n",
      "Epoch 0[16991/17270] Time:0.244, Train Loss:1.037245750427246\n",
      "Epoch 0[16992/17270] Time:0.23, Train Loss:0.6606407165527344\n",
      "Epoch 0[16993/17270] Time:0.225, Train Loss:0.5534423589706421\n",
      "Epoch 0[16994/17270] Time:0.247, Train Loss:0.7259719967842102\n",
      "Epoch 0[16995/17270] Time:0.242, Train Loss:0.6373445987701416\n",
      "Epoch 0[16996/17270] Time:0.232, Train Loss:0.4142261743545532\n",
      "Epoch 0[16997/17270] Time:0.223, Train Loss:0.5825377702713013\n",
      "Epoch 0[16998/17270] Time:0.25, Train Loss:0.7100091576576233\n",
      "Epoch 0[16999/17270] Time:0.238, Train Loss:0.4597037732601166\n",
      "Epoch 0[17000/17270] Time:0.22, Train Loss:0.3708117604255676\n",
      "Epoch 0[17001/17270] Time:0.235, Train Loss:0.3751004934310913\n",
      "Epoch 0[17002/17270] Time:0.231, Train Loss:0.49557071924209595\n",
      "Epoch 0[17003/17270] Time:0.242, Train Loss:0.3929941654205322\n",
      "Epoch 0[17004/17270] Time:0.241, Train Loss:0.39318421483039856\n",
      "Epoch 0[17005/17270] Time:0.234, Train Loss:0.38709211349487305\n",
      "Epoch 0[17006/17270] Time:0.23, Train Loss:0.46333393454551697\n",
      "Epoch 0[17007/17270] Time:0.232, Train Loss:0.41015252470970154\n",
      "Epoch 0[17008/17270] Time:0.223, Train Loss:0.6044065356254578\n",
      "Epoch 0[17009/17270] Time:0.242, Train Loss:0.46434342861175537\n",
      "Epoch 0[17010/17270] Time:0.238, Train Loss:0.9557751417160034\n",
      "Epoch 0[17011/17270] Time:0.24, Train Loss:0.3858536183834076\n",
      "Epoch 0[17012/17270] Time:0.23, Train Loss:1.266464114189148\n",
      "Epoch 0[17013/17270] Time:0.234, Train Loss:0.592185378074646\n",
      "Epoch 0[17014/17270] Time:0.235, Train Loss:0.706804096698761\n",
      "Epoch 0[17015/17270] Time:0.232, Train Loss:0.9306607842445374\n",
      "Epoch 0[17016/17270] Time:0.23, Train Loss:0.9697989225387573\n",
      "Epoch 0[17017/17270] Time:0.234, Train Loss:0.49823033809661865\n",
      "Epoch 0[17018/17270] Time:0.233, Train Loss:0.6557275652885437\n",
      "Epoch 0[17019/17270] Time:0.231, Train Loss:0.5305315852165222\n",
      "Epoch 0[17020/17270] Time:0.232, Train Loss:0.6570400595664978\n",
      "Epoch 0[17021/17270] Time:0.221, Train Loss:0.2735981345176697\n",
      "Epoch 0[17022/17270] Time:0.245, Train Loss:0.636618971824646\n",
      "Epoch 0[17023/17270] Time:0.237, Train Loss:0.9674383997917175\n",
      "Epoch 0[17024/17270] Time:0.223, Train Loss:0.4481886625289917\n",
      "Epoch 0[17025/17270] Time:0.225, Train Loss:0.46421176195144653\n",
      "Epoch 0[17026/17270] Time:0.231, Train Loss:0.44154706597328186\n",
      "Epoch 0[17027/17270] Time:0.235, Train Loss:0.44624876976013184\n",
      "Epoch 0[17028/17270] Time:0.238, Train Loss:0.7373090386390686\n",
      "Epoch 0[17029/17270] Time:0.237, Train Loss:0.38489359617233276\n",
      "Epoch 0[17030/17270] Time:0.248, Train Loss:0.7491244673728943\n",
      "Epoch 0[17031/17270] Time:0.239, Train Loss:0.42904189229011536\n",
      "Epoch 0[17032/17270] Time:0.241, Train Loss:0.4221132695674896\n",
      "Epoch 0[17033/17270] Time:0.253, Train Loss:0.35844817757606506\n",
      "Epoch 0[17034/17270] Time:0.228, Train Loss:1.0727061033248901\n",
      "Epoch 0[17035/17270] Time:0.232, Train Loss:0.9172058701515198\n",
      "Epoch 0[17036/17270] Time:0.231, Train Loss:0.4287871718406677\n",
      "Epoch 0[17037/17270] Time:0.231, Train Loss:0.9031288623809814\n",
      "Epoch 0[17038/17270] Time:0.234, Train Loss:0.5175666213035583\n",
      "Epoch 0[17039/17270] Time:0.232, Train Loss:0.5533150434494019\n",
      "Epoch 0[17040/17270] Time:0.231, Train Loss:0.4995200037956238\n",
      "Epoch 0[17041/17270] Time:0.238, Train Loss:0.48374101519584656\n",
      "Epoch 0[17042/17270] Time:0.241, Train Loss:0.5521470904350281\n",
      "Epoch 0[17043/17270] Time:0.229, Train Loss:0.3457791805267334\n",
      "Epoch 0[17044/17270] Time:0.244, Train Loss:0.6274070739746094\n",
      "Epoch 0[17045/17270] Time:0.233, Train Loss:0.36596840620040894\n",
      "Epoch 0[17046/17270] Time:0.226, Train Loss:0.3572774827480316\n",
      "Epoch 0[17047/17270] Time:0.241, Train Loss:0.4297267496585846\n",
      "Epoch 0[17048/17270] Time:0.229, Train Loss:0.47125643491744995\n",
      "Epoch 0[17049/17270] Time:0.242, Train Loss:0.5054457783699036\n",
      "Epoch 0[17050/17270] Time:0.229, Train Loss:0.40775975584983826\n",
      "Epoch 0[17051/17270] Time:0.233, Train Loss:0.4903312921524048\n",
      "Epoch 0[17052/17270] Time:0.22, Train Loss:0.3563120365142822\n",
      "Epoch 0[17053/17270] Time:0.24, Train Loss:0.5867183208465576\n",
      "Epoch 0[17054/17270] Time:0.224, Train Loss:0.35611414909362793\n",
      "Epoch 0[17055/17270] Time:0.237, Train Loss:0.6583141088485718\n",
      "Epoch 0[17056/17270] Time:0.225, Train Loss:0.5863828659057617\n",
      "Epoch 0[17057/17270] Time:0.237, Train Loss:0.7620251178741455\n",
      "Epoch 0[17058/17270] Time:0.234, Train Loss:0.5639276504516602\n",
      "Epoch 0[17059/17270] Time:0.241, Train Loss:0.7697927355766296\n",
      "Epoch 0[17060/17270] Time:0.232, Train Loss:1.600035548210144\n",
      "Epoch 0[17061/17270] Time:0.242, Train Loss:0.3573538362979889\n",
      "Epoch 0[17062/17270] Time:0.235, Train Loss:0.7046125531196594\n",
      "Epoch 0[17063/17270] Time:0.234, Train Loss:0.3518703579902649\n",
      "Epoch 0[17064/17270] Time:0.223, Train Loss:0.3945227563381195\n",
      "Epoch 0[17065/17270] Time:0.246, Train Loss:0.27131423354148865\n",
      "Epoch 0[17066/17270] Time:0.235, Train Loss:0.42853933572769165\n",
      "Epoch 0[17067/17270] Time:0.238, Train Loss:0.3864225745201111\n",
      "Epoch 0[17068/17270] Time:0.237, Train Loss:0.4797273576259613\n",
      "Epoch 0[17069/17270] Time:0.234, Train Loss:0.8644058108329773\n",
      "Epoch 0[17070/17270] Time:0.225, Train Loss:0.4086286127567291\n",
      "Epoch 0[17071/17270] Time:0.23, Train Loss:0.3027324080467224\n",
      "Epoch 0[17072/17270] Time:0.229, Train Loss:0.32769185304641724\n",
      "Epoch 0[17073/17270] Time:0.232, Train Loss:0.4743230938911438\n",
      "Epoch 0[17074/17270] Time:0.226, Train Loss:1.2226749658584595\n",
      "Epoch 0[17075/17270] Time:0.254, Train Loss:0.56649249792099\n",
      "Epoch 0[17076/17270] Time:0.227, Train Loss:0.5575717091560364\n",
      "Epoch 0[17077/17270] Time:0.234, Train Loss:0.5071148872375488\n",
      "Epoch 0[17078/17270] Time:0.242, Train Loss:0.37136268615722656\n",
      "Epoch 0[17079/17270] Time:0.231, Train Loss:0.5301824808120728\n",
      "Epoch 0[17080/17270] Time:0.237, Train Loss:0.5073665976524353\n",
      "Epoch 0[17081/17270] Time:0.223, Train Loss:0.5823394060134888\n",
      "Epoch 0[17082/17270] Time:0.229, Train Loss:0.8204705715179443\n",
      "Epoch 0[17083/17270] Time:0.234, Train Loss:0.761085569858551\n",
      "Epoch 0[17084/17270] Time:0.237, Train Loss:0.6236623525619507\n",
      "Epoch 0[17085/17270] Time:0.24, Train Loss:0.4313507378101349\n",
      "Epoch 0[17086/17270] Time:0.224, Train Loss:0.7369270920753479\n",
      "Epoch 0[17087/17270] Time:0.236, Train Loss:0.345599889755249\n",
      "Epoch 0[17088/17270] Time:0.231, Train Loss:0.48645851016044617\n",
      "Epoch 0[17089/17270] Time:0.226, Train Loss:0.674640417098999\n",
      "Epoch 0[17090/17270] Time:0.244, Train Loss:0.372122198343277\n",
      "Epoch 0[17091/17270] Time:0.229, Train Loss:0.9152472019195557\n",
      "Epoch 0[17092/17270] Time:0.23, Train Loss:0.3123116195201874\n",
      "Epoch 0[17093/17270] Time:0.25, Train Loss:0.8956738114356995\n",
      "Epoch 0[17094/17270] Time:0.234, Train Loss:0.3832525312900543\n",
      "Epoch 0[17095/17270] Time:0.23, Train Loss:0.6326804757118225\n",
      "Epoch 0[17096/17270] Time:0.236, Train Loss:0.3802909851074219\n",
      "Epoch 0[17097/17270] Time:0.238, Train Loss:0.6263367533683777\n",
      "Epoch 0[17098/17270] Time:0.242, Train Loss:0.6347535848617554\n",
      "Epoch 0[17099/17270] Time:0.237, Train Loss:0.4262387454509735\n",
      "Epoch 0[17100/17270] Time:0.239, Train Loss:0.4861544668674469\n",
      "Epoch 0[17101/17270] Time:0.232, Train Loss:0.6240677237510681\n",
      "Epoch 0[17102/17270] Time:0.245, Train Loss:0.37875983119010925\n",
      "Epoch 0[17103/17270] Time:0.236, Train Loss:0.44694703817367554\n",
      "Epoch 0[17104/17270] Time:0.238, Train Loss:0.35845834016799927\n",
      "Epoch 0[17105/17270] Time:0.233, Train Loss:0.4527847468852997\n",
      "Epoch 0[17106/17270] Time:0.246, Train Loss:1.0137592554092407\n",
      "Epoch 0[17107/17270] Time:0.239, Train Loss:0.3373069167137146\n",
      "Epoch 0[17108/17270] Time:0.238, Train Loss:0.430681973695755\n",
      "Epoch 0[17109/17270] Time:0.238, Train Loss:0.7108868360519409\n",
      "Epoch 0[17110/17270] Time:0.245, Train Loss:0.6289795637130737\n",
      "Epoch 0[17111/17270] Time:0.227, Train Loss:0.7296704649925232\n",
      "Epoch 0[17112/17270] Time:0.224, Train Loss:0.568915069103241\n",
      "Epoch 0[17113/17270] Time:0.228, Train Loss:0.47965654730796814\n",
      "Epoch 0[17114/17270] Time:0.236, Train Loss:0.36125263571739197\n",
      "Epoch 0[17115/17270] Time:0.236, Train Loss:0.6336395740509033\n",
      "Epoch 0[17116/17270] Time:0.243, Train Loss:0.30949023365974426\n",
      "Epoch 0[17117/17270] Time:0.226, Train Loss:0.32163968682289124\n",
      "Epoch 0[17118/17270] Time:0.241, Train Loss:0.5998774170875549\n",
      "Epoch 0[17119/17270] Time:0.231, Train Loss:0.26334986090660095\n",
      "Epoch 0[17120/17270] Time:0.238, Train Loss:0.2980731129646301\n",
      "Epoch 0[17121/17270] Time:0.235, Train Loss:0.5875173211097717\n",
      "Epoch 0[17122/17270] Time:0.238, Train Loss:0.5399479269981384\n",
      "Epoch 0[17123/17270] Time:0.241, Train Loss:0.3426479697227478\n",
      "Epoch 0[17124/17270] Time:0.238, Train Loss:0.6194640398025513\n",
      "Epoch 0[17125/17270] Time:0.236, Train Loss:0.7657980918884277\n",
      "Epoch 0[17126/17270] Time:0.235, Train Loss:0.26112306118011475\n",
      "Epoch 0[17127/17270] Time:0.236, Train Loss:0.48283320665359497\n",
      "Epoch 0[17128/17270] Time:0.237, Train Loss:0.5031415820121765\n",
      "Epoch 0[17129/17270] Time:0.236, Train Loss:0.978919267654419\n",
      "Epoch 0[17130/17270] Time:0.238, Train Loss:0.3073333203792572\n",
      "Epoch 0[17131/17270] Time:0.235, Train Loss:0.4443899095058441\n",
      "Epoch 0[17132/17270] Time:0.238, Train Loss:0.22480344772338867\n",
      "Epoch 0[17133/17270] Time:0.231, Train Loss:0.4905759394168854\n",
      "Epoch 0[17134/17270] Time:0.226, Train Loss:0.5169685482978821\n",
      "Epoch 0[17135/17270] Time:0.237, Train Loss:0.5056631565093994\n",
      "Epoch 0[17136/17270] Time:0.237, Train Loss:0.4144614338874817\n",
      "Epoch 0[17137/17270] Time:0.23, Train Loss:0.35280904173851013\n",
      "Epoch 0[17138/17270] Time:0.236, Train Loss:0.9736191630363464\n",
      "Epoch 0[17139/17270] Time:0.234, Train Loss:0.8150238990783691\n",
      "Epoch 0[17140/17270] Time:0.227, Train Loss:0.3653331696987152\n",
      "Epoch 0[17141/17270] Time:0.236, Train Loss:0.6982235908508301\n",
      "Epoch 0[17142/17270] Time:0.229, Train Loss:0.2738950550556183\n",
      "Epoch 0[17143/17270] Time:0.231, Train Loss:0.5178313851356506\n",
      "Epoch 0[17144/17270] Time:0.228, Train Loss:0.5218462944030762\n",
      "Epoch 0[17145/17270] Time:0.239, Train Loss:0.5166248679161072\n",
      "Epoch 0[17146/17270] Time:0.228, Train Loss:0.7567048668861389\n",
      "Epoch 0[17147/17270] Time:0.229, Train Loss:0.3712747395038605\n",
      "Epoch 0[17148/17270] Time:0.23, Train Loss:0.48706454038619995\n",
      "Epoch 0[17149/17270] Time:0.239, Train Loss:0.3952379822731018\n",
      "Epoch 0[17150/17270] Time:0.228, Train Loss:0.6472831964492798\n",
      "Epoch 0[17151/17270] Time:0.235, Train Loss:0.49858421087265015\n",
      "Epoch 0[17152/17270] Time:0.239, Train Loss:0.6549544930458069\n",
      "Epoch 0[17153/17270] Time:0.229, Train Loss:0.5856161713600159\n",
      "Epoch 0[17154/17270] Time:0.227, Train Loss:0.3775869905948639\n",
      "Epoch 0[17155/17270] Time:0.23, Train Loss:0.2957119047641754\n",
      "Epoch 0[17156/17270] Time:0.237, Train Loss:0.24173790216445923\n",
      "Epoch 0[17157/17270] Time:0.236, Train Loss:0.291409969329834\n",
      "Epoch 0[17158/17270] Time:0.236, Train Loss:0.5230268836021423\n",
      "Epoch 0[17159/17270] Time:0.23, Train Loss:0.4104713201522827\n",
      "Epoch 0[17160/17270] Time:0.254, Train Loss:0.5252691507339478\n",
      "Epoch 0[17161/17270] Time:0.24, Train Loss:0.5320744514465332\n",
      "Epoch 0[17162/17270] Time:0.237, Train Loss:0.4778231382369995\n",
      "Epoch 0[17163/17270] Time:0.226, Train Loss:0.33070680499076843\n",
      "Epoch 0[17164/17270] Time:0.224, Train Loss:0.38836726546287537\n",
      "Epoch 0[17165/17270] Time:0.245, Train Loss:0.27065813541412354\n",
      "Epoch 0[17166/17270] Time:0.234, Train Loss:0.549899697303772\n",
      "Epoch 0[17167/17270] Time:0.242, Train Loss:0.5103015303611755\n",
      "Epoch 0[17168/17270] Time:0.237, Train Loss:0.3779136538505554\n",
      "Epoch 0[17169/17270] Time:0.226, Train Loss:0.36396336555480957\n",
      "Epoch 0[17170/17270] Time:0.232, Train Loss:0.5760446786880493\n",
      "Epoch 0[17171/17270] Time:0.239, Train Loss:0.5907304286956787\n",
      "Epoch 0[17172/17270] Time:0.239, Train Loss:0.42317575216293335\n",
      "Epoch 0[17173/17270] Time:0.226, Train Loss:0.7839164137840271\n",
      "Epoch 0[17174/17270] Time:0.239, Train Loss:0.5858887434005737\n",
      "Epoch 0[17175/17270] Time:0.229, Train Loss:0.2765111029148102\n",
      "Epoch 0[17176/17270] Time:0.237, Train Loss:0.5317447781562805\n",
      "Epoch 0[17177/17270] Time:0.242, Train Loss:0.34507322311401367\n",
      "Epoch 0[17178/17270] Time:0.23, Train Loss:0.5404053926467896\n",
      "Epoch 0[17179/17270] Time:0.234, Train Loss:0.4453269839286804\n",
      "Epoch 0[17180/17270] Time:0.244, Train Loss:0.7209057211875916\n",
      "Epoch 0[17181/17270] Time:0.223, Train Loss:0.6528573632240295\n",
      "Epoch 0[17182/17270] Time:0.231, Train Loss:0.2699379026889801\n",
      "Epoch 0[17183/17270] Time:0.228, Train Loss:0.3946102559566498\n",
      "Epoch 0[17184/17270] Time:0.237, Train Loss:0.34989455342292786\n",
      "Epoch 0[17185/17270] Time:0.226, Train Loss:0.417292058467865\n",
      "Epoch 0[17186/17270] Time:0.238, Train Loss:0.43202242255210876\n",
      "Epoch 0[17187/17270] Time:0.237, Train Loss:0.5599352717399597\n",
      "Epoch 0[17188/17270] Time:0.231, Train Loss:1.4863094091415405\n",
      "Epoch 0[17189/17270] Time:0.238, Train Loss:0.8841428756713867\n",
      "Epoch 0[17190/17270] Time:0.231, Train Loss:0.43054404854774475\n",
      "Epoch 0[17191/17270] Time:0.231, Train Loss:0.6521443128585815\n",
      "Epoch 0[17192/17270] Time:0.231, Train Loss:0.26536211371421814\n",
      "Epoch 0[17193/17270] Time:0.237, Train Loss:0.49213606119155884\n",
      "Epoch 0[17194/17270] Time:0.23, Train Loss:0.34941455721855164\n",
      "Epoch 0[17195/17270] Time:0.228, Train Loss:0.3760944604873657\n",
      "Epoch 0[17196/17270] Time:0.238, Train Loss:0.6092860698699951\n",
      "Epoch 0[17197/17270] Time:0.25, Train Loss:0.39077773690223694\n",
      "Epoch 0[17198/17270] Time:0.23, Train Loss:0.37028154730796814\n",
      "Epoch 0[17199/17270] Time:0.23, Train Loss:0.902976930141449\n",
      "Epoch 0[17200/17270] Time:0.244, Train Loss:0.47070688009262085\n",
      "Epoch 0[17201/17270] Time:0.231, Train Loss:0.4157036542892456\n",
      "Epoch 0[17202/17270] Time:0.246, Train Loss:0.8752240538597107\n",
      "Epoch 0[17203/17270] Time:0.238, Train Loss:0.8773005604743958\n",
      "Epoch 0[17204/17270] Time:0.232, Train Loss:0.4039601981639862\n",
      "Epoch 0[17205/17270] Time:0.235, Train Loss:1.9980348348617554\n",
      "Epoch 0[17206/17270] Time:0.235, Train Loss:0.9061175584793091\n",
      "Epoch 0[17207/17270] Time:0.225, Train Loss:0.6277347803115845\n",
      "Epoch 0[17208/17270] Time:0.229, Train Loss:0.4214438796043396\n",
      "Epoch 0[17209/17270] Time:0.223, Train Loss:0.3532577157020569\n",
      "Epoch 0[17210/17270] Time:0.23, Train Loss:0.9871888160705566\n",
      "Epoch 0[17211/17270] Time:0.237, Train Loss:0.3055077791213989\n",
      "Epoch 0[17212/17270] Time:0.239, Train Loss:0.6384719014167786\n",
      "Epoch 0[17213/17270] Time:0.228, Train Loss:0.43921253085136414\n",
      "Epoch 0[17214/17270] Time:0.234, Train Loss:0.5204719305038452\n",
      "Epoch 0[17215/17270] Time:0.238, Train Loss:0.44459816813468933\n",
      "Epoch 0[17216/17270] Time:0.234, Train Loss:0.6043303608894348\n",
      "Epoch 0[17217/17270] Time:0.232, Train Loss:0.32816261053085327\n",
      "Epoch 0[17218/17270] Time:0.225, Train Loss:0.47596636414527893\n",
      "Epoch 0[17219/17270] Time:0.232, Train Loss:1.2486395835876465\n",
      "Epoch 0[17220/17270] Time:0.237, Train Loss:0.7077410221099854\n",
      "Epoch 0[17221/17270] Time:0.225, Train Loss:0.5740377306938171\n",
      "Epoch 0[17222/17270] Time:0.226, Train Loss:0.8291067481040955\n",
      "Epoch 0[17223/17270] Time:0.238, Train Loss:1.2579635381698608\n",
      "Epoch 0[17224/17270] Time:0.235, Train Loss:0.7239279747009277\n",
      "Epoch 0[17225/17270] Time:0.228, Train Loss:0.9572597742080688\n",
      "Epoch 0[17226/17270] Time:0.229, Train Loss:0.6106390953063965\n",
      "Epoch 0[17227/17270] Time:0.232, Train Loss:0.5434093475341797\n",
      "Epoch 0[17228/17270] Time:0.23, Train Loss:0.4681127369403839\n",
      "Epoch 0[17229/17270] Time:0.242, Train Loss:0.44044962525367737\n",
      "Epoch 0[17230/17270] Time:0.233, Train Loss:0.714235246181488\n",
      "Epoch 0[17231/17270] Time:0.233, Train Loss:0.6761830449104309\n",
      "Epoch 0[17232/17270] Time:0.252, Train Loss:0.46630099415779114\n",
      "Epoch 0[17233/17270] Time:0.247, Train Loss:0.6405367851257324\n",
      "Epoch 0[17234/17270] Time:0.233, Train Loss:0.2552492618560791\n",
      "Epoch 0[17235/17270] Time:0.228, Train Loss:0.3787648677825928\n",
      "Epoch 0[17236/17270] Time:0.222, Train Loss:1.097220778465271\n",
      "Epoch 0[17237/17270] Time:0.23, Train Loss:0.4150756001472473\n",
      "Epoch 0[17238/17270] Time:0.234, Train Loss:0.5097262859344482\n",
      "Epoch 0[17239/17270] Time:0.244, Train Loss:0.49992379546165466\n",
      "Epoch 0[17240/17270] Time:0.233, Train Loss:0.7070800065994263\n",
      "Epoch 0[17241/17270] Time:0.231, Train Loss:0.6747578382492065\n",
      "Epoch 0[17242/17270] Time:0.236, Train Loss:0.5167376399040222\n",
      "Epoch 0[17243/17270] Time:0.231, Train Loss:0.7652511596679688\n",
      "Epoch 0[17244/17270] Time:0.232, Train Loss:0.6336290836334229\n",
      "Epoch 0[17245/17270] Time:0.227, Train Loss:0.7087491154670715\n",
      "Epoch 0[17246/17270] Time:0.236, Train Loss:0.4817831516265869\n",
      "Epoch 0[17247/17270] Time:0.23, Train Loss:0.5776033401489258\n",
      "Epoch 0[17248/17270] Time:0.226, Train Loss:0.7882130146026611\n",
      "Epoch 0[17249/17270] Time:0.239, Train Loss:1.0561527013778687\n",
      "Epoch 0[17250/17270] Time:0.239, Train Loss:0.4396226406097412\n",
      "Epoch 0[17251/17270] Time:0.231, Train Loss:0.6414889097213745\n",
      "Epoch 0[17252/17270] Time:0.23, Train Loss:0.6709375381469727\n",
      "Epoch 0[17253/17270] Time:0.236, Train Loss:0.512239396572113\n",
      "Epoch 0[17254/17270] Time:0.236, Train Loss:1.2426615953445435\n",
      "Epoch 0[17255/17270] Time:0.238, Train Loss:0.3766188323497772\n",
      "Epoch 0[17256/17270] Time:0.229, Train Loss:0.5040774345397949\n",
      "Epoch 0[17257/17270] Time:0.226, Train Loss:0.6581546664237976\n",
      "Epoch 0[17258/17270] Time:0.227, Train Loss:0.5572305917739868\n",
      "Epoch 0[17259/17270] Time:0.231, Train Loss:0.7841271162033081\n",
      "Epoch 0[17260/17270] Time:0.234, Train Loss:0.6767372488975525\n",
      "Epoch 0[17261/17270] Time:0.218, Train Loss:0.5184649229049683\n",
      "Epoch 0[17262/17270] Time:0.252, Train Loss:0.6479711532592773\n",
      "Epoch 0[17263/17270] Time:0.241, Train Loss:0.9009969234466553\n",
      "Epoch 0[17264/17270] Time:0.225, Train Loss:1.4690873622894287\n",
      "Epoch 0[17265/17270] Time:0.223, Train Loss:0.5005223751068115\n",
      "Epoch 0[17266/17270] Time:0.216, Train Loss:1.4130100011825562\n",
      "Epoch 0[17267/17270] Time:0.228, Train Loss:0.8613373041152954\n",
      "Epoch 0[17268/17270] Time:0.236, Train Loss:0.3719412684440613\n",
      "Epoch 0[17269/17270] Time:0.233, Train Loss:0.5278113484382629\n",
      "Epoch 0[0/4499] Val Loss:0.35952210426330566\n",
      "Epoch 0[1/4499] Val Loss:0.27725380659103394\n",
      "Epoch 0[2/4499] Val Loss:0.24080976843833923\n",
      "Epoch 0[3/4499] Val Loss:0.1242411732673645\n",
      "Epoch 0[4/4499] Val Loss:0.07378589361906052\n",
      "Epoch 0[5/4499] Val Loss:0.06166745722293854\n",
      "Epoch 0[6/4499] Val Loss:0.07863134145736694\n",
      "Epoch 0[7/4499] Val Loss:0.15920260548591614\n",
      "Epoch 0[8/4499] Val Loss:0.10209888219833374\n",
      "Epoch 0[9/4499] Val Loss:0.07516516000032425\n",
      "Epoch 0[10/4499] Val Loss:0.12547814846038818\n",
      "Epoch 0[11/4499] Val Loss:0.1360599398612976\n",
      "Epoch 0[12/4499] Val Loss:0.15913134813308716\n",
      "Epoch 0[13/4499] Val Loss:0.12844513356685638\n",
      "Epoch 0[14/4499] Val Loss:0.08465401083230972\n",
      "Epoch 0[15/4499] Val Loss:0.11514615267515182\n",
      "Epoch 0[16/4499] Val Loss:0.07574166357517242\n",
      "Epoch 0[17/4499] Val Loss:0.08110274374485016\n",
      "Epoch 0[18/4499] Val Loss:0.10281360894441605\n",
      "Epoch 0[19/4499] Val Loss:0.11986522376537323\n",
      "Epoch 0[20/4499] Val Loss:0.16684822738170624\n",
      "Epoch 0[21/4499] Val Loss:0.17380709946155548\n",
      "Epoch 0[22/4499] Val Loss:0.18709544837474823\n",
      "Epoch 0[23/4499] Val Loss:0.16931894421577454\n",
      "Epoch 0[24/4499] Val Loss:0.2820710837841034\n",
      "Epoch 0[25/4499] Val Loss:0.14815813302993774\n",
      "Epoch 0[26/4499] Val Loss:0.39246243238449097\n",
      "Epoch 0[27/4499] Val Loss:0.11373954266309738\n",
      "Epoch 0[28/4499] Val Loss:0.41777920722961426\n",
      "Epoch 0[29/4499] Val Loss:0.240630105137825\n",
      "Epoch 0[30/4499] Val Loss:0.4267849624156952\n",
      "Epoch 0[31/4499] Val Loss:0.3030378222465515\n",
      "Epoch 0[32/4499] Val Loss:0.3084656000137329\n",
      "Epoch 0[33/4499] Val Loss:0.3784015476703644\n",
      "Epoch 0[34/4499] Val Loss:0.42632701992988586\n",
      "Epoch 0[35/4499] Val Loss:0.493888795375824\n",
      "Epoch 0[36/4499] Val Loss:0.16789397597312927\n",
      "Epoch 0[37/4499] Val Loss:0.17809613049030304\n",
      "Epoch 0[38/4499] Val Loss:0.2613057494163513\n",
      "Epoch 0[39/4499] Val Loss:0.3787180185317993\n",
      "Epoch 0[40/4499] Val Loss:0.28346356749534607\n",
      "Epoch 0[41/4499] Val Loss:0.27018406987190247\n",
      "Epoch 0[42/4499] Val Loss:0.35186514258384705\n",
      "Epoch 0[43/4499] Val Loss:0.26111289858818054\n",
      "Epoch 0[44/4499] Val Loss:0.11585875600576401\n",
      "Epoch 0[45/4499] Val Loss:0.12426293641328812\n",
      "Epoch 0[46/4499] Val Loss:0.29929661750793457\n",
      "Epoch 0[47/4499] Val Loss:0.1442987620830536\n",
      "Epoch 0[48/4499] Val Loss:0.18054316937923431\n",
      "Epoch 0[49/4499] Val Loss:0.19945570826530457\n",
      "Epoch 0[50/4499] Val Loss:0.2187039703130722\n",
      "Epoch 0[51/4499] Val Loss:0.20240190625190735\n",
      "Epoch 0[52/4499] Val Loss:0.23034854233264923\n",
      "Epoch 0[53/4499] Val Loss:0.3309488594532013\n",
      "Epoch 0[54/4499] Val Loss:0.3706889748573303\n",
      "Epoch 0[55/4499] Val Loss:0.44114950299263\n",
      "Epoch 0[56/4499] Val Loss:0.47261765599250793\n",
      "Epoch 0[57/4499] Val Loss:0.4359578490257263\n",
      "Epoch 0[58/4499] Val Loss:0.5240475535392761\n",
      "Epoch 0[59/4499] Val Loss:0.518776535987854\n",
      "Epoch 0[60/4499] Val Loss:0.4837074875831604\n",
      "Epoch 0[61/4499] Val Loss:0.547373354434967\n",
      "Epoch 0[62/4499] Val Loss:0.6684494614601135\n",
      "Epoch 0[63/4499] Val Loss:0.4861508309841156\n",
      "Epoch 0[64/4499] Val Loss:0.36002984642982483\n",
      "Epoch 0[65/4499] Val Loss:0.47684934735298157\n",
      "Epoch 0[66/4499] Val Loss:0.7185606956481934\n",
      "Epoch 0[67/4499] Val Loss:0.4282195270061493\n",
      "Epoch 0[68/4499] Val Loss:0.6787766814231873\n",
      "Epoch 0[69/4499] Val Loss:0.6272583603858948\n",
      "Epoch 0[70/4499] Val Loss:0.47580620646476746\n",
      "Epoch 0[71/4499] Val Loss:0.4631168842315674\n",
      "Epoch 0[72/4499] Val Loss:0.4375346302986145\n",
      "Epoch 0[73/4499] Val Loss:0.34652721881866455\n",
      "Epoch 0[74/4499] Val Loss:0.2910537123680115\n",
      "Epoch 0[75/4499] Val Loss:0.22265969216823578\n",
      "Epoch 0[76/4499] Val Loss:0.2584138810634613\n",
      "Epoch 0[77/4499] Val Loss:0.27452799677848816\n",
      "Epoch 0[78/4499] Val Loss:0.22895270586013794\n",
      "Epoch 0[79/4499] Val Loss:0.20592306554317474\n",
      "Epoch 0[80/4499] Val Loss:0.23954521119594574\n",
      "Epoch 0[81/4499] Val Loss:0.2100212275981903\n",
      "Epoch 0[82/4499] Val Loss:0.2788394093513489\n",
      "Epoch 0[83/4499] Val Loss:0.2586953938007355\n",
      "Epoch 0[84/4499] Val Loss:0.20250961184501648\n",
      "Epoch 0[85/4499] Val Loss:0.19321584701538086\n",
      "Epoch 0[86/4499] Val Loss:0.20859526097774506\n",
      "Epoch 0[87/4499] Val Loss:0.2171279788017273\n",
      "Epoch 0[88/4499] Val Loss:0.25929850339889526\n",
      "Epoch 0[89/4499] Val Loss:0.20984488725662231\n",
      "Epoch 0[90/4499] Val Loss:0.2376289665699005\n",
      "Epoch 0[91/4499] Val Loss:0.28979942202568054\n",
      "Epoch 0[92/4499] Val Loss:0.24988523125648499\n",
      "Epoch 0[93/4499] Val Loss:0.24498191475868225\n",
      "Epoch 0[94/4499] Val Loss:0.2412320375442505\n",
      "Epoch 0[95/4499] Val Loss:0.21122276782989502\n",
      "Epoch 0[96/4499] Val Loss:0.24703682959079742\n",
      "Epoch 0[97/4499] Val Loss:0.2372453361749649\n",
      "Epoch 0[98/4499] Val Loss:0.2763924300670624\n",
      "Epoch 0[99/4499] Val Loss:0.31450405716896057\n",
      "Epoch 0[100/4499] Val Loss:0.40752100944519043\n",
      "Epoch 0[101/4499] Val Loss:0.3908490240573883\n",
      "Epoch 0[102/4499] Val Loss:0.28557461500167847\n",
      "Epoch 0[103/4499] Val Loss:0.7358972430229187\n",
      "Epoch 0[104/4499] Val Loss:1.0325006246566772\n",
      "Epoch 0[105/4499] Val Loss:0.8069262504577637\n",
      "Epoch 0[106/4499] Val Loss:0.5513923168182373\n",
      "Epoch 0[107/4499] Val Loss:0.5141173005104065\n",
      "Epoch 0[108/4499] Val Loss:0.39711320400238037\n",
      "Epoch 0[109/4499] Val Loss:0.5457912683486938\n",
      "Epoch 0[110/4499] Val Loss:0.44833800196647644\n",
      "Epoch 0[111/4499] Val Loss:0.3461824357509613\n",
      "Epoch 0[112/4499] Val Loss:0.3593413531780243\n",
      "Epoch 0[113/4499] Val Loss:0.37522590160369873\n",
      "Epoch 0[114/4499] Val Loss:0.25931769609451294\n",
      "Epoch 0[115/4499] Val Loss:0.2926841676235199\n",
      "Epoch 0[116/4499] Val Loss:0.3922736942768097\n",
      "Epoch 0[117/4499] Val Loss:0.3036494851112366\n",
      "Epoch 0[118/4499] Val Loss:0.29772958159446716\n",
      "Epoch 0[119/4499] Val Loss:0.41991937160491943\n",
      "Epoch 0[120/4499] Val Loss:0.3839798867702484\n",
      "Epoch 0[121/4499] Val Loss:0.27310898900032043\n",
      "Epoch 0[122/4499] Val Loss:0.21776309609413147\n",
      "Epoch 0[123/4499] Val Loss:0.22170695662498474\n",
      "Epoch 0[124/4499] Val Loss:0.381115198135376\n",
      "Epoch 0[125/4499] Val Loss:0.26589080691337585\n",
      "Epoch 0[126/4499] Val Loss:0.3508750796318054\n",
      "Epoch 0[127/4499] Val Loss:0.24008728563785553\n",
      "Epoch 0[128/4499] Val Loss:0.20691296458244324\n",
      "Epoch 0[129/4499] Val Loss:0.151767760515213\n",
      "Epoch 0[130/4499] Val Loss:0.11106379330158234\n",
      "Epoch 0[131/4499] Val Loss:0.1943243145942688\n",
      "Epoch 0[132/4499] Val Loss:0.19662930071353912\n",
      "Epoch 0[133/4499] Val Loss:0.3719435930252075\n",
      "Epoch 0[134/4499] Val Loss:0.48702749609947205\n",
      "Epoch 0[135/4499] Val Loss:0.42980214953422546\n",
      "Epoch 0[136/4499] Val Loss:0.2936931252479553\n",
      "Epoch 0[137/4499] Val Loss:0.3037733733654022\n",
      "Epoch 0[138/4499] Val Loss:0.36714351177215576\n",
      "Epoch 0[139/4499] Val Loss:0.4072363078594208\n",
      "Epoch 0[140/4499] Val Loss:0.37885743379592896\n",
      "Epoch 0[141/4499] Val Loss:0.40112271904945374\n",
      "Epoch 0[142/4499] Val Loss:0.38876986503601074\n",
      "Epoch 0[143/4499] Val Loss:0.4535906910896301\n",
      "Epoch 0[144/4499] Val Loss:0.3366025984287262\n",
      "Epoch 0[145/4499] Val Loss:0.456699013710022\n",
      "Epoch 0[146/4499] Val Loss:0.4144236743450165\n",
      "Epoch 0[147/4499] Val Loss:0.20586104691028595\n",
      "Epoch 0[148/4499] Val Loss:0.32849469780921936\n",
      "Epoch 0[149/4499] Val Loss:0.36831948161125183\n",
      "Epoch 0[150/4499] Val Loss:0.22031733393669128\n",
      "Epoch 0[151/4499] Val Loss:0.36995720863342285\n",
      "Epoch 0[152/4499] Val Loss:0.38928747177124023\n",
      "Epoch 0[153/4499] Val Loss:0.21314343810081482\n",
      "Epoch 0[154/4499] Val Loss:0.419811874628067\n",
      "Epoch 0[155/4499] Val Loss:0.4916623532772064\n",
      "Epoch 0[156/4499] Val Loss:0.15442563593387604\n",
      "Epoch 0[157/4499] Val Loss:0.3640456795692444\n",
      "Epoch 0[158/4499] Val Loss:0.20765165984630585\n",
      "Epoch 0[159/4499] Val Loss:0.29441794753074646\n",
      "Epoch 0[160/4499] Val Loss:0.2664165496826172\n",
      "Epoch 0[161/4499] Val Loss:0.3170922100543976\n",
      "Epoch 0[162/4499] Val Loss:0.3627941906452179\n",
      "Epoch 0[163/4499] Val Loss:0.36233264207839966\n",
      "Epoch 0[164/4499] Val Loss:0.2337590903043747\n",
      "Epoch 0[165/4499] Val Loss:0.5285816788673401\n",
      "Epoch 0[166/4499] Val Loss:0.6929603815078735\n",
      "Epoch 0[167/4499] Val Loss:0.6049461364746094\n",
      "Epoch 0[168/4499] Val Loss:0.48574262857437134\n",
      "Epoch 0[169/4499] Val Loss:0.3937969207763672\n",
      "Epoch 0[170/4499] Val Loss:0.3926512598991394\n",
      "Epoch 0[171/4499] Val Loss:0.5620030760765076\n",
      "Epoch 0[172/4499] Val Loss:0.26987168192863464\n",
      "Epoch 0[173/4499] Val Loss:0.3899330794811249\n",
      "Epoch 0[174/4499] Val Loss:0.4326399564743042\n",
      "Epoch 0[175/4499] Val Loss:0.4334459900856018\n",
      "Epoch 0[176/4499] Val Loss:0.4466947317123413\n",
      "Epoch 0[177/4499] Val Loss:0.6055582761764526\n",
      "Epoch 0[178/4499] Val Loss:0.3735284209251404\n",
      "Epoch 0[179/4499] Val Loss:0.5203393697738647\n",
      "Epoch 0[180/4499] Val Loss:0.6293574571609497\n",
      "Epoch 0[181/4499] Val Loss:0.5778011679649353\n",
      "Epoch 0[182/4499] Val Loss:0.4961602985858917\n",
      "Epoch 0[183/4499] Val Loss:0.523139476776123\n",
      "Epoch 0[184/4499] Val Loss:0.5332419872283936\n",
      "Epoch 0[185/4499] Val Loss:0.3668957054615021\n",
      "Epoch 0[186/4499] Val Loss:0.7808862328529358\n",
      "Epoch 0[187/4499] Val Loss:0.7442218065261841\n",
      "Epoch 0[188/4499] Val Loss:0.4292632043361664\n",
      "Epoch 0[189/4499] Val Loss:0.4784564673900604\n",
      "Epoch 0[190/4499] Val Loss:0.25942718982696533\n",
      "Epoch 0[191/4499] Val Loss:0.2592894732952118\n",
      "Epoch 0[192/4499] Val Loss:0.2804061770439148\n",
      "Epoch 0[193/4499] Val Loss:0.2762559950351715\n",
      "Epoch 0[194/4499] Val Loss:0.33054035902023315\n",
      "Epoch 0[195/4499] Val Loss:0.30376002192497253\n",
      "Epoch 0[196/4499] Val Loss:0.33797070384025574\n",
      "Epoch 0[197/4499] Val Loss:0.2848813533782959\n",
      "Epoch 0[198/4499] Val Loss:0.2657676637172699\n",
      "Epoch 0[199/4499] Val Loss:0.27154025435447693\n",
      "Epoch 0[200/4499] Val Loss:0.20203061401844025\n",
      "Epoch 0[201/4499] Val Loss:0.33062744140625\n",
      "Epoch 0[202/4499] Val Loss:0.23468540608882904\n",
      "Epoch 0[203/4499] Val Loss:0.22996120154857635\n",
      "Epoch 0[204/4499] Val Loss:0.30321750044822693\n",
      "Epoch 0[205/4499] Val Loss:0.3617185354232788\n",
      "Epoch 0[206/4499] Val Loss:0.261420875787735\n",
      "Epoch 0[207/4499] Val Loss:0.4087377190589905\n",
      "Epoch 0[208/4499] Val Loss:0.3392403721809387\n",
      "Epoch 0[209/4499] Val Loss:0.24887558817863464\n",
      "Epoch 0[210/4499] Val Loss:0.35843661427497864\n",
      "Epoch 0[211/4499] Val Loss:0.23542505502700806\n",
      "Epoch 0[212/4499] Val Loss:0.23274989426136017\n",
      "Epoch 0[213/4499] Val Loss:0.29118990898132324\n",
      "Epoch 0[214/4499] Val Loss:0.3739510774612427\n",
      "Epoch 0[215/4499] Val Loss:0.3253490924835205\n",
      "Epoch 0[216/4499] Val Loss:0.42090895771980286\n",
      "Epoch 0[217/4499] Val Loss:0.5487275719642639\n",
      "Epoch 0[218/4499] Val Loss:0.28307968378067017\n",
      "Epoch 0[219/4499] Val Loss:0.465926855802536\n",
      "Epoch 0[220/4499] Val Loss:0.6942951679229736\n",
      "Epoch 0[221/4499] Val Loss:0.6844559907913208\n",
      "Epoch 0[222/4499] Val Loss:0.6964545249938965\n",
      "Epoch 0[223/4499] Val Loss:0.5555598139762878\n",
      "Epoch 0[224/4499] Val Loss:0.3677324056625366\n",
      "Epoch 0[225/4499] Val Loss:0.45686373114585876\n",
      "Epoch 0[226/4499] Val Loss:0.4310740828514099\n",
      "Epoch 0[227/4499] Val Loss:0.36995336413383484\n",
      "Epoch 0[228/4499] Val Loss:0.38300982117652893\n",
      "Epoch 0[229/4499] Val Loss:0.4480670094490051\n",
      "Epoch 0[230/4499] Val Loss:0.39623674750328064\n",
      "Epoch 0[231/4499] Val Loss:0.3370283246040344\n",
      "Epoch 0[232/4499] Val Loss:0.359769731760025\n",
      "Epoch 0[233/4499] Val Loss:0.3842775225639343\n",
      "Epoch 0[234/4499] Val Loss:0.2905772030353546\n",
      "Epoch 0[235/4499] Val Loss:0.35517847537994385\n",
      "Epoch 0[236/4499] Val Loss:0.36387908458709717\n",
      "Epoch 0[237/4499] Val Loss:0.32464683055877686\n",
      "Epoch 0[238/4499] Val Loss:0.38069742918014526\n",
      "Epoch 0[239/4499] Val Loss:0.38636860251426697\n",
      "Epoch 0[240/4499] Val Loss:0.4453604519367218\n",
      "Epoch 0[241/4499] Val Loss:0.39658620953559875\n",
      "Epoch 0[242/4499] Val Loss:0.2829669713973999\n",
      "Epoch 0[243/4499] Val Loss:0.18121612071990967\n",
      "Epoch 0[244/4499] Val Loss:0.17575344443321228\n",
      "Epoch 0[245/4499] Val Loss:0.23921184241771698\n",
      "Epoch 0[246/4499] Val Loss:0.2676974833011627\n",
      "Epoch 0[247/4499] Val Loss:0.3339785635471344\n",
      "Epoch 0[248/4499] Val Loss:0.4529103636741638\n",
      "Epoch 0[249/4499] Val Loss:0.3898817300796509\n",
      "Epoch 0[250/4499] Val Loss:0.34648609161376953\n",
      "Epoch 0[251/4499] Val Loss:0.48961442708969116\n",
      "Epoch 0[252/4499] Val Loss:0.790377676486969\n",
      "Epoch 0[253/4499] Val Loss:0.35818102955818176\n",
      "Epoch 0[254/4499] Val Loss:0.5078722238540649\n",
      "Epoch 0[255/4499] Val Loss:0.3405284881591797\n",
      "Epoch 0[256/4499] Val Loss:0.5474317073822021\n",
      "Epoch 0[257/4499] Val Loss:0.4240906536579132\n",
      "Epoch 0[258/4499] Val Loss:0.2981668710708618\n",
      "Epoch 0[259/4499] Val Loss:0.40156012773513794\n",
      "Epoch 0[260/4499] Val Loss:0.3395402133464813\n",
      "Epoch 0[261/4499] Val Loss:0.26444071531295776\n",
      "Epoch 0[262/4499] Val Loss:0.22888517379760742\n",
      "Epoch 0[263/4499] Val Loss:0.3591553866863251\n",
      "Epoch 0[264/4499] Val Loss:0.300412118434906\n",
      "Epoch 0[265/4499] Val Loss:0.18899670243263245\n",
      "Epoch 0[266/4499] Val Loss:0.2789457440376282\n",
      "Epoch 0[267/4499] Val Loss:0.5361321568489075\n",
      "Epoch 0[268/4499] Val Loss:0.5010021328926086\n",
      "Epoch 0[269/4499] Val Loss:0.14711520075798035\n",
      "Epoch 0[270/4499] Val Loss:0.3012753427028656\n",
      "Epoch 0[271/4499] Val Loss:0.1808944195508957\n",
      "Epoch 0[272/4499] Val Loss:0.1623295545578003\n",
      "Epoch 0[273/4499] Val Loss:0.14963753521442413\n",
      "Epoch 0[274/4499] Val Loss:0.4736540913581848\n",
      "Epoch 0[275/4499] Val Loss:0.48713502287864685\n",
      "Epoch 0[276/4499] Val Loss:0.38507774472236633\n",
      "Epoch 0[277/4499] Val Loss:0.4243139326572418\n",
      "Epoch 0[278/4499] Val Loss:0.7603364586830139\n",
      "Epoch 0[279/4499] Val Loss:0.48664289712905884\n",
      "Epoch 0[280/4499] Val Loss:0.5362250804901123\n",
      "Epoch 0[281/4499] Val Loss:0.5571953058242798\n",
      "Epoch 0[282/4499] Val Loss:0.6410427689552307\n",
      "Epoch 0[283/4499] Val Loss:0.5063231587409973\n",
      "Epoch 0[284/4499] Val Loss:0.4735679030418396\n",
      "Epoch 0[285/4499] Val Loss:0.4992945194244385\n",
      "Epoch 0[286/4499] Val Loss:0.6812396049499512\n",
      "Epoch 0[287/4499] Val Loss:0.5474583506584167\n",
      "Epoch 0[288/4499] Val Loss:0.4713136851787567\n",
      "Epoch 0[289/4499] Val Loss:0.6682730913162231\n",
      "Epoch 0[290/4499] Val Loss:0.697589099407196\n",
      "Epoch 0[291/4499] Val Loss:0.48672983050346375\n",
      "Epoch 0[292/4499] Val Loss:0.8316673636436462\n",
      "Epoch 0[293/4499] Val Loss:0.694542407989502\n",
      "Epoch 0[294/4499] Val Loss:0.23016344010829926\n",
      "Epoch 0[295/4499] Val Loss:0.7585389018058777\n",
      "Epoch 0[296/4499] Val Loss:0.6736631393432617\n",
      "Epoch 0[297/4499] Val Loss:0.2940945029258728\n",
      "Epoch 0[298/4499] Val Loss:0.7328240871429443\n",
      "Epoch 0[299/4499] Val Loss:0.6417099833488464\n",
      "Epoch 0[300/4499] Val Loss:0.4336991012096405\n",
      "Epoch 0[301/4499] Val Loss:0.38259854912757874\n",
      "Epoch 0[302/4499] Val Loss:0.3969863951206207\n",
      "Epoch 0[303/4499] Val Loss:0.3172035217285156\n",
      "Epoch 0[304/4499] Val Loss:0.28364625573158264\n",
      "Epoch 0[305/4499] Val Loss:0.3106735646724701\n",
      "Epoch 0[306/4499] Val Loss:0.30604350566864014\n",
      "Epoch 0[307/4499] Val Loss:0.3368247449398041\n",
      "Epoch 0[308/4499] Val Loss:0.28378671407699585\n",
      "Epoch 0[309/4499] Val Loss:0.3461554944515228\n",
      "Epoch 0[310/4499] Val Loss:0.36345720291137695\n",
      "Epoch 0[311/4499] Val Loss:0.448639839887619\n",
      "Epoch 0[312/4499] Val Loss:0.3655560612678528\n",
      "Epoch 0[313/4499] Val Loss:0.31296253204345703\n",
      "Epoch 0[314/4499] Val Loss:0.2877535820007324\n",
      "Epoch 0[315/4499] Val Loss:0.3031924068927765\n",
      "Epoch 0[316/4499] Val Loss:0.34820857644081116\n",
      "Epoch 0[317/4499] Val Loss:0.30591151118278503\n",
      "Epoch 0[318/4499] Val Loss:0.21645420789718628\n",
      "Epoch 0[319/4499] Val Loss:0.2799813747406006\n",
      "Epoch 0[320/4499] Val Loss:0.2742263674736023\n",
      "Epoch 0[321/4499] Val Loss:0.3133227527141571\n",
      "Epoch 0[322/4499] Val Loss:0.3209095001220703\n",
      "Epoch 0[323/4499] Val Loss:0.267936110496521\n",
      "Epoch 0[324/4499] Val Loss:0.22357526421546936\n",
      "Epoch 0[325/4499] Val Loss:0.2400406002998352\n",
      "Epoch 0[326/4499] Val Loss:0.2897671163082123\n",
      "Epoch 0[327/4499] Val Loss:0.68446946144104\n",
      "Epoch 0[328/4499] Val Loss:0.47063523530960083\n",
      "Epoch 0[329/4499] Val Loss:0.4839614927768707\n",
      "Epoch 0[330/4499] Val Loss:0.4333803951740265\n",
      "Epoch 0[331/4499] Val Loss:0.35226309299468994\n",
      "Epoch 0[332/4499] Val Loss:0.511864960193634\n",
      "Epoch 0[333/4499] Val Loss:0.6281805634498596\n",
      "Epoch 0[334/4499] Val Loss:0.34504663944244385\n",
      "Epoch 0[335/4499] Val Loss:0.18159325420856476\n",
      "Epoch 0[336/4499] Val Loss:0.20195263624191284\n",
      "Epoch 0[337/4499] Val Loss:0.17945657670497894\n",
      "Epoch 0[338/4499] Val Loss:0.291557252407074\n",
      "Epoch 0[339/4499] Val Loss:0.24091820418834686\n",
      "Epoch 0[340/4499] Val Loss:0.21760903298854828\n",
      "Epoch 0[341/4499] Val Loss:0.2349114716053009\n",
      "Epoch 0[342/4499] Val Loss:0.18005682528018951\n",
      "Epoch 0[343/4499] Val Loss:0.18586640059947968\n",
      "Epoch 0[344/4499] Val Loss:0.18218202888965607\n",
      "Epoch 0[345/4499] Val Loss:0.2146771252155304\n",
      "Epoch 0[346/4499] Val Loss:0.2627938687801361\n",
      "Epoch 0[347/4499] Val Loss:0.3056819438934326\n",
      "Epoch 0[348/4499] Val Loss:0.264271080493927\n",
      "Epoch 0[349/4499] Val Loss:0.24879848957061768\n",
      "Epoch 0[350/4499] Val Loss:0.1476721316576004\n",
      "Epoch 0[351/4499] Val Loss:0.1362033188343048\n",
      "Epoch 0[352/4499] Val Loss:0.1925511509180069\n",
      "Epoch 0[353/4499] Val Loss:0.1866961121559143\n",
      "Epoch 0[354/4499] Val Loss:0.15543240308761597\n",
      "Epoch 0[355/4499] Val Loss:0.206414595246315\n",
      "Epoch 0[356/4499] Val Loss:0.1157333180308342\n",
      "Epoch 0[357/4499] Val Loss:0.1533185988664627\n",
      "Epoch 0[358/4499] Val Loss:0.1525844931602478\n",
      "Epoch 0[359/4499] Val Loss:0.29421767592430115\n",
      "Epoch 0[360/4499] Val Loss:0.47085049748420715\n",
      "Epoch 0[361/4499] Val Loss:0.30298930406570435\n",
      "Epoch 0[362/4499] Val Loss:0.4421955943107605\n",
      "Epoch 0[363/4499] Val Loss:0.30516478419303894\n",
      "Epoch 0[364/4499] Val Loss:0.5762247443199158\n",
      "Epoch 0[365/4499] Val Loss:0.5226655602455139\n",
      "Epoch 0[366/4499] Val Loss:0.5282086730003357\n",
      "Epoch 0[367/4499] Val Loss:0.3259228765964508\n",
      "Epoch 0[368/4499] Val Loss:0.5318468809127808\n",
      "Epoch 0[369/4499] Val Loss:0.5084211826324463\n",
      "Epoch 0[370/4499] Val Loss:0.4429004192352295\n",
      "Epoch 0[371/4499] Val Loss:0.39521124958992004\n",
      "Epoch 0[372/4499] Val Loss:0.4116328954696655\n",
      "Epoch 0[373/4499] Val Loss:0.5149006247520447\n",
      "Epoch 0[374/4499] Val Loss:0.2671189308166504\n",
      "Epoch 0[375/4499] Val Loss:0.4510248005390167\n",
      "Epoch 0[376/4499] Val Loss:0.4890209138393402\n",
      "Epoch 0[377/4499] Val Loss:0.4796784520149231\n",
      "Epoch 0[378/4499] Val Loss:0.3959273099899292\n",
      "Epoch 0[379/4499] Val Loss:0.44509243965148926\n",
      "Epoch 0[380/4499] Val Loss:0.32097694277763367\n",
      "Epoch 0[381/4499] Val Loss:0.5111271739006042\n",
      "Epoch 0[382/4499] Val Loss:0.44558531045913696\n",
      "Epoch 0[383/4499] Val Loss:0.5354471206665039\n",
      "Epoch 0[384/4499] Val Loss:0.38298672437667847\n",
      "Epoch 0[385/4499] Val Loss:0.35998693108558655\n",
      "Epoch 0[386/4499] Val Loss:0.2818416655063629\n",
      "Epoch 0[387/4499] Val Loss:0.47926896810531616\n",
      "Epoch 0[388/4499] Val Loss:0.5244422554969788\n",
      "Epoch 0[389/4499] Val Loss:0.5970010757446289\n",
      "Epoch 0[390/4499] Val Loss:0.5230965614318848\n",
      "Epoch 0[391/4499] Val Loss:0.53854900598526\n",
      "Epoch 0[392/4499] Val Loss:0.5159614682197571\n",
      "Epoch 0[393/4499] Val Loss:0.4941655099391937\n",
      "Epoch 0[394/4499] Val Loss:0.47959232330322266\n",
      "Epoch 0[395/4499] Val Loss:0.5538356900215149\n",
      "Epoch 0[396/4499] Val Loss:0.48384615778923035\n",
      "Epoch 0[397/4499] Val Loss:0.6261063814163208\n",
      "Epoch 0[398/4499] Val Loss:0.5713754296302795\n",
      "Epoch 0[399/4499] Val Loss:0.3583305776119232\n",
      "Epoch 0[400/4499] Val Loss:0.44072696566581726\n",
      "Epoch 0[401/4499] Val Loss:0.6360758543014526\n",
      "Epoch 0[402/4499] Val Loss:0.6100109219551086\n",
      "Epoch 0[403/4499] Val Loss:0.4101610779762268\n",
      "Epoch 0[404/4499] Val Loss:0.4687890410423279\n",
      "Epoch 0[405/4499] Val Loss:0.5438815355300903\n",
      "Epoch 0[406/4499] Val Loss:0.43418315052986145\n",
      "Epoch 0[407/4499] Val Loss:0.3606027364730835\n",
      "Epoch 0[408/4499] Val Loss:0.4248717427253723\n",
      "Epoch 0[409/4499] Val Loss:0.4655565023422241\n",
      "Epoch 0[410/4499] Val Loss:0.5682398676872253\n",
      "Epoch 0[411/4499] Val Loss:0.45301663875579834\n",
      "Epoch 0[412/4499] Val Loss:0.40072381496429443\n",
      "Epoch 0[413/4499] Val Loss:0.3743729591369629\n",
      "Epoch 0[414/4499] Val Loss:0.30845266580581665\n",
      "Epoch 0[415/4499] Val Loss:0.3190482556819916\n",
      "Epoch 0[416/4499] Val Loss:0.2556172013282776\n",
      "Epoch 0[417/4499] Val Loss:0.21955496072769165\n",
      "Epoch 0[418/4499] Val Loss:0.20049290359020233\n",
      "Epoch 0[419/4499] Val Loss:0.2001866102218628\n",
      "Epoch 0[420/4499] Val Loss:0.2329634726047516\n",
      "Epoch 0[421/4499] Val Loss:0.20163050293922424\n",
      "Epoch 0[422/4499] Val Loss:0.22951187193393707\n",
      "Epoch 0[423/4499] Val Loss:0.31596794724464417\n",
      "Epoch 0[424/4499] Val Loss:0.3492487072944641\n",
      "Epoch 0[425/4499] Val Loss:0.2973906993865967\n",
      "Epoch 0[426/4499] Val Loss:0.28455090522766113\n",
      "Epoch 0[427/4499] Val Loss:0.2854408919811249\n",
      "Epoch 0[428/4499] Val Loss:0.40317073464393616\n",
      "Epoch 0[429/4499] Val Loss:0.3845820724964142\n",
      "Epoch 0[430/4499] Val Loss:0.4428845942020416\n",
      "Epoch 0[431/4499] Val Loss:0.5017839074134827\n",
      "Epoch 0[432/4499] Val Loss:0.4163757264614105\n",
      "Epoch 0[433/4499] Val Loss:0.4727919399738312\n",
      "Epoch 0[434/4499] Val Loss:0.32092049717903137\n",
      "Epoch 0[435/4499] Val Loss:0.37995558977127075\n",
      "Epoch 0[436/4499] Val Loss:0.6676062941551208\n",
      "Epoch 0[437/4499] Val Loss:0.5075663328170776\n",
      "Epoch 0[438/4499] Val Loss:0.3569159209728241\n",
      "Epoch 0[439/4499] Val Loss:0.3543879985809326\n",
      "Epoch 0[440/4499] Val Loss:0.4348246455192566\n",
      "Epoch 0[441/4499] Val Loss:0.48580166697502136\n",
      "Epoch 0[442/4499] Val Loss:0.5581989288330078\n",
      "Epoch 0[443/4499] Val Loss:0.5767247080802917\n",
      "Epoch 0[444/4499] Val Loss:0.4288969933986664\n",
      "Epoch 0[445/4499] Val Loss:0.4330189526081085\n",
      "Epoch 0[446/4499] Val Loss:0.4475081264972687\n",
      "Epoch 0[447/4499] Val Loss:0.3705439865589142\n",
      "Epoch 0[448/4499] Val Loss:0.40370973944664\n",
      "Epoch 0[449/4499] Val Loss:0.3975042700767517\n",
      "Epoch 0[450/4499] Val Loss:0.4351467788219452\n",
      "Epoch 0[451/4499] Val Loss:0.43218234181404114\n",
      "Epoch 0[452/4499] Val Loss:0.49260902404785156\n",
      "Epoch 0[453/4499] Val Loss:0.4861806035041809\n",
      "Epoch 0[454/4499] Val Loss:0.41514259576797485\n",
      "Epoch 0[455/4499] Val Loss:0.5313869714736938\n",
      "Epoch 0[456/4499] Val Loss:0.3915903866291046\n",
      "Epoch 0[457/4499] Val Loss:0.4898468255996704\n",
      "Epoch 0[458/4499] Val Loss:0.4647281765937805\n",
      "Epoch 0[459/4499] Val Loss:0.2995608150959015\n",
      "Epoch 0[460/4499] Val Loss:0.21269281208515167\n",
      "Epoch 0[461/4499] Val Loss:0.30901068449020386\n",
      "Epoch 0[462/4499] Val Loss:0.26363134384155273\n",
      "Epoch 0[463/4499] Val Loss:0.2741827070713043\n",
      "Epoch 0[464/4499] Val Loss:0.3202613890171051\n",
      "Epoch 0[465/4499] Val Loss:0.5339738130569458\n",
      "Epoch 0[466/4499] Val Loss:0.5898768901824951\n",
      "Epoch 0[467/4499] Val Loss:0.6399916410446167\n",
      "Epoch 0[468/4499] Val Loss:0.5213939547538757\n",
      "Epoch 0[469/4499] Val Loss:0.6028813123703003\n",
      "Epoch 0[470/4499] Val Loss:0.5855826735496521\n",
      "Epoch 0[471/4499] Val Loss:0.5813512206077576\n",
      "Epoch 0[472/4499] Val Loss:0.6158581972122192\n",
      "Epoch 0[473/4499] Val Loss:0.47612136602401733\n",
      "Epoch 0[474/4499] Val Loss:0.6830736398696899\n",
      "Epoch 0[475/4499] Val Loss:0.6288232803344727\n",
      "Epoch 0[476/4499] Val Loss:0.4924579858779907\n",
      "Epoch 0[477/4499] Val Loss:0.552262544631958\n",
      "Epoch 0[478/4499] Val Loss:0.5352769494056702\n",
      "Epoch 0[479/4499] Val Loss:0.3038140833377838\n",
      "Epoch 0[480/4499] Val Loss:0.40940722823143005\n",
      "Epoch 0[481/4499] Val Loss:0.5783028602600098\n",
      "Epoch 0[482/4499] Val Loss:0.5273185968399048\n",
      "Epoch 0[483/4499] Val Loss:0.44969794154167175\n",
      "Epoch 0[484/4499] Val Loss:0.5409554243087769\n",
      "Epoch 0[485/4499] Val Loss:0.3889095187187195\n",
      "Epoch 0[486/4499] Val Loss:0.35498806834220886\n",
      "Epoch 0[487/4499] Val Loss:0.3805365264415741\n",
      "Epoch 0[488/4499] Val Loss:0.4379185736179352\n",
      "Epoch 0[489/4499] Val Loss:0.3126721978187561\n",
      "Epoch 0[490/4499] Val Loss:0.4812549948692322\n",
      "Epoch 0[491/4499] Val Loss:0.40909475088119507\n",
      "Epoch 0[492/4499] Val Loss:0.5925472974777222\n",
      "Epoch 0[493/4499] Val Loss:0.5854663252830505\n",
      "Epoch 0[494/4499] Val Loss:0.6833113431930542\n",
      "Epoch 0[495/4499] Val Loss:0.7119286060333252\n",
      "Epoch 0[496/4499] Val Loss:0.670569658279419\n",
      "Epoch 0[497/4499] Val Loss:0.5839049220085144\n",
      "Epoch 0[498/4499] Val Loss:0.5948092341423035\n",
      "Epoch 0[499/4499] Val Loss:0.443722665309906\n",
      "Epoch 0[500/4499] Val Loss:0.510590136051178\n",
      "Epoch 0[501/4499] Val Loss:0.48206138610839844\n",
      "Epoch 0[502/4499] Val Loss:0.4773119390010834\n",
      "Epoch 0[503/4499] Val Loss:0.327932745218277\n",
      "Epoch 0[504/4499] Val Loss:0.5460761189460754\n",
      "Epoch 0[505/4499] Val Loss:0.5638010501861572\n",
      "Epoch 0[506/4499] Val Loss:0.4514876902103424\n",
      "Epoch 0[507/4499] Val Loss:0.4509522020816803\n",
      "Epoch 0[508/4499] Val Loss:0.5449208617210388\n",
      "Epoch 0[509/4499] Val Loss:0.3577045202255249\n",
      "Epoch 0[510/4499] Val Loss:0.5132060647010803\n",
      "Epoch 0[511/4499] Val Loss:0.6529551148414612\n",
      "Epoch 0[512/4499] Val Loss:0.6353705525398254\n",
      "Epoch 0[513/4499] Val Loss:0.37156471610069275\n",
      "Epoch 0[514/4499] Val Loss:0.6666727662086487\n",
      "Epoch 0[515/4499] Val Loss:0.6009920239448547\n",
      "Epoch 0[516/4499] Val Loss:0.6520510315895081\n",
      "Epoch 0[517/4499] Val Loss:0.4833427667617798\n",
      "Epoch 0[518/4499] Val Loss:0.4528794586658478\n",
      "Epoch 0[519/4499] Val Loss:0.3425986170768738\n",
      "Epoch 0[520/4499] Val Loss:0.4395456314086914\n",
      "Epoch 0[521/4499] Val Loss:0.3338998854160309\n",
      "Epoch 0[522/4499] Val Loss:0.32376182079315186\n",
      "Epoch 0[523/4499] Val Loss:0.3157289922237396\n",
      "Epoch 0[524/4499] Val Loss:0.3366738259792328\n",
      "Epoch 0[525/4499] Val Loss:0.30842283368110657\n",
      "Epoch 0[526/4499] Val Loss:0.35779112577438354\n",
      "Epoch 0[527/4499] Val Loss:0.25477322936058044\n",
      "Epoch 0[528/4499] Val Loss:0.3321148455142975\n",
      "Epoch 0[529/4499] Val Loss:0.32453587651252747\n",
      "Epoch 0[530/4499] Val Loss:0.2602120339870453\n",
      "Epoch 0[531/4499] Val Loss:0.2761012017726898\n",
      "Epoch 0[532/4499] Val Loss:0.41033482551574707\n",
      "Epoch 0[533/4499] Val Loss:0.37237563729286194\n",
      "Epoch 0[534/4499] Val Loss:0.3633360266685486\n",
      "Epoch 0[535/4499] Val Loss:0.33212539553642273\n",
      "Epoch 0[536/4499] Val Loss:0.4415266215801239\n",
      "Epoch 0[537/4499] Val Loss:0.3799683451652527\n",
      "Epoch 0[538/4499] Val Loss:0.41405439376831055\n",
      "Epoch 0[539/4499] Val Loss:0.5037780404090881\n",
      "Epoch 0[540/4499] Val Loss:0.4806199073791504\n",
      "Epoch 0[541/4499] Val Loss:0.6724564433097839\n",
      "Epoch 0[542/4499] Val Loss:0.6880362629890442\n",
      "Epoch 0[543/4499] Val Loss:0.33248651027679443\n",
      "Epoch 0[544/4499] Val Loss:0.6366305351257324\n",
      "Epoch 0[545/4499] Val Loss:0.7410993576049805\n",
      "Epoch 0[546/4499] Val Loss:0.6015831828117371\n",
      "Epoch 0[547/4499] Val Loss:0.543524980545044\n",
      "Epoch 0[548/4499] Val Loss:0.5520372986793518\n",
      "Epoch 0[549/4499] Val Loss:0.6120759844779968\n",
      "Epoch 0[550/4499] Val Loss:0.5940787196159363\n",
      "Epoch 0[551/4499] Val Loss:0.6682590246200562\n",
      "Epoch 0[552/4499] Val Loss:0.5691630244255066\n",
      "Epoch 0[553/4499] Val Loss:0.6362797617912292\n",
      "Epoch 0[554/4499] Val Loss:0.5536214113235474\n",
      "Epoch 0[555/4499] Val Loss:0.6389455795288086\n",
      "Epoch 0[556/4499] Val Loss:0.5692434906959534\n",
      "Epoch 0[557/4499] Val Loss:0.6556469202041626\n",
      "Epoch 0[558/4499] Val Loss:0.6127431392669678\n",
      "Epoch 0[559/4499] Val Loss:0.5850322246551514\n",
      "Epoch 0[560/4499] Val Loss:0.5990784764289856\n",
      "Epoch 0[561/4499] Val Loss:0.5828818678855896\n",
      "Epoch 0[562/4499] Val Loss:0.5823814868927002\n",
      "Epoch 0[563/4499] Val Loss:0.5869261026382446\n",
      "Epoch 0[564/4499] Val Loss:0.5682234168052673\n",
      "Epoch 0[565/4499] Val Loss:0.5063533186912537\n",
      "Epoch 0[566/4499] Val Loss:0.5246700048446655\n",
      "Epoch 0[567/4499] Val Loss:0.4244517683982849\n",
      "Epoch 0[568/4499] Val Loss:0.47308075428009033\n",
      "Epoch 0[569/4499] Val Loss:0.5063779950141907\n",
      "Epoch 0[570/4499] Val Loss:0.4993423819541931\n",
      "Epoch 0[571/4499] Val Loss:0.4059887230396271\n",
      "Epoch 0[572/4499] Val Loss:0.4454670548439026\n",
      "Epoch 0[573/4499] Val Loss:0.44869178533554077\n",
      "Epoch 0[574/4499] Val Loss:0.5873597264289856\n",
      "Epoch 0[575/4499] Val Loss:0.5597090125083923\n",
      "Epoch 0[576/4499] Val Loss:0.6619066596031189\n",
      "Epoch 0[577/4499] Val Loss:0.761831521987915\n",
      "Epoch 0[578/4499] Val Loss:0.8815370798110962\n",
      "Epoch 0[579/4499] Val Loss:0.6346835494041443\n",
      "Epoch 0[580/4499] Val Loss:0.7679151296615601\n",
      "Epoch 0[581/4499] Val Loss:0.7565276622772217\n",
      "Epoch 0[582/4499] Val Loss:0.659346342086792\n",
      "Epoch 0[583/4499] Val Loss:0.795591413974762\n",
      "Epoch 0[584/4499] Val Loss:0.6570788621902466\n",
      "Epoch 0[585/4499] Val Loss:0.5146298408508301\n",
      "Epoch 0[586/4499] Val Loss:0.6843319535255432\n",
      "Epoch 0[587/4499] Val Loss:0.6355865597724915\n",
      "Epoch 0[588/4499] Val Loss:0.5177485942840576\n",
      "Epoch 0[589/4499] Val Loss:0.6380659341812134\n",
      "Epoch 0[590/4499] Val Loss:0.5416414141654968\n",
      "Epoch 0[591/4499] Val Loss:0.4485655426979065\n",
      "Epoch 0[592/4499] Val Loss:0.47929465770721436\n",
      "Epoch 0[593/4499] Val Loss:0.4940285086631775\n",
      "Epoch 0[594/4499] Val Loss:0.4635241627693176\n",
      "Epoch 0[595/4499] Val Loss:0.46319326758384705\n",
      "Epoch 0[596/4499] Val Loss:0.39064279198646545\n",
      "Epoch 0[597/4499] Val Loss:0.35692083835601807\n",
      "Epoch 0[598/4499] Val Loss:0.4906063377857208\n",
      "Epoch 0[599/4499] Val Loss:0.4652262032032013\n",
      "Epoch 0[600/4499] Val Loss:0.474467396736145\n",
      "Epoch 0[601/4499] Val Loss:0.6354911923408508\n",
      "Epoch 0[602/4499] Val Loss:0.5903818607330322\n",
      "Epoch 0[603/4499] Val Loss:0.6890197992324829\n",
      "Epoch 0[604/4499] Val Loss:0.5916311740875244\n",
      "Epoch 0[605/4499] Val Loss:0.7030623555183411\n",
      "Epoch 0[606/4499] Val Loss:0.6331241726875305\n",
      "Epoch 0[607/4499] Val Loss:0.6585303544998169\n",
      "Epoch 0[608/4499] Val Loss:0.6637151837348938\n",
      "Epoch 0[609/4499] Val Loss:0.5976160764694214\n",
      "Epoch 0[610/4499] Val Loss:0.4014734625816345\n",
      "Epoch 0[611/4499] Val Loss:0.5005339980125427\n",
      "Epoch 0[612/4499] Val Loss:0.5635421276092529\n",
      "Epoch 0[613/4499] Val Loss:0.4433410167694092\n",
      "Epoch 0[614/4499] Val Loss:0.5340992212295532\n",
      "Epoch 0[615/4499] Val Loss:0.5081815123558044\n",
      "Epoch 0[616/4499] Val Loss:0.39433905482292175\n",
      "Epoch 0[617/4499] Val Loss:0.577057421207428\n",
      "Epoch 0[618/4499] Val Loss:0.44083598256111145\n",
      "Epoch 0[619/4499] Val Loss:0.3983798623085022\n",
      "Epoch 0[620/4499] Val Loss:0.47949841618537903\n",
      "Epoch 0[621/4499] Val Loss:0.5446462631225586\n",
      "Epoch 0[622/4499] Val Loss:0.5562908053398132\n",
      "Epoch 0[623/4499] Val Loss:0.47554445266723633\n",
      "Epoch 0[624/4499] Val Loss:0.5434105396270752\n",
      "Epoch 0[625/4499] Val Loss:0.6365526914596558\n",
      "Epoch 0[626/4499] Val Loss:0.440481036901474\n",
      "Epoch 0[627/4499] Val Loss:0.4626538157463074\n",
      "Epoch 0[628/4499] Val Loss:0.41380149126052856\n",
      "Epoch 0[629/4499] Val Loss:0.5533013343811035\n",
      "Epoch 0[630/4499] Val Loss:0.4642197787761688\n",
      "Epoch 0[631/4499] Val Loss:0.40957775712013245\n",
      "Epoch 0[632/4499] Val Loss:0.38132038712501526\n",
      "Epoch 0[633/4499] Val Loss:0.4044564664363861\n",
      "Epoch 0[634/4499] Val Loss:0.3790138065814972\n",
      "Epoch 0[635/4499] Val Loss:0.4002634584903717\n",
      "Epoch 0[636/4499] Val Loss:0.36247718334198\n",
      "Epoch 0[637/4499] Val Loss:0.30243754386901855\n",
      "Epoch 0[638/4499] Val Loss:0.4153578579425812\n",
      "Epoch 0[639/4499] Val Loss:0.3270893096923828\n",
      "Epoch 0[640/4499] Val Loss:0.38598984479904175\n",
      "Epoch 0[641/4499] Val Loss:0.36296549439430237\n",
      "Epoch 0[642/4499] Val Loss:0.33948448300361633\n",
      "Epoch 0[643/4499] Val Loss:0.3932805061340332\n",
      "Epoch 0[644/4499] Val Loss:0.332823246717453\n",
      "Epoch 0[645/4499] Val Loss:0.33130815625190735\n",
      "Epoch 0[646/4499] Val Loss:0.3660215139389038\n",
      "Epoch 0[647/4499] Val Loss:0.39612507820129395\n",
      "Epoch 0[648/4499] Val Loss:0.41383102536201477\n",
      "Epoch 0[649/4499] Val Loss:0.3816036880016327\n",
      "Epoch 0[650/4499] Val Loss:0.46062734723091125\n",
      "Epoch 0[651/4499] Val Loss:0.48505815863609314\n",
      "Epoch 0[652/4499] Val Loss:0.6996828317642212\n",
      "Epoch 0[653/4499] Val Loss:0.6459810733795166\n",
      "Epoch 0[654/4499] Val Loss:0.7365742921829224\n",
      "Epoch 0[655/4499] Val Loss:0.48505279421806335\n",
      "Epoch 0[656/4499] Val Loss:0.6409134268760681\n",
      "Epoch 0[657/4499] Val Loss:0.6965433955192566\n",
      "Epoch 0[658/4499] Val Loss:0.8199974894523621\n",
      "Epoch 0[659/4499] Val Loss:0.5974858999252319\n",
      "Epoch 0[660/4499] Val Loss:0.564139723777771\n",
      "Epoch 0[661/4499] Val Loss:0.3899204134941101\n",
      "Epoch 0[662/4499] Val Loss:0.4563522934913635\n",
      "Epoch 0[663/4499] Val Loss:0.49286890029907227\n",
      "Epoch 0[664/4499] Val Loss:0.7060524225234985\n",
      "Epoch 0[665/4499] Val Loss:0.38405612111091614\n",
      "Epoch 0[666/4499] Val Loss:0.3892534673213959\n",
      "Epoch 0[667/4499] Val Loss:0.43403205275535583\n",
      "Epoch 0[668/4499] Val Loss:0.4084293246269226\n",
      "Epoch 0[669/4499] Val Loss:0.32163217663764954\n",
      "Epoch 0[670/4499] Val Loss:0.22470533847808838\n",
      "Epoch 0[671/4499] Val Loss:0.3167984187602997\n",
      "Epoch 0[672/4499] Val Loss:0.4253704249858856\n",
      "Epoch 0[673/4499] Val Loss:0.44089120626449585\n",
      "Epoch 0[674/4499] Val Loss:0.42071768641471863\n",
      "Epoch 0[675/4499] Val Loss:0.427434504032135\n",
      "Epoch 0[676/4499] Val Loss:0.4996020793914795\n",
      "Epoch 0[677/4499] Val Loss:0.3826633393764496\n",
      "Epoch 0[678/4499] Val Loss:0.15273316204547882\n",
      "Epoch 0[679/4499] Val Loss:0.2721216380596161\n",
      "Epoch 0[680/4499] Val Loss:0.4179200232028961\n",
      "Epoch 0[681/4499] Val Loss:0.7802429795265198\n",
      "Epoch 0[682/4499] Val Loss:0.4769340455532074\n",
      "Epoch 0[683/4499] Val Loss:0.4666454792022705\n",
      "Epoch 0[684/4499] Val Loss:0.4992426633834839\n",
      "Epoch 0[685/4499] Val Loss:0.45666754245758057\n",
      "Epoch 0[686/4499] Val Loss:0.6683943271636963\n",
      "Epoch 0[687/4499] Val Loss:0.7241075038909912\n",
      "Epoch 0[688/4499] Val Loss:0.5236355662345886\n",
      "Epoch 0[689/4499] Val Loss:0.6761392951011658\n",
      "Epoch 0[690/4499] Val Loss:0.7034282088279724\n",
      "Epoch 0[691/4499] Val Loss:0.7741898894309998\n",
      "Epoch 0[692/4499] Val Loss:0.6024228930473328\n",
      "Epoch 0[693/4499] Val Loss:0.6288725733757019\n",
      "Epoch 0[694/4499] Val Loss:0.6949872970581055\n",
      "Epoch 0[695/4499] Val Loss:0.8163279891014099\n",
      "Epoch 0[696/4499] Val Loss:0.633151113986969\n",
      "Epoch 0[697/4499] Val Loss:0.7355742454528809\n",
      "Epoch 0[698/4499] Val Loss:0.7455353736877441\n",
      "Epoch 0[699/4499] Val Loss:0.674274742603302\n",
      "Epoch 0[700/4499] Val Loss:0.7404505610466003\n",
      "Epoch 0[701/4499] Val Loss:0.5240637063980103\n",
      "Epoch 0[702/4499] Val Loss:0.7079479098320007\n",
      "Epoch 0[703/4499] Val Loss:0.6993530988693237\n",
      "Epoch 0[704/4499] Val Loss:0.7567034959793091\n",
      "Epoch 0[705/4499] Val Loss:0.6358689069747925\n",
      "Epoch 0[706/4499] Val Loss:0.6691226959228516\n",
      "Epoch 0[707/4499] Val Loss:0.5570844411849976\n",
      "Epoch 0[708/4499] Val Loss:0.7190432548522949\n",
      "Epoch 0[709/4499] Val Loss:0.744662344455719\n",
      "Epoch 0[710/4499] Val Loss:0.5856351852416992\n",
      "Epoch 0[711/4499] Val Loss:0.567730724811554\n",
      "Epoch 0[712/4499] Val Loss:0.4814881384372711\n",
      "Epoch 0[713/4499] Val Loss:0.8468765616416931\n",
      "Epoch 0[714/4499] Val Loss:0.5447849035263062\n",
      "Epoch 0[715/4499] Val Loss:0.527020275592804\n",
      "Epoch 0[716/4499] Val Loss:0.5994637608528137\n",
      "Epoch 0[717/4499] Val Loss:0.6493310928344727\n",
      "Epoch 0[718/4499] Val Loss:0.531333327293396\n",
      "Epoch 0[719/4499] Val Loss:0.5647386312484741\n",
      "Epoch 0[720/4499] Val Loss:0.7698403000831604\n",
      "Epoch 0[721/4499] Val Loss:0.7087090015411377\n",
      "Epoch 0[722/4499] Val Loss:0.557789146900177\n",
      "Epoch 0[723/4499] Val Loss:0.7525894045829773\n",
      "Epoch 0[724/4499] Val Loss:0.6517268419265747\n",
      "Epoch 0[725/4499] Val Loss:0.552954912185669\n",
      "Epoch 0[726/4499] Val Loss:0.7209171056747437\n",
      "Epoch 0[727/4499] Val Loss:0.7383505702018738\n",
      "Epoch 0[728/4499] Val Loss:1.208204984664917\n",
      "Epoch 0[729/4499] Val Loss:1.3907233476638794\n",
      "Epoch 0[730/4499] Val Loss:1.233627200126648\n",
      "Epoch 0[731/4499] Val Loss:1.19765305519104\n",
      "Epoch 0[732/4499] Val Loss:1.0092719793319702\n",
      "Epoch 0[733/4499] Val Loss:0.6761386394500732\n",
      "Epoch 0[734/4499] Val Loss:0.6841321587562561\n",
      "Epoch 0[735/4499] Val Loss:0.4779027998447418\n",
      "Epoch 0[736/4499] Val Loss:0.4088963270187378\n",
      "Epoch 0[737/4499] Val Loss:0.5333701968193054\n",
      "Epoch 0[738/4499] Val Loss:0.4852485954761505\n",
      "Epoch 0[739/4499] Val Loss:0.37995290756225586\n",
      "Epoch 0[740/4499] Val Loss:0.4436744749546051\n",
      "Epoch 0[741/4499] Val Loss:0.432552307844162\n",
      "Epoch 0[742/4499] Val Loss:0.5210708975791931\n",
      "Epoch 0[743/4499] Val Loss:0.45991256833076477\n",
      "Epoch 0[744/4499] Val Loss:0.2849287986755371\n",
      "Epoch 0[745/4499] Val Loss:0.6711819767951965\n",
      "Epoch 0[746/4499] Val Loss:0.6157879829406738\n",
      "Epoch 0[747/4499] Val Loss:0.5365458726882935\n",
      "Epoch 0[748/4499] Val Loss:0.3165464699268341\n",
      "Epoch 0[749/4499] Val Loss:0.39713501930236816\n",
      "Epoch 0[750/4499] Val Loss:0.23516421020030975\n",
      "Epoch 0[751/4499] Val Loss:0.3152259290218353\n",
      "Epoch 0[752/4499] Val Loss:0.37018823623657227\n",
      "Epoch 0[753/4499] Val Loss:0.24217644333839417\n",
      "Epoch 0[754/4499] Val Loss:0.27788811922073364\n",
      "Epoch 0[755/4499] Val Loss:0.24517270922660828\n",
      "Epoch 0[756/4499] Val Loss:0.31258460879325867\n",
      "Epoch 0[757/4499] Val Loss:0.3576408326625824\n",
      "Epoch 0[758/4499] Val Loss:0.22789476811885834\n",
      "Epoch 0[759/4499] Val Loss:0.24820572137832642\n",
      "Epoch 0[760/4499] Val Loss:0.37505218386650085\n",
      "Epoch 0[761/4499] Val Loss:0.2745784521102905\n",
      "Epoch 0[762/4499] Val Loss:0.3262377977371216\n",
      "Epoch 0[763/4499] Val Loss:0.2871415615081787\n",
      "Epoch 0[764/4499] Val Loss:0.25705230236053467\n",
      "Epoch 0[765/4499] Val Loss:0.3003811240196228\n",
      "Epoch 0[766/4499] Val Loss:0.22208663821220398\n",
      "Epoch 0[767/4499] Val Loss:0.21097402274608612\n",
      "Epoch 0[768/4499] Val Loss:0.29391828179359436\n",
      "Epoch 0[769/4499] Val Loss:0.3286803960800171\n",
      "Epoch 0[770/4499] Val Loss:0.4214005172252655\n",
      "Epoch 0[771/4499] Val Loss:0.3399498462677002\n",
      "Epoch 0[772/4499] Val Loss:0.3982243239879608\n",
      "Epoch 0[773/4499] Val Loss:0.4802752733230591\n",
      "Epoch 0[774/4499] Val Loss:0.5418949723243713\n",
      "Epoch 0[775/4499] Val Loss:0.5037182569503784\n",
      "Epoch 0[776/4499] Val Loss:0.6123595237731934\n",
      "Epoch 0[777/4499] Val Loss:0.6893153190612793\n",
      "Epoch 0[778/4499] Val Loss:0.6639547348022461\n",
      "Epoch 0[779/4499] Val Loss:0.4292193055152893\n",
      "Epoch 0[780/4499] Val Loss:0.6008830666542053\n",
      "Epoch 0[781/4499] Val Loss:0.705817699432373\n",
      "Epoch 0[782/4499] Val Loss:0.9043049216270447\n",
      "Epoch 0[783/4499] Val Loss:0.781596839427948\n",
      "Epoch 0[784/4499] Val Loss:0.6059357523918152\n",
      "Epoch 0[785/4499] Val Loss:0.9628917574882507\n",
      "Epoch 0[786/4499] Val Loss:0.8519459962844849\n",
      "Epoch 0[787/4499] Val Loss:1.0081814527511597\n",
      "Epoch 0[788/4499] Val Loss:0.7973165512084961\n",
      "Epoch 0[789/4499] Val Loss:0.8137137293815613\n",
      "Epoch 0[790/4499] Val Loss:0.6850845217704773\n",
      "Epoch 0[791/4499] Val Loss:0.7281025052070618\n",
      "Epoch 0[792/4499] Val Loss:0.6897525787353516\n",
      "Epoch 0[793/4499] Val Loss:0.6312066316604614\n",
      "Epoch 0[794/4499] Val Loss:0.6116616129875183\n",
      "Epoch 0[795/4499] Val Loss:0.7046852111816406\n",
      "Epoch 0[796/4499] Val Loss:0.577262818813324\n",
      "Epoch 0[797/4499] Val Loss:0.6290932893753052\n",
      "Epoch 0[798/4499] Val Loss:0.584820568561554\n",
      "Epoch 0[799/4499] Val Loss:0.5791473984718323\n",
      "Epoch 0[800/4499] Val Loss:0.620782196521759\n",
      "Epoch 0[801/4499] Val Loss:0.43552345037460327\n",
      "Epoch 0[802/4499] Val Loss:0.6278866529464722\n",
      "Epoch 0[803/4499] Val Loss:0.5801254510879517\n",
      "Epoch 0[804/4499] Val Loss:0.8497987389564514\n",
      "Epoch 0[805/4499] Val Loss:0.7901474833488464\n",
      "Epoch 0[806/4499] Val Loss:0.46042707562446594\n",
      "Epoch 0[807/4499] Val Loss:0.5411272644996643\n",
      "Epoch 0[808/4499] Val Loss:0.7914096713066101\n",
      "Epoch 0[809/4499] Val Loss:0.6768622398376465\n",
      "Epoch 0[810/4499] Val Loss:0.6695550680160522\n",
      "Epoch 0[811/4499] Val Loss:0.6805205345153809\n",
      "Epoch 0[812/4499] Val Loss:0.6197200417518616\n",
      "Epoch 0[813/4499] Val Loss:0.7312136888504028\n",
      "Epoch 0[814/4499] Val Loss:0.959138810634613\n",
      "Epoch 0[815/4499] Val Loss:1.1553438901901245\n",
      "Epoch 0[816/4499] Val Loss:1.1988389492034912\n",
      "Epoch 0[817/4499] Val Loss:0.8766396641731262\n",
      "Epoch 0[818/4499] Val Loss:0.4552150368690491\n",
      "Epoch 0[819/4499] Val Loss:0.4325349032878876\n",
      "Epoch 0[820/4499] Val Loss:0.38162463903427124\n",
      "Epoch 0[821/4499] Val Loss:0.2843789756298065\n",
      "Epoch 0[822/4499] Val Loss:0.2892949879169464\n",
      "Epoch 0[823/4499] Val Loss:0.2959022521972656\n",
      "Epoch 0[824/4499] Val Loss:0.39494526386260986\n",
      "Epoch 0[825/4499] Val Loss:0.39051079750061035\n",
      "Epoch 0[826/4499] Val Loss:0.3368988335132599\n",
      "Epoch 0[827/4499] Val Loss:0.4715707004070282\n",
      "Epoch 0[828/4499] Val Loss:0.4804231524467468\n",
      "Epoch 0[829/4499] Val Loss:0.5795348286628723\n",
      "Epoch 0[830/4499] Val Loss:0.507965624332428\n",
      "Epoch 0[831/4499] Val Loss:0.5733372569084167\n",
      "Epoch 0[832/4499] Val Loss:0.5028102993965149\n",
      "Epoch 0[833/4499] Val Loss:0.344791442155838\n",
      "Epoch 0[834/4499] Val Loss:0.5399074554443359\n",
      "Epoch 0[835/4499] Val Loss:0.5050503015518188\n",
      "Epoch 0[836/4499] Val Loss:0.5596359372138977\n",
      "Epoch 0[837/4499] Val Loss:0.4711287319660187\n",
      "Epoch 0[838/4499] Val Loss:0.2713848054409027\n",
      "Epoch 0[839/4499] Val Loss:0.2650778591632843\n",
      "Epoch 0[840/4499] Val Loss:0.2586306929588318\n",
      "Epoch 0[841/4499] Val Loss:0.24430622160434723\n",
      "Epoch 0[842/4499] Val Loss:0.2596849799156189\n",
      "Epoch 0[843/4499] Val Loss:0.371467649936676\n",
      "Epoch 0[844/4499] Val Loss:0.3823355436325073\n",
      "Epoch 0[845/4499] Val Loss:0.25861114263534546\n",
      "Epoch 0[846/4499] Val Loss:0.1769891381263733\n",
      "Epoch 0[847/4499] Val Loss:0.2731783986091614\n",
      "Epoch 0[848/4499] Val Loss:0.3287568986415863\n",
      "Epoch 0[849/4499] Val Loss:0.34825366735458374\n",
      "Epoch 0[850/4499] Val Loss:0.26790866255760193\n",
      "Epoch 0[851/4499] Val Loss:0.41098907589912415\n",
      "Epoch 0[852/4499] Val Loss:0.3573794662952423\n",
      "Epoch 0[853/4499] Val Loss:0.2892504334449768\n",
      "Epoch 0[854/4499] Val Loss:0.29107218980789185\n",
      "Epoch 0[855/4499] Val Loss:0.14374539256095886\n",
      "Epoch 0[856/4499] Val Loss:0.16325266659259796\n",
      "Epoch 0[857/4499] Val Loss:0.26036977767944336\n",
      "Epoch 0[858/4499] Val Loss:0.28473007678985596\n",
      "Epoch 0[859/4499] Val Loss:0.34680578112602234\n",
      "Epoch 0[860/4499] Val Loss:0.35323449969291687\n",
      "Epoch 0[861/4499] Val Loss:0.3845600187778473\n",
      "Epoch 0[862/4499] Val Loss:0.5199066400527954\n",
      "Epoch 0[863/4499] Val Loss:0.4484606981277466\n",
      "Epoch 0[864/4499] Val Loss:0.529580295085907\n",
      "Epoch 0[865/4499] Val Loss:0.5151402950286865\n",
      "Epoch 0[866/4499] Val Loss:0.4821280241012573\n",
      "Epoch 0[867/4499] Val Loss:0.43611621856689453\n",
      "Epoch 0[868/4499] Val Loss:0.4380847215652466\n",
      "Epoch 0[869/4499] Val Loss:0.4978262186050415\n",
      "Epoch 0[870/4499] Val Loss:0.721947968006134\n",
      "Epoch 0[871/4499] Val Loss:0.7591306567192078\n",
      "Epoch 0[872/4499] Val Loss:0.46555396914482117\n",
      "Epoch 0[873/4499] Val Loss:0.6212692856788635\n",
      "Epoch 0[874/4499] Val Loss:0.8144528269767761\n",
      "Epoch 0[875/4499] Val Loss:0.646957516670227\n",
      "Epoch 0[876/4499] Val Loss:1.1420823335647583\n",
      "Epoch 0[877/4499] Val Loss:0.7178834080696106\n",
      "Epoch 0[878/4499] Val Loss:1.0087814331054688\n",
      "Epoch 0[879/4499] Val Loss:0.7381260991096497\n",
      "Epoch 0[880/4499] Val Loss:0.47408267855644226\n",
      "Epoch 0[881/4499] Val Loss:0.5962543487548828\n",
      "Epoch 0[882/4499] Val Loss:0.6782656908035278\n",
      "Epoch 0[883/4499] Val Loss:0.6072317957878113\n",
      "Epoch 0[884/4499] Val Loss:0.667673647403717\n",
      "Epoch 0[885/4499] Val Loss:0.5631468892097473\n",
      "Epoch 0[886/4499] Val Loss:0.6419519186019897\n",
      "Epoch 0[887/4499] Val Loss:0.8605336546897888\n",
      "Epoch 0[888/4499] Val Loss:0.5106171369552612\n",
      "Epoch 0[889/4499] Val Loss:0.7389663457870483\n",
      "Epoch 0[890/4499] Val Loss:0.8897532820701599\n",
      "Epoch 0[891/4499] Val Loss:1.0019830465316772\n",
      "Epoch 0[892/4499] Val Loss:0.9228819012641907\n",
      "Epoch 0[893/4499] Val Loss:0.6316583156585693\n",
      "Epoch 0[894/4499] Val Loss:0.6497546434402466\n",
      "Epoch 0[895/4499] Val Loss:0.6536054015159607\n",
      "Epoch 0[896/4499] Val Loss:0.6813022494316101\n",
      "Epoch 0[897/4499] Val Loss:0.9870312213897705\n",
      "Epoch 0[898/4499] Val Loss:0.6724953651428223\n",
      "Epoch 0[899/4499] Val Loss:0.8712835907936096\n",
      "Epoch 0[900/4499] Val Loss:0.8906942009925842\n",
      "Epoch 0[901/4499] Val Loss:1.0523134469985962\n",
      "Epoch 0[902/4499] Val Loss:1.4291692972183228\n",
      "Epoch 0[903/4499] Val Loss:1.5141521692276\n",
      "Epoch 0[904/4499] Val Loss:0.8709978461265564\n",
      "Epoch 0[905/4499] Val Loss:0.44784945249557495\n",
      "Epoch 0[906/4499] Val Loss:0.4888961911201477\n",
      "Epoch 0[907/4499] Val Loss:0.3306565582752228\n",
      "Epoch 0[908/4499] Val Loss:0.35360926389694214\n",
      "Epoch 0[909/4499] Val Loss:0.3777162432670593\n",
      "Epoch 0[910/4499] Val Loss:0.25178003311157227\n",
      "Epoch 0[911/4499] Val Loss:0.34439778327941895\n",
      "Epoch 0[912/4499] Val Loss:0.24757640063762665\n",
      "Epoch 0[913/4499] Val Loss:0.34711533784866333\n",
      "Epoch 0[914/4499] Val Loss:0.5378031730651855\n",
      "Epoch 0[915/4499] Val Loss:0.49668067693710327\n",
      "Epoch 0[916/4499] Val Loss:0.5025895833969116\n",
      "Epoch 0[917/4499] Val Loss:0.4797036647796631\n",
      "Epoch 0[918/4499] Val Loss:0.5344236493110657\n",
      "Epoch 0[919/4499] Val Loss:0.5717469453811646\n",
      "Epoch 0[920/4499] Val Loss:0.5669923424720764\n",
      "Epoch 0[921/4499] Val Loss:0.4275267720222473\n",
      "Epoch 0[922/4499] Val Loss:0.658104658126831\n",
      "Epoch 0[923/4499] Val Loss:0.6172758340835571\n",
      "Epoch 0[924/4499] Val Loss:0.4592633843421936\n",
      "Epoch 0[925/4499] Val Loss:0.42351558804512024\n",
      "Epoch 0[926/4499] Val Loss:0.5311082005500793\n",
      "Epoch 0[927/4499] Val Loss:0.678382396697998\n",
      "Epoch 0[928/4499] Val Loss:0.4865056574344635\n",
      "Epoch 0[929/4499] Val Loss:0.5129435658454895\n",
      "Epoch 0[930/4499] Val Loss:0.5601806044578552\n",
      "Epoch 0[931/4499] Val Loss:0.4965408444404602\n",
      "Epoch 0[932/4499] Val Loss:0.3840259313583374\n",
      "Epoch 0[933/4499] Val Loss:0.33620917797088623\n",
      "Epoch 0[934/4499] Val Loss:0.40018585324287415\n",
      "Epoch 0[935/4499] Val Loss:0.42027169466018677\n",
      "Epoch 0[936/4499] Val Loss:0.48456960916519165\n",
      "Epoch 0[937/4499] Val Loss:0.5960621237754822\n",
      "Epoch 0[938/4499] Val Loss:0.7196452021598816\n",
      "Epoch 0[939/4499] Val Loss:0.5999395251274109\n",
      "Epoch 0[940/4499] Val Loss:0.5966566205024719\n",
      "Epoch 0[941/4499] Val Loss:0.7172146439552307\n",
      "Epoch 0[942/4499] Val Loss:0.6352142095565796\n",
      "Epoch 0[943/4499] Val Loss:0.6056578755378723\n",
      "Epoch 0[944/4499] Val Loss:0.4625634253025055\n",
      "Epoch 0[945/4499] Val Loss:0.6639468669891357\n",
      "Epoch 0[946/4499] Val Loss:0.7193252444267273\n",
      "Epoch 0[947/4499] Val Loss:0.8203911781311035\n",
      "Epoch 0[948/4499] Val Loss:0.6008217930793762\n",
      "Epoch 0[949/4499] Val Loss:0.6475928425788879\n",
      "Epoch 0[950/4499] Val Loss:0.7447034120559692\n",
      "Epoch 0[951/4499] Val Loss:0.6396589875221252\n",
      "Epoch 0[952/4499] Val Loss:0.8551977276802063\n",
      "Epoch 0[953/4499] Val Loss:0.6472752094268799\n",
      "Epoch 0[954/4499] Val Loss:0.7568265199661255\n",
      "Epoch 0[955/4499] Val Loss:0.877216637134552\n",
      "Epoch 0[956/4499] Val Loss:0.976034939289093\n",
      "Epoch 0[957/4499] Val Loss:1.064725637435913\n",
      "Epoch 0[958/4499] Val Loss:0.9697099328041077\n",
      "Epoch 0[959/4499] Val Loss:1.117323875427246\n",
      "Epoch 0[960/4499] Val Loss:0.9412084221839905\n",
      "Epoch 0[961/4499] Val Loss:1.1368701457977295\n",
      "Epoch 0[962/4499] Val Loss:1.1388314962387085\n",
      "Epoch 0[963/4499] Val Loss:0.8493480086326599\n",
      "Epoch 0[964/4499] Val Loss:0.6912842392921448\n",
      "Epoch 0[965/4499] Val Loss:0.8135597705841064\n",
      "Epoch 0[966/4499] Val Loss:0.8300894498825073\n",
      "Epoch 0[967/4499] Val Loss:0.7214095592498779\n",
      "Epoch 0[968/4499] Val Loss:0.6974802613258362\n",
      "Epoch 0[969/4499] Val Loss:0.7407344579696655\n",
      "Epoch 0[970/4499] Val Loss:0.8220068216323853\n",
      "Epoch 0[971/4499] Val Loss:0.7944860458374023\n",
      "Epoch 0[972/4499] Val Loss:0.7804661989212036\n",
      "Epoch 0[973/4499] Val Loss:0.9323943257331848\n",
      "Epoch 0[974/4499] Val Loss:0.7950247526168823\n",
      "Epoch 0[975/4499] Val Loss:0.8195323944091797\n",
      "Epoch 0[976/4499] Val Loss:0.7837854623794556\n",
      "Epoch 0[977/4499] Val Loss:0.6757010817527771\n",
      "Epoch 0[978/4499] Val Loss:0.7451491355895996\n",
      "Epoch 0[979/4499] Val Loss:0.8189716339111328\n",
      "Epoch 0[980/4499] Val Loss:0.7001464366912842\n",
      "Epoch 0[981/4499] Val Loss:0.7079480886459351\n",
      "Epoch 0[982/4499] Val Loss:0.6947911381721497\n",
      "Epoch 0[983/4499] Val Loss:0.6272090673446655\n",
      "Epoch 0[984/4499] Val Loss:0.7396957874298096\n",
      "Epoch 0[985/4499] Val Loss:0.7698432803153992\n",
      "Epoch 0[986/4499] Val Loss:0.742983341217041\n",
      "Epoch 0[987/4499] Val Loss:0.7456060647964478\n",
      "Epoch 0[988/4499] Val Loss:0.853500247001648\n",
      "Epoch 0[989/4499] Val Loss:0.8346077799797058\n",
      "Epoch 0[990/4499] Val Loss:0.9764678478240967\n",
      "Epoch 0[991/4499] Val Loss:0.9525502920150757\n",
      "Epoch 0[992/4499] Val Loss:0.9073046445846558\n",
      "Epoch 0[993/4499] Val Loss:0.9886406064033508\n",
      "Epoch 0[994/4499] Val Loss:0.8411821722984314\n",
      "Epoch 0[995/4499] Val Loss:0.6780999898910522\n",
      "Epoch 0[996/4499] Val Loss:0.6733641028404236\n",
      "Epoch 0[997/4499] Val Loss:0.5824581980705261\n",
      "Epoch 0[998/4499] Val Loss:0.5898681879043579\n",
      "Epoch 0[999/4499] Val Loss:0.6525498628616333\n",
      "Epoch 0[1000/4499] Val Loss:0.6478520035743713\n",
      "Epoch 0[1001/4499] Val Loss:0.5512190461158752\n",
      "Epoch 0[1002/4499] Val Loss:0.4890262484550476\n",
      "Epoch 0[1003/4499] Val Loss:0.4785798192024231\n",
      "Epoch 0[1004/4499] Val Loss:0.4023685157299042\n",
      "Epoch 0[1005/4499] Val Loss:0.6963335275650024\n",
      "Epoch 0[1006/4499] Val Loss:0.5089832544326782\n",
      "Epoch 0[1007/4499] Val Loss:0.49354976415634155\n",
      "Epoch 0[1008/4499] Val Loss:0.6212104558944702\n",
      "Epoch 0[1009/4499] Val Loss:0.5107413530349731\n",
      "Epoch 0[1010/4499] Val Loss:0.44169044494628906\n",
      "Epoch 0[1011/4499] Val Loss:0.35198450088500977\n",
      "Epoch 0[1012/4499] Val Loss:0.39063775539398193\n",
      "Epoch 0[1013/4499] Val Loss:0.4412786364555359\n",
      "Epoch 0[1014/4499] Val Loss:0.35156041383743286\n",
      "Epoch 0[1015/4499] Val Loss:0.3225550651550293\n",
      "Epoch 0[1016/4499] Val Loss:0.2819257080554962\n",
      "Epoch 0[1017/4499] Val Loss:0.334340363740921\n",
      "Epoch 0[1018/4499] Val Loss:0.4419046640396118\n",
      "Epoch 0[1019/4499] Val Loss:0.5766775012016296\n",
      "Epoch 0[1020/4499] Val Loss:0.563810408115387\n",
      "Epoch 0[1021/4499] Val Loss:0.5223573446273804\n",
      "Epoch 0[1022/4499] Val Loss:0.4195144772529602\n",
      "Epoch 0[1023/4499] Val Loss:0.7176843881607056\n",
      "Epoch 0[1024/4499] Val Loss:0.6853663921356201\n",
      "Epoch 0[1025/4499] Val Loss:0.7553877830505371\n",
      "Epoch 0[1026/4499] Val Loss:0.6224530339241028\n",
      "Epoch 0[1027/4499] Val Loss:0.8466967940330505\n",
      "Epoch 0[1028/4499] Val Loss:0.876078188419342\n",
      "Epoch 0[1029/4499] Val Loss:0.7473048567771912\n",
      "Epoch 0[1030/4499] Val Loss:0.6102660894393921\n",
      "Epoch 0[1031/4499] Val Loss:0.8007338047027588\n",
      "Epoch 0[1032/4499] Val Loss:0.5889054536819458\n",
      "Epoch 0[1033/4499] Val Loss:0.7784854769706726\n",
      "Epoch 0[1034/4499] Val Loss:0.7703481912612915\n",
      "Epoch 0[1035/4499] Val Loss:0.7949147820472717\n",
      "Epoch 0[1036/4499] Val Loss:0.7719821333885193\n",
      "Epoch 0[1037/4499] Val Loss:1.0085219144821167\n",
      "Epoch 0[1038/4499] Val Loss:1.0195674896240234\n",
      "Epoch 0[1039/4499] Val Loss:1.0726308822631836\n",
      "Epoch 0[1040/4499] Val Loss:0.9796689748764038\n",
      "Epoch 0[1041/4499] Val Loss:0.6174139380455017\n",
      "Epoch 0[1042/4499] Val Loss:0.7042916417121887\n",
      "Epoch 0[1043/4499] Val Loss:0.804179847240448\n",
      "Epoch 0[1044/4499] Val Loss:0.719113290309906\n",
      "Epoch 0[1045/4499] Val Loss:0.7996668219566345\n",
      "Epoch 0[1046/4499] Val Loss:0.6759305000305176\n",
      "Epoch 0[1047/4499] Val Loss:0.4587417244911194\n",
      "Epoch 0[1048/4499] Val Loss:0.7514116764068604\n",
      "Epoch 0[1049/4499] Val Loss:0.8432892560958862\n",
      "Epoch 0[1050/4499] Val Loss:0.8959141969680786\n",
      "Epoch 0[1051/4499] Val Loss:0.746126651763916\n",
      "Epoch 0[1052/4499] Val Loss:0.8115100860595703\n",
      "Epoch 0[1053/4499] Val Loss:0.7043325901031494\n",
      "Epoch 0[1054/4499] Val Loss:0.677200973033905\n",
      "Epoch 0[1055/4499] Val Loss:0.7288119792938232\n",
      "Epoch 0[1056/4499] Val Loss:0.770786702632904\n",
      "Epoch 0[1057/4499] Val Loss:0.78959059715271\n",
      "Epoch 0[1058/4499] Val Loss:0.7428169846534729\n",
      "Epoch 0[1059/4499] Val Loss:0.8131780028343201\n",
      "Epoch 0[1060/4499] Val Loss:0.8531249165534973\n",
      "Epoch 0[1061/4499] Val Loss:1.1868209838867188\n",
      "Epoch 0[1062/4499] Val Loss:1.3665646314620972\n",
      "Epoch 0[1063/4499] Val Loss:1.4879626035690308\n",
      "Epoch 0[1064/4499] Val Loss:0.9820248484611511\n",
      "Epoch 0[1065/4499] Val Loss:1.0004713535308838\n",
      "Epoch 0[1066/4499] Val Loss:0.6949918270111084\n",
      "Epoch 0[1067/4499] Val Loss:0.6954871416091919\n",
      "Epoch 0[1068/4499] Val Loss:0.6516764163970947\n",
      "Epoch 0[1069/4499] Val Loss:0.6784824728965759\n",
      "Epoch 0[1070/4499] Val Loss:0.7105253338813782\n",
      "Epoch 0[1071/4499] Val Loss:0.6598327159881592\n",
      "Epoch 0[1072/4499] Val Loss:0.6236688494682312\n",
      "Epoch 0[1073/4499] Val Loss:0.8950138688087463\n",
      "Epoch 0[1074/4499] Val Loss:0.7056622505187988\n",
      "Epoch 0[1075/4499] Val Loss:0.4111477732658386\n",
      "Epoch 0[1076/4499] Val Loss:0.6559743881225586\n",
      "Epoch 0[1077/4499] Val Loss:0.4864175617694855\n",
      "Epoch 0[1078/4499] Val Loss:0.6324415802955627\n",
      "Epoch 0[1079/4499] Val Loss:0.5604209303855896\n",
      "Epoch 0[1080/4499] Val Loss:0.6188690066337585\n",
      "Epoch 0[1081/4499] Val Loss:0.4671326279640198\n",
      "Epoch 0[1082/4499] Val Loss:0.41623160243034363\n",
      "Epoch 0[1083/4499] Val Loss:0.3376733660697937\n",
      "Epoch 0[1084/4499] Val Loss:0.4417635202407837\n",
      "Epoch 0[1085/4499] Val Loss:0.3149980306625366\n",
      "Epoch 0[1086/4499] Val Loss:0.3530195951461792\n",
      "Epoch 0[1087/4499] Val Loss:0.31993111968040466\n",
      "Epoch 0[1088/4499] Val Loss:0.3583363890647888\n",
      "Epoch 0[1089/4499] Val Loss:0.34593912959098816\n",
      "Epoch 0[1090/4499] Val Loss:0.3149566650390625\n",
      "Epoch 0[1091/4499] Val Loss:0.4465999901294708\n",
      "Epoch 0[1092/4499] Val Loss:0.4563409686088562\n",
      "Epoch 0[1093/4499] Val Loss:0.527019739151001\n",
      "Epoch 0[1094/4499] Val Loss:0.43310803174972534\n",
      "Epoch 0[1095/4499] Val Loss:0.2570318877696991\n",
      "Epoch 0[1096/4499] Val Loss:0.25975003838539124\n",
      "Epoch 0[1097/4499] Val Loss:0.18501684069633484\n",
      "Epoch 0[1098/4499] Val Loss:0.1946977823972702\n",
      "Epoch 0[1099/4499] Val Loss:0.483016699552536\n",
      "Epoch 0[1100/4499] Val Loss:0.7372293472290039\n",
      "Epoch 0[1101/4499] Val Loss:0.7843821048736572\n",
      "Epoch 0[1102/4499] Val Loss:0.8641355037689209\n",
      "Epoch 0[1103/4499] Val Loss:0.7642776370048523\n",
      "Epoch 0[1104/4499] Val Loss:0.7012004852294922\n",
      "Epoch 0[1105/4499] Val Loss:0.7713316082954407\n",
      "Epoch 0[1106/4499] Val Loss:0.5481286644935608\n",
      "Epoch 0[1107/4499] Val Loss:0.7624015808105469\n",
      "Epoch 0[1108/4499] Val Loss:0.9170165061950684\n",
      "Epoch 0[1109/4499] Val Loss:0.8584669828414917\n",
      "Epoch 0[1110/4499] Val Loss:0.6498846411705017\n",
      "Epoch 0[1111/4499] Val Loss:0.562356173992157\n",
      "Epoch 0[1112/4499] Val Loss:0.7756326198577881\n",
      "Epoch 0[1113/4499] Val Loss:0.9068409204483032\n",
      "Epoch 0[1114/4499] Val Loss:1.1343896389007568\n",
      "Epoch 0[1115/4499] Val Loss:1.18939208984375\n",
      "Epoch 0[1116/4499] Val Loss:0.7200308442115784\n",
      "Epoch 0[1117/4499] Val Loss:0.9605859518051147\n",
      "Epoch 0[1118/4499] Val Loss:0.8341864943504333\n",
      "Epoch 0[1119/4499] Val Loss:0.9117386341094971\n",
      "Epoch 0[1120/4499] Val Loss:1.1687713861465454\n",
      "Epoch 0[1121/4499] Val Loss:1.29194974899292\n",
      "Epoch 0[1122/4499] Val Loss:0.9911180734634399\n",
      "Epoch 0[1123/4499] Val Loss:1.0477399826049805\n",
      "Epoch 0[1124/4499] Val Loss:1.1776822805404663\n",
      "Epoch 0[1125/4499] Val Loss:1.2049689292907715\n",
      "Epoch 0[1126/4499] Val Loss:1.148041009902954\n",
      "Epoch 0[1127/4499] Val Loss:1.1587494611740112\n",
      "Epoch 0[1128/4499] Val Loss:1.2158204317092896\n",
      "Epoch 0[1129/4499] Val Loss:0.9280192255973816\n",
      "Epoch 0[1130/4499] Val Loss:0.873619556427002\n",
      "Epoch 0[1131/4499] Val Loss:0.9753092527389526\n",
      "Epoch 0[1132/4499] Val Loss:0.9689975380897522\n",
      "Epoch 0[1133/4499] Val Loss:1.1112053394317627\n",
      "Epoch 0[1134/4499] Val Loss:1.1915485858917236\n",
      "Epoch 0[1135/4499] Val Loss:1.1723014116287231\n",
      "Epoch 0[1136/4499] Val Loss:1.526187777519226\n",
      "Epoch 0[1137/4499] Val Loss:1.6132575273513794\n",
      "Epoch 0[1138/4499] Val Loss:1.4714478254318237\n",
      "Epoch 0[1139/4499] Val Loss:1.067368984222412\n",
      "Epoch 0[1140/4499] Val Loss:1.106961727142334\n",
      "Epoch 0[1141/4499] Val Loss:0.6764278411865234\n",
      "Epoch 0[1142/4499] Val Loss:0.5279499888420105\n",
      "Epoch 0[1143/4499] Val Loss:0.6062330603599548\n",
      "Epoch 0[1144/4499] Val Loss:0.6552380919456482\n",
      "Epoch 0[1145/4499] Val Loss:0.9127247333526611\n",
      "Epoch 0[1146/4499] Val Loss:0.7507493495941162\n",
      "Epoch 0[1147/4499] Val Loss:0.8220553398132324\n",
      "Epoch 0[1148/4499] Val Loss:0.7436413764953613\n",
      "Epoch 0[1149/4499] Val Loss:0.6976416110992432\n",
      "Epoch 0[1150/4499] Val Loss:0.6255481243133545\n",
      "Epoch 0[1151/4499] Val Loss:0.43823671340942383\n",
      "Epoch 0[1152/4499] Val Loss:0.2536041736602783\n",
      "Epoch 0[1153/4499] Val Loss:0.2767978608608246\n",
      "Epoch 0[1154/4499] Val Loss:0.18559101223945618\n",
      "Epoch 0[1155/4499] Val Loss:0.3475363254547119\n",
      "Epoch 0[1156/4499] Val Loss:0.2731274664402008\n",
      "Epoch 0[1157/4499] Val Loss:0.4655715227127075\n",
      "Epoch 0[1158/4499] Val Loss:0.49840620160102844\n",
      "Epoch 0[1159/4499] Val Loss:0.5141929984092712\n",
      "Epoch 0[1160/4499] Val Loss:0.446403443813324\n",
      "Epoch 0[1161/4499] Val Loss:0.4861465096473694\n",
      "Epoch 0[1162/4499] Val Loss:0.5178343057632446\n",
      "Epoch 0[1163/4499] Val Loss:0.7000736594200134\n",
      "Epoch 0[1164/4499] Val Loss:0.8124959468841553\n",
      "Epoch 0[1165/4499] Val Loss:0.5931487679481506\n",
      "Epoch 0[1166/4499] Val Loss:0.6774150133132935\n",
      "Epoch 0[1167/4499] Val Loss:0.8535938262939453\n",
      "Epoch 0[1168/4499] Val Loss:0.8061537742614746\n",
      "Epoch 0[1169/4499] Val Loss:0.618709921836853\n",
      "Epoch 0[1170/4499] Val Loss:0.7528184652328491\n",
      "Epoch 0[1171/4499] Val Loss:0.5734542608261108\n",
      "Epoch 0[1172/4499] Val Loss:0.8254754543304443\n",
      "Epoch 0[1173/4499] Val Loss:0.79160076379776\n",
      "Epoch 0[1174/4499] Val Loss:0.6958224177360535\n",
      "Epoch 0[1175/4499] Val Loss:0.8665317296981812\n",
      "Epoch 0[1176/4499] Val Loss:0.7500666379928589\n",
      "Epoch 0[1177/4499] Val Loss:0.8102095127105713\n",
      "Epoch 0[1178/4499] Val Loss:0.6519818902015686\n",
      "Epoch 0[1179/4499] Val Loss:0.8405263423919678\n",
      "Epoch 0[1180/4499] Val Loss:0.7091946601867676\n",
      "Epoch 0[1181/4499] Val Loss:1.511544108390808\n",
      "Epoch 0[1182/4499] Val Loss:0.5810495018959045\n",
      "Epoch 0[1183/4499] Val Loss:0.6704393029212952\n",
      "Epoch 0[1184/4499] Val Loss:0.6261820197105408\n",
      "Epoch 0[1185/4499] Val Loss:0.522463321685791\n",
      "Epoch 0[1186/4499] Val Loss:0.5932047367095947\n",
      "Epoch 0[1187/4499] Val Loss:0.4628620445728302\n",
      "Epoch 0[1188/4499] Val Loss:0.5248798727989197\n",
      "Epoch 0[1189/4499] Val Loss:0.4494030773639679\n",
      "Epoch 0[1190/4499] Val Loss:0.4940216839313507\n",
      "Epoch 0[1191/4499] Val Loss:0.5799896121025085\n",
      "Epoch 0[1192/4499] Val Loss:0.6041727662086487\n",
      "Epoch 0[1193/4499] Val Loss:0.5277668237686157\n",
      "Epoch 0[1194/4499] Val Loss:0.715665340423584\n",
      "Epoch 0[1195/4499] Val Loss:0.7317057847976685\n",
      "Epoch 0[1196/4499] Val Loss:0.6959936022758484\n",
      "Epoch 0[1197/4499] Val Loss:0.7114557027816772\n",
      "Epoch 0[1198/4499] Val Loss:0.6709664463996887\n",
      "Epoch 0[1199/4499] Val Loss:0.6415427327156067\n",
      "Epoch 0[1200/4499] Val Loss:0.6598007678985596\n",
      "Epoch 0[1201/4499] Val Loss:0.6648598313331604\n",
      "Epoch 0[1202/4499] Val Loss:0.6481303572654724\n",
      "Epoch 0[1203/4499] Val Loss:0.640647292137146\n",
      "Epoch 0[1204/4499] Val Loss:0.6898828744888306\n",
      "Epoch 0[1205/4499] Val Loss:0.8400033712387085\n",
      "Epoch 0[1206/4499] Val Loss:0.7986645698547363\n",
      "Epoch 0[1207/4499] Val Loss:0.9721176028251648\n",
      "Epoch 0[1208/4499] Val Loss:0.7329604029655457\n",
      "Epoch 0[1209/4499] Val Loss:0.71975177526474\n",
      "Epoch 0[1210/4499] Val Loss:0.6195930242538452\n",
      "Epoch 0[1211/4499] Val Loss:0.6003391146659851\n",
      "Epoch 0[1212/4499] Val Loss:0.6367124915122986\n",
      "Epoch 0[1213/4499] Val Loss:0.7080221772193909\n",
      "Epoch 0[1214/4499] Val Loss:0.7220758199691772\n",
      "Epoch 0[1215/4499] Val Loss:0.6152257919311523\n",
      "Epoch 0[1216/4499] Val Loss:0.5839194059371948\n",
      "Epoch 0[1217/4499] Val Loss:0.5866126418113708\n",
      "Epoch 0[1218/4499] Val Loss:0.45089206099510193\n",
      "Epoch 0[1219/4499] Val Loss:0.4150179624557495\n",
      "Epoch 0[1220/4499] Val Loss:0.38000544905662537\n",
      "Epoch 0[1221/4499] Val Loss:0.5371851325035095\n",
      "Epoch 0[1222/4499] Val Loss:0.5408139824867249\n",
      "Epoch 0[1223/4499] Val Loss:0.5810367465019226\n",
      "Epoch 0[1224/4499] Val Loss:0.6294523477554321\n",
      "Epoch 0[1225/4499] Val Loss:0.5560342073440552\n",
      "Epoch 0[1226/4499] Val Loss:0.4368182122707367\n",
      "Epoch 0[1227/4499] Val Loss:0.47145533561706543\n",
      "Epoch 0[1228/4499] Val Loss:0.4565485119819641\n",
      "Epoch 0[1229/4499] Val Loss:0.4908530116081238\n",
      "Epoch 0[1230/4499] Val Loss:0.4742797017097473\n",
      "Epoch 0[1231/4499] Val Loss:0.4418603181838989\n",
      "Epoch 0[1232/4499] Val Loss:0.38394564390182495\n",
      "Epoch 0[1233/4499] Val Loss:0.3945358097553253\n",
      "Epoch 0[1234/4499] Val Loss:0.3752323091030121\n",
      "Epoch 0[1235/4499] Val Loss:0.4722335636615753\n",
      "Epoch 0[1236/4499] Val Loss:0.48627328872680664\n",
      "Epoch 0[1237/4499] Val Loss:0.5675869584083557\n",
      "Epoch 0[1238/4499] Val Loss:0.5211506485939026\n",
      "Epoch 0[1239/4499] Val Loss:0.5020677447319031\n",
      "Epoch 0[1240/4499] Val Loss:0.5424758791923523\n",
      "Epoch 0[1241/4499] Val Loss:0.637542724609375\n",
      "Epoch 0[1242/4499] Val Loss:0.6156754493713379\n",
      "Epoch 0[1243/4499] Val Loss:0.6206497550010681\n",
      "Epoch 0[1244/4499] Val Loss:0.6199283003807068\n",
      "Epoch 0[1245/4499] Val Loss:0.637185275554657\n",
      "Epoch 0[1246/4499] Val Loss:0.5709434747695923\n",
      "Epoch 0[1247/4499] Val Loss:0.6430431008338928\n",
      "Epoch 0[1248/4499] Val Loss:0.6349828839302063\n",
      "Epoch 0[1249/4499] Val Loss:0.6087847352027893\n",
      "Epoch 0[1250/4499] Val Loss:0.5866291522979736\n",
      "Epoch 0[1251/4499] Val Loss:0.5903651118278503\n",
      "Epoch 0[1252/4499] Val Loss:0.552638828754425\n",
      "Epoch 0[1253/4499] Val Loss:0.6515087485313416\n",
      "Epoch 0[1254/4499] Val Loss:0.5563707947731018\n",
      "Epoch 0[1255/4499] Val Loss:0.5047882795333862\n",
      "Epoch 0[1256/4499] Val Loss:0.70671147108078\n",
      "Epoch 0[1257/4499] Val Loss:0.8080242872238159\n",
      "Epoch 0[1258/4499] Val Loss:0.774316668510437\n",
      "Epoch 0[1259/4499] Val Loss:0.8456246852874756\n",
      "Epoch 0[1260/4499] Val Loss:0.7687392234802246\n",
      "Epoch 0[1261/4499] Val Loss:0.9324081540107727\n",
      "Epoch 0[1262/4499] Val Loss:0.5045408010482788\n",
      "Epoch 0[1263/4499] Val Loss:0.7065384387969971\n",
      "Epoch 0[1264/4499] Val Loss:0.7280111908912659\n",
      "Epoch 0[1265/4499] Val Loss:0.6412472128868103\n",
      "Epoch 0[1266/4499] Val Loss:0.46721047163009644\n",
      "Epoch 0[1267/4499] Val Loss:0.5178030729293823\n",
      "Epoch 0[1268/4499] Val Loss:0.592585027217865\n",
      "Epoch 0[1269/4499] Val Loss:0.43449610471725464\n",
      "Epoch 0[1270/4499] Val Loss:0.4881467819213867\n",
      "Epoch 0[1271/4499] Val Loss:0.5831691026687622\n",
      "Epoch 0[1272/4499] Val Loss:0.5212766528129578\n",
      "Epoch 0[1273/4499] Val Loss:0.5689443945884705\n",
      "Epoch 0[1274/4499] Val Loss:0.5352287888526917\n",
      "Epoch 0[1275/4499] Val Loss:0.5592809319496155\n",
      "Epoch 0[1276/4499] Val Loss:0.7174138426780701\n",
      "Epoch 0[1277/4499] Val Loss:0.5362465381622314\n",
      "Epoch 0[1278/4499] Val Loss:0.531987726688385\n",
      "Epoch 0[1279/4499] Val Loss:0.5816957950592041\n",
      "Epoch 0[1280/4499] Val Loss:0.6988772749900818\n",
      "Epoch 0[1281/4499] Val Loss:0.5198019742965698\n",
      "Epoch 0[1282/4499] Val Loss:0.6473692059516907\n",
      "Epoch 0[1283/4499] Val Loss:0.610585629940033\n",
      "Epoch 0[1284/4499] Val Loss:0.6432989835739136\n",
      "Epoch 0[1285/4499] Val Loss:0.6582337617874146\n",
      "Epoch 0[1286/4499] Val Loss:0.8831334114074707\n",
      "Epoch 0[1287/4499] Val Loss:1.049155592918396\n",
      "Epoch 0[1288/4499] Val Loss:0.798473596572876\n",
      "Epoch 0[1289/4499] Val Loss:0.9752432703971863\n",
      "Epoch 0[1290/4499] Val Loss:0.9284522533416748\n",
      "Epoch 0[1291/4499] Val Loss:0.6248233914375305\n",
      "Epoch 0[1292/4499] Val Loss:0.6298266649246216\n",
      "Epoch 0[1293/4499] Val Loss:0.6345971822738647\n",
      "Epoch 0[1294/4499] Val Loss:0.7440347671508789\n",
      "Epoch 0[1295/4499] Val Loss:0.7200921177864075\n",
      "Epoch 0[1296/4499] Val Loss:0.6929017305374146\n",
      "Epoch 0[1297/4499] Val Loss:0.6275668740272522\n",
      "Epoch 0[1298/4499] Val Loss:0.48582127690315247\n",
      "Epoch 0[1299/4499] Val Loss:0.4634310305118561\n",
      "Epoch 0[1300/4499] Val Loss:0.46676957607269287\n",
      "Epoch 0[1301/4499] Val Loss:0.5119117498397827\n",
      "Epoch 0[1302/4499] Val Loss:0.562075138092041\n",
      "Epoch 0[1303/4499] Val Loss:0.5893957018852234\n",
      "Epoch 0[1304/4499] Val Loss:0.5136216282844543\n",
      "Epoch 0[1305/4499] Val Loss:0.5144922137260437\n",
      "Epoch 0[1306/4499] Val Loss:0.46696147322654724\n",
      "Epoch 0[1307/4499] Val Loss:0.4191288948059082\n",
      "Epoch 0[1308/4499] Val Loss:0.5025023818016052\n",
      "Epoch 0[1309/4499] Val Loss:0.6788010597229004\n",
      "Epoch 0[1310/4499] Val Loss:0.420437216758728\n",
      "Epoch 0[1311/4499] Val Loss:0.4117448627948761\n",
      "Epoch 0[1312/4499] Val Loss:0.32915666699409485\n",
      "Epoch 0[1313/4499] Val Loss:0.4648350775241852\n",
      "Epoch 0[1314/4499] Val Loss:0.6431930065155029\n",
      "Epoch 0[1315/4499] Val Loss:0.5430013537406921\n",
      "Epoch 0[1316/4499] Val Loss:0.5484466552734375\n",
      "Epoch 0[1317/4499] Val Loss:0.5940467715263367\n",
      "Epoch 0[1318/4499] Val Loss:0.5296940207481384\n",
      "Epoch 0[1319/4499] Val Loss:0.5249190330505371\n",
      "Epoch 0[1320/4499] Val Loss:0.666341245174408\n",
      "Epoch 0[1321/4499] Val Loss:0.7645153999328613\n",
      "Epoch 0[1322/4499] Val Loss:0.6109275221824646\n",
      "Epoch 0[1323/4499] Val Loss:0.5725210905075073\n",
      "Epoch 0[1324/4499] Val Loss:0.5167987942695618\n",
      "Epoch 0[1325/4499] Val Loss:0.6502431631088257\n",
      "Epoch 0[1326/4499] Val Loss:0.7097558975219727\n",
      "Epoch 0[1327/4499] Val Loss:0.6259241104125977\n",
      "Epoch 0[1328/4499] Val Loss:0.7613657712936401\n",
      "Epoch 0[1329/4499] Val Loss:0.5548993945121765\n",
      "Epoch 0[1330/4499] Val Loss:0.6358373761177063\n",
      "Epoch 0[1331/4499] Val Loss:0.6421313285827637\n",
      "Epoch 0[1332/4499] Val Loss:0.7533536553382874\n",
      "Epoch 0[1333/4499] Val Loss:0.6593807935714722\n",
      "Epoch 0[1334/4499] Val Loss:0.5948923230171204\n",
      "Epoch 0[1335/4499] Val Loss:0.7233997583389282\n",
      "Epoch 0[1336/4499] Val Loss:0.6570213437080383\n",
      "Epoch 0[1337/4499] Val Loss:0.668615460395813\n",
      "Epoch 0[1338/4499] Val Loss:0.7279680371284485\n",
      "Epoch 0[1339/4499] Val Loss:0.8406644463539124\n",
      "Epoch 0[1340/4499] Val Loss:0.7214705348014832\n",
      "Epoch 0[1341/4499] Val Loss:0.9177688956260681\n",
      "Epoch 0[1342/4499] Val Loss:0.7431812882423401\n",
      "Epoch 0[1343/4499] Val Loss:0.7055851817131042\n",
      "Epoch 0[1344/4499] Val Loss:0.7187818288803101\n",
      "Epoch 0[1345/4499] Val Loss:0.7115480899810791\n",
      "Epoch 0[1346/4499] Val Loss:0.6134723424911499\n",
      "Epoch 0[1347/4499] Val Loss:0.5639589428901672\n",
      "Epoch 0[1348/4499] Val Loss:0.6394240260124207\n",
      "Epoch 0[1349/4499] Val Loss:0.5861881971359253\n",
      "Epoch 0[1350/4499] Val Loss:0.6804149150848389\n",
      "Epoch 0[1351/4499] Val Loss:0.642939567565918\n",
      "Epoch 0[1352/4499] Val Loss:0.5118324756622314\n",
      "Epoch 0[1353/4499] Val Loss:0.5968177914619446\n",
      "Epoch 0[1354/4499] Val Loss:0.5947438478469849\n",
      "Epoch 0[1355/4499] Val Loss:0.6650606393814087\n",
      "Epoch 0[1356/4499] Val Loss:0.6081182956695557\n",
      "Epoch 0[1357/4499] Val Loss:0.6025537252426147\n",
      "Epoch 0[1358/4499] Val Loss:0.6270942687988281\n",
      "Epoch 0[1359/4499] Val Loss:0.6334433555603027\n",
      "Epoch 0[1360/4499] Val Loss:0.5527898669242859\n",
      "Epoch 0[1361/4499] Val Loss:0.6890251040458679\n",
      "Epoch 0[1362/4499] Val Loss:0.6173549294471741\n",
      "Epoch 0[1363/4499] Val Loss:0.6640439033508301\n",
      "Epoch 0[1364/4499] Val Loss:0.828253448009491\n",
      "Epoch 0[1365/4499] Val Loss:0.8301919102668762\n",
      "Epoch 0[1366/4499] Val Loss:0.9787526726722717\n",
      "Epoch 0[1367/4499] Val Loss:0.9024924039840698\n",
      "Epoch 0[1368/4499] Val Loss:0.9232875108718872\n",
      "Epoch 0[1369/4499] Val Loss:1.00093674659729\n",
      "Epoch 0[1370/4499] Val Loss:0.8701355457305908\n",
      "Epoch 0[1371/4499] Val Loss:0.7100575566291809\n",
      "Epoch 0[1372/4499] Val Loss:0.6285267472267151\n",
      "Epoch 0[1373/4499] Val Loss:0.7267593741416931\n",
      "Epoch 0[1374/4499] Val Loss:0.6462072730064392\n",
      "Epoch 0[1375/4499] Val Loss:0.5915679335594177\n",
      "Epoch 0[1376/4499] Val Loss:0.5564534664154053\n",
      "Epoch 0[1377/4499] Val Loss:0.5966262221336365\n",
      "Epoch 0[1378/4499] Val Loss:0.5954198241233826\n",
      "Epoch 0[1379/4499] Val Loss:0.5353000164031982\n",
      "Epoch 0[1380/4499] Val Loss:0.4422881007194519\n",
      "Epoch 0[1381/4499] Val Loss:0.4587916433811188\n",
      "Epoch 0[1382/4499] Val Loss:0.35878217220306396\n",
      "Epoch 0[1383/4499] Val Loss:0.25018614530563354\n",
      "Epoch 0[1384/4499] Val Loss:0.2874358594417572\n",
      "Epoch 0[1385/4499] Val Loss:0.3282707929611206\n",
      "Epoch 0[1386/4499] Val Loss:0.2576500177383423\n",
      "Epoch 0[1387/4499] Val Loss:0.2716996669769287\n",
      "Epoch 0[1388/4499] Val Loss:0.23141945898532867\n",
      "Epoch 0[1389/4499] Val Loss:0.133647620677948\n",
      "Epoch 0[1390/4499] Val Loss:0.13939489424228668\n",
      "Epoch 0[1391/4499] Val Loss:0.3930380046367645\n",
      "Epoch 0[1392/4499] Val Loss:0.16532745957374573\n",
      "Epoch 0[1393/4499] Val Loss:0.16448763012886047\n",
      "Epoch 0[1394/4499] Val Loss:0.24051280319690704\n",
      "Epoch 0[1395/4499] Val Loss:0.162846177816391\n",
      "Epoch 0[1396/4499] Val Loss:0.3530845046043396\n",
      "Epoch 0[1397/4499] Val Loss:0.5796395540237427\n",
      "Epoch 0[1398/4499] Val Loss:0.3344837427139282\n",
      "Epoch 0[1399/4499] Val Loss:0.5727159380912781\n",
      "Epoch 0[1400/4499] Val Loss:0.5852466821670532\n",
      "Epoch 0[1401/4499] Val Loss:0.6249422430992126\n",
      "Epoch 0[1402/4499] Val Loss:0.7437918186187744\n",
      "Epoch 0[1403/4499] Val Loss:0.6119734644889832\n",
      "Epoch 0[1404/4499] Val Loss:0.5435642600059509\n",
      "Epoch 0[1405/4499] Val Loss:0.5864323973655701\n",
      "Epoch 0[1406/4499] Val Loss:0.780992865562439\n",
      "Epoch 0[1407/4499] Val Loss:0.7790641188621521\n",
      "Epoch 0[1408/4499] Val Loss:0.8004909157752991\n",
      "Epoch 0[1409/4499] Val Loss:0.7206706404685974\n",
      "Epoch 0[1410/4499] Val Loss:0.8260873556137085\n",
      "Epoch 0[1411/4499] Val Loss:0.6021667718887329\n",
      "Epoch 0[1412/4499] Val Loss:0.6196826100349426\n",
      "Epoch 0[1413/4499] Val Loss:0.6029871106147766\n",
      "Epoch 0[1414/4499] Val Loss:0.7208470106124878\n",
      "Epoch 0[1415/4499] Val Loss:0.9232398271560669\n",
      "Epoch 0[1416/4499] Val Loss:0.9049858450889587\n",
      "Epoch 0[1417/4499] Val Loss:0.8588977456092834\n",
      "Epoch 0[1418/4499] Val Loss:0.9266394972801208\n",
      "Epoch 0[1419/4499] Val Loss:0.3169984519481659\n",
      "Epoch 0[1420/4499] Val Loss:0.2060718536376953\n",
      "Epoch 0[1421/4499] Val Loss:0.1774549037218094\n",
      "Epoch 0[1422/4499] Val Loss:0.17693009972572327\n",
      "Epoch 0[1423/4499] Val Loss:0.21975943446159363\n",
      "Epoch 0[1424/4499] Val Loss:0.23699140548706055\n",
      "Epoch 0[1425/4499] Val Loss:0.2481006681919098\n",
      "Epoch 0[1426/4499] Val Loss:0.1815899908542633\n",
      "Epoch 0[1427/4499] Val Loss:0.15953178703784943\n",
      "Epoch 0[1428/4499] Val Loss:0.18141305446624756\n",
      "Epoch 0[1429/4499] Val Loss:0.1778494119644165\n",
      "Epoch 0[1430/4499] Val Loss:0.18079644441604614\n",
      "Epoch 0[1431/4499] Val Loss:0.19049173593521118\n",
      "Epoch 0[1432/4499] Val Loss:0.19096465408802032\n",
      "Epoch 0[1433/4499] Val Loss:0.17189528048038483\n",
      "Epoch 0[1434/4499] Val Loss:0.2321203351020813\n",
      "Epoch 0[1435/4499] Val Loss:0.23414456844329834\n",
      "Epoch 0[1436/4499] Val Loss:0.16781635582447052\n",
      "Epoch 0[1437/4499] Val Loss:0.2028055489063263\n",
      "Epoch 0[1438/4499] Val Loss:0.18909992277622223\n",
      "Epoch 0[1439/4499] Val Loss:0.15784357488155365\n",
      "Epoch 0[1440/4499] Val Loss:0.24184145033359528\n",
      "Epoch 0[1441/4499] Val Loss:0.16903668642044067\n",
      "Epoch 0[1442/4499] Val Loss:0.2248517870903015\n",
      "Epoch 0[1443/4499] Val Loss:0.24200811982154846\n",
      "Epoch 0[1444/4499] Val Loss:0.24479065835475922\n",
      "Epoch 0[1445/4499] Val Loss:0.2005227506160736\n",
      "Epoch 0[1446/4499] Val Loss:0.17792825400829315\n",
      "Epoch 0[1447/4499] Val Loss:0.1856270432472229\n",
      "Epoch 0[1448/4499] Val Loss:0.1530054807662964\n",
      "Epoch 0[1449/4499] Val Loss:0.1694011688232422\n",
      "Epoch 0[1450/4499] Val Loss:0.1964002251625061\n",
      "Epoch 0[1451/4499] Val Loss:0.2142251431941986\n",
      "Epoch 0[1452/4499] Val Loss:0.21909776329994202\n",
      "Epoch 0[1453/4499] Val Loss:0.29896998405456543\n",
      "Epoch 0[1454/4499] Val Loss:0.1937374770641327\n",
      "Epoch 0[1455/4499] Val Loss:0.1798790842294693\n",
      "Epoch 0[1456/4499] Val Loss:0.2697286605834961\n",
      "Epoch 0[1457/4499] Val Loss:0.22959741950035095\n",
      "Epoch 0[1458/4499] Val Loss:0.18405084311962128\n",
      "Epoch 0[1459/4499] Val Loss:0.21353289484977722\n",
      "Epoch 0[1460/4499] Val Loss:0.26181429624557495\n",
      "Epoch 0[1461/4499] Val Loss:0.211290642619133\n",
      "Epoch 0[1462/4499] Val Loss:0.2289741337299347\n",
      "Epoch 0[1463/4499] Val Loss:0.20926597714424133\n",
      "Epoch 0[1464/4499] Val Loss:0.1892836093902588\n",
      "Epoch 0[1465/4499] Val Loss:0.15870486199855804\n",
      "Epoch 0[1466/4499] Val Loss:0.20394831895828247\n",
      "Epoch 0[1467/4499] Val Loss:0.25771623849868774\n",
      "Epoch 0[1468/4499] Val Loss:0.1340359002351761\n",
      "Epoch 0[1469/4499] Val Loss:0.30328503251075745\n",
      "Epoch 0[1470/4499] Val Loss:0.25856247544288635\n",
      "Epoch 0[1471/4499] Val Loss:0.21870191395282745\n",
      "Epoch 0[1472/4499] Val Loss:0.15143144130706787\n",
      "Epoch 0[1473/4499] Val Loss:0.14161476492881775\n",
      "Epoch 0[1474/4499] Val Loss:0.16689755022525787\n",
      "Epoch 0[1475/4499] Val Loss:0.20742373168468475\n",
      "Epoch 0[1476/4499] Val Loss:0.24022483825683594\n",
      "Epoch 0[1477/4499] Val Loss:0.182822585105896\n",
      "Epoch 0[1478/4499] Val Loss:0.15621618926525116\n",
      "Epoch 0[1479/4499] Val Loss:0.16931138932704926\n",
      "Epoch 0[1480/4499] Val Loss:0.14388099312782288\n",
      "Epoch 0[1481/4499] Val Loss:0.25804924964904785\n",
      "Epoch 0[1482/4499] Val Loss:0.16888079047203064\n",
      "Epoch 0[1483/4499] Val Loss:0.27569809556007385\n",
      "Epoch 0[1484/4499] Val Loss:0.18259529769420624\n",
      "Epoch 0[1485/4499] Val Loss:0.21162022650241852\n",
      "Epoch 0[1486/4499] Val Loss:0.17437899112701416\n",
      "Epoch 0[1487/4499] Val Loss:0.23631609976291656\n",
      "Epoch 0[1488/4499] Val Loss:0.17840537428855896\n",
      "Epoch 0[1489/4499] Val Loss:0.38366058468818665\n",
      "Epoch 0[1490/4499] Val Loss:0.19084949791431427\n",
      "Epoch 0[1491/4499] Val Loss:0.6180441975593567\n",
      "Epoch 0[1492/4499] Val Loss:0.17602720856666565\n",
      "Epoch 0[1493/4499] Val Loss:0.5405756831169128\n",
      "Epoch 0[1494/4499] Val Loss:0.29102832078933716\n",
      "Epoch 0[1495/4499] Val Loss:0.5641335844993591\n",
      "Epoch 0[1496/4499] Val Loss:0.348746657371521\n",
      "Epoch 0[1497/4499] Val Loss:0.5340637564659119\n",
      "Epoch 0[1498/4499] Val Loss:0.4349416196346283\n",
      "Epoch 0[1499/4499] Val Loss:0.5930388569831848\n",
      "Epoch 0[1500/4499] Val Loss:0.4414235055446625\n",
      "Epoch 0[1501/4499] Val Loss:0.3690826892852783\n",
      "Epoch 0[1502/4499] Val Loss:0.3974367678165436\n",
      "Epoch 0[1503/4499] Val Loss:0.4528628885746002\n",
      "Epoch 0[1504/4499] Val Loss:0.39421817660331726\n",
      "Epoch 0[1505/4499] Val Loss:0.4148826599121094\n",
      "Epoch 0[1506/4499] Val Loss:0.559705376625061\n",
      "Epoch 0[1507/4499] Val Loss:0.5296919345855713\n",
      "Epoch 0[1508/4499] Val Loss:0.5879665017127991\n",
      "Epoch 0[1509/4499] Val Loss:0.39611849188804626\n",
      "Epoch 0[1510/4499] Val Loss:0.5163918137550354\n",
      "Epoch 0[1511/4499] Val Loss:0.4318792521953583\n",
      "Epoch 0[1512/4499] Val Loss:0.6571898460388184\n",
      "Epoch 0[1513/4499] Val Loss:0.5736804008483887\n",
      "Epoch 0[1514/4499] Val Loss:0.8302474021911621\n",
      "Epoch 0[1515/4499] Val Loss:0.6399078369140625\n",
      "Epoch 0[1516/4499] Val Loss:0.7483829259872437\n",
      "Epoch 0[1517/4499] Val Loss:0.35543739795684814\n",
      "Epoch 0[1518/4499] Val Loss:0.5735499262809753\n",
      "Epoch 0[1519/4499] Val Loss:0.418317586183548\n",
      "Epoch 0[1520/4499] Val Loss:0.7004247307777405\n",
      "Epoch 0[1521/4499] Val Loss:0.4819319248199463\n",
      "Epoch 0[1522/4499] Val Loss:0.9075773358345032\n",
      "Epoch 0[1523/4499] Val Loss:0.4387173652648926\n",
      "Epoch 0[1524/4499] Val Loss:0.7974602580070496\n",
      "Epoch 0[1525/4499] Val Loss:0.5606526732444763\n",
      "Epoch 0[1526/4499] Val Loss:0.6808516979217529\n",
      "Epoch 0[1527/4499] Val Loss:0.5787684917449951\n",
      "Epoch 0[1528/4499] Val Loss:0.8561115264892578\n",
      "Epoch 0[1529/4499] Val Loss:0.6004282832145691\n",
      "Epoch 0[1530/4499] Val Loss:1.0199358463287354\n",
      "Epoch 0[1531/4499] Val Loss:0.5173594951629639\n",
      "Epoch 0[1532/4499] Val Loss:0.7958390712738037\n",
      "Epoch 0[1533/4499] Val Loss:0.3965834081172943\n",
      "Epoch 0[1534/4499] Val Loss:0.5791234970092773\n",
      "Epoch 0[1535/4499] Val Loss:0.606281042098999\n",
      "Epoch 0[1536/4499] Val Loss:0.7714077234268188\n",
      "Epoch 0[1537/4499] Val Loss:0.7961323857307434\n",
      "Epoch 0[1538/4499] Val Loss:0.6762710809707642\n",
      "Epoch 0[1539/4499] Val Loss:0.7609431147575378\n",
      "Epoch 0[1540/4499] Val Loss:0.6331472992897034\n",
      "Epoch 0[1541/4499] Val Loss:0.8009159564971924\n",
      "Epoch 0[1542/4499] Val Loss:0.841094970703125\n",
      "Epoch 0[1543/4499] Val Loss:0.8793216347694397\n",
      "Epoch 0[1544/4499] Val Loss:0.9674240946769714\n",
      "Epoch 0[1545/4499] Val Loss:0.8831325173377991\n",
      "Epoch 0[1546/4499] Val Loss:0.7048549652099609\n",
      "Epoch 0[1547/4499] Val Loss:0.8423047661781311\n",
      "Epoch 0[1548/4499] Val Loss:0.6567692756652832\n",
      "Epoch 0[1549/4499] Val Loss:1.0000115633010864\n",
      "Epoch 0[1550/4499] Val Loss:0.7925689816474915\n",
      "Epoch 0[1551/4499] Val Loss:1.001953125\n",
      "Epoch 0[1552/4499] Val Loss:0.9853764176368713\n",
      "Epoch 0[1553/4499] Val Loss:1.1431909799575806\n",
      "Epoch 0[1554/4499] Val Loss:1.0335934162139893\n",
      "Epoch 0[1555/4499] Val Loss:1.271066665649414\n",
      "Epoch 0[1556/4499] Val Loss:1.0253627300262451\n",
      "Epoch 0[1557/4499] Val Loss:1.372151255607605\n",
      "Epoch 0[1558/4499] Val Loss:1.1315217018127441\n",
      "Epoch 0[1559/4499] Val Loss:1.1105276346206665\n",
      "Epoch 0[1560/4499] Val Loss:1.265964150428772\n",
      "Epoch 0[1561/4499] Val Loss:0.7879287600517273\n",
      "Epoch 0[1562/4499] Val Loss:1.2141776084899902\n",
      "Epoch 0[1563/4499] Val Loss:1.2043087482452393\n",
      "Epoch 0[1564/4499] Val Loss:0.8443928956985474\n",
      "Epoch 0[1565/4499] Val Loss:1.2717516422271729\n",
      "Epoch 0[1566/4499] Val Loss:0.9329240322113037\n",
      "Epoch 0[1567/4499] Val Loss:1.3030555248260498\n",
      "Epoch 0[1568/4499] Val Loss:1.0540239810943604\n",
      "Epoch 0[1569/4499] Val Loss:1.113468885421753\n",
      "Epoch 0[1570/4499] Val Loss:1.2025078535079956\n",
      "Epoch 0[1571/4499] Val Loss:1.3731894493103027\n",
      "Epoch 0[1572/4499] Val Loss:1.0609188079833984\n",
      "Epoch 0[1573/4499] Val Loss:1.0452221632003784\n",
      "Epoch 0[1574/4499] Val Loss:0.8672897815704346\n",
      "Epoch 0[1575/4499] Val Loss:1.106549859046936\n",
      "Epoch 0[1576/4499] Val Loss:1.1642735004425049\n",
      "Epoch 0[1577/4499] Val Loss:1.4167858362197876\n",
      "Epoch 0[1578/4499] Val Loss:1.4494000673294067\n",
      "Epoch 0[1579/4499] Val Loss:1.2718141078948975\n",
      "Epoch 0[1580/4499] Val Loss:1.3926128149032593\n",
      "Epoch 0[1581/4499] Val Loss:1.3729525804519653\n",
      "Epoch 0[1582/4499] Val Loss:1.3005977869033813\n",
      "Epoch 0[1583/4499] Val Loss:1.003326654434204\n",
      "Epoch 0[1584/4499] Val Loss:1.0157403945922852\n",
      "Epoch 0[1585/4499] Val Loss:1.2170679569244385\n",
      "Epoch 0[1586/4499] Val Loss:0.994719386100769\n",
      "Epoch 0[1587/4499] Val Loss:0.7811497449874878\n",
      "Epoch 0[1588/4499] Val Loss:0.7110303640365601\n",
      "Epoch 0[1589/4499] Val Loss:0.5665954351425171\n",
      "Epoch 0[1590/4499] Val Loss:0.8207735419273376\n",
      "Epoch 0[1591/4499] Val Loss:0.5615385174751282\n",
      "Epoch 0[1592/4499] Val Loss:0.4920446276664734\n",
      "Epoch 0[1593/4499] Val Loss:0.7241288423538208\n",
      "Epoch 0[1594/4499] Val Loss:0.9146103858947754\n",
      "Epoch 0[1595/4499] Val Loss:0.6871247291564941\n",
      "Epoch 0[1596/4499] Val Loss:0.5835356116294861\n",
      "Epoch 0[1597/4499] Val Loss:0.9347206354141235\n",
      "Epoch 0[1598/4499] Val Loss:1.051142930984497\n",
      "Epoch 0[1599/4499] Val Loss:0.7998937964439392\n",
      "Epoch 0[1600/4499] Val Loss:0.6515771150588989\n",
      "Epoch 0[1601/4499] Val Loss:0.32013076543807983\n",
      "Epoch 0[1602/4499] Val Loss:0.35930147767066956\n",
      "Epoch 0[1603/4499] Val Loss:0.5717533230781555\n",
      "Epoch 0[1604/4499] Val Loss:0.31895875930786133\n",
      "Epoch 0[1605/4499] Val Loss:0.3574513792991638\n",
      "Epoch 0[1606/4499] Val Loss:0.41885411739349365\n",
      "Epoch 0[1607/4499] Val Loss:0.6210641264915466\n",
      "Epoch 0[1608/4499] Val Loss:0.5325765609741211\n",
      "Epoch 0[1609/4499] Val Loss:0.6176044940948486\n",
      "Epoch 0[1610/4499] Val Loss:0.6550174951553345\n",
      "Epoch 0[1611/4499] Val Loss:0.5924808979034424\n",
      "Epoch 0[1612/4499] Val Loss:0.6484693884849548\n",
      "Epoch 0[1613/4499] Val Loss:0.7285723090171814\n",
      "Epoch 0[1614/4499] Val Loss:0.7801700234413147\n",
      "Epoch 0[1615/4499] Val Loss:0.7812371253967285\n",
      "Epoch 0[1616/4499] Val Loss:0.8069419264793396\n",
      "Epoch 0[1617/4499] Val Loss:0.7183637619018555\n",
      "Epoch 0[1618/4499] Val Loss:0.7775965929031372\n",
      "Epoch 0[1619/4499] Val Loss:0.6786560416221619\n",
      "Epoch 0[1620/4499] Val Loss:0.8264774084091187\n",
      "Epoch 0[1621/4499] Val Loss:0.7279152870178223\n",
      "Epoch 0[1622/4499] Val Loss:0.9811920523643494\n",
      "Epoch 0[1623/4499] Val Loss:1.2255616188049316\n",
      "Epoch 0[1624/4499] Val Loss:1.0709389448165894\n",
      "Epoch 0[1625/4499] Val Loss:0.8560970425605774\n",
      "Epoch 0[1626/4499] Val Loss:0.9806869029998779\n",
      "Epoch 0[1627/4499] Val Loss:1.0167790651321411\n",
      "Epoch 0[1628/4499] Val Loss:1.0851233005523682\n",
      "Epoch 0[1629/4499] Val Loss:0.9303246140480042\n",
      "Epoch 0[1630/4499] Val Loss:0.5650869607925415\n",
      "Epoch 0[1631/4499] Val Loss:0.5883342027664185\n",
      "Epoch 0[1632/4499] Val Loss:0.7321817874908447\n",
      "Epoch 0[1633/4499] Val Loss:1.0106555223464966\n",
      "Epoch 0[1634/4499] Val Loss:0.7504993677139282\n",
      "Epoch 0[1635/4499] Val Loss:0.49432405829429626\n",
      "Epoch 0[1636/4499] Val Loss:0.7057685256004333\n",
      "Epoch 0[1637/4499] Val Loss:0.3234616219997406\n",
      "Epoch 0[1638/4499] Val Loss:0.3696646988391876\n",
      "Epoch 0[1639/4499] Val Loss:0.2429303526878357\n",
      "Epoch 0[1640/4499] Val Loss:0.22961187362670898\n",
      "Epoch 0[1641/4499] Val Loss:0.16603510081768036\n",
      "Epoch 0[1642/4499] Val Loss:0.19198693335056305\n",
      "Epoch 0[1643/4499] Val Loss:0.17478597164154053\n",
      "Epoch 0[1644/4499] Val Loss:0.14908063411712646\n",
      "Epoch 0[1645/4499] Val Loss:0.12298692017793655\n",
      "Epoch 0[1646/4499] Val Loss:0.13352149724960327\n",
      "Epoch 0[1647/4499] Val Loss:0.12043551355600357\n",
      "Epoch 0[1648/4499] Val Loss:0.13369004428386688\n",
      "Epoch 0[1649/4499] Val Loss:0.11247196048498154\n",
      "Epoch 0[1650/4499] Val Loss:0.11365343630313873\n",
      "Epoch 0[1651/4499] Val Loss:0.1714046597480774\n",
      "Epoch 0[1652/4499] Val Loss:0.14199143648147583\n",
      "Epoch 0[1653/4499] Val Loss:0.13347598910331726\n",
      "Epoch 0[1654/4499] Val Loss:0.10264650732278824\n",
      "Epoch 0[1655/4499] Val Loss:0.10478267073631287\n",
      "Epoch 0[1656/4499] Val Loss:0.12991662323474884\n",
      "Epoch 0[1657/4499] Val Loss:0.1078382134437561\n",
      "Epoch 0[1658/4499] Val Loss:0.08202481269836426\n",
      "Epoch 0[1659/4499] Val Loss:0.08436284214258194\n",
      "Epoch 0[1660/4499] Val Loss:0.12338660657405853\n",
      "Epoch 0[1661/4499] Val Loss:0.1286110281944275\n",
      "Epoch 0[1662/4499] Val Loss:0.157323956489563\n",
      "Epoch 0[1663/4499] Val Loss:0.13839611411094666\n",
      "Epoch 0[1664/4499] Val Loss:0.1537155956029892\n",
      "Epoch 0[1665/4499] Val Loss:0.13886211812496185\n",
      "Epoch 0[1666/4499] Val Loss:0.1621781289577484\n",
      "Epoch 0[1667/4499] Val Loss:0.1415269523859024\n",
      "Epoch 0[1668/4499] Val Loss:0.10606449842453003\n",
      "Epoch 0[1669/4499] Val Loss:0.11366812139749527\n",
      "Epoch 0[1670/4499] Val Loss:0.16212931275367737\n",
      "Epoch 0[1671/4499] Val Loss:0.18253713846206665\n",
      "Epoch 0[1672/4499] Val Loss:0.15335938334465027\n",
      "Epoch 0[1673/4499] Val Loss:0.13546501100063324\n",
      "Epoch 0[1674/4499] Val Loss:0.14097769558429718\n",
      "Epoch 0[1675/4499] Val Loss:0.11695945262908936\n",
      "Epoch 0[1676/4499] Val Loss:0.13580593466758728\n",
      "Epoch 0[1677/4499] Val Loss:0.11218968778848648\n",
      "Epoch 0[1678/4499] Val Loss:0.1390744000673294\n",
      "Epoch 0[1679/4499] Val Loss:0.11379516869783401\n",
      "Epoch 0[1680/4499] Val Loss:0.10899592190980911\n",
      "Epoch 0[1681/4499] Val Loss:0.12780733406543732\n",
      "Epoch 0[1682/4499] Val Loss:0.12050308287143707\n",
      "Epoch 0[1683/4499] Val Loss:0.12794075906276703\n",
      "Epoch 0[1684/4499] Val Loss:0.1416737586259842\n",
      "Epoch 0[1685/4499] Val Loss:0.14753437042236328\n",
      "Epoch 0[1686/4499] Val Loss:0.1374293565750122\n",
      "Epoch 0[1687/4499] Val Loss:0.15068799257278442\n",
      "Epoch 0[1688/4499] Val Loss:0.14752964675426483\n",
      "Epoch 0[1689/4499] Val Loss:0.14654488861560822\n",
      "Epoch 0[1690/4499] Val Loss:0.1551292985677719\n",
      "Epoch 0[1691/4499] Val Loss:0.17675884068012238\n",
      "Epoch 0[1692/4499] Val Loss:0.12884493172168732\n",
      "Epoch 0[1693/4499] Val Loss:0.23596955835819244\n",
      "Epoch 0[1694/4499] Val Loss:0.20261280238628387\n",
      "Epoch 0[1695/4499] Val Loss:0.18797336518764496\n",
      "Epoch 0[1696/4499] Val Loss:0.13319502770900726\n",
      "Epoch 0[1697/4499] Val Loss:0.13812899589538574\n",
      "Epoch 0[1698/4499] Val Loss:0.13437806069850922\n",
      "Epoch 0[1699/4499] Val Loss:0.10316549986600876\n",
      "Epoch 0[1700/4499] Val Loss:0.12801894545555115\n",
      "Epoch 0[1701/4499] Val Loss:0.11343055963516235\n",
      "Epoch 0[1702/4499] Val Loss:0.17137359082698822\n",
      "Epoch 0[1703/4499] Val Loss:0.1545078605413437\n",
      "Epoch 0[1704/4499] Val Loss:0.1414194107055664\n",
      "Epoch 0[1705/4499] Val Loss:0.11124470829963684\n",
      "Epoch 0[1706/4499] Val Loss:0.14537321031093597\n",
      "Epoch 0[1707/4499] Val Loss:0.18383553624153137\n",
      "Epoch 0[1708/4499] Val Loss:0.1316678673028946\n",
      "Epoch 0[1709/4499] Val Loss:0.20441262423992157\n",
      "Epoch 0[1710/4499] Val Loss:0.11162041872739792\n",
      "Epoch 0[1711/4499] Val Loss:0.22311322391033173\n",
      "Epoch 0[1712/4499] Val Loss:0.1528315544128418\n",
      "Epoch 0[1713/4499] Val Loss:0.1298881620168686\n",
      "Epoch 0[1714/4499] Val Loss:0.2214648425579071\n",
      "Epoch 0[1715/4499] Val Loss:0.11612820625305176\n",
      "Epoch 0[1716/4499] Val Loss:0.2327679544687271\n",
      "Epoch 0[1717/4499] Val Loss:0.1621139645576477\n",
      "Epoch 0[1718/4499] Val Loss:0.17208439111709595\n",
      "Epoch 0[1719/4499] Val Loss:0.19149845838546753\n",
      "Epoch 0[1720/4499] Val Loss:0.15105633437633514\n",
      "Epoch 0[1721/4499] Val Loss:0.156985804438591\n",
      "Epoch 0[1722/4499] Val Loss:0.13361112773418427\n",
      "Epoch 0[1723/4499] Val Loss:0.23227033019065857\n",
      "Epoch 0[1724/4499] Val Loss:0.15217767655849457\n",
      "Epoch 0[1725/4499] Val Loss:0.23384344577789307\n",
      "Epoch 0[1726/4499] Val Loss:0.1474057286977768\n",
      "Epoch 0[1727/4499] Val Loss:0.14132791757583618\n",
      "Epoch 0[1728/4499] Val Loss:0.17963624000549316\n",
      "Epoch 0[1729/4499] Val Loss:0.20710089802742004\n",
      "Epoch 0[1730/4499] Val Loss:0.1811673790216446\n",
      "Epoch 0[1731/4499] Val Loss:0.2587173581123352\n",
      "Epoch 0[1732/4499] Val Loss:0.1642281413078308\n",
      "Epoch 0[1733/4499] Val Loss:0.25159919261932373\n",
      "Epoch 0[1734/4499] Val Loss:0.1804075390100479\n",
      "Epoch 0[1735/4499] Val Loss:0.159449964761734\n",
      "Epoch 0[1736/4499] Val Loss:0.23488199710845947\n",
      "Epoch 0[1737/4499] Val Loss:0.2702130973339081\n",
      "Epoch 0[1738/4499] Val Loss:0.2767200767993927\n",
      "Epoch 0[1739/4499] Val Loss:0.33083969354629517\n",
      "Epoch 0[1740/4499] Val Loss:0.21111449599266052\n",
      "Epoch 0[1741/4499] Val Loss:0.2949916124343872\n",
      "Epoch 0[1742/4499] Val Loss:0.15735919773578644\n",
      "Epoch 0[1743/4499] Val Loss:0.30584415793418884\n",
      "Epoch 0[1744/4499] Val Loss:0.18174970149993896\n",
      "Epoch 0[1745/4499] Val Loss:0.3063191771507263\n",
      "Epoch 0[1746/4499] Val Loss:0.22896845638751984\n",
      "Epoch 0[1747/4499] Val Loss:0.4724310636520386\n",
      "Epoch 0[1748/4499] Val Loss:0.28442010283470154\n",
      "Epoch 0[1749/4499] Val Loss:0.39308544993400574\n",
      "Epoch 0[1750/4499] Val Loss:0.2654031813144684\n",
      "Epoch 0[1751/4499] Val Loss:0.33298635482788086\n",
      "Epoch 0[1752/4499] Val Loss:0.2975938022136688\n",
      "Epoch 0[1753/4499] Val Loss:0.3402908742427826\n",
      "Epoch 0[1754/4499] Val Loss:0.29933854937553406\n",
      "Epoch 0[1755/4499] Val Loss:0.41483381390571594\n",
      "Epoch 0[1756/4499] Val Loss:0.2979635000228882\n",
      "Epoch 0[1757/4499] Val Loss:0.27949023246765137\n",
      "Epoch 0[1758/4499] Val Loss:0.33680906891822815\n",
      "Epoch 0[1759/4499] Val Loss:0.30119481682777405\n",
      "Epoch 0[1760/4499] Val Loss:0.34887078404426575\n",
      "Epoch 0[1761/4499] Val Loss:0.4404679536819458\n",
      "Epoch 0[1762/4499] Val Loss:0.4052642583847046\n",
      "Epoch 0[1763/4499] Val Loss:0.36414068937301636\n",
      "Epoch 0[1764/4499] Val Loss:0.5445724725723267\n",
      "Epoch 0[1765/4499] Val Loss:0.34995830059051514\n",
      "Epoch 0[1766/4499] Val Loss:0.5412365198135376\n",
      "Epoch 0[1767/4499] Val Loss:0.45986926555633545\n",
      "Epoch 0[1768/4499] Val Loss:0.47801727056503296\n",
      "Epoch 0[1769/4499] Val Loss:0.5714709162712097\n",
      "Epoch 0[1770/4499] Val Loss:0.6549427509307861\n",
      "Epoch 0[1771/4499] Val Loss:0.481139600276947\n",
      "Epoch 0[1772/4499] Val Loss:0.5003759264945984\n",
      "Epoch 0[1773/4499] Val Loss:0.32620275020599365\n",
      "Epoch 0[1774/4499] Val Loss:0.4565325081348419\n",
      "Epoch 0[1775/4499] Val Loss:0.4390908479690552\n",
      "Epoch 0[1776/4499] Val Loss:0.547770619392395\n",
      "Epoch 0[1777/4499] Val Loss:0.552529513835907\n",
      "Epoch 0[1778/4499] Val Loss:0.5754427909851074\n",
      "Epoch 0[1779/4499] Val Loss:0.7248792052268982\n",
      "Epoch 0[1780/4499] Val Loss:0.5580883026123047\n",
      "Epoch 0[1781/4499] Val Loss:0.5676915645599365\n",
      "Epoch 0[1782/4499] Val Loss:0.6947057247161865\n",
      "Epoch 0[1783/4499] Val Loss:0.6655989289283752\n",
      "Epoch 0[1784/4499] Val Loss:0.6838405132293701\n",
      "Epoch 0[1785/4499] Val Loss:0.7967121005058289\n",
      "Epoch 0[1786/4499] Val Loss:0.6217160820960999\n",
      "Epoch 0[1787/4499] Val Loss:0.49241650104522705\n",
      "Epoch 0[1788/4499] Val Loss:0.6490378975868225\n",
      "Epoch 0[1789/4499] Val Loss:0.4851105809211731\n",
      "Epoch 0[1790/4499] Val Loss:0.5753876566886902\n",
      "Epoch 0[1791/4499] Val Loss:0.6303420662879944\n",
      "Epoch 0[1792/4499] Val Loss:0.6259034872055054\n",
      "Epoch 0[1793/4499] Val Loss:0.913605272769928\n",
      "Epoch 0[1794/4499] Val Loss:0.6404596567153931\n",
      "Epoch 0[1795/4499] Val Loss:1.0330214500427246\n",
      "Epoch 0[1796/4499] Val Loss:0.6516424417495728\n",
      "Epoch 0[1797/4499] Val Loss:1.1860138177871704\n",
      "Epoch 0[1798/4499] Val Loss:0.7995914816856384\n",
      "Epoch 0[1799/4499] Val Loss:0.9220772385597229\n",
      "Epoch 0[1800/4499] Val Loss:1.0018067359924316\n",
      "Epoch 0[1801/4499] Val Loss:0.7057402729988098\n",
      "Epoch 0[1802/4499] Val Loss:1.0906685590744019\n",
      "Epoch 0[1803/4499] Val Loss:0.42025235295295715\n",
      "Epoch 0[1804/4499] Val Loss:0.7822991609573364\n",
      "Epoch 0[1805/4499] Val Loss:0.5054193735122681\n",
      "Epoch 0[1806/4499] Val Loss:0.9591401219367981\n",
      "Epoch 0[1807/4499] Val Loss:0.7875713109970093\n",
      "Epoch 0[1808/4499] Val Loss:0.5878521203994751\n",
      "Epoch 0[1809/4499] Val Loss:0.845144510269165\n",
      "Epoch 0[1810/4499] Val Loss:0.47111114859580994\n",
      "Epoch 0[1811/4499] Val Loss:0.7809030413627625\n",
      "Epoch 0[1812/4499] Val Loss:0.7140005826950073\n",
      "Epoch 0[1813/4499] Val Loss:0.7275636196136475\n",
      "Epoch 0[1814/4499] Val Loss:0.9057751893997192\n",
      "Epoch 0[1815/4499] Val Loss:0.5518477559089661\n",
      "Epoch 0[1816/4499] Val Loss:0.7569141387939453\n",
      "Epoch 0[1817/4499] Val Loss:0.6286352872848511\n",
      "Epoch 0[1818/4499] Val Loss:0.6055232286453247\n",
      "Epoch 0[1819/4499] Val Loss:0.6786960363388062\n",
      "Epoch 0[1820/4499] Val Loss:0.573158860206604\n",
      "Epoch 0[1821/4499] Val Loss:0.7544972896575928\n",
      "Epoch 0[1822/4499] Val Loss:0.9128623008728027\n",
      "Epoch 0[1823/4499] Val Loss:0.6482847332954407\n",
      "Epoch 0[1824/4499] Val Loss:0.6130511164665222\n",
      "Epoch 0[1825/4499] Val Loss:0.8402100205421448\n",
      "Epoch 0[1826/4499] Val Loss:0.911466121673584\n",
      "Epoch 0[1827/4499] Val Loss:0.8565736413002014\n",
      "Epoch 0[1828/4499] Val Loss:0.8645667433738708\n",
      "Epoch 0[1829/4499] Val Loss:0.826850414276123\n",
      "Epoch 0[1830/4499] Val Loss:0.656665027141571\n",
      "Epoch 0[1831/4499] Val Loss:0.7166188359260559\n",
      "Epoch 0[1832/4499] Val Loss:0.657753586769104\n",
      "Epoch 0[1833/4499] Val Loss:0.6699845194816589\n",
      "Epoch 0[1834/4499] Val Loss:0.4421194791793823\n",
      "Epoch 0[1835/4499] Val Loss:0.4664129316806793\n",
      "Epoch 0[1836/4499] Val Loss:0.7974701523780823\n",
      "Epoch 0[1837/4499] Val Loss:0.6672496795654297\n",
      "Epoch 0[1838/4499] Val Loss:0.9381842017173767\n",
      "Epoch 0[1839/4499] Val Loss:0.9136377573013306\n",
      "Epoch 0[1840/4499] Val Loss:0.7566320896148682\n",
      "Epoch 0[1841/4499] Val Loss:0.5747529864311218\n",
      "Epoch 0[1842/4499] Val Loss:0.6137858629226685\n",
      "Epoch 0[1843/4499] Val Loss:0.8761398196220398\n",
      "Epoch 0[1844/4499] Val Loss:1.0357716083526611\n",
      "Epoch 0[1845/4499] Val Loss:1.0620932579040527\n",
      "Epoch 0[1846/4499] Val Loss:0.8349096775054932\n",
      "Epoch 0[1847/4499] Val Loss:0.3324369192123413\n",
      "Epoch 0[1848/4499] Val Loss:0.2504998743534088\n",
      "Epoch 0[1849/4499] Val Loss:0.2528265118598938\n",
      "Epoch 0[1850/4499] Val Loss:0.38545000553131104\n",
      "Epoch 0[1851/4499] Val Loss:0.5111812353134155\n",
      "Epoch 0[1852/4499] Val Loss:0.45802727341651917\n",
      "Epoch 0[1853/4499] Val Loss:0.4017547369003296\n",
      "Epoch 0[1854/4499] Val Loss:0.47237882018089294\n",
      "Epoch 0[1855/4499] Val Loss:0.43998146057128906\n",
      "Epoch 0[1856/4499] Val Loss:0.4038614332675934\n",
      "Epoch 0[1857/4499] Val Loss:0.41495251655578613\n",
      "Epoch 0[1858/4499] Val Loss:0.4706694781780243\n",
      "Epoch 0[1859/4499] Val Loss:0.5669012069702148\n",
      "Epoch 0[1860/4499] Val Loss:0.47939738631248474\n",
      "Epoch 0[1861/4499] Val Loss:0.4960753321647644\n",
      "Epoch 0[1862/4499] Val Loss:0.6294278502464294\n",
      "Epoch 0[1863/4499] Val Loss:0.5755757093429565\n",
      "Epoch 0[1864/4499] Val Loss:0.5102109313011169\n",
      "Epoch 0[1865/4499] Val Loss:0.5615955591201782\n",
      "Epoch 0[1866/4499] Val Loss:0.6737640500068665\n",
      "Epoch 0[1867/4499] Val Loss:0.7665599584579468\n",
      "Epoch 0[1868/4499] Val Loss:0.7936371564865112\n",
      "Epoch 0[1869/4499] Val Loss:0.6251951456069946\n",
      "Epoch 0[1870/4499] Val Loss:0.6507704257965088\n",
      "Epoch 0[1871/4499] Val Loss:0.7756516337394714\n",
      "Epoch 0[1872/4499] Val Loss:0.7848777174949646\n",
      "Epoch 0[1873/4499] Val Loss:0.5305101275444031\n",
      "Epoch 0[1874/4499] Val Loss:0.31110256910324097\n",
      "Epoch 0[1875/4499] Val Loss:0.6221017837524414\n",
      "Epoch 0[1876/4499] Val Loss:0.6997001767158508\n",
      "Epoch 0[1877/4499] Val Loss:0.6533952355384827\n",
      "Epoch 0[1878/4499] Val Loss:0.45521026849746704\n",
      "Epoch 0[1879/4499] Val Loss:0.43141642212867737\n",
      "Epoch 0[1880/4499] Val Loss:0.6283541917800903\n",
      "Epoch 0[1881/4499] Val Loss:0.6253682374954224\n",
      "Epoch 0[1882/4499] Val Loss:0.39205750823020935\n",
      "Epoch 0[1883/4499] Val Loss:0.42774274945259094\n",
      "Epoch 0[1884/4499] Val Loss:0.2777644395828247\n",
      "Epoch 0[1885/4499] Val Loss:0.2343684583902359\n",
      "Epoch 0[1886/4499] Val Loss:0.18225909769535065\n",
      "Epoch 0[1887/4499] Val Loss:0.2509360611438751\n",
      "Epoch 0[1888/4499] Val Loss:0.26648661494255066\n",
      "Epoch 0[1889/4499] Val Loss:0.13350306451320648\n",
      "Epoch 0[1890/4499] Val Loss:0.1918058842420578\n",
      "Epoch 0[1891/4499] Val Loss:0.1936342716217041\n",
      "Epoch 0[1892/4499] Val Loss:0.14492623507976532\n",
      "Epoch 0[1893/4499] Val Loss:0.1795717179775238\n",
      "Epoch 0[1894/4499] Val Loss:0.1507032811641693\n",
      "Epoch 0[1895/4499] Val Loss:0.1430986374616623\n",
      "Epoch 0[1896/4499] Val Loss:0.11773942410945892\n",
      "Epoch 0[1897/4499] Val Loss:0.11235041171312332\n",
      "Epoch 0[1898/4499] Val Loss:0.09225054830312729\n",
      "Epoch 0[1899/4499] Val Loss:0.16594398021697998\n",
      "Epoch 0[1900/4499] Val Loss:0.10946490615606308\n",
      "Epoch 0[1901/4499] Val Loss:0.12027399986982346\n",
      "Epoch 0[1902/4499] Val Loss:0.11393725126981735\n",
      "Epoch 0[1903/4499] Val Loss:0.1115613728761673\n",
      "Epoch 0[1904/4499] Val Loss:0.10633715242147446\n",
      "Epoch 0[1905/4499] Val Loss:0.11798375844955444\n",
      "Epoch 0[1906/4499] Val Loss:0.1188158318400383\n",
      "Epoch 0[1907/4499] Val Loss:0.10346847772598267\n",
      "Epoch 0[1908/4499] Val Loss:0.0854368805885315\n",
      "Epoch 0[1909/4499] Val Loss:0.09018837660551071\n",
      "Epoch 0[1910/4499] Val Loss:0.11593732982873917\n",
      "Epoch 0[1911/4499] Val Loss:0.10247471928596497\n",
      "Epoch 0[1912/4499] Val Loss:0.11117533594369888\n",
      "Epoch 0[1913/4499] Val Loss:0.11939723044633865\n",
      "Epoch 0[1914/4499] Val Loss:0.08571046590805054\n",
      "Epoch 0[1915/4499] Val Loss:0.11539456993341446\n",
      "Epoch 0[1916/4499] Val Loss:0.09381000697612762\n",
      "Epoch 0[1917/4499] Val Loss:0.10401736199855804\n",
      "Epoch 0[1918/4499] Val Loss:0.12288264185190201\n",
      "Epoch 0[1919/4499] Val Loss:0.11508245766162872\n",
      "Epoch 0[1920/4499] Val Loss:0.08929553627967834\n",
      "Epoch 0[1921/4499] Val Loss:0.10991241782903671\n",
      "Epoch 0[1922/4499] Val Loss:0.06769058108329773\n",
      "Epoch 0[1923/4499] Val Loss:0.11169026792049408\n",
      "Epoch 0[1924/4499] Val Loss:0.11990852653980255\n",
      "Epoch 0[1925/4499] Val Loss:0.1038120910525322\n",
      "Epoch 0[1926/4499] Val Loss:0.16341236233711243\n",
      "Epoch 0[1927/4499] Val Loss:0.09686441719532013\n",
      "Epoch 0[1928/4499] Val Loss:0.14897207915782928\n",
      "Epoch 0[1929/4499] Val Loss:0.08919768780469894\n",
      "Epoch 0[1930/4499] Val Loss:0.16419117152690887\n",
      "Epoch 0[1931/4499] Val Loss:0.08383630216121674\n",
      "Epoch 0[1932/4499] Val Loss:0.12609067559242249\n",
      "Epoch 0[1933/4499] Val Loss:0.07996593415737152\n",
      "Epoch 0[1934/4499] Val Loss:0.08732639998197556\n",
      "Epoch 0[1935/4499] Val Loss:0.11062846332788467\n",
      "Epoch 0[1936/4499] Val Loss:0.08281359076499939\n",
      "Epoch 0[1937/4499] Val Loss:0.09947505593299866\n",
      "Epoch 0[1938/4499] Val Loss:0.0803418755531311\n",
      "Epoch 0[1939/4499] Val Loss:0.10663805156946182\n",
      "Epoch 0[1940/4499] Val Loss:0.07951796054840088\n",
      "Epoch 0[1941/4499] Val Loss:0.07363070547580719\n",
      "Epoch 0[1942/4499] Val Loss:0.08342771977186203\n",
      "Epoch 0[1943/4499] Val Loss:0.08406028151512146\n",
      "Epoch 0[1944/4499] Val Loss:0.13595977425575256\n",
      "Epoch 0[1945/4499] Val Loss:0.08048317581415176\n",
      "Epoch 0[1946/4499] Val Loss:0.09575328230857849\n",
      "Epoch 0[1947/4499] Val Loss:0.08692729473114014\n",
      "Epoch 0[1948/4499] Val Loss:0.09762968868017197\n",
      "Epoch 0[1949/4499] Val Loss:0.08981746435165405\n",
      "Epoch 0[1950/4499] Val Loss:0.10190897434949875\n",
      "Epoch 0[1951/4499] Val Loss:0.08939804136753082\n",
      "Epoch 0[1952/4499] Val Loss:0.07268927246332169\n",
      "Epoch 0[1953/4499] Val Loss:0.08009655773639679\n",
      "Epoch 0[1954/4499] Val Loss:0.078995481133461\n",
      "Epoch 0[1955/4499] Val Loss:0.08328358829021454\n",
      "Epoch 0[1956/4499] Val Loss:0.10849429666996002\n",
      "Epoch 0[1957/4499] Val Loss:0.07960133999586105\n",
      "Epoch 0[1958/4499] Val Loss:0.1319914013147354\n",
      "Epoch 0[1959/4499] Val Loss:0.06659369170665741\n",
      "Epoch 0[1960/4499] Val Loss:0.11352004110813141\n",
      "Epoch 0[1961/4499] Val Loss:0.07030045241117477\n",
      "Epoch 0[1962/4499] Val Loss:0.1140998974442482\n",
      "Epoch 0[1963/4499] Val Loss:0.08268622308969498\n",
      "Epoch 0[1964/4499] Val Loss:0.1339545100927353\n",
      "Epoch 0[1965/4499] Val Loss:0.07392152398824692\n",
      "Epoch 0[1966/4499] Val Loss:0.16776832938194275\n",
      "Epoch 0[1967/4499] Val Loss:0.06696318089962006\n",
      "Epoch 0[1968/4499] Val Loss:0.14677459001541138\n",
      "Epoch 0[1969/4499] Val Loss:0.10540034621953964\n",
      "Epoch 0[1970/4499] Val Loss:0.12441040575504303\n",
      "Epoch 0[1971/4499] Val Loss:0.06802558898925781\n",
      "Epoch 0[1972/4499] Val Loss:0.14009462296962738\n",
      "Epoch 0[1973/4499] Val Loss:0.07456881552934647\n",
      "Epoch 0[1974/4499] Val Loss:0.08627951890230179\n",
      "Epoch 0[1975/4499] Val Loss:0.08264195919036865\n",
      "Epoch 0[1976/4499] Val Loss:0.062222205102443695\n",
      "Epoch 0[1977/4499] Val Loss:0.0839221179485321\n",
      "Epoch 0[1978/4499] Val Loss:0.100385881960392\n",
      "Epoch 0[1979/4499] Val Loss:0.07881437242031097\n",
      "Epoch 0[1980/4499] Val Loss:0.07769503444433212\n",
      "Epoch 0[1981/4499] Val Loss:0.09758104383945465\n",
      "Epoch 0[1982/4499] Val Loss:0.10867879539728165\n",
      "Epoch 0[1983/4499] Val Loss:0.06748081743717194\n",
      "Epoch 0[1984/4499] Val Loss:0.08313976228237152\n",
      "Epoch 0[1985/4499] Val Loss:0.07268670201301575\n",
      "Epoch 0[1986/4499] Val Loss:0.075498566031456\n",
      "Epoch 0[1987/4499] Val Loss:0.07410319149494171\n",
      "Epoch 0[1988/4499] Val Loss:0.07573878020048141\n",
      "Epoch 0[1989/4499] Val Loss:0.14539653062820435\n",
      "Epoch 0[1990/4499] Val Loss:0.09839126467704773\n",
      "Epoch 0[1991/4499] Val Loss:0.17768704891204834\n",
      "Epoch 0[1992/4499] Val Loss:0.15323404967784882\n",
      "Epoch 0[1993/4499] Val Loss:0.08742627501487732\n",
      "Epoch 0[1994/4499] Val Loss:0.17379237711429596\n",
      "Epoch 0[1995/4499] Val Loss:0.09687434881925583\n",
      "Epoch 0[1996/4499] Val Loss:0.17999376356601715\n",
      "Epoch 0[1997/4499] Val Loss:0.14342905580997467\n",
      "Epoch 0[1998/4499] Val Loss:0.2305629700422287\n",
      "Epoch 0[1999/4499] Val Loss:0.14502675831317902\n",
      "Epoch 0[2000/4499] Val Loss:0.1685238480567932\n",
      "Epoch 0[2001/4499] Val Loss:0.15667995810508728\n",
      "Epoch 0[2002/4499] Val Loss:0.09771023690700531\n",
      "Epoch 0[2003/4499] Val Loss:0.13319355249404907\n",
      "Epoch 0[2004/4499] Val Loss:0.09455125033855438\n",
      "Epoch 0[2005/4499] Val Loss:0.1355610340833664\n",
      "Epoch 0[2006/4499] Val Loss:0.15370792150497437\n",
      "Epoch 0[2007/4499] Val Loss:0.1289616972208023\n",
      "Epoch 0[2008/4499] Val Loss:0.15949873626232147\n",
      "Epoch 0[2009/4499] Val Loss:0.1347568929195404\n",
      "Epoch 0[2010/4499] Val Loss:0.16968314349651337\n",
      "Epoch 0[2011/4499] Val Loss:0.16928119957447052\n",
      "Epoch 0[2012/4499] Val Loss:0.1722380816936493\n",
      "Epoch 0[2013/4499] Val Loss:0.20851942896842957\n",
      "Epoch 0[2014/4499] Val Loss:0.18400968611240387\n",
      "Epoch 0[2015/4499] Val Loss:0.18184544146060944\n",
      "Epoch 0[2016/4499] Val Loss:0.22882042825222015\n",
      "Epoch 0[2017/4499] Val Loss:0.12059967964887619\n",
      "Epoch 0[2018/4499] Val Loss:0.22640423476696014\n",
      "Epoch 0[2019/4499] Val Loss:0.1389884054660797\n",
      "Epoch 0[2020/4499] Val Loss:0.25249573588371277\n",
      "Epoch 0[2021/4499] Val Loss:0.21188198029994965\n",
      "Epoch 0[2022/4499] Val Loss:0.3702552616596222\n",
      "Epoch 0[2023/4499] Val Loss:0.2640203535556793\n",
      "Epoch 0[2024/4499] Val Loss:0.24324482679367065\n",
      "Epoch 0[2025/4499] Val Loss:0.21634331345558167\n",
      "Epoch 0[2026/4499] Val Loss:0.22774526476860046\n",
      "Epoch 0[2027/4499] Val Loss:0.27663278579711914\n",
      "Epoch 0[2028/4499] Val Loss:0.2482939511537552\n",
      "Epoch 0[2029/4499] Val Loss:0.2995516359806061\n",
      "Epoch 0[2030/4499] Val Loss:0.29659149050712585\n",
      "Epoch 0[2031/4499] Val Loss:0.3140922784805298\n",
      "Epoch 0[2032/4499] Val Loss:0.3178618252277374\n",
      "Epoch 0[2033/4499] Val Loss:0.28868481516838074\n",
      "Epoch 0[2034/4499] Val Loss:0.32209908962249756\n",
      "Epoch 0[2035/4499] Val Loss:0.32595571875572205\n",
      "Epoch 0[2036/4499] Val Loss:0.40202832221984863\n",
      "Epoch 0[2037/4499] Val Loss:0.3739927113056183\n",
      "Epoch 0[2038/4499] Val Loss:0.3963640332221985\n",
      "Epoch 0[2039/4499] Val Loss:0.560086190700531\n",
      "Epoch 0[2040/4499] Val Loss:0.36736851930618286\n",
      "Epoch 0[2041/4499] Val Loss:0.47650325298309326\n",
      "Epoch 0[2042/4499] Val Loss:0.39233770966529846\n",
      "Epoch 0[2043/4499] Val Loss:0.4979632794857025\n",
      "Epoch 0[2044/4499] Val Loss:0.4740898013114929\n",
      "Epoch 0[2045/4499] Val Loss:0.4193194508552551\n",
      "Epoch 0[2046/4499] Val Loss:0.2975190579891205\n",
      "Epoch 0[2047/4499] Val Loss:0.4062102735042572\n",
      "Epoch 0[2048/4499] Val Loss:0.2864420711994171\n",
      "Epoch 0[2049/4499] Val Loss:0.3576945960521698\n",
      "Epoch 0[2050/4499] Val Loss:0.2544774115085602\n",
      "Epoch 0[2051/4499] Val Loss:0.49553269147872925\n",
      "Epoch 0[2052/4499] Val Loss:0.4417381286621094\n",
      "Epoch 0[2053/4499] Val Loss:0.47502633929252625\n",
      "Epoch 0[2054/4499] Val Loss:0.5806923508644104\n",
      "Epoch 0[2055/4499] Val Loss:0.34969744086265564\n",
      "Epoch 0[2056/4499] Val Loss:0.6204522848129272\n",
      "Epoch 0[2057/4499] Val Loss:0.506418764591217\n",
      "Epoch 0[2058/4499] Val Loss:0.5513737201690674\n",
      "Epoch 0[2059/4499] Val Loss:0.6628005504608154\n",
      "Epoch 0[2060/4499] Val Loss:0.6823499798774719\n",
      "Epoch 0[2061/4499] Val Loss:0.5765734910964966\n",
      "Epoch 0[2062/4499] Val Loss:0.5981308817863464\n",
      "Epoch 0[2063/4499] Val Loss:0.4067358374595642\n",
      "Epoch 0[2064/4499] Val Loss:0.5867224335670471\n",
      "Epoch 0[2065/4499] Val Loss:0.4907447397708893\n",
      "Epoch 0[2066/4499] Val Loss:0.5698209404945374\n",
      "Epoch 0[2067/4499] Val Loss:0.7243468761444092\n",
      "Epoch 0[2068/4499] Val Loss:0.6752657294273376\n",
      "Epoch 0[2069/4499] Val Loss:0.5998573303222656\n",
      "Epoch 0[2070/4499] Val Loss:0.7049250602722168\n",
      "Epoch 0[2071/4499] Val Loss:0.5633048415184021\n",
      "Epoch 0[2072/4499] Val Loss:0.6967272162437439\n",
      "Epoch 0[2073/4499] Val Loss:0.644997239112854\n",
      "Epoch 0[2074/4499] Val Loss:0.7677886486053467\n",
      "Epoch 0[2075/4499] Val Loss:0.7818607687950134\n",
      "Epoch 0[2076/4499] Val Loss:0.6077767014503479\n",
      "Epoch 0[2077/4499] Val Loss:0.7135136127471924\n",
      "Epoch 0[2078/4499] Val Loss:0.605050802230835\n",
      "Epoch 0[2079/4499] Val Loss:0.7435787916183472\n",
      "Epoch 0[2080/4499] Val Loss:0.6822493672370911\n",
      "Epoch 0[2081/4499] Val Loss:0.7291799187660217\n",
      "Epoch 0[2082/4499] Val Loss:0.8777538537979126\n",
      "Epoch 0[2083/4499] Val Loss:0.7923820614814758\n",
      "Epoch 0[2084/4499] Val Loss:1.053400993347168\n",
      "Epoch 0[2085/4499] Val Loss:0.7365168333053589\n",
      "Epoch 0[2086/4499] Val Loss:1.0418543815612793\n",
      "Epoch 0[2087/4499] Val Loss:0.903974175453186\n",
      "Epoch 0[2088/4499] Val Loss:1.0730711221694946\n",
      "Epoch 0[2089/4499] Val Loss:1.0436912775039673\n",
      "Epoch 0[2090/4499] Val Loss:0.6735800504684448\n",
      "Epoch 0[2091/4499] Val Loss:1.057334065437317\n",
      "Epoch 0[2092/4499] Val Loss:0.6309791803359985\n",
      "Epoch 0[2093/4499] Val Loss:0.8379920125007629\n",
      "Epoch 0[2094/4499] Val Loss:0.8857643604278564\n",
      "Epoch 0[2095/4499] Val Loss:0.5267934799194336\n",
      "Epoch 0[2096/4499] Val Loss:0.8602752089500427\n",
      "Epoch 0[2097/4499] Val Loss:0.7038843035697937\n",
      "Epoch 0[2098/4499] Val Loss:0.6639460325241089\n",
      "Epoch 0[2099/4499] Val Loss:0.9861968755722046\n",
      "Epoch 0[2100/4499] Val Loss:0.5336161851882935\n",
      "Epoch 0[2101/4499] Val Loss:0.6362541913986206\n",
      "Epoch 0[2102/4499] Val Loss:0.968151330947876\n",
      "Epoch 0[2103/4499] Val Loss:0.6121437549591064\n",
      "Epoch 0[2104/4499] Val Loss:0.69403475522995\n",
      "Epoch 0[2105/4499] Val Loss:0.6467836499214172\n",
      "Epoch 0[2106/4499] Val Loss:0.7859280109405518\n",
      "Epoch 0[2107/4499] Val Loss:0.9656512141227722\n",
      "Epoch 0[2108/4499] Val Loss:0.6791519522666931\n",
      "Epoch 0[2109/4499] Val Loss:0.8676644563674927\n",
      "Epoch 0[2110/4499] Val Loss:0.7879668474197388\n",
      "Epoch 0[2111/4499] Val Loss:0.8308577537536621\n",
      "Epoch 0[2112/4499] Val Loss:0.6453877091407776\n",
      "Epoch 0[2113/4499] Val Loss:0.7791844010353088\n",
      "Epoch 0[2114/4499] Val Loss:0.8683148622512817\n",
      "Epoch 0[2115/4499] Val Loss:0.8791478276252747\n",
      "Epoch 0[2116/4499] Val Loss:0.7065525054931641\n",
      "Epoch 0[2117/4499] Val Loss:0.6547620892524719\n",
      "Epoch 0[2118/4499] Val Loss:0.5521091222763062\n",
      "Epoch 0[2119/4499] Val Loss:0.38970866799354553\n",
      "Epoch 0[2120/4499] Val Loss:0.4547310173511505\n",
      "Epoch 0[2121/4499] Val Loss:0.23235808312892914\n",
      "Epoch 0[2122/4499] Val Loss:0.09087280929088593\n",
      "Epoch 0[2123/4499] Val Loss:0.09136165678501129\n",
      "Epoch 0[2124/4499] Val Loss:0.1265905201435089\n",
      "Epoch 0[2125/4499] Val Loss:0.14398403465747833\n",
      "Epoch 0[2126/4499] Val Loss:0.13673394918441772\n",
      "Epoch 0[2127/4499] Val Loss:0.13860023021697998\n",
      "Epoch 0[2128/4499] Val Loss:0.1642649620771408\n",
      "Epoch 0[2129/4499] Val Loss:0.11683093011379242\n",
      "Epoch 0[2130/4499] Val Loss:0.09155469387769699\n",
      "Epoch 0[2131/4499] Val Loss:0.07407022267580032\n",
      "Epoch 0[2132/4499] Val Loss:0.11323563754558563\n",
      "Epoch 0[2133/4499] Val Loss:0.09056359529495239\n",
      "Epoch 0[2134/4499] Val Loss:0.09510248899459839\n",
      "Epoch 0[2135/4499] Val Loss:0.13612419366836548\n",
      "Epoch 0[2136/4499] Val Loss:0.09860814362764359\n",
      "Epoch 0[2137/4499] Val Loss:0.33522582054138184\n",
      "Epoch 0[2138/4499] Val Loss:0.26183366775512695\n",
      "Epoch 0[2139/4499] Val Loss:0.12540484964847565\n",
      "Epoch 0[2140/4499] Val Loss:0.09528475999832153\n",
      "Epoch 0[2141/4499] Val Loss:0.2212403118610382\n",
      "Epoch 0[2142/4499] Val Loss:0.1821102648973465\n",
      "Epoch 0[2143/4499] Val Loss:0.42224690318107605\n",
      "Epoch 0[2144/4499] Val Loss:0.3790085017681122\n",
      "Epoch 0[2145/4499] Val Loss:0.41837674379348755\n",
      "Epoch 0[2146/4499] Val Loss:0.3862701654434204\n",
      "Epoch 0[2147/4499] Val Loss:0.4207199811935425\n",
      "Epoch 0[2148/4499] Val Loss:0.5422409176826477\n",
      "Epoch 0[2149/4499] Val Loss:0.30320316553115845\n",
      "Epoch 0[2150/4499] Val Loss:0.23574431240558624\n",
      "Epoch 0[2151/4499] Val Loss:0.3931072950363159\n",
      "Epoch 0[2152/4499] Val Loss:0.40879708528518677\n",
      "Epoch 0[2153/4499] Val Loss:0.5263058543205261\n",
      "Epoch 0[2154/4499] Val Loss:0.29877424240112305\n",
      "Epoch 0[2155/4499] Val Loss:0.3673318028450012\n",
      "Epoch 0[2156/4499] Val Loss:0.496998131275177\n",
      "Epoch 0[2157/4499] Val Loss:0.4223434627056122\n",
      "Epoch 0[2158/4499] Val Loss:0.5570471882820129\n",
      "Epoch 0[2159/4499] Val Loss:0.5117619633674622\n",
      "Epoch 0[2160/4499] Val Loss:0.4947716295719147\n",
      "Epoch 0[2161/4499] Val Loss:0.44512084126472473\n",
      "Epoch 0[2162/4499] Val Loss:0.3914380967617035\n",
      "Epoch 0[2163/4499] Val Loss:0.3859991431236267\n",
      "Epoch 0[2164/4499] Val Loss:0.42266905307769775\n",
      "Epoch 0[2165/4499] Val Loss:0.25012028217315674\n",
      "Epoch 0[2166/4499] Val Loss:0.33426612615585327\n",
      "Epoch 0[2167/4499] Val Loss:0.3528996407985687\n",
      "Epoch 0[2168/4499] Val Loss:0.40928351879119873\n",
      "Epoch 0[2169/4499] Val Loss:0.36552736163139343\n",
      "Epoch 0[2170/4499] Val Loss:0.4285484552383423\n",
      "Epoch 0[2171/4499] Val Loss:0.5100213289260864\n",
      "Epoch 0[2172/4499] Val Loss:0.30655771493911743\n",
      "Epoch 0[2173/4499] Val Loss:0.3135918974876404\n",
      "Epoch 0[2174/4499] Val Loss:0.2703436017036438\n",
      "Epoch 0[2175/4499] Val Loss:0.30319905281066895\n",
      "Epoch 0[2176/4499] Val Loss:0.2711736559867859\n",
      "Epoch 0[2177/4499] Val Loss:0.38124194741249084\n",
      "Epoch 0[2178/4499] Val Loss:0.31316500902175903\n",
      "Epoch 0[2179/4499] Val Loss:0.4223087430000305\n",
      "Epoch 0[2180/4499] Val Loss:0.20950442552566528\n",
      "Epoch 0[2181/4499] Val Loss:0.1388247162103653\n",
      "Epoch 0[2182/4499] Val Loss:0.14896062016487122\n",
      "Epoch 0[2183/4499] Val Loss:0.25711682438850403\n",
      "Epoch 0[2184/4499] Val Loss:0.319408118724823\n",
      "Epoch 0[2185/4499] Val Loss:0.2689432203769684\n",
      "Epoch 0[2186/4499] Val Loss:0.19993415474891663\n",
      "Epoch 0[2187/4499] Val Loss:0.21288171410560608\n",
      "Epoch 0[2188/4499] Val Loss:0.24165762960910797\n",
      "Epoch 0[2189/4499] Val Loss:0.2628946304321289\n",
      "Epoch 0[2190/4499] Val Loss:0.2693353295326233\n",
      "Epoch 0[2191/4499] Val Loss:0.2311887890100479\n",
      "Epoch 0[2192/4499] Val Loss:0.26285192370414734\n",
      "Epoch 0[2193/4499] Val Loss:0.27939456701278687\n",
      "Epoch 0[2194/4499] Val Loss:0.2655477225780487\n",
      "Epoch 0[2195/4499] Val Loss:0.328486829996109\n",
      "Epoch 0[2196/4499] Val Loss:0.2356557697057724\n",
      "Epoch 0[2197/4499] Val Loss:0.20317454636096954\n",
      "Epoch 0[2198/4499] Val Loss:0.16672417521476746\n",
      "Epoch 0[2199/4499] Val Loss:0.07366827130317688\n",
      "Epoch 0[2200/4499] Val Loss:0.06972242891788483\n",
      "Epoch 0[2201/4499] Val Loss:0.07761014997959137\n",
      "Epoch 0[2202/4499] Val Loss:0.0676145851612091\n",
      "Epoch 0[2203/4499] Val Loss:0.06784195452928543\n",
      "Epoch 0[2204/4499] Val Loss:0.07486148178577423\n",
      "Epoch 0[2205/4499] Val Loss:0.048940420150756836\n",
      "Epoch 0[2206/4499] Val Loss:0.06458619982004166\n",
      "Epoch 0[2207/4499] Val Loss:0.061398137360811234\n",
      "Epoch 0[2208/4499] Val Loss:0.05956409499049187\n",
      "Epoch 0[2209/4499] Val Loss:0.062208060175180435\n",
      "Epoch 0[2210/4499] Val Loss:0.06826372444629669\n",
      "Epoch 0[2211/4499] Val Loss:0.06377677619457245\n",
      "Epoch 0[2212/4499] Val Loss:0.09044012427330017\n",
      "Epoch 0[2213/4499] Val Loss:0.09937158972024918\n",
      "Epoch 0[2214/4499] Val Loss:0.08922605961561203\n",
      "Epoch 0[2215/4499] Val Loss:0.1121034249663353\n",
      "Epoch 0[2216/4499] Val Loss:0.07963833212852478\n",
      "Epoch 0[2217/4499] Val Loss:0.061483725905418396\n",
      "Epoch 0[2218/4499] Val Loss:0.06523044407367706\n",
      "Epoch 0[2219/4499] Val Loss:0.06401406228542328\n",
      "Epoch 0[2220/4499] Val Loss:0.06512793898582458\n",
      "Epoch 0[2221/4499] Val Loss:0.06752987205982208\n",
      "Epoch 0[2222/4499] Val Loss:0.07459431141614914\n",
      "Epoch 0[2223/4499] Val Loss:0.10554128885269165\n",
      "Epoch 0[2224/4499] Val Loss:0.05982544645667076\n",
      "Epoch 0[2225/4499] Val Loss:0.09026175737380981\n",
      "Epoch 0[2226/4499] Val Loss:0.08339221030473709\n",
      "Epoch 0[2227/4499] Val Loss:0.07460449635982513\n",
      "Epoch 0[2228/4499] Val Loss:0.07595618069171906\n",
      "Epoch 0[2229/4499] Val Loss:0.09241977334022522\n",
      "Epoch 0[2230/4499] Val Loss:0.14645475149154663\n",
      "Epoch 0[2231/4499] Val Loss:0.16240337491035461\n",
      "Epoch 0[2232/4499] Val Loss:0.1698962152004242\n",
      "Epoch 0[2233/4499] Val Loss:0.1865980178117752\n",
      "Epoch 0[2234/4499] Val Loss:0.2754514813423157\n",
      "Epoch 0[2235/4499] Val Loss:0.20717760920524597\n",
      "Epoch 0[2236/4499] Val Loss:0.22766272723674774\n",
      "Epoch 0[2237/4499] Val Loss:0.1749061644077301\n",
      "Epoch 0[2238/4499] Val Loss:0.21277952194213867\n",
      "Epoch 0[2239/4499] Val Loss:0.1948058009147644\n",
      "Epoch 0[2240/4499] Val Loss:0.21711264550685883\n",
      "Epoch 0[2241/4499] Val Loss:0.23717869818210602\n",
      "Epoch 0[2242/4499] Val Loss:0.34108996391296387\n",
      "Epoch 0[2243/4499] Val Loss:0.35763195157051086\n",
      "Epoch 0[2244/4499] Val Loss:0.3570294976234436\n",
      "Epoch 0[2245/4499] Val Loss:0.327609121799469\n",
      "Epoch 0[2246/4499] Val Loss:0.3750244677066803\n",
      "Epoch 0[2247/4499] Val Loss:0.5599762797355652\n",
      "Epoch 0[2248/4499] Val Loss:0.42849141359329224\n",
      "Epoch 0[2249/4499] Val Loss:0.42511633038520813\n",
      "Epoch 0[2250/4499] Val Loss:0.42786261439323425\n",
      "Epoch 0[2251/4499] Val Loss:0.4950140118598938\n",
      "Epoch 0[2252/4499] Val Loss:0.5411633253097534\n",
      "Epoch 0[2253/4499] Val Loss:0.2657656669616699\n",
      "Epoch 0[2254/4499] Val Loss:0.2527395188808441\n",
      "Epoch 0[2255/4499] Val Loss:0.19885383546352386\n",
      "Epoch 0[2256/4499] Val Loss:0.17259429395198822\n",
      "Epoch 0[2257/4499] Val Loss:0.1928359419107437\n",
      "Epoch 0[2258/4499] Val Loss:0.2320614904165268\n",
      "Epoch 0[2259/4499] Val Loss:0.2372129112482071\n",
      "Epoch 0[2260/4499] Val Loss:0.17231670022010803\n",
      "Epoch 0[2261/4499] Val Loss:0.12997889518737793\n",
      "Epoch 0[2262/4499] Val Loss:0.16471204161643982\n",
      "Epoch 0[2263/4499] Val Loss:0.1400575041770935\n",
      "Epoch 0[2264/4499] Val Loss:0.17702257633209229\n",
      "Epoch 0[2265/4499] Val Loss:0.18990199267864227\n",
      "Epoch 0[2266/4499] Val Loss:0.38079795241355896\n",
      "Epoch 0[2267/4499] Val Loss:0.5442931056022644\n",
      "Epoch 0[2268/4499] Val Loss:0.5003636479377747\n",
      "Epoch 0[2269/4499] Val Loss:0.4829348623752594\n",
      "Epoch 0[2270/4499] Val Loss:0.4953262209892273\n",
      "Epoch 0[2271/4499] Val Loss:0.5376814603805542\n",
      "Epoch 0[2272/4499] Val Loss:0.5403714776039124\n",
      "Epoch 0[2273/4499] Val Loss:0.4612821042537689\n",
      "Epoch 0[2274/4499] Val Loss:0.4925149083137512\n",
      "Epoch 0[2275/4499] Val Loss:0.5547276735305786\n",
      "Epoch 0[2276/4499] Val Loss:0.4696342945098877\n",
      "Epoch 0[2277/4499] Val Loss:0.5466657876968384\n",
      "Epoch 0[2278/4499] Val Loss:0.5136331915855408\n",
      "Epoch 0[2279/4499] Val Loss:0.4831450879573822\n",
      "Epoch 0[2280/4499] Val Loss:0.4777553081512451\n",
      "Epoch 0[2281/4499] Val Loss:0.4339837431907654\n",
      "Epoch 0[2282/4499] Val Loss:0.7212092280387878\n",
      "Epoch 0[2283/4499] Val Loss:0.4934011995792389\n",
      "Epoch 0[2284/4499] Val Loss:0.5152261853218079\n",
      "Epoch 0[2285/4499] Val Loss:0.4197751581668854\n",
      "Epoch 0[2286/4499] Val Loss:0.25748980045318604\n",
      "Epoch 0[2287/4499] Val Loss:0.19232751429080963\n",
      "Epoch 0[2288/4499] Val Loss:0.16179464757442474\n",
      "Epoch 0[2289/4499] Val Loss:0.14724968373775482\n",
      "Epoch 0[2290/4499] Val Loss:0.20739871263504028\n",
      "Epoch 0[2291/4499] Val Loss:0.1993928700685501\n",
      "Epoch 0[2292/4499] Val Loss:0.20602209866046906\n",
      "Epoch 0[2293/4499] Val Loss:0.1946350485086441\n",
      "Epoch 0[2294/4499] Val Loss:0.1753365844488144\n",
      "Epoch 0[2295/4499] Val Loss:0.17791138589382172\n",
      "Epoch 0[2296/4499] Val Loss:0.16491229832172394\n",
      "Epoch 0[2297/4499] Val Loss:0.14376690983772278\n",
      "Epoch 0[2298/4499] Val Loss:0.14879770576953888\n",
      "Epoch 0[2299/4499] Val Loss:0.1373833864927292\n",
      "Epoch 0[2300/4499] Val Loss:0.13322822749614716\n",
      "Epoch 0[2301/4499] Val Loss:0.11612788587808609\n",
      "Epoch 0[2302/4499] Val Loss:0.18083101511001587\n",
      "Epoch 0[2303/4499] Val Loss:0.2613717317581177\n",
      "Epoch 0[2304/4499] Val Loss:0.3065115511417389\n",
      "Epoch 0[2305/4499] Val Loss:0.23849807679653168\n",
      "Epoch 0[2306/4499] Val Loss:0.21093913912773132\n",
      "Epoch 0[2307/4499] Val Loss:0.20420078933238983\n",
      "Epoch 0[2308/4499] Val Loss:0.24306364357471466\n",
      "Epoch 0[2309/4499] Val Loss:0.2932262122631073\n",
      "Epoch 0[2310/4499] Val Loss:0.2396518886089325\n",
      "Epoch 0[2311/4499] Val Loss:0.25332239270210266\n",
      "Epoch 0[2312/4499] Val Loss:0.26370105147361755\n",
      "Epoch 0[2313/4499] Val Loss:0.19764944911003113\n",
      "Epoch 0[2314/4499] Val Loss:0.2069033682346344\n",
      "Epoch 0[2315/4499] Val Loss:0.1769392192363739\n",
      "Epoch 0[2316/4499] Val Loss:0.20156651735305786\n",
      "Epoch 0[2317/4499] Val Loss:0.16221760213375092\n",
      "Epoch 0[2318/4499] Val Loss:0.21482998132705688\n",
      "Epoch 0[2319/4499] Val Loss:0.24304652214050293\n",
      "Epoch 0[2320/4499] Val Loss:0.26495495438575745\n",
      "Epoch 0[2321/4499] Val Loss:0.21779271960258484\n",
      "Epoch 0[2322/4499] Val Loss:0.22590371966362\n",
      "Epoch 0[2323/4499] Val Loss:0.18273422122001648\n",
      "Epoch 0[2324/4499] Val Loss:0.2539708614349365\n",
      "Epoch 0[2325/4499] Val Loss:0.3693537414073944\n",
      "Epoch 0[2326/4499] Val Loss:0.5273010730743408\n",
      "Epoch 0[2327/4499] Val Loss:0.3738788962364197\n",
      "Epoch 0[2328/4499] Val Loss:0.24971681833267212\n",
      "Epoch 0[2329/4499] Val Loss:0.12332778424024582\n",
      "Epoch 0[2330/4499] Val Loss:0.13760870695114136\n",
      "Epoch 0[2331/4499] Val Loss:0.05799999460577965\n",
      "Epoch 0[2332/4499] Val Loss:0.07109368592500687\n",
      "Epoch 0[2333/4499] Val Loss:0.07214239984750748\n",
      "Epoch 0[2334/4499] Val Loss:0.0980013832449913\n",
      "Epoch 0[2335/4499] Val Loss:0.101777084171772\n",
      "Epoch 0[2336/4499] Val Loss:0.09332605451345444\n",
      "Epoch 0[2337/4499] Val Loss:0.08948488533496857\n",
      "Epoch 0[2338/4499] Val Loss:0.07732362300157547\n",
      "Epoch 0[2339/4499] Val Loss:0.09512101113796234\n",
      "Epoch 0[2340/4499] Val Loss:0.10162685066461563\n",
      "Epoch 0[2341/4499] Val Loss:0.08362598717212677\n",
      "Epoch 0[2342/4499] Val Loss:0.12399112433195114\n",
      "Epoch 0[2343/4499] Val Loss:0.16700641810894012\n",
      "Epoch 0[2344/4499] Val Loss:0.34922054409980774\n",
      "Epoch 0[2345/4499] Val Loss:0.33552274107933044\n",
      "Epoch 0[2346/4499] Val Loss:0.4373392164707184\n",
      "Epoch 0[2347/4499] Val Loss:0.3944109380245209\n",
      "Epoch 0[2348/4499] Val Loss:0.43377000093460083\n",
      "Epoch 0[2349/4499] Val Loss:0.4812677502632141\n",
      "Epoch 0[2350/4499] Val Loss:0.44268250465393066\n",
      "Epoch 0[2351/4499] Val Loss:0.3653368055820465\n",
      "Epoch 0[2352/4499] Val Loss:0.4354721009731293\n",
      "Epoch 0[2353/4499] Val Loss:0.569525957107544\n",
      "Epoch 0[2354/4499] Val Loss:0.5540115237236023\n",
      "Epoch 0[2355/4499] Val Loss:0.6132429242134094\n",
      "Epoch 0[2356/4499] Val Loss:0.6206611394882202\n",
      "Epoch 0[2357/4499] Val Loss:0.48683542013168335\n",
      "Epoch 0[2358/4499] Val Loss:0.41230908036231995\n",
      "Epoch 0[2359/4499] Val Loss:0.36781513690948486\n",
      "Epoch 0[2360/4499] Val Loss:0.3320777118206024\n",
      "Epoch 0[2361/4499] Val Loss:0.38592684268951416\n",
      "Epoch 0[2362/4499] Val Loss:0.36116349697113037\n",
      "Epoch 0[2363/4499] Val Loss:0.3784496486186981\n",
      "Epoch 0[2364/4499] Val Loss:0.4493826627731323\n",
      "Epoch 0[2365/4499] Val Loss:0.37829580903053284\n",
      "Epoch 0[2366/4499] Val Loss:0.2666189968585968\n",
      "Epoch 0[2367/4499] Val Loss:0.35227030515670776\n",
      "Epoch 0[2368/4499] Val Loss:0.392881840467453\n",
      "Epoch 0[2369/4499] Val Loss:0.37443578243255615\n",
      "Epoch 0[2370/4499] Val Loss:0.1349898725748062\n",
      "Epoch 0[2371/4499] Val Loss:0.25686681270599365\n",
      "Epoch 0[2372/4499] Val Loss:0.34754303097724915\n",
      "Epoch 0[2373/4499] Val Loss:0.3521944284439087\n",
      "Epoch 0[2374/4499] Val Loss:0.28500238060951233\n",
      "Epoch 0[2375/4499] Val Loss:0.3104976415634155\n",
      "Epoch 0[2376/4499] Val Loss:0.32060614228248596\n",
      "Epoch 0[2377/4499] Val Loss:0.3540169298648834\n",
      "Epoch 0[2378/4499] Val Loss:0.34414979815483093\n",
      "Epoch 0[2379/4499] Val Loss:0.2333463877439499\n",
      "Epoch 0[2380/4499] Val Loss:0.2139550745487213\n",
      "Epoch 0[2381/4499] Val Loss:0.32401084899902344\n",
      "Epoch 0[2382/4499] Val Loss:0.37724488973617554\n",
      "Epoch 0[2383/4499] Val Loss:0.4117465615272522\n",
      "Epoch 0[2384/4499] Val Loss:0.36196550726890564\n",
      "Epoch 0[2385/4499] Val Loss:0.30762219429016113\n",
      "Epoch 0[2386/4499] Val Loss:0.2589191794395447\n",
      "Epoch 0[2387/4499] Val Loss:0.27184048295021057\n",
      "Epoch 0[2388/4499] Val Loss:0.3571987450122833\n",
      "Epoch 0[2389/4499] Val Loss:0.4132627546787262\n",
      "Epoch 0[2390/4499] Val Loss:0.3331867456436157\n",
      "Epoch 0[2391/4499] Val Loss:0.2527639865875244\n",
      "Epoch 0[2392/4499] Val Loss:0.2556861340999603\n",
      "Epoch 0[2393/4499] Val Loss:0.2919859290122986\n",
      "Epoch 0[2394/4499] Val Loss:0.29881078004837036\n",
      "Epoch 0[2395/4499] Val Loss:0.22544720768928528\n",
      "Epoch 0[2396/4499] Val Loss:0.14961431920528412\n",
      "Epoch 0[2397/4499] Val Loss:0.22143569588661194\n",
      "Epoch 0[2398/4499] Val Loss:0.20843175053596497\n",
      "Epoch 0[2399/4499] Val Loss:0.24540574848651886\n",
      "Epoch 0[2400/4499] Val Loss:0.31146278977394104\n",
      "Epoch 0[2401/4499] Val Loss:0.33165326714515686\n",
      "Epoch 0[2402/4499] Val Loss:0.35598039627075195\n",
      "Epoch 0[2403/4499] Val Loss:0.3949800431728363\n",
      "Epoch 0[2404/4499] Val Loss:0.15781162679195404\n",
      "Epoch 0[2405/4499] Val Loss:0.38683512806892395\n",
      "Epoch 0[2406/4499] Val Loss:0.4108113646507263\n",
      "Epoch 0[2407/4499] Val Loss:0.46074843406677246\n",
      "Epoch 0[2408/4499] Val Loss:0.43818387389183044\n",
      "Epoch 0[2409/4499] Val Loss:0.2086205631494522\n",
      "Epoch 0[2410/4499] Val Loss:0.4434939920902252\n",
      "Epoch 0[2411/4499] Val Loss:0.4686179757118225\n",
      "Epoch 0[2412/4499] Val Loss:0.4174123704433441\n",
      "Epoch 0[2413/4499] Val Loss:0.29922452569007874\n",
      "Epoch 0[2414/4499] Val Loss:0.1881408989429474\n",
      "Epoch 0[2415/4499] Val Loss:0.3178383708000183\n",
      "Epoch 0[2416/4499] Val Loss:0.4115836024284363\n",
      "Epoch 0[2417/4499] Val Loss:0.18693074584007263\n",
      "Epoch 0[2418/4499] Val Loss:0.2285328209400177\n",
      "Epoch 0[2419/4499] Val Loss:0.38260817527770996\n",
      "Epoch 0[2420/4499] Val Loss:0.4746581017971039\n",
      "Epoch 0[2421/4499] Val Loss:0.114106185734272\n",
      "Epoch 0[2422/4499] Val Loss:0.4073958992958069\n",
      "Epoch 0[2423/4499] Val Loss:0.4318557381629944\n",
      "Epoch 0[2424/4499] Val Loss:0.36284711956977844\n",
      "Epoch 0[2425/4499] Val Loss:0.11383391916751862\n",
      "Epoch 0[2426/4499] Val Loss:0.07024697214365005\n",
      "Epoch 0[2427/4499] Val Loss:0.07043972611427307\n",
      "Epoch 0[2428/4499] Val Loss:0.08479823917150497\n",
      "Epoch 0[2429/4499] Val Loss:0.06594845652580261\n",
      "Epoch 0[2430/4499] Val Loss:0.06952053308486938\n",
      "Epoch 0[2431/4499] Val Loss:0.07054213434457779\n",
      "Epoch 0[2432/4499] Val Loss:0.10460491478443146\n",
      "Epoch 0[2433/4499] Val Loss:0.09675998240709305\n",
      "Epoch 0[2434/4499] Val Loss:0.0981956422328949\n",
      "Epoch 0[2435/4499] Val Loss:0.08380056172609329\n",
      "Epoch 0[2436/4499] Val Loss:0.08433900028467178\n",
      "Epoch 0[2437/4499] Val Loss:0.12042917311191559\n",
      "Epoch 0[2438/4499] Val Loss:0.09831617772579193\n",
      "Epoch 0[2439/4499] Val Loss:0.0988045409321785\n",
      "Epoch 0[2440/4499] Val Loss:0.08715297281742096\n",
      "Epoch 0[2441/4499] Val Loss:0.11101622134447098\n",
      "Epoch 0[2442/4499] Val Loss:0.0912080928683281\n",
      "Epoch 0[2443/4499] Val Loss:0.09903505444526672\n",
      "Epoch 0[2444/4499] Val Loss:0.07960959523916245\n",
      "Epoch 0[2445/4499] Val Loss:0.0885014683008194\n",
      "Epoch 0[2446/4499] Val Loss:0.07351180911064148\n",
      "Epoch 0[2447/4499] Val Loss:0.09670968353748322\n",
      "Epoch 0[2448/4499] Val Loss:0.10846555978059769\n",
      "Epoch 0[2449/4499] Val Loss:0.1239490956068039\n",
      "Epoch 0[2450/4499] Val Loss:0.14721566438674927\n",
      "Epoch 0[2451/4499] Val Loss:0.1565685123205185\n",
      "Epoch 0[2452/4499] Val Loss:0.12080325186252594\n",
      "Epoch 0[2453/4499] Val Loss:0.06137993931770325\n",
      "Epoch 0[2454/4499] Val Loss:0.07076350599527359\n",
      "Epoch 0[2455/4499] Val Loss:0.09331710636615753\n",
      "Epoch 0[2456/4499] Val Loss:0.131547212600708\n",
      "Epoch 0[2457/4499] Val Loss:0.10658696293830872\n",
      "Epoch 0[2458/4499] Val Loss:0.11289273947477341\n",
      "Epoch 0[2459/4499] Val Loss:0.10350839048624039\n",
      "Epoch 0[2460/4499] Val Loss:0.08157593756914139\n",
      "Epoch 0[2461/4499] Val Loss:0.08825600147247314\n",
      "Epoch 0[2462/4499] Val Loss:0.07546636462211609\n",
      "Epoch 0[2463/4499] Val Loss:0.09873388707637787\n",
      "Epoch 0[2464/4499] Val Loss:0.08946022391319275\n",
      "Epoch 0[2465/4499] Val Loss:0.11428200453519821\n",
      "Epoch 0[2466/4499] Val Loss:0.09152887016534805\n",
      "Epoch 0[2467/4499] Val Loss:0.1806592345237732\n",
      "Epoch 0[2468/4499] Val Loss:0.16019698977470398\n",
      "Epoch 0[2469/4499] Val Loss:0.12494570016860962\n",
      "Epoch 0[2470/4499] Val Loss:0.17994734644889832\n",
      "Epoch 0[2471/4499] Val Loss:0.1596001535654068\n",
      "Epoch 0[2472/4499] Val Loss:0.251394122838974\n",
      "Epoch 0[2473/4499] Val Loss:0.18199388682842255\n",
      "Epoch 0[2474/4499] Val Loss:0.22230102121829987\n",
      "Epoch 0[2475/4499] Val Loss:0.15902356803417206\n",
      "Epoch 0[2476/4499] Val Loss:0.12852142751216888\n",
      "Epoch 0[2477/4499] Val Loss:0.1334458738565445\n",
      "Epoch 0[2478/4499] Val Loss:0.12160447984933853\n",
      "Epoch 0[2479/4499] Val Loss:0.14789824187755585\n",
      "Epoch 0[2480/4499] Val Loss:0.1985502392053604\n",
      "Epoch 0[2481/4499] Val Loss:0.15170955657958984\n",
      "Epoch 0[2482/4499] Val Loss:0.22984562814235687\n",
      "Epoch 0[2483/4499] Val Loss:0.11383593082427979\n",
      "Epoch 0[2484/4499] Val Loss:0.21703781187534332\n",
      "Epoch 0[2485/4499] Val Loss:0.2493613362312317\n",
      "Epoch 0[2486/4499] Val Loss:0.2514438033103943\n",
      "Epoch 0[2487/4499] Val Loss:0.3362998366355896\n",
      "Epoch 0[2488/4499] Val Loss:0.2277861386537552\n",
      "Epoch 0[2489/4499] Val Loss:0.3056598901748657\n",
      "Epoch 0[2490/4499] Val Loss:0.4262712895870209\n",
      "Epoch 0[2491/4499] Val Loss:0.295319527387619\n",
      "Epoch 0[2492/4499] Val Loss:0.456382155418396\n",
      "Epoch 0[2493/4499] Val Loss:0.3469140827655792\n",
      "Epoch 0[2494/4499] Val Loss:0.448599636554718\n",
      "Epoch 0[2495/4499] Val Loss:0.42889922857284546\n",
      "Epoch 0[2496/4499] Val Loss:0.3220910131931305\n",
      "Epoch 0[2497/4499] Val Loss:0.4750070869922638\n",
      "Epoch 0[2498/4499] Val Loss:0.28848183155059814\n",
      "Epoch 0[2499/4499] Val Loss:0.5644361972808838\n",
      "Epoch 0[2500/4499] Val Loss:0.5634353756904602\n",
      "Epoch 0[2501/4499] Val Loss:0.2964279055595398\n",
      "Epoch 0[2502/4499] Val Loss:0.462600976228714\n",
      "Epoch 0[2503/4499] Val Loss:0.3996380567550659\n",
      "Epoch 0[2504/4499] Val Loss:0.7870050668716431\n",
      "Epoch 0[2505/4499] Val Loss:0.730776846408844\n",
      "Epoch 0[2506/4499] Val Loss:0.2678707540035248\n",
      "Epoch 0[2507/4499] Val Loss:0.08519912511110306\n",
      "Epoch 0[2508/4499] Val Loss:0.07123788446187973\n",
      "Epoch 0[2509/4499] Val Loss:0.08792701363563538\n",
      "Epoch 0[2510/4499] Val Loss:0.11604811996221542\n",
      "Epoch 0[2511/4499] Val Loss:0.10366801172494888\n",
      "Epoch 0[2512/4499] Val Loss:0.21102049946784973\n",
      "Epoch 0[2513/4499] Val Loss:0.11709316074848175\n",
      "Epoch 0[2514/4499] Val Loss:0.12197884172201157\n",
      "Epoch 0[2515/4499] Val Loss:0.08828004449605942\n",
      "Epoch 0[2516/4499] Val Loss:0.10795530676841736\n",
      "Epoch 0[2517/4499] Val Loss:0.14755584299564362\n",
      "Epoch 0[2518/4499] Val Loss:0.12092594802379608\n",
      "Epoch 0[2519/4499] Val Loss:0.12631367146968842\n",
      "Epoch 0[2520/4499] Val Loss:0.10171408206224442\n",
      "Epoch 0[2521/4499] Val Loss:0.08682417124509811\n",
      "Epoch 0[2522/4499] Val Loss:0.12938900291919708\n",
      "Epoch 0[2523/4499] Val Loss:0.08818145096302032\n",
      "Epoch 0[2524/4499] Val Loss:0.12809588015079498\n",
      "Epoch 0[2525/4499] Val Loss:0.15773442387580872\n",
      "Epoch 0[2526/4499] Val Loss:0.2474907487630844\n",
      "Epoch 0[2527/4499] Val Loss:0.31302952766418457\n",
      "Epoch 0[2528/4499] Val Loss:0.17127487063407898\n",
      "Epoch 0[2529/4499] Val Loss:0.19388069212436676\n",
      "Epoch 0[2530/4499] Val Loss:0.07773568481206894\n",
      "Epoch 0[2531/4499] Val Loss:0.08746904879808426\n",
      "Epoch 0[2532/4499] Val Loss:0.07511867582798004\n",
      "Epoch 0[2533/4499] Val Loss:0.13804593682289124\n",
      "Epoch 0[2534/4499] Val Loss:0.16549614071846008\n",
      "Epoch 0[2535/4499] Val Loss:0.1640443056821823\n",
      "Epoch 0[2536/4499] Val Loss:0.1421600580215454\n",
      "Epoch 0[2537/4499] Val Loss:0.19654305279254913\n",
      "Epoch 0[2538/4499] Val Loss:0.6982364058494568\n",
      "Epoch 0[2539/4499] Val Loss:0.5633591413497925\n",
      "Epoch 0[2540/4499] Val Loss:0.3597198724746704\n",
      "Epoch 0[2541/4499] Val Loss:0.4715059995651245\n",
      "Epoch 0[2542/4499] Val Loss:0.30826452374458313\n",
      "Epoch 0[2543/4499] Val Loss:0.11029277741909027\n",
      "Epoch 0[2544/4499] Val Loss:0.38048258423805237\n",
      "Epoch 0[2545/4499] Val Loss:0.2420763224363327\n",
      "Epoch 0[2546/4499] Val Loss:0.29587844014167786\n",
      "Epoch 0[2547/4499] Val Loss:0.536797821521759\n",
      "Epoch 0[2548/4499] Val Loss:0.3083103597164154\n",
      "Epoch 0[2549/4499] Val Loss:0.33400359749794006\n",
      "Epoch 0[2550/4499] Val Loss:0.5902987122535706\n",
      "Epoch 0[2551/4499] Val Loss:0.3882996439933777\n",
      "Epoch 0[2552/4499] Val Loss:0.3136216998100281\n",
      "Epoch 0[2553/4499] Val Loss:0.49350595474243164\n",
      "Epoch 0[2554/4499] Val Loss:0.16958282887935638\n",
      "Epoch 0[2555/4499] Val Loss:0.4044416844844818\n",
      "Epoch 0[2556/4499] Val Loss:0.3066769540309906\n",
      "Epoch 0[2557/4499] Val Loss:0.1855667680501938\n",
      "Epoch 0[2558/4499] Val Loss:0.40338876843452454\n",
      "Epoch 0[2559/4499] Val Loss:0.24719320237636566\n",
      "Epoch 0[2560/4499] Val Loss:0.30832380056381226\n",
      "Epoch 0[2561/4499] Val Loss:0.40538546442985535\n",
      "Epoch 0[2562/4499] Val Loss:0.23016279935836792\n",
      "Epoch 0[2563/4499] Val Loss:0.3106425404548645\n",
      "Epoch 0[2564/4499] Val Loss:0.32337436079978943\n",
      "Epoch 0[2565/4499] Val Loss:0.1522177755832672\n",
      "Epoch 0[2566/4499] Val Loss:0.29836583137512207\n",
      "Epoch 0[2567/4499] Val Loss:0.45744791626930237\n",
      "Epoch 0[2568/4499] Val Loss:0.21948331594467163\n",
      "Epoch 0[2569/4499] Val Loss:0.388385146856308\n",
      "Epoch 0[2570/4499] Val Loss:0.4984623193740845\n",
      "Epoch 0[2571/4499] Val Loss:0.36669695377349854\n",
      "Epoch 0[2572/4499] Val Loss:0.302222341299057\n",
      "Epoch 0[2573/4499] Val Loss:0.4698433578014374\n",
      "Epoch 0[2574/4499] Val Loss:0.4080835282802582\n",
      "Epoch 0[2575/4499] Val Loss:0.24570871889591217\n",
      "Epoch 0[2576/4499] Val Loss:0.6239644885063171\n",
      "Epoch 0[2577/4499] Val Loss:0.2768087685108185\n",
      "Epoch 0[2578/4499] Val Loss:0.15789838135242462\n",
      "Epoch 0[2579/4499] Val Loss:0.2528681755065918\n",
      "Epoch 0[2580/4499] Val Loss:0.22860845923423767\n",
      "Epoch 0[2581/4499] Val Loss:0.18816514313220978\n",
      "Epoch 0[2582/4499] Val Loss:0.09953608363866806\n",
      "Epoch 0[2583/4499] Val Loss:0.20226024091243744\n",
      "Epoch 0[2584/4499] Val Loss:0.2603924572467804\n",
      "Epoch 0[2585/4499] Val Loss:0.3488125205039978\n",
      "Epoch 0[2586/4499] Val Loss:0.13922850787639618\n",
      "Epoch 0[2587/4499] Val Loss:0.34028562903404236\n",
      "Epoch 0[2588/4499] Val Loss:0.4837267994880676\n",
      "Epoch 0[2589/4499] Val Loss:0.40903568267822266\n",
      "Epoch 0[2590/4499] Val Loss:0.19298885762691498\n",
      "Epoch 0[2591/4499] Val Loss:0.34505411982536316\n",
      "Epoch 0[2592/4499] Val Loss:0.41022053360939026\n",
      "Epoch 0[2593/4499] Val Loss:0.4198502004146576\n",
      "Epoch 0[2594/4499] Val Loss:0.11886467039585114\n",
      "Epoch 0[2595/4499] Val Loss:0.40820547938346863\n",
      "Epoch 0[2596/4499] Val Loss:0.3623528778553009\n",
      "Epoch 0[2597/4499] Val Loss:0.33712831139564514\n",
      "Epoch 0[2598/4499] Val Loss:0.2926173210144043\n",
      "Epoch 0[2599/4499] Val Loss:0.28094351291656494\n",
      "Epoch 0[2600/4499] Val Loss:0.41578784584999084\n",
      "Epoch 0[2601/4499] Val Loss:0.3165866434574127\n",
      "Epoch 0[2602/4499] Val Loss:0.34111288189888\n",
      "Epoch 0[2603/4499] Val Loss:0.14830023050308228\n",
      "Epoch 0[2604/4499] Val Loss:0.40662282705307007\n",
      "Epoch 0[2605/4499] Val Loss:0.49125710129737854\n",
      "Epoch 0[2606/4499] Val Loss:0.38799506425857544\n",
      "Epoch 0[2607/4499] Val Loss:0.13465556502342224\n",
      "Epoch 0[2608/4499] Val Loss:0.30673399567604065\n",
      "Epoch 0[2609/4499] Val Loss:0.23367036879062653\n",
      "Epoch 0[2610/4499] Val Loss:0.16551809012889862\n",
      "Epoch 0[2611/4499] Val Loss:0.18420839309692383\n",
      "Epoch 0[2612/4499] Val Loss:0.13848239183425903\n",
      "Epoch 0[2613/4499] Val Loss:0.12354690581560135\n",
      "Epoch 0[2614/4499] Val Loss:0.0972481518983841\n",
      "Epoch 0[2615/4499] Val Loss:0.12093523144721985\n",
      "Epoch 0[2616/4499] Val Loss:0.12634986639022827\n",
      "Epoch 0[2617/4499] Val Loss:0.12500885128974915\n",
      "Epoch 0[2618/4499] Val Loss:0.09564923495054245\n",
      "Epoch 0[2619/4499] Val Loss:0.12690502405166626\n",
      "Epoch 0[2620/4499] Val Loss:0.13698051869869232\n",
      "Epoch 0[2621/4499] Val Loss:0.12071707844734192\n",
      "Epoch 0[2622/4499] Val Loss:0.10830430686473846\n",
      "Epoch 0[2623/4499] Val Loss:0.1520465612411499\n",
      "Epoch 0[2624/4499] Val Loss:0.2334418147802353\n",
      "Epoch 0[2625/4499] Val Loss:0.26500988006591797\n",
      "Epoch 0[2626/4499] Val Loss:0.16562692821025848\n",
      "Epoch 0[2627/4499] Val Loss:0.151557058095932\n",
      "Epoch 0[2628/4499] Val Loss:0.15500830113887787\n",
      "Epoch 0[2629/4499] Val Loss:0.18057118356227875\n",
      "Epoch 0[2630/4499] Val Loss:0.2217983901500702\n",
      "Epoch 0[2631/4499] Val Loss:0.12780188024044037\n",
      "Epoch 0[2632/4499] Val Loss:0.2203681915998459\n",
      "Epoch 0[2633/4499] Val Loss:0.16778361797332764\n",
      "Epoch 0[2634/4499] Val Loss:0.20281514525413513\n",
      "Epoch 0[2635/4499] Val Loss:0.2277013510465622\n",
      "Epoch 0[2636/4499] Val Loss:0.19878457486629486\n",
      "Epoch 0[2637/4499] Val Loss:0.2921447455883026\n",
      "Epoch 0[2638/4499] Val Loss:0.1760927140712738\n",
      "Epoch 0[2639/4499] Val Loss:0.2593240439891815\n",
      "Epoch 0[2640/4499] Val Loss:0.35763972997665405\n",
      "Epoch 0[2641/4499] Val Loss:0.22310063242912292\n",
      "Epoch 0[2642/4499] Val Loss:0.20991036295890808\n",
      "Epoch 0[2643/4499] Val Loss:0.2555772066116333\n",
      "Epoch 0[2644/4499] Val Loss:0.23551145195960999\n",
      "Epoch 0[2645/4499] Val Loss:0.2364904284477234\n",
      "Epoch 0[2646/4499] Val Loss:0.16003860533237457\n",
      "Epoch 0[2647/4499] Val Loss:0.22445885837078094\n",
      "Epoch 0[2648/4499] Val Loss:0.29622942209243774\n",
      "Epoch 0[2649/4499] Val Loss:0.25800344347953796\n",
      "Epoch 0[2650/4499] Val Loss:0.18795476853847504\n",
      "Epoch 0[2651/4499] Val Loss:0.2079722136259079\n",
      "Epoch 0[2652/4499] Val Loss:0.16630446910858154\n",
      "Epoch 0[2653/4499] Val Loss:0.24885165691375732\n",
      "Epoch 0[2654/4499] Val Loss:0.3007619380950928\n",
      "Epoch 0[2655/4499] Val Loss:0.3091527819633484\n",
      "Epoch 0[2656/4499] Val Loss:0.25392937660217285\n",
      "Epoch 0[2657/4499] Val Loss:0.3822212815284729\n",
      "Epoch 0[2658/4499] Val Loss:0.3306172788143158\n",
      "Epoch 0[2659/4499] Val Loss:0.2534049451351166\n",
      "Epoch 0[2660/4499] Val Loss:0.22134892642498016\n",
      "Epoch 0[2661/4499] Val Loss:0.2132236808538437\n",
      "Epoch 0[2662/4499] Val Loss:0.2506248354911804\n",
      "Epoch 0[2663/4499] Val Loss:0.3530081808567047\n",
      "Epoch 0[2664/4499] Val Loss:0.3697732090950012\n",
      "Epoch 0[2665/4499] Val Loss:0.34809404611587524\n",
      "Epoch 0[2666/4499] Val Loss:0.3680381178855896\n",
      "Epoch 0[2667/4499] Val Loss:0.3165767788887024\n",
      "Epoch 0[2668/4499] Val Loss:0.5320532917976379\n",
      "Epoch 0[2669/4499] Val Loss:0.4685717225074768\n",
      "Epoch 0[2670/4499] Val Loss:0.15403424203395844\n",
      "Epoch 0[2671/4499] Val Loss:0.18030411005020142\n",
      "Epoch 0[2672/4499] Val Loss:0.17759442329406738\n",
      "Epoch 0[2673/4499] Val Loss:0.1516147404909134\n",
      "Epoch 0[2674/4499] Val Loss:0.08031954616308212\n",
      "Epoch 0[2675/4499] Val Loss:0.07224895060062408\n",
      "Epoch 0[2676/4499] Val Loss:0.08009636402130127\n",
      "Epoch 0[2677/4499] Val Loss:0.08641181141138077\n",
      "Epoch 0[2678/4499] Val Loss:0.10392753034830093\n",
      "Epoch 0[2679/4499] Val Loss:0.07801163196563721\n",
      "Epoch 0[2680/4499] Val Loss:0.09749668836593628\n",
      "Epoch 0[2681/4499] Val Loss:0.07644590735435486\n",
      "Epoch 0[2682/4499] Val Loss:0.09819541871547699\n",
      "Epoch 0[2683/4499] Val Loss:0.09515286982059479\n",
      "Epoch 0[2684/4499] Val Loss:0.08525650203227997\n",
      "Epoch 0[2685/4499] Val Loss:0.07217719405889511\n",
      "Epoch 0[2686/4499] Val Loss:0.110369972884655\n",
      "Epoch 0[2687/4499] Val Loss:0.17821523547172546\n",
      "Epoch 0[2688/4499] Val Loss:0.1377253234386444\n",
      "Epoch 0[2689/4499] Val Loss:0.09492751210927963\n",
      "Epoch 0[2690/4499] Val Loss:0.08600904792547226\n",
      "Epoch 0[2691/4499] Val Loss:0.16058887541294098\n",
      "Epoch 0[2692/4499] Val Loss:0.28722402453422546\n",
      "Epoch 0[2693/4499] Val Loss:0.379878431558609\n",
      "Epoch 0[2694/4499] Val Loss:0.49604228138923645\n",
      "Epoch 0[2695/4499] Val Loss:0.2600323259830475\n",
      "Epoch 0[2696/4499] Val Loss:0.2862466275691986\n",
      "Epoch 0[2697/4499] Val Loss:0.24342872202396393\n",
      "Epoch 0[2698/4499] Val Loss:0.3264019787311554\n",
      "Epoch 0[2699/4499] Val Loss:0.2703726589679718\n",
      "Epoch 0[2700/4499] Val Loss:0.28062704205513\n",
      "Epoch 0[2701/4499] Val Loss:0.3948625922203064\n",
      "Epoch 0[2702/4499] Val Loss:0.5463229417800903\n",
      "Epoch 0[2703/4499] Val Loss:0.4610184133052826\n",
      "Epoch 0[2704/4499] Val Loss:0.13603854179382324\n",
      "Epoch 0[2705/4499] Val Loss:0.501220166683197\n",
      "Epoch 0[2706/4499] Val Loss:0.4047258198261261\n",
      "Epoch 0[2707/4499] Val Loss:0.3800829350948334\n",
      "Epoch 0[2708/4499] Val Loss:0.15277181565761566\n",
      "Epoch 0[2709/4499] Val Loss:0.34218353033065796\n",
      "Epoch 0[2710/4499] Val Loss:0.3438085913658142\n",
      "Epoch 0[2711/4499] Val Loss:0.2832864224910736\n",
      "Epoch 0[2712/4499] Val Loss:0.16718530654907227\n",
      "Epoch 0[2713/4499] Val Loss:0.2364046573638916\n",
      "Epoch 0[2714/4499] Val Loss:0.39839625358581543\n",
      "Epoch 0[2715/4499] Val Loss:0.3266700804233551\n",
      "Epoch 0[2716/4499] Val Loss:0.14217515289783478\n",
      "Epoch 0[2717/4499] Val Loss:0.30935168266296387\n",
      "Epoch 0[2718/4499] Val Loss:0.337254136800766\n",
      "Epoch 0[2719/4499] Val Loss:0.2513977587223053\n",
      "Epoch 0[2720/4499] Val Loss:0.14670361578464508\n",
      "Epoch 0[2721/4499] Val Loss:0.2243276983499527\n",
      "Epoch 0[2722/4499] Val Loss:0.32009005546569824\n",
      "Epoch 0[2723/4499] Val Loss:0.3816906809806824\n",
      "Epoch 0[2724/4499] Val Loss:0.15753568708896637\n",
      "Epoch 0[2725/4499] Val Loss:0.5771176218986511\n",
      "Epoch 0[2726/4499] Val Loss:0.637901782989502\n",
      "Epoch 0[2727/4499] Val Loss:0.4375841021537781\n",
      "Epoch 0[2728/4499] Val Loss:0.12188953161239624\n",
      "Epoch 0[2729/4499] Val Loss:0.21637733280658722\n",
      "Epoch 0[2730/4499] Val Loss:0.34389206767082214\n",
      "Epoch 0[2731/4499] Val Loss:0.4984561800956726\n",
      "Epoch 0[2732/4499] Val Loss:0.1828734427690506\n",
      "Epoch 0[2733/4499] Val Loss:0.2612386643886566\n",
      "Epoch 0[2734/4499] Val Loss:0.3510999083518982\n",
      "Epoch 0[2735/4499] Val Loss:0.37844783067703247\n",
      "Epoch 0[2736/4499] Val Loss:0.39653509855270386\n",
      "Epoch 0[2737/4499] Val Loss:0.21973873674869537\n",
      "Epoch 0[2738/4499] Val Loss:0.27256929874420166\n",
      "Epoch 0[2739/4499] Val Loss:0.40025797486305237\n",
      "Epoch 0[2740/4499] Val Loss:0.3561655879020691\n",
      "Epoch 0[2741/4499] Val Loss:0.4277012050151825\n",
      "Epoch 0[2742/4499] Val Loss:0.28604790568351746\n",
      "Epoch 0[2743/4499] Val Loss:0.0910467803478241\n",
      "Epoch 0[2744/4499] Val Loss:0.23115795850753784\n",
      "Epoch 0[2745/4499] Val Loss:0.33304381370544434\n",
      "Epoch 0[2746/4499] Val Loss:0.2554217576980591\n",
      "Epoch 0[2747/4499] Val Loss:0.2432359755039215\n",
      "Epoch 0[2748/4499] Val Loss:0.33625176548957825\n",
      "Epoch 0[2749/4499] Val Loss:0.18484646081924438\n",
      "Epoch 0[2750/4499] Val Loss:0.12900985777378082\n",
      "Epoch 0[2751/4499] Val Loss:0.37717005610466003\n",
      "Epoch 0[2752/4499] Val Loss:0.28756487369537354\n",
      "Epoch 0[2753/4499] Val Loss:0.21618273854255676\n",
      "Epoch 0[2754/4499] Val Loss:0.1072942465543747\n",
      "Epoch 0[2755/4499] Val Loss:0.16383633017539978\n",
      "Epoch 0[2756/4499] Val Loss:0.29746297001838684\n",
      "Epoch 0[2757/4499] Val Loss:0.3500988185405731\n",
      "Epoch 0[2758/4499] Val Loss:0.3148174583911896\n",
      "Epoch 0[2759/4499] Val Loss:0.09113415330648422\n",
      "Epoch 0[2760/4499] Val Loss:0.2597983479499817\n",
      "Epoch 0[2761/4499] Val Loss:0.24012374877929688\n",
      "Epoch 0[2762/4499] Val Loss:0.29194626212120056\n",
      "Epoch 0[2763/4499] Val Loss:0.08416244387626648\n",
      "Epoch 0[2764/4499] Val Loss:0.30496326088905334\n",
      "Epoch 0[2765/4499] Val Loss:0.2456788718700409\n",
      "Epoch 0[2766/4499] Val Loss:0.3604462146759033\n",
      "Epoch 0[2767/4499] Val Loss:0.15391448140144348\n",
      "Epoch 0[2768/4499] Val Loss:0.4318922460079193\n",
      "Epoch 0[2769/4499] Val Loss:0.2814944088459015\n",
      "Epoch 0[2770/4499] Val Loss:0.12704433500766754\n",
      "Epoch 0[2771/4499] Val Loss:0.29168394207954407\n",
      "Epoch 0[2772/4499] Val Loss:0.25277039408683777\n",
      "Epoch 0[2773/4499] Val Loss:0.18284697830677032\n",
      "Epoch 0[2774/4499] Val Loss:0.20064017176628113\n",
      "Epoch 0[2775/4499] Val Loss:0.2454669028520584\n",
      "Epoch 0[2776/4499] Val Loss:0.11713254451751709\n",
      "Epoch 0[2777/4499] Val Loss:0.0638468787074089\n",
      "Epoch 0[2778/4499] Val Loss:0.04754115641117096\n",
      "Epoch 0[2779/4499] Val Loss:0.05832386016845703\n",
      "Epoch 0[2780/4499] Val Loss:0.060525503009557724\n",
      "Epoch 0[2781/4499] Val Loss:0.0662192702293396\n",
      "Epoch 0[2782/4499] Val Loss:0.05046888440847397\n",
      "Epoch 0[2783/4499] Val Loss:0.057458631694316864\n",
      "Epoch 0[2784/4499] Val Loss:0.052099086344242096\n",
      "Epoch 0[2785/4499] Val Loss:0.06206720694899559\n",
      "Epoch 0[2786/4499] Val Loss:0.09812768548727036\n",
      "Epoch 0[2787/4499] Val Loss:0.07248514145612717\n",
      "Epoch 0[2788/4499] Val Loss:0.053526848554611206\n",
      "Epoch 0[2789/4499] Val Loss:0.07054769992828369\n",
      "Epoch 0[2790/4499] Val Loss:0.05682157725095749\n",
      "Epoch 0[2791/4499] Val Loss:0.05815490707755089\n",
      "Epoch 0[2792/4499] Val Loss:0.06162075325846672\n",
      "Epoch 0[2793/4499] Val Loss:0.056501638144254684\n",
      "Epoch 0[2794/4499] Val Loss:0.08646522462368011\n",
      "Epoch 0[2795/4499] Val Loss:0.06006941199302673\n",
      "Epoch 0[2796/4499] Val Loss:0.06690314412117004\n",
      "Epoch 0[2797/4499] Val Loss:0.11337123811244965\n",
      "Epoch 0[2798/4499] Val Loss:0.1060200035572052\n",
      "Epoch 0[2799/4499] Val Loss:0.08096683770418167\n",
      "Epoch 0[2800/4499] Val Loss:0.05148666352033615\n",
      "Epoch 0[2801/4499] Val Loss:0.0510576069355011\n",
      "Epoch 0[2802/4499] Val Loss:0.05919269844889641\n",
      "Epoch 0[2803/4499] Val Loss:0.06494220346212387\n",
      "Epoch 0[2804/4499] Val Loss:0.06462065875530243\n",
      "Epoch 0[2805/4499] Val Loss:0.06691586226224899\n",
      "Epoch 0[2806/4499] Val Loss:0.06463182717561722\n",
      "Epoch 0[2807/4499] Val Loss:0.06737348437309265\n",
      "Epoch 0[2808/4499] Val Loss:0.07983596622943878\n",
      "Epoch 0[2809/4499] Val Loss:0.06662225723266602\n",
      "Epoch 0[2810/4499] Val Loss:0.07932009547948837\n",
      "Epoch 0[2811/4499] Val Loss:0.07669656723737717\n",
      "Epoch 0[2812/4499] Val Loss:0.07234172523021698\n",
      "Epoch 0[2813/4499] Val Loss:0.06769821047782898\n",
      "Epoch 0[2814/4499] Val Loss:0.09507465362548828\n",
      "Epoch 0[2815/4499] Val Loss:0.059691667556762695\n",
      "Epoch 0[2816/4499] Val Loss:0.08694762736558914\n",
      "Epoch 0[2817/4499] Val Loss:0.07743994891643524\n",
      "Epoch 0[2818/4499] Val Loss:0.09205245971679688\n",
      "Epoch 0[2819/4499] Val Loss:0.11018423736095428\n",
      "Epoch 0[2820/4499] Val Loss:0.10797302424907684\n",
      "Epoch 0[2821/4499] Val Loss:0.12433723360300064\n",
      "Epoch 0[2822/4499] Val Loss:0.14009147882461548\n",
      "Epoch 0[2823/4499] Val Loss:0.15585291385650635\n",
      "Epoch 0[2824/4499] Val Loss:0.1082434207201004\n",
      "Epoch 0[2825/4499] Val Loss:0.0975625067949295\n",
      "Epoch 0[2826/4499] Val Loss:0.0978916808962822\n",
      "Epoch 0[2827/4499] Val Loss:0.11313769221305847\n",
      "Epoch 0[2828/4499] Val Loss:0.14002956449985504\n",
      "Epoch 0[2829/4499] Val Loss:0.14631426334381104\n",
      "Epoch 0[2830/4499] Val Loss:0.09289634227752686\n",
      "Epoch 0[2831/4499] Val Loss:0.18454188108444214\n",
      "Epoch 0[2832/4499] Val Loss:0.11738497018814087\n",
      "Epoch 0[2833/4499] Val Loss:0.1622813642024994\n",
      "Epoch 0[2834/4499] Val Loss:0.17612498998641968\n",
      "Epoch 0[2835/4499] Val Loss:0.1562192291021347\n",
      "Epoch 0[2836/4499] Val Loss:0.14135295152664185\n",
      "Epoch 0[2837/4499] Val Loss:0.11001954227685928\n",
      "Epoch 0[2838/4499] Val Loss:0.20060168206691742\n",
      "Epoch 0[2839/4499] Val Loss:0.09083414077758789\n",
      "Epoch 0[2840/4499] Val Loss:0.18522688746452332\n",
      "Epoch 0[2841/4499] Val Loss:0.13261136412620544\n",
      "Epoch 0[2842/4499] Val Loss:0.15207308530807495\n",
      "Epoch 0[2843/4499] Val Loss:0.09466424584388733\n",
      "Epoch 0[2844/4499] Val Loss:0.17964762449264526\n",
      "Epoch 0[2845/4499] Val Loss:0.0811656266450882\n",
      "Epoch 0[2846/4499] Val Loss:0.24376878142356873\n",
      "Epoch 0[2847/4499] Val Loss:0.20707817375659943\n",
      "Epoch 0[2848/4499] Val Loss:0.25883248448371887\n",
      "Epoch 0[2849/4499] Val Loss:0.16956131160259247\n",
      "Epoch 0[2850/4499] Val Loss:0.20551908016204834\n",
      "Epoch 0[2851/4499] Val Loss:0.18443724513053894\n",
      "Epoch 0[2852/4499] Val Loss:0.19606822729110718\n",
      "Epoch 0[2853/4499] Val Loss:0.22550740838050842\n",
      "Epoch 0[2854/4499] Val Loss:0.1446438878774643\n",
      "Epoch 0[2855/4499] Val Loss:0.23374828696250916\n",
      "Epoch 0[2856/4499] Val Loss:0.15279759466648102\n",
      "Epoch 0[2857/4499] Val Loss:0.3014289140701294\n",
      "Epoch 0[2858/4499] Val Loss:0.23112940788269043\n",
      "Epoch 0[2859/4499] Val Loss:0.35435497760772705\n",
      "Epoch 0[2860/4499] Val Loss:0.39773136377334595\n",
      "Epoch 0[2861/4499] Val Loss:0.1516791582107544\n",
      "Epoch 0[2862/4499] Val Loss:0.39177262783050537\n",
      "Epoch 0[2863/4499] Val Loss:0.2729105055332184\n",
      "Epoch 0[2864/4499] Val Loss:0.3688082695007324\n",
      "Epoch 0[2865/4499] Val Loss:0.35834044218063354\n",
      "Epoch 0[2866/4499] Val Loss:0.26927557587623596\n",
      "Epoch 0[2867/4499] Val Loss:0.2906050980091095\n",
      "Epoch 0[2868/4499] Val Loss:0.22425100207328796\n",
      "Epoch 0[2869/4499] Val Loss:0.16074497997760773\n",
      "Epoch 0[2870/4499] Val Loss:0.12720231711864471\n",
      "Epoch 0[2871/4499] Val Loss:0.13334035873413086\n",
      "Epoch 0[2872/4499] Val Loss:0.1450764387845993\n",
      "Epoch 0[2873/4499] Val Loss:0.12383021414279938\n",
      "Epoch 0[2874/4499] Val Loss:0.11410567909479141\n",
      "Epoch 0[2875/4499] Val Loss:0.10759127140045166\n",
      "Epoch 0[2876/4499] Val Loss:0.1239747554063797\n",
      "Epoch 0[2877/4499] Val Loss:0.10272622108459473\n",
      "Epoch 0[2878/4499] Val Loss:0.13418827950954437\n",
      "Epoch 0[2879/4499] Val Loss:0.13516920804977417\n",
      "Epoch 0[2880/4499] Val Loss:0.10193345695734024\n",
      "Epoch 0[2881/4499] Val Loss:0.10259830951690674\n",
      "Epoch 0[2882/4499] Val Loss:0.12265094369649887\n",
      "Epoch 0[2883/4499] Val Loss:0.1828981190919876\n",
      "Epoch 0[2884/4499] Val Loss:0.35253480076789856\n",
      "Epoch 0[2885/4499] Val Loss:0.34329044818878174\n",
      "Epoch 0[2886/4499] Val Loss:0.3369327485561371\n",
      "Epoch 0[2887/4499] Val Loss:0.624767541885376\n",
      "Epoch 0[2888/4499] Val Loss:0.2920089662075043\n",
      "Epoch 0[2889/4499] Val Loss:0.19564655423164368\n",
      "Epoch 0[2890/4499] Val Loss:0.1549663245677948\n",
      "Epoch 0[2891/4499] Val Loss:0.24323415756225586\n",
      "Epoch 0[2892/4499] Val Loss:0.2585024833679199\n",
      "Epoch 0[2893/4499] Val Loss:0.2622181177139282\n",
      "Epoch 0[2894/4499] Val Loss:0.1550317108631134\n",
      "Epoch 0[2895/4499] Val Loss:0.34266820549964905\n",
      "Epoch 0[2896/4499] Val Loss:0.41404783725738525\n",
      "Epoch 0[2897/4499] Val Loss:0.274364173412323\n",
      "Epoch 0[2898/4499] Val Loss:0.2818087339401245\n",
      "Epoch 0[2899/4499] Val Loss:0.40806424617767334\n",
      "Epoch 0[2900/4499] Val Loss:0.4461866319179535\n",
      "Epoch 0[2901/4499] Val Loss:0.15356801450252533\n",
      "Epoch 0[2902/4499] Val Loss:0.3276466131210327\n",
      "Epoch 0[2903/4499] Val Loss:0.3279041349887848\n",
      "Epoch 0[2904/4499] Val Loss:0.20535734295845032\n",
      "Epoch 0[2905/4499] Val Loss:0.2389250248670578\n",
      "Epoch 0[2906/4499] Val Loss:0.3575717508792877\n",
      "Epoch 0[2907/4499] Val Loss:0.34239262342453003\n",
      "Epoch 0[2908/4499] Val Loss:0.18416661024093628\n",
      "Epoch 0[2909/4499] Val Loss:0.32237234711647034\n",
      "Epoch 0[2910/4499] Val Loss:0.36570143699645996\n",
      "Epoch 0[2911/4499] Val Loss:0.18123142421245575\n",
      "Epoch 0[2912/4499] Val Loss:0.24947701394557953\n",
      "Epoch 0[2913/4499] Val Loss:0.37974128127098083\n",
      "Epoch 0[2914/4499] Val Loss:0.29596588015556335\n",
      "Epoch 0[2915/4499] Val Loss:0.26093655824661255\n",
      "Epoch 0[2916/4499] Val Loss:0.3880775570869446\n",
      "Epoch 0[2917/4499] Val Loss:0.45864933729171753\n",
      "Epoch 0[2918/4499] Val Loss:0.2115614414215088\n",
      "Epoch 0[2919/4499] Val Loss:0.356294184923172\n",
      "Epoch 0[2920/4499] Val Loss:0.43008938431739807\n",
      "Epoch 0[2921/4499] Val Loss:0.32628002762794495\n",
      "Epoch 0[2922/4499] Val Loss:0.22878152132034302\n",
      "Epoch 0[2923/4499] Val Loss:0.3170858919620514\n",
      "Epoch 0[2924/4499] Val Loss:0.4275640845298767\n",
      "Epoch 0[2925/4499] Val Loss:0.2137373834848404\n",
      "Epoch 0[2926/4499] Val Loss:0.32967713475227356\n",
      "Epoch 0[2927/4499] Val Loss:0.4390069246292114\n",
      "Epoch 0[2928/4499] Val Loss:0.3340352177619934\n",
      "Epoch 0[2929/4499] Val Loss:0.28547748923301697\n",
      "Epoch 0[2930/4499] Val Loss:0.21089142560958862\n",
      "Epoch 0[2931/4499] Val Loss:0.4164929687976837\n",
      "Epoch 0[2932/4499] Val Loss:0.4115828573703766\n",
      "Epoch 0[2933/4499] Val Loss:0.34984317421913147\n",
      "Epoch 0[2934/4499] Val Loss:0.18466676771640778\n",
      "Epoch 0[2935/4499] Val Loss:0.29530009627342224\n",
      "Epoch 0[2936/4499] Val Loss:0.4707128703594208\n",
      "Epoch 0[2937/4499] Val Loss:0.4183831512928009\n",
      "Epoch 0[2938/4499] Val Loss:0.4173453152179718\n",
      "Epoch 0[2939/4499] Val Loss:0.12066055834293365\n",
      "Epoch 0[2940/4499] Val Loss:0.36150410771369934\n",
      "Epoch 0[2941/4499] Val Loss:0.3168451189994812\n",
      "Epoch 0[2942/4499] Val Loss:0.318928599357605\n",
      "Epoch 0[2943/4499] Val Loss:0.1604350507259369\n",
      "Epoch 0[2944/4499] Val Loss:0.1320786327123642\n",
      "Epoch 0[2945/4499] Val Loss:0.2767733335494995\n",
      "Epoch 0[2946/4499] Val Loss:0.285442590713501\n",
      "Epoch 0[2947/4499] Val Loss:0.34671342372894287\n",
      "Epoch 0[2948/4499] Val Loss:0.2704715430736542\n",
      "Epoch 0[2949/4499] Val Loss:0.17730657756328583\n",
      "Epoch 0[2950/4499] Val Loss:0.3227553963661194\n",
      "Epoch 0[2951/4499] Val Loss:0.3958401679992676\n",
      "Epoch 0[2952/4499] Val Loss:0.13539332151412964\n",
      "Epoch 0[2953/4499] Val Loss:0.2670203447341919\n",
      "Epoch 0[2954/4499] Val Loss:0.32977354526519775\n",
      "Epoch 0[2955/4499] Val Loss:0.2868272066116333\n",
      "Epoch 0[2956/4499] Val Loss:0.08411810547113419\n",
      "Epoch 0[2957/4499] Val Loss:0.36375749111175537\n",
      "Epoch 0[2958/4499] Val Loss:0.3961316645145416\n",
      "Epoch 0[2959/4499] Val Loss:0.1415848731994629\n",
      "Epoch 0[2960/4499] Val Loss:0.4500727653503418\n",
      "Epoch 0[2961/4499] Val Loss:0.3602951765060425\n",
      "Epoch 0[2962/4499] Val Loss:0.12681244313716888\n",
      "Epoch 0[2963/4499] Val Loss:0.39746376872062683\n",
      "Epoch 0[2964/4499] Val Loss:0.33642861247062683\n",
      "Epoch 0[2965/4499] Val Loss:0.0898851528763771\n",
      "Epoch 0[2966/4499] Val Loss:0.38393551111221313\n",
      "Epoch 0[2967/4499] Val Loss:0.3799278438091278\n",
      "Epoch 0[2968/4499] Val Loss:0.08762221038341522\n",
      "Epoch 0[2969/4499] Val Loss:0.36827558279037476\n",
      "Epoch 0[2970/4499] Val Loss:0.33078980445861816\n",
      "Epoch 0[2971/4499] Val Loss:0.07251608371734619\n",
      "Epoch 0[2972/4499] Val Loss:0.5124409794807434\n",
      "Epoch 0[2973/4499] Val Loss:0.46332287788391113\n",
      "Epoch 0[2974/4499] Val Loss:0.06464751809835434\n",
      "Epoch 0[2975/4499] Val Loss:0.40020444989204407\n",
      "Epoch 0[2976/4499] Val Loss:0.4202834367752075\n",
      "Epoch 0[2977/4499] Val Loss:0.3435046970844269\n",
      "Epoch 0[2978/4499] Val Loss:0.21846215426921844\n",
      "Epoch 0[2979/4499] Val Loss:0.06992901861667633\n",
      "Epoch 0[2980/4499] Val Loss:0.07746149599552155\n",
      "Epoch 0[2981/4499] Val Loss:0.08336275070905685\n",
      "Epoch 0[2982/4499] Val Loss:0.06670991331338882\n",
      "Epoch 0[2983/4499] Val Loss:0.07150853425264359\n",
      "Epoch 0[2984/4499] Val Loss:0.09746839106082916\n",
      "Epoch 0[2985/4499] Val Loss:0.08128724247217178\n",
      "Epoch 0[2986/4499] Val Loss:0.09161650389432907\n",
      "Epoch 0[2987/4499] Val Loss:0.10051807016134262\n",
      "Epoch 0[2988/4499] Val Loss:0.07182475924491882\n",
      "Epoch 0[2989/4499] Val Loss:0.09000670909881592\n",
      "Epoch 0[2990/4499] Val Loss:0.08575335890054703\n",
      "Epoch 0[2991/4499] Val Loss:0.06764665246009827\n",
      "Epoch 0[2992/4499] Val Loss:0.07396715134382248\n",
      "Epoch 0[2993/4499] Val Loss:0.05600693076848984\n",
      "Epoch 0[2994/4499] Val Loss:0.09754817932844162\n",
      "Epoch 0[2995/4499] Val Loss:0.09113604575395584\n",
      "Epoch 0[2996/4499] Val Loss:0.08417368680238724\n",
      "Epoch 0[2997/4499] Val Loss:0.0884610116481781\n",
      "Epoch 0[2998/4499] Val Loss:0.09583678841590881\n",
      "Epoch 0[2999/4499] Val Loss:0.11183467507362366\n",
      "Epoch 0[3000/4499] Val Loss:0.08378192037343979\n",
      "Epoch 0[3001/4499] Val Loss:0.09375224262475967\n",
      "Epoch 0[3002/4499] Val Loss:0.07945507019758224\n",
      "Epoch 0[3003/4499] Val Loss:0.09225670993328094\n",
      "Epoch 0[3004/4499] Val Loss:0.07922002673149109\n",
      "Epoch 0[3005/4499] Val Loss:0.0783560648560524\n",
      "Epoch 0[3006/4499] Val Loss:0.076731838285923\n",
      "Epoch 0[3007/4499] Val Loss:0.11336962878704071\n",
      "Epoch 0[3008/4499] Val Loss:0.06665735691785812\n",
      "Epoch 0[3009/4499] Val Loss:0.12716993689537048\n",
      "Epoch 0[3010/4499] Val Loss:0.15270838141441345\n",
      "Epoch 0[3011/4499] Val Loss:0.08934196084737778\n",
      "Epoch 0[3012/4499] Val Loss:0.06746615469455719\n",
      "Epoch 0[3013/4499] Val Loss:0.08789816498756409\n",
      "Epoch 0[3014/4499] Val Loss:0.06186376512050629\n",
      "Epoch 0[3015/4499] Val Loss:0.06288731843233109\n",
      "Epoch 0[3016/4499] Val Loss:0.06534916907548904\n",
      "Epoch 0[3017/4499] Val Loss:0.07300853729248047\n",
      "Epoch 0[3018/4499] Val Loss:0.07456960529088974\n",
      "Epoch 0[3019/4499] Val Loss:0.09351631253957748\n",
      "Epoch 0[3020/4499] Val Loss:0.09855800122022629\n",
      "Epoch 0[3021/4499] Val Loss:0.07771934568881989\n",
      "Epoch 0[3022/4499] Val Loss:0.08507699519395828\n",
      "Epoch 0[3023/4499] Val Loss:0.0984896868467331\n",
      "Epoch 0[3024/4499] Val Loss:0.08310510218143463\n",
      "Epoch 0[3025/4499] Val Loss:0.13346639275550842\n",
      "Epoch 0[3026/4499] Val Loss:0.1424974948167801\n",
      "Epoch 0[3027/4499] Val Loss:0.08765403181314468\n",
      "Epoch 0[3028/4499] Val Loss:0.1399327516555786\n",
      "Epoch 0[3029/4499] Val Loss:0.17982697486877441\n",
      "Epoch 0[3030/4499] Val Loss:0.08500493317842484\n",
      "Epoch 0[3031/4499] Val Loss:0.19403043389320374\n",
      "Epoch 0[3032/4499] Val Loss:0.13939394056797028\n",
      "Epoch 0[3033/4499] Val Loss:0.09998507052659988\n",
      "Epoch 0[3034/4499] Val Loss:0.11269106715917587\n",
      "Epoch 0[3035/4499] Val Loss:0.1821233481168747\n",
      "Epoch 0[3036/4499] Val Loss:0.2081475555896759\n",
      "Epoch 0[3037/4499] Val Loss:0.15156391263008118\n",
      "Epoch 0[3038/4499] Val Loss:0.16640609502792358\n",
      "Epoch 0[3039/4499] Val Loss:0.1856880784034729\n",
      "Epoch 0[3040/4499] Val Loss:0.17871083319187164\n",
      "Epoch 0[3041/4499] Val Loss:0.14310456812381744\n",
      "Epoch 0[3042/4499] Val Loss:0.18702220916748047\n",
      "Epoch 0[3043/4499] Val Loss:0.2264281064271927\n",
      "Epoch 0[3044/4499] Val Loss:0.22139060497283936\n",
      "Epoch 0[3045/4499] Val Loss:0.16144819557666779\n",
      "Epoch 0[3046/4499] Val Loss:0.3128037452697754\n",
      "Epoch 0[3047/4499] Val Loss:0.3342834413051605\n",
      "Epoch 0[3048/4499] Val Loss:0.2514636516571045\n",
      "Epoch 0[3049/4499] Val Loss:0.300792932510376\n",
      "Epoch 0[3050/4499] Val Loss:0.19207100570201874\n",
      "Epoch 0[3051/4499] Val Loss:0.18600238859653473\n",
      "Epoch 0[3052/4499] Val Loss:0.2427937537431717\n",
      "Epoch 0[3053/4499] Val Loss:0.22911417484283447\n",
      "Epoch 0[3054/4499] Val Loss:0.3084173798561096\n",
      "Epoch 0[3055/4499] Val Loss:0.38761889934539795\n",
      "Epoch 0[3056/4499] Val Loss:0.24272318184375763\n",
      "Epoch 0[3057/4499] Val Loss:0.4621289074420929\n",
      "Epoch 0[3058/4499] Val Loss:0.5414552092552185\n",
      "Epoch 0[3059/4499] Val Loss:0.3167545795440674\n",
      "Epoch 0[3060/4499] Val Loss:0.5850615501403809\n",
      "Epoch 0[3061/4499] Val Loss:0.6037651896476746\n",
      "Epoch 0[3062/4499] Val Loss:0.27056974172592163\n",
      "Epoch 0[3063/4499] Val Loss:0.6798969507217407\n",
      "Epoch 0[3064/4499] Val Loss:0.6676387786865234\n",
      "Epoch 0[3065/4499] Val Loss:0.6526443958282471\n",
      "Epoch 0[3066/4499] Val Loss:0.5446098446846008\n",
      "Epoch 0[3067/4499] Val Loss:0.5445811152458191\n",
      "Epoch 0[3068/4499] Val Loss:0.547554612159729\n",
      "Epoch 0[3069/4499] Val Loss:0.7866019010543823\n",
      "Epoch 0[3070/4499] Val Loss:0.918611466884613\n",
      "Epoch 0[3071/4499] Val Loss:0.6036899089813232\n",
      "Epoch 0[3072/4499] Val Loss:0.7807645797729492\n",
      "Epoch 0[3073/4499] Val Loss:0.6785843372344971\n",
      "Epoch 0[3074/4499] Val Loss:0.6182827949523926\n",
      "Epoch 0[3075/4499] Val Loss:0.6525006890296936\n",
      "Epoch 0[3076/4499] Val Loss:0.580165684223175\n",
      "Epoch 0[3077/4499] Val Loss:0.44360649585723877\n",
      "Epoch 0[3078/4499] Val Loss:0.417733758687973\n",
      "Epoch 0[3079/4499] Val Loss:0.6474006175994873\n",
      "Epoch 0[3080/4499] Val Loss:0.5369158983230591\n",
      "Epoch 0[3081/4499] Val Loss:0.410234659910202\n",
      "Epoch 0[3082/4499] Val Loss:0.32043424248695374\n",
      "Epoch 0[3083/4499] Val Loss:0.5541383028030396\n",
      "Epoch 0[3084/4499] Val Loss:0.4369317293167114\n",
      "Epoch 0[3085/4499] Val Loss:0.26250848174095154\n",
      "Epoch 0[3086/4499] Val Loss:0.42362698912620544\n",
      "Epoch 0[3087/4499] Val Loss:0.4234597980976105\n",
      "Epoch 0[3088/4499] Val Loss:0.27348592877388\n",
      "Epoch 0[3089/4499] Val Loss:0.45202016830444336\n",
      "Epoch 0[3090/4499] Val Loss:0.41049644351005554\n",
      "Epoch 0[3091/4499] Val Loss:0.2858309745788574\n",
      "Epoch 0[3092/4499] Val Loss:0.38361597061157227\n",
      "Epoch 0[3093/4499] Val Loss:0.2965857982635498\n",
      "Epoch 0[3094/4499] Val Loss:0.16257968544960022\n",
      "Epoch 0[3095/4499] Val Loss:0.1746962070465088\n",
      "Epoch 0[3096/4499] Val Loss:0.07419853657484055\n",
      "Epoch 0[3097/4499] Val Loss:0.2023749053478241\n",
      "Epoch 0[3098/4499] Val Loss:0.09271728992462158\n",
      "Epoch 0[3099/4499] Val Loss:0.13386161625385284\n",
      "Epoch 0[3100/4499] Val Loss:0.11619570851325989\n",
      "Epoch 0[3101/4499] Val Loss:0.06282538175582886\n",
      "Epoch 0[3102/4499] Val Loss:0.11238719522953033\n",
      "Epoch 0[3103/4499] Val Loss:0.07936917990446091\n",
      "Epoch 0[3104/4499] Val Loss:0.08468220382928848\n",
      "Epoch 0[3105/4499] Val Loss:0.11070499569177628\n",
      "Epoch 0[3106/4499] Val Loss:0.07288786768913269\n",
      "Epoch 0[3107/4499] Val Loss:0.14719200134277344\n",
      "Epoch 0[3108/4499] Val Loss:0.06620890647172928\n",
      "Epoch 0[3109/4499] Val Loss:0.1229240745306015\n",
      "Epoch 0[3110/4499] Val Loss:0.0709962323307991\n",
      "Epoch 0[3111/4499] Val Loss:0.08204087615013123\n",
      "Epoch 0[3112/4499] Val Loss:0.0869859904050827\n",
      "Epoch 0[3113/4499] Val Loss:0.060489293187856674\n",
      "Epoch 0[3114/4499] Val Loss:0.1072583869099617\n",
      "Epoch 0[3115/4499] Val Loss:0.08727578073740005\n",
      "Epoch 0[3116/4499] Val Loss:0.055872756987810135\n",
      "Epoch 0[3117/4499] Val Loss:0.10787282884120941\n",
      "Epoch 0[3118/4499] Val Loss:0.10394944250583649\n",
      "Epoch 0[3119/4499] Val Loss:0.11452122777700424\n",
      "Epoch 0[3120/4499] Val Loss:0.20046377182006836\n",
      "Epoch 0[3121/4499] Val Loss:0.10702045261859894\n",
      "Epoch 0[3122/4499] Val Loss:0.09776340425014496\n",
      "Epoch 0[3123/4499] Val Loss:0.16172103583812714\n",
      "Epoch 0[3124/4499] Val Loss:0.08513864874839783\n",
      "Epoch 0[3125/4499] Val Loss:0.10498688369989395\n",
      "Epoch 0[3126/4499] Val Loss:0.1393551230430603\n",
      "Epoch 0[3127/4499] Val Loss:0.11972241848707199\n",
      "Epoch 0[3128/4499] Val Loss:0.0919320210814476\n",
      "Epoch 0[3129/4499] Val Loss:0.23580613732337952\n",
      "Epoch 0[3130/4499] Val Loss:0.1266624629497528\n",
      "Epoch 0[3131/4499] Val Loss:0.10419253259897232\n",
      "Epoch 0[3132/4499] Val Loss:0.1484096199274063\n",
      "Epoch 0[3133/4499] Val Loss:0.11411796510219574\n",
      "Epoch 0[3134/4499] Val Loss:0.12714649736881256\n",
      "Epoch 0[3135/4499] Val Loss:0.12825313210487366\n",
      "Epoch 0[3136/4499] Val Loss:0.10966741293668747\n",
      "Epoch 0[3137/4499] Val Loss:0.10477414727210999\n",
      "Epoch 0[3138/4499] Val Loss:0.09950447082519531\n",
      "Epoch 0[3139/4499] Val Loss:0.11483222991228104\n",
      "Epoch 0[3140/4499] Val Loss:0.11453405022621155\n",
      "Epoch 0[3141/4499] Val Loss:0.20044852793216705\n",
      "Epoch 0[3142/4499] Val Loss:0.13410697877407074\n",
      "Epoch 0[3143/4499] Val Loss:0.16184863448143005\n",
      "Epoch 0[3144/4499] Val Loss:0.09763337671756744\n",
      "Epoch 0[3145/4499] Val Loss:0.13004446029663086\n",
      "Epoch 0[3146/4499] Val Loss:0.10115983337163925\n",
      "Epoch 0[3147/4499] Val Loss:0.09211620688438416\n",
      "Epoch 0[3148/4499] Val Loss:0.13960058987140656\n",
      "Epoch 0[3149/4499] Val Loss:0.0819297805428505\n",
      "Epoch 0[3150/4499] Val Loss:0.08708599954843521\n",
      "Epoch 0[3151/4499] Val Loss:0.06950893253087997\n",
      "Epoch 0[3152/4499] Val Loss:0.1498437374830246\n",
      "Epoch 0[3153/4499] Val Loss:0.05454171076416969\n",
      "Epoch 0[3154/4499] Val Loss:0.07935543358325958\n",
      "Epoch 0[3155/4499] Val Loss:0.09899143129587173\n",
      "Epoch 0[3156/4499] Val Loss:0.08038409054279327\n",
      "Epoch 0[3157/4499] Val Loss:0.09341958165168762\n",
      "Epoch 0[3158/4499] Val Loss:0.10663244873285294\n",
      "Epoch 0[3159/4499] Val Loss:0.1301157921552658\n",
      "Epoch 0[3160/4499] Val Loss:0.19016601145267487\n",
      "Epoch 0[3161/4499] Val Loss:0.13041923940181732\n",
      "Epoch 0[3162/4499] Val Loss:0.08635155111551285\n",
      "Epoch 0[3163/4499] Val Loss:0.06225452944636345\n",
      "Epoch 0[3164/4499] Val Loss:0.06463685631752014\n",
      "Epoch 0[3165/4499] Val Loss:0.11885812878608704\n",
      "Epoch 0[3166/4499] Val Loss:0.14245985448360443\n",
      "Epoch 0[3167/4499] Val Loss:0.11468227207660675\n",
      "Epoch 0[3168/4499] Val Loss:0.0645340234041214\n",
      "Epoch 0[3169/4499] Val Loss:0.08201184123754501\n",
      "Epoch 0[3170/4499] Val Loss:0.2250594198703766\n",
      "Epoch 0[3171/4499] Val Loss:0.10704826563596725\n",
      "Epoch 0[3172/4499] Val Loss:0.14390529692173004\n",
      "Epoch 0[3173/4499] Val Loss:0.17234109342098236\n",
      "Epoch 0[3174/4499] Val Loss:0.09212794899940491\n",
      "Epoch 0[3175/4499] Val Loss:0.08540346473455429\n",
      "Epoch 0[3176/4499] Val Loss:0.12263353914022446\n",
      "Epoch 0[3177/4499] Val Loss:0.12225859612226486\n",
      "Epoch 0[3178/4499] Val Loss:0.12478873878717422\n",
      "Epoch 0[3179/4499] Val Loss:0.16241025924682617\n",
      "Epoch 0[3180/4499] Val Loss:0.1087939590215683\n",
      "Epoch 0[3181/4499] Val Loss:0.11559142917394638\n",
      "Epoch 0[3182/4499] Val Loss:0.08460568636655807\n",
      "Epoch 0[3183/4499] Val Loss:0.12629927694797516\n",
      "Epoch 0[3184/4499] Val Loss:0.12792561948299408\n",
      "Epoch 0[3185/4499] Val Loss:0.09761137515306473\n",
      "Epoch 0[3186/4499] Val Loss:0.0918920561671257\n",
      "Epoch 0[3187/4499] Val Loss:0.08711698651313782\n",
      "Epoch 0[3188/4499] Val Loss:0.09841592609882355\n",
      "Epoch 0[3189/4499] Val Loss:0.08832687884569168\n",
      "Epoch 0[3190/4499] Val Loss:0.07000020891427994\n",
      "Epoch 0[3191/4499] Val Loss:0.1278521567583084\n",
      "Epoch 0[3192/4499] Val Loss:0.10364161431789398\n",
      "Epoch 0[3193/4499] Val Loss:0.10263736546039581\n",
      "Epoch 0[3194/4499] Val Loss:0.0861779972910881\n",
      "Epoch 0[3195/4499] Val Loss:0.3736130893230438\n",
      "Epoch 0[3196/4499] Val Loss:0.49411311745643616\n",
      "Epoch 0[3197/4499] Val Loss:0.26473328471183777\n",
      "Epoch 0[3198/4499] Val Loss:0.1651088148355484\n",
      "Epoch 0[3199/4499] Val Loss:0.09166674315929413\n",
      "Epoch 0[3200/4499] Val Loss:0.06710986793041229\n",
      "Epoch 0[3201/4499] Val Loss:0.08890039473772049\n",
      "Epoch 0[3202/4499] Val Loss:0.08573023229837418\n",
      "Epoch 0[3203/4499] Val Loss:0.08571575582027435\n",
      "Epoch 0[3204/4499] Val Loss:0.12578773498535156\n",
      "Epoch 0[3205/4499] Val Loss:0.09084885567426682\n",
      "Epoch 0[3206/4499] Val Loss:0.09800799190998077\n",
      "Epoch 0[3207/4499] Val Loss:0.15186238288879395\n",
      "Epoch 0[3208/4499] Val Loss:0.17295631766319275\n",
      "Epoch 0[3209/4499] Val Loss:0.17943905293941498\n",
      "Epoch 0[3210/4499] Val Loss:0.12247417122125626\n",
      "Epoch 0[3211/4499] Val Loss:0.09927099943161011\n",
      "Epoch 0[3212/4499] Val Loss:0.1596052050590515\n",
      "Epoch 0[3213/4499] Val Loss:0.19270335137844086\n",
      "Epoch 0[3214/4499] Val Loss:0.19833138585090637\n",
      "Epoch 0[3215/4499] Val Loss:0.11990240216255188\n",
      "Epoch 0[3216/4499] Val Loss:0.20109502971172333\n",
      "Epoch 0[3217/4499] Val Loss:0.22948512434959412\n",
      "Epoch 0[3218/4499] Val Loss:0.12544289231300354\n",
      "Epoch 0[3219/4499] Val Loss:0.20653119683265686\n",
      "Epoch 0[3220/4499] Val Loss:0.30913689732551575\n",
      "Epoch 0[3221/4499] Val Loss:0.34411340951919556\n",
      "Epoch 0[3222/4499] Val Loss:0.09599610418081284\n",
      "Epoch 0[3223/4499] Val Loss:0.4007924795150757\n",
      "Epoch 0[3224/4499] Val Loss:0.4168519675731659\n",
      "Epoch 0[3225/4499] Val Loss:0.21054992079734802\n",
      "Epoch 0[3226/4499] Val Loss:0.22168798744678497\n",
      "Epoch 0[3227/4499] Val Loss:0.32009410858154297\n",
      "Epoch 0[3228/4499] Val Loss:0.28706595301628113\n",
      "Epoch 0[3229/4499] Val Loss:0.17291045188903809\n",
      "Epoch 0[3230/4499] Val Loss:0.23054789006710052\n",
      "Epoch 0[3231/4499] Val Loss:0.23681193590164185\n",
      "Epoch 0[3232/4499] Val Loss:0.29454246163368225\n",
      "Epoch 0[3233/4499] Val Loss:0.167957603931427\n",
      "Epoch 0[3234/4499] Val Loss:0.26068511605262756\n",
      "Epoch 0[3235/4499] Val Loss:0.2861083745956421\n",
      "Epoch 0[3236/4499] Val Loss:0.08785304427146912\n",
      "Epoch 0[3237/4499] Val Loss:0.40294477343559265\n",
      "Epoch 0[3238/4499] Val Loss:0.2254062443971634\n",
      "Epoch 0[3239/4499] Val Loss:0.24334575235843658\n",
      "Epoch 0[3240/4499] Val Loss:0.41298815608024597\n",
      "Epoch 0[3241/4499] Val Loss:0.0645333081483841\n",
      "Epoch 0[3242/4499] Val Loss:0.41086044907569885\n",
      "Epoch 0[3243/4499] Val Loss:0.14794817566871643\n",
      "Epoch 0[3244/4499] Val Loss:0.18538174033164978\n",
      "Epoch 0[3245/4499] Val Loss:0.19184604287147522\n",
      "Epoch 0[3246/4499] Val Loss:0.09432445466518402\n",
      "Epoch 0[3247/4499] Val Loss:0.21460318565368652\n",
      "Epoch 0[3248/4499] Val Loss:0.049902867525815964\n",
      "Epoch 0[3249/4499] Val Loss:0.1491648107767105\n",
      "Epoch 0[3250/4499] Val Loss:0.06762990355491638\n",
      "Epoch 0[3251/4499] Val Loss:0.17520657181739807\n",
      "Epoch 0[3252/4499] Val Loss:0.10744795948266983\n",
      "Epoch 0[3253/4499] Val Loss:0.1087367907166481\n",
      "Epoch 0[3254/4499] Val Loss:0.1389823704957962\n",
      "Epoch 0[3255/4499] Val Loss:0.06907077878713608\n",
      "Epoch 0[3256/4499] Val Loss:0.14398907124996185\n",
      "Epoch 0[3257/4499] Val Loss:0.11079588532447815\n",
      "Epoch 0[3258/4499] Val Loss:0.16412268579006195\n",
      "Epoch 0[3259/4499] Val Loss:0.16058504581451416\n",
      "Epoch 0[3260/4499] Val Loss:0.18781965970993042\n",
      "Epoch 0[3261/4499] Val Loss:0.1804073005914688\n",
      "Epoch 0[3262/4499] Val Loss:0.07670020312070847\n",
      "Epoch 0[3263/4499] Val Loss:0.1543191522359848\n",
      "Epoch 0[3264/4499] Val Loss:0.08256358653306961\n",
      "Epoch 0[3265/4499] Val Loss:0.10558091849088669\n",
      "Epoch 0[3266/4499] Val Loss:0.09528146684169769\n",
      "Epoch 0[3267/4499] Val Loss:0.052980199456214905\n",
      "Epoch 0[3268/4499] Val Loss:0.10462094098329544\n",
      "Epoch 0[3269/4499] Val Loss:0.06210026517510414\n",
      "Epoch 0[3270/4499] Val Loss:0.15449081361293793\n",
      "Epoch 0[3271/4499] Val Loss:0.08156351745128632\n",
      "Epoch 0[3272/4499] Val Loss:0.05032646283507347\n",
      "Epoch 0[3273/4499] Val Loss:0.08413255214691162\n",
      "Epoch 0[3274/4499] Val Loss:0.05626331642270088\n",
      "Epoch 0[3275/4499] Val Loss:0.06890209019184113\n",
      "Epoch 0[3276/4499] Val Loss:0.09952563047409058\n",
      "Epoch 0[3277/4499] Val Loss:0.05033092200756073\n",
      "Epoch 0[3278/4499] Val Loss:0.12710903584957123\n",
      "Epoch 0[3279/4499] Val Loss:0.10031889379024506\n",
      "Epoch 0[3280/4499] Val Loss:0.06360307335853577\n",
      "Epoch 0[3281/4499] Val Loss:0.07214216142892838\n",
      "Epoch 0[3282/4499] Val Loss:0.08163057267665863\n",
      "Epoch 0[3283/4499] Val Loss:0.15521815419197083\n",
      "Epoch 0[3284/4499] Val Loss:0.12719522416591644\n",
      "Epoch 0[3285/4499] Val Loss:0.09265803545713425\n",
      "Epoch 0[3286/4499] Val Loss:0.1084691733121872\n",
      "Epoch 0[3287/4499] Val Loss:0.09208770841360092\n",
      "Epoch 0[3288/4499] Val Loss:0.12442723661661148\n",
      "Epoch 0[3289/4499] Val Loss:0.0818931832909584\n",
      "Epoch 0[3290/4499] Val Loss:0.13063734769821167\n",
      "Epoch 0[3291/4499] Val Loss:0.13376300036907196\n",
      "Epoch 0[3292/4499] Val Loss:0.13749994337558746\n",
      "Epoch 0[3293/4499] Val Loss:0.09028693288564682\n",
      "Epoch 0[3294/4499] Val Loss:0.0859842449426651\n",
      "Epoch 0[3295/4499] Val Loss:0.11593452841043472\n",
      "Epoch 0[3296/4499] Val Loss:0.11483696848154068\n",
      "Epoch 0[3297/4499] Val Loss:0.14638233184814453\n",
      "Epoch 0[3298/4499] Val Loss:0.14671002328395844\n",
      "Epoch 0[3299/4499] Val Loss:0.10050774365663528\n",
      "Epoch 0[3300/4499] Val Loss:0.13700319826602936\n",
      "Epoch 0[3301/4499] Val Loss:0.12469237297773361\n",
      "Epoch 0[3302/4499] Val Loss:0.155025914311409\n",
      "Epoch 0[3303/4499] Val Loss:0.14032860100269318\n",
      "Epoch 0[3304/4499] Val Loss:0.11317142099142075\n",
      "Epoch 0[3305/4499] Val Loss:0.0918571874499321\n",
      "Epoch 0[3306/4499] Val Loss:0.12141762673854828\n",
      "Epoch 0[3307/4499] Val Loss:0.2613605558872223\n",
      "Epoch 0[3308/4499] Val Loss:0.44339466094970703\n",
      "Epoch 0[3309/4499] Val Loss:0.14778271317481995\n",
      "Epoch 0[3310/4499] Val Loss:0.27929869294166565\n",
      "Epoch 0[3311/4499] Val Loss:0.21878263354301453\n",
      "Epoch 0[3312/4499] Val Loss:0.14867034554481506\n",
      "Epoch 0[3313/4499] Val Loss:0.16919758915901184\n",
      "Epoch 0[3314/4499] Val Loss:0.1460881233215332\n",
      "Epoch 0[3315/4499] Val Loss:0.2651823163032532\n",
      "Epoch 0[3316/4499] Val Loss:0.20712140202522278\n",
      "Epoch 0[3317/4499] Val Loss:0.21079161763191223\n",
      "Epoch 0[3318/4499] Val Loss:0.14563189446926117\n",
      "Epoch 0[3319/4499] Val Loss:0.18490532040596008\n",
      "Epoch 0[3320/4499] Val Loss:0.2434649020433426\n",
      "Epoch 0[3321/4499] Val Loss:0.23732267320156097\n",
      "Epoch 0[3322/4499] Val Loss:0.2843076288700104\n",
      "Epoch 0[3323/4499] Val Loss:0.25617873668670654\n",
      "Epoch 0[3324/4499] Val Loss:0.15165694057941437\n",
      "Epoch 0[3325/4499] Val Loss:0.12285362929105759\n",
      "Epoch 0[3326/4499] Val Loss:0.10988961160182953\n",
      "Epoch 0[3327/4499] Val Loss:0.12676040828227997\n",
      "Epoch 0[3328/4499] Val Loss:0.1599714607000351\n",
      "Epoch 0[3329/4499] Val Loss:0.16675156354904175\n",
      "Epoch 0[3330/4499] Val Loss:0.1888900250196457\n",
      "Epoch 0[3331/4499] Val Loss:0.16076739132404327\n",
      "Epoch 0[3332/4499] Val Loss:0.14558319747447968\n",
      "Epoch 0[3333/4499] Val Loss:0.09572610259056091\n",
      "Epoch 0[3334/4499] Val Loss:0.19481122493743896\n",
      "Epoch 0[3335/4499] Val Loss:0.1976245939731598\n",
      "Epoch 0[3336/4499] Val Loss:0.19276675581932068\n",
      "Epoch 0[3337/4499] Val Loss:0.10737720876932144\n",
      "Epoch 0[3338/4499] Val Loss:0.13969355821609497\n",
      "Epoch 0[3339/4499] Val Loss:0.1415535807609558\n",
      "Epoch 0[3340/4499] Val Loss:0.13318300247192383\n",
      "Epoch 0[3341/4499] Val Loss:0.17467260360717773\n",
      "Epoch 0[3342/4499] Val Loss:0.16340456902980804\n",
      "Epoch 0[3343/4499] Val Loss:0.21880453824996948\n",
      "Epoch 0[3344/4499] Val Loss:0.16149570047855377\n",
      "Epoch 0[3345/4499] Val Loss:0.1887420266866684\n",
      "Epoch 0[3346/4499] Val Loss:0.0962318703532219\n",
      "Epoch 0[3347/4499] Val Loss:0.15152254700660706\n",
      "Epoch 0[3348/4499] Val Loss:0.2402399480342865\n",
      "Epoch 0[3349/4499] Val Loss:0.1387384980916977\n",
      "Epoch 0[3350/4499] Val Loss:0.139621764421463\n",
      "Epoch 0[3351/4499] Val Loss:0.22755908966064453\n",
      "Epoch 0[3352/4499] Val Loss:0.17429989576339722\n",
      "Epoch 0[3353/4499] Val Loss:0.13428647816181183\n",
      "Epoch 0[3354/4499] Val Loss:0.1713094860315323\n",
      "Epoch 0[3355/4499] Val Loss:0.16061373054981232\n",
      "Epoch 0[3356/4499] Val Loss:0.15996481478214264\n",
      "Epoch 0[3357/4499] Val Loss:0.1479317545890808\n",
      "Epoch 0[3358/4499] Val Loss:0.07516351342201233\n",
      "Epoch 0[3359/4499] Val Loss:0.14523114264011383\n",
      "Epoch 0[3360/4499] Val Loss:0.2224777191877365\n",
      "Epoch 0[3361/4499] Val Loss:0.12049126625061035\n",
      "Epoch 0[3362/4499] Val Loss:0.10188597440719604\n",
      "Epoch 0[3363/4499] Val Loss:0.09302706271409988\n",
      "Epoch 0[3364/4499] Val Loss:0.11497148871421814\n",
      "Epoch 0[3365/4499] Val Loss:0.10614017397165298\n",
      "Epoch 0[3366/4499] Val Loss:0.11222647875547409\n",
      "Epoch 0[3367/4499] Val Loss:0.13552504777908325\n",
      "Epoch 0[3368/4499] Val Loss:0.18829411268234253\n",
      "Epoch 0[3369/4499] Val Loss:0.13030113279819489\n",
      "Epoch 0[3370/4499] Val Loss:0.16455145180225372\n",
      "Epoch 0[3371/4499] Val Loss:0.11364000290632248\n",
      "Epoch 0[3372/4499] Val Loss:0.1696900725364685\n",
      "Epoch 0[3373/4499] Val Loss:0.276614248752594\n",
      "Epoch 0[3374/4499] Val Loss:0.21428699791431427\n",
      "Epoch 0[3375/4499] Val Loss:0.2837235927581787\n",
      "Epoch 0[3376/4499] Val Loss:0.3039388060569763\n",
      "Epoch 0[3377/4499] Val Loss:0.23053137958049774\n",
      "Epoch 0[3378/4499] Val Loss:0.30034127831459045\n",
      "Epoch 0[3379/4499] Val Loss:0.2210022211074829\n",
      "Epoch 0[3380/4499] Val Loss:0.2150091826915741\n",
      "Epoch 0[3381/4499] Val Loss:0.3040349781513214\n",
      "Epoch 0[3382/4499] Val Loss:0.29260188341140747\n",
      "Epoch 0[3383/4499] Val Loss:0.4034731686115265\n",
      "Epoch 0[3384/4499] Val Loss:0.25454825162887573\n",
      "Epoch 0[3385/4499] Val Loss:0.2824244201183319\n",
      "Epoch 0[3386/4499] Val Loss:0.3163147270679474\n",
      "Epoch 0[3387/4499] Val Loss:0.3425706923007965\n",
      "Epoch 0[3388/4499] Val Loss:0.37133851647377014\n",
      "Epoch 0[3389/4499] Val Loss:0.4966089725494385\n",
      "Epoch 0[3390/4499] Val Loss:0.40280285477638245\n",
      "Epoch 0[3391/4499] Val Loss:0.48192718625068665\n",
      "Epoch 0[3392/4499] Val Loss:0.37292665243148804\n",
      "Epoch 0[3393/4499] Val Loss:0.49644583463668823\n",
      "Epoch 0[3394/4499] Val Loss:0.5096651315689087\n",
      "Epoch 0[3395/4499] Val Loss:0.5588871836662292\n",
      "Epoch 0[3396/4499] Val Loss:0.49745023250579834\n",
      "Epoch 0[3397/4499] Val Loss:0.5216261744499207\n",
      "Epoch 0[3398/4499] Val Loss:0.48642677068710327\n",
      "Epoch 0[3399/4499] Val Loss:0.5731079578399658\n",
      "Epoch 0[3400/4499] Val Loss:0.43885380029678345\n",
      "Epoch 0[3401/4499] Val Loss:0.5361056923866272\n",
      "Epoch 0[3402/4499] Val Loss:0.6316948533058167\n",
      "Epoch 0[3403/4499] Val Loss:0.46219027042388916\n",
      "Epoch 0[3404/4499] Val Loss:0.41568833589553833\n",
      "Epoch 0[3405/4499] Val Loss:0.48490458726882935\n",
      "Epoch 0[3406/4499] Val Loss:0.7128584384918213\n",
      "Epoch 0[3407/4499] Val Loss:0.5781877636909485\n",
      "Epoch 0[3408/4499] Val Loss:0.5261501669883728\n",
      "Epoch 0[3409/4499] Val Loss:0.38442787528038025\n",
      "Epoch 0[3410/4499] Val Loss:0.42107442021369934\n",
      "Epoch 0[3411/4499] Val Loss:0.44972312450408936\n",
      "Epoch 0[3412/4499] Val Loss:0.42815127968788147\n",
      "Epoch 0[3413/4499] Val Loss:0.42878782749176025\n",
      "Epoch 0[3414/4499] Val Loss:0.3574574291706085\n",
      "Epoch 0[3415/4499] Val Loss:0.23338159918785095\n",
      "Epoch 0[3416/4499] Val Loss:0.19132910668849945\n",
      "Epoch 0[3417/4499] Val Loss:0.2267853319644928\n",
      "Epoch 0[3418/4499] Val Loss:0.26223644614219666\n",
      "Epoch 0[3419/4499] Val Loss:0.24924147129058838\n",
      "Epoch 0[3420/4499] Val Loss:0.25031670928001404\n",
      "Epoch 0[3421/4499] Val Loss:0.15377210080623627\n",
      "Epoch 0[3422/4499] Val Loss:0.2722605764865875\n",
      "Epoch 0[3423/4499] Val Loss:0.2753622233867645\n",
      "Epoch 0[3424/4499] Val Loss:0.32916077971458435\n",
      "Epoch 0[3425/4499] Val Loss:0.3508608043193817\n",
      "Epoch 0[3426/4499] Val Loss:0.37796977162361145\n",
      "Epoch 0[3427/4499] Val Loss:0.5480220913887024\n",
      "Epoch 0[3428/4499] Val Loss:0.28704404830932617\n",
      "Epoch 0[3429/4499] Val Loss:0.3389383852481842\n",
      "Epoch 0[3430/4499] Val Loss:0.3810884356498718\n",
      "Epoch 0[3431/4499] Val Loss:0.19927865266799927\n",
      "Epoch 0[3432/4499] Val Loss:0.14795739948749542\n",
      "Epoch 0[3433/4499] Val Loss:0.17279933393001556\n",
      "Epoch 0[3434/4499] Val Loss:0.18970976769924164\n",
      "Epoch 0[3435/4499] Val Loss:0.13209372758865356\n",
      "Epoch 0[3436/4499] Val Loss:0.11892160773277283\n",
      "Epoch 0[3437/4499] Val Loss:0.10507020354270935\n",
      "Epoch 0[3438/4499] Val Loss:0.1575741320848465\n",
      "Epoch 0[3439/4499] Val Loss:0.11510695517063141\n",
      "Epoch 0[3440/4499] Val Loss:0.12569455802440643\n",
      "Epoch 0[3441/4499] Val Loss:0.2246544510126114\n",
      "Epoch 0[3442/4499] Val Loss:0.32600831985473633\n",
      "Epoch 0[3443/4499] Val Loss:0.2510763704776764\n",
      "Epoch 0[3444/4499] Val Loss:0.18682798743247986\n",
      "Epoch 0[3445/4499] Val Loss:0.23320364952087402\n",
      "Epoch 0[3446/4499] Val Loss:0.20494122803211212\n",
      "Epoch 0[3447/4499] Val Loss:0.1384928971529007\n",
      "Epoch 0[3448/4499] Val Loss:0.14494317770004272\n",
      "Epoch 0[3449/4499] Val Loss:0.18217653036117554\n",
      "Epoch 0[3450/4499] Val Loss:0.20466645061969757\n",
      "Epoch 0[3451/4499] Val Loss:0.13812284171581268\n",
      "Epoch 0[3452/4499] Val Loss:0.13809986412525177\n",
      "Epoch 0[3453/4499] Val Loss:0.1775728166103363\n",
      "Epoch 0[3454/4499] Val Loss:0.1870688796043396\n",
      "Epoch 0[3455/4499] Val Loss:0.12654997408390045\n",
      "Epoch 0[3456/4499] Val Loss:0.08459648489952087\n",
      "Epoch 0[3457/4499] Val Loss:0.1725064367055893\n",
      "Epoch 0[3458/4499] Val Loss:0.13175725936889648\n",
      "Epoch 0[3459/4499] Val Loss:0.08549758046865463\n",
      "Epoch 0[3460/4499] Val Loss:0.19101335108280182\n",
      "Epoch 0[3461/4499] Val Loss:0.13394427299499512\n",
      "Epoch 0[3462/4499] Val Loss:0.09691637009382248\n",
      "Epoch 0[3463/4499] Val Loss:0.10305525362491608\n",
      "Epoch 0[3464/4499] Val Loss:0.1239752247929573\n",
      "Epoch 0[3465/4499] Val Loss:0.11242049932479858\n",
      "Epoch 0[3466/4499] Val Loss:0.13470683991909027\n",
      "Epoch 0[3467/4499] Val Loss:0.17271900177001953\n",
      "Epoch 0[3468/4499] Val Loss:0.11061365157365799\n",
      "Epoch 0[3469/4499] Val Loss:0.16308026015758514\n",
      "Epoch 0[3470/4499] Val Loss:0.18312007188796997\n",
      "Epoch 0[3471/4499] Val Loss:0.15074294805526733\n",
      "Epoch 0[3472/4499] Val Loss:0.1591346710920334\n",
      "Epoch 0[3473/4499] Val Loss:0.20531730353832245\n",
      "Epoch 0[3474/4499] Val Loss:0.17465269565582275\n",
      "Epoch 0[3475/4499] Val Loss:0.2151639610528946\n",
      "Epoch 0[3476/4499] Val Loss:0.2001231461763382\n",
      "Epoch 0[3477/4499] Val Loss:0.25150787830352783\n",
      "Epoch 0[3478/4499] Val Loss:0.2401387244462967\n",
      "Epoch 0[3479/4499] Val Loss:0.33572080731391907\n",
      "Epoch 0[3480/4499] Val Loss:0.2097732275724411\n",
      "Epoch 0[3481/4499] Val Loss:0.33115702867507935\n",
      "Epoch 0[3482/4499] Val Loss:0.25633859634399414\n",
      "Epoch 0[3483/4499] Val Loss:0.3213212490081787\n",
      "Epoch 0[3484/4499] Val Loss:0.33287814259529114\n",
      "Epoch 0[3485/4499] Val Loss:0.35516229271888733\n",
      "Epoch 0[3486/4499] Val Loss:0.3573666214942932\n",
      "Epoch 0[3487/4499] Val Loss:0.24269799888134003\n",
      "Epoch 0[3488/4499] Val Loss:0.4941105246543884\n",
      "Epoch 0[3489/4499] Val Loss:0.3759753704071045\n",
      "Epoch 0[3490/4499] Val Loss:0.4565609097480774\n",
      "Epoch 0[3491/4499] Val Loss:0.4939410090446472\n",
      "Epoch 0[3492/4499] Val Loss:0.4497951865196228\n",
      "Epoch 0[3493/4499] Val Loss:0.5235008001327515\n",
      "Epoch 0[3494/4499] Val Loss:0.3741920292377472\n",
      "Epoch 0[3495/4499] Val Loss:0.64445960521698\n",
      "Epoch 0[3496/4499] Val Loss:0.6946188807487488\n",
      "Epoch 0[3497/4499] Val Loss:0.5393150448799133\n",
      "Epoch 0[3498/4499] Val Loss:0.4433041214942932\n",
      "Epoch 0[3499/4499] Val Loss:0.6250683665275574\n",
      "Epoch 0[3500/4499] Val Loss:0.5965577363967896\n",
      "Epoch 0[3501/4499] Val Loss:0.6046348214149475\n",
      "Epoch 0[3502/4499] Val Loss:0.6909143924713135\n",
      "Epoch 0[3503/4499] Val Loss:0.47039663791656494\n",
      "Epoch 0[3504/4499] Val Loss:0.5485502481460571\n",
      "Epoch 0[3505/4499] Val Loss:0.4674876630306244\n",
      "Epoch 0[3506/4499] Val Loss:0.534761369228363\n",
      "Epoch 0[3507/4499] Val Loss:0.6272813081741333\n",
      "Epoch 0[3508/4499] Val Loss:0.24152976274490356\n",
      "Epoch 0[3509/4499] Val Loss:0.2620016038417816\n",
      "Epoch 0[3510/4499] Val Loss:0.4881604015827179\n",
      "Epoch 0[3511/4499] Val Loss:0.26451846957206726\n",
      "Epoch 0[3512/4499] Val Loss:0.29161536693573\n",
      "Epoch 0[3513/4499] Val Loss:0.34903183579444885\n",
      "Epoch 0[3514/4499] Val Loss:0.3095214366912842\n",
      "Epoch 0[3515/4499] Val Loss:0.31040966510772705\n",
      "Epoch 0[3516/4499] Val Loss:0.2192080318927765\n",
      "Epoch 0[3517/4499] Val Loss:0.27271774411201477\n",
      "Epoch 0[3518/4499] Val Loss:0.26858532428741455\n",
      "Epoch 0[3519/4499] Val Loss:0.3684423565864563\n",
      "Epoch 0[3520/4499] Val Loss:0.6471121907234192\n",
      "Epoch 0[3521/4499] Val Loss:0.4856327474117279\n",
      "Epoch 0[3522/4499] Val Loss:0.2418963462114334\n",
      "Epoch 0[3523/4499] Val Loss:0.15760956704616547\n",
      "Epoch 0[3524/4499] Val Loss:0.2331528663635254\n",
      "Epoch 0[3525/4499] Val Loss:0.23853716254234314\n",
      "Epoch 0[3526/4499] Val Loss:0.1917710304260254\n",
      "Epoch 0[3527/4499] Val Loss:0.16331787407398224\n",
      "Epoch 0[3528/4499] Val Loss:0.15272502601146698\n",
      "Epoch 0[3529/4499] Val Loss:0.2039594054222107\n",
      "Epoch 0[3530/4499] Val Loss:0.12576672434806824\n",
      "Epoch 0[3531/4499] Val Loss:0.1626121550798416\n",
      "Epoch 0[3532/4499] Val Loss:0.15680627524852753\n",
      "Epoch 0[3533/4499] Val Loss:0.11269828677177429\n",
      "Epoch 0[3534/4499] Val Loss:0.1244405061006546\n",
      "Epoch 0[3535/4499] Val Loss:0.1976359784603119\n",
      "Epoch 0[3536/4499] Val Loss:0.1558423638343811\n",
      "Epoch 0[3537/4499] Val Loss:0.16537576913833618\n",
      "Epoch 0[3538/4499] Val Loss:0.15376701951026917\n",
      "Epoch 0[3539/4499] Val Loss:0.09515181183815002\n",
      "Epoch 0[3540/4499] Val Loss:0.12709921598434448\n",
      "Epoch 0[3541/4499] Val Loss:0.08359301090240479\n",
      "Epoch 0[3542/4499] Val Loss:0.12668922543525696\n",
      "Epoch 0[3543/4499] Val Loss:0.133151575922966\n",
      "Epoch 0[3544/4499] Val Loss:0.07905754446983337\n",
      "Epoch 0[3545/4499] Val Loss:0.14567260444164276\n",
      "Epoch 0[3546/4499] Val Loss:0.12664982676506042\n",
      "Epoch 0[3547/4499] Val Loss:0.12407572567462921\n",
      "Epoch 0[3548/4499] Val Loss:0.11218898743391037\n",
      "Epoch 0[3549/4499] Val Loss:0.2047891467809677\n",
      "Epoch 0[3550/4499] Val Loss:0.3536376655101776\n",
      "Epoch 0[3551/4499] Val Loss:0.3465064465999603\n",
      "Epoch 0[3552/4499] Val Loss:0.20521019399166107\n",
      "Epoch 0[3553/4499] Val Loss:0.15323406457901\n",
      "Epoch 0[3554/4499] Val Loss:0.21873457729816437\n",
      "Epoch 0[3555/4499] Val Loss:0.16608057916164398\n",
      "Epoch 0[3556/4499] Val Loss:0.3624397814273834\n",
      "Epoch 0[3557/4499] Val Loss:0.3115208148956299\n",
      "Epoch 0[3558/4499] Val Loss:0.207187220454216\n",
      "Epoch 0[3559/4499] Val Loss:0.15051530301570892\n",
      "Epoch 0[3560/4499] Val Loss:0.22197377681732178\n",
      "Epoch 0[3561/4499] Val Loss:0.17854073643684387\n",
      "Epoch 0[3562/4499] Val Loss:0.14893034100532532\n",
      "Epoch 0[3563/4499] Val Loss:0.10808749496936798\n",
      "Epoch 0[3564/4499] Val Loss:0.13980010151863098\n",
      "Epoch 0[3565/4499] Val Loss:0.2581663131713867\n",
      "Epoch 0[3566/4499] Val Loss:0.13694748282432556\n",
      "Epoch 0[3567/4499] Val Loss:0.30214783549308777\n",
      "Epoch 0[3568/4499] Val Loss:0.14251141250133514\n",
      "Epoch 0[3569/4499] Val Loss:0.23381371796131134\n",
      "Epoch 0[3570/4499] Val Loss:0.33139392733573914\n",
      "Epoch 0[3571/4499] Val Loss:0.24835991859436035\n",
      "Epoch 0[3572/4499] Val Loss:0.2721672058105469\n",
      "Epoch 0[3573/4499] Val Loss:0.229803666472435\n",
      "Epoch 0[3574/4499] Val Loss:0.1956922560930252\n",
      "Epoch 0[3575/4499] Val Loss:0.32460007071495056\n",
      "Epoch 0[3576/4499] Val Loss:0.1914166510105133\n",
      "Epoch 0[3577/4499] Val Loss:0.23171524703502655\n",
      "Epoch 0[3578/4499] Val Loss:0.3163159191608429\n",
      "Epoch 0[3579/4499] Val Loss:0.3072521984577179\n",
      "Epoch 0[3580/4499] Val Loss:0.17158815264701843\n",
      "Epoch 0[3581/4499] Val Loss:0.18519547581672668\n",
      "Epoch 0[3582/4499] Val Loss:0.15520019829273224\n",
      "Epoch 0[3583/4499] Val Loss:0.1444057822227478\n",
      "Epoch 0[3584/4499] Val Loss:0.10703867673873901\n",
      "Epoch 0[3585/4499] Val Loss:0.14416871964931488\n",
      "Epoch 0[3586/4499] Val Loss:0.10492011904716492\n",
      "Epoch 0[3587/4499] Val Loss:0.12463029474020004\n",
      "Epoch 0[3588/4499] Val Loss:0.2666596472263336\n",
      "Epoch 0[3589/4499] Val Loss:0.2381601780653\n",
      "Epoch 0[3590/4499] Val Loss:0.24731707572937012\n",
      "Epoch 0[3591/4499] Val Loss:0.29292604327201843\n",
      "Epoch 0[3592/4499] Val Loss:0.21528221666812897\n",
      "Epoch 0[3593/4499] Val Loss:0.08704888075590134\n",
      "Epoch 0[3594/4499] Val Loss:0.10460889339447021\n",
      "Epoch 0[3595/4499] Val Loss:0.07393784075975418\n",
      "Epoch 0[3596/4499] Val Loss:0.14530602097511292\n",
      "Epoch 0[3597/4499] Val Loss:0.22996941208839417\n",
      "Epoch 0[3598/4499] Val Loss:0.10837845504283905\n",
      "Epoch 0[3599/4499] Val Loss:0.13353636860847473\n",
      "Epoch 0[3600/4499] Val Loss:0.13548806309700012\n",
      "Epoch 0[3601/4499] Val Loss:0.21185477077960968\n",
      "Epoch 0[3602/4499] Val Loss:0.14445307850837708\n",
      "Epoch 0[3603/4499] Val Loss:0.18144123256206512\n",
      "Epoch 0[3604/4499] Val Loss:0.15856988728046417\n",
      "Epoch 0[3605/4499] Val Loss:0.17518623173236847\n",
      "Epoch 0[3606/4499] Val Loss:0.23572257161140442\n",
      "Epoch 0[3607/4499] Val Loss:0.15654806792736053\n",
      "Epoch 0[3608/4499] Val Loss:0.19803734123706818\n",
      "Epoch 0[3609/4499] Val Loss:0.2892454266548157\n",
      "Epoch 0[3610/4499] Val Loss:0.1367102861404419\n",
      "Epoch 0[3611/4499] Val Loss:0.23554886877536774\n",
      "Epoch 0[3612/4499] Val Loss:0.18755637109279633\n",
      "Epoch 0[3613/4499] Val Loss:0.245436429977417\n",
      "Epoch 0[3614/4499] Val Loss:0.19067680835723877\n",
      "Epoch 0[3615/4499] Val Loss:0.30256232619285583\n",
      "Epoch 0[3616/4499] Val Loss:0.2902250587940216\n",
      "Epoch 0[3617/4499] Val Loss:0.20513421297073364\n",
      "Epoch 0[3618/4499] Val Loss:0.32613274455070496\n",
      "Epoch 0[3619/4499] Val Loss:0.3732243776321411\n",
      "Epoch 0[3620/4499] Val Loss:0.3196873664855957\n",
      "Epoch 0[3621/4499] Val Loss:0.39034438133239746\n",
      "Epoch 0[3622/4499] Val Loss:0.4043116569519043\n",
      "Epoch 0[3623/4499] Val Loss:0.34011057019233704\n",
      "Epoch 0[3624/4499] Val Loss:0.30093830823898315\n",
      "Epoch 0[3625/4499] Val Loss:0.34607023000717163\n",
      "Epoch 0[3626/4499] Val Loss:0.3333292603492737\n",
      "Epoch 0[3627/4499] Val Loss:0.36808183789253235\n",
      "Epoch 0[3628/4499] Val Loss:0.32220837473869324\n",
      "Epoch 0[3629/4499] Val Loss:0.28203284740448\n",
      "Epoch 0[3630/4499] Val Loss:0.344948410987854\n",
      "Epoch 0[3631/4499] Val Loss:0.29211533069610596\n",
      "Epoch 0[3632/4499] Val Loss:0.36839890480041504\n",
      "Epoch 0[3633/4499] Val Loss:0.5091100931167603\n",
      "Epoch 0[3634/4499] Val Loss:0.7384804487228394\n",
      "Epoch 0[3635/4499] Val Loss:0.6556559205055237\n",
      "Epoch 0[3636/4499] Val Loss:0.8297720551490784\n",
      "Epoch 0[3637/4499] Val Loss:0.9349650144577026\n",
      "Epoch 0[3638/4499] Val Loss:0.6681194305419922\n",
      "Epoch 0[3639/4499] Val Loss:0.6015446782112122\n",
      "Epoch 0[3640/4499] Val Loss:0.5811834335327148\n",
      "Epoch 0[3641/4499] Val Loss:0.4169465899467468\n",
      "Epoch 0[3642/4499] Val Loss:0.6829219460487366\n",
      "Epoch 0[3643/4499] Val Loss:0.6375086903572083\n",
      "Epoch 0[3644/4499] Val Loss:0.6480535864830017\n",
      "Epoch 0[3645/4499] Val Loss:0.45013049244880676\n",
      "Epoch 0[3646/4499] Val Loss:0.6155263185501099\n",
      "Epoch 0[3647/4499] Val Loss:0.55231773853302\n",
      "Epoch 0[3648/4499] Val Loss:0.5686116218566895\n",
      "Epoch 0[3649/4499] Val Loss:0.5021191239356995\n",
      "Epoch 0[3650/4499] Val Loss:0.5573008060455322\n",
      "Epoch 0[3651/4499] Val Loss:0.6094828844070435\n",
      "Epoch 0[3652/4499] Val Loss:0.5090188384056091\n",
      "Epoch 0[3653/4499] Val Loss:0.33170241117477417\n",
      "Epoch 0[3654/4499] Val Loss:0.328067809343338\n",
      "Epoch 0[3655/4499] Val Loss:0.3499925434589386\n",
      "Epoch 0[3656/4499] Val Loss:0.32428687810897827\n",
      "Epoch 0[3657/4499] Val Loss:0.4569967985153198\n",
      "Epoch 0[3658/4499] Val Loss:0.4598780572414398\n",
      "Epoch 0[3659/4499] Val Loss:0.4813561737537384\n",
      "Epoch 0[3660/4499] Val Loss:0.5196774005889893\n",
      "Epoch 0[3661/4499] Val Loss:0.4541287422180176\n",
      "Epoch 0[3662/4499] Val Loss:0.5318859219551086\n",
      "Epoch 0[3663/4499] Val Loss:0.5238872170448303\n",
      "Epoch 0[3664/4499] Val Loss:0.5397895574569702\n",
      "Epoch 0[3665/4499] Val Loss:0.46929195523262024\n",
      "Epoch 0[3666/4499] Val Loss:0.3570384383201599\n",
      "Epoch 0[3667/4499] Val Loss:0.3492761254310608\n",
      "Epoch 0[3668/4499] Val Loss:0.4084461033344269\n",
      "Epoch 0[3669/4499] Val Loss:0.41197189688682556\n",
      "Epoch 0[3670/4499] Val Loss:0.4418523907661438\n",
      "Epoch 0[3671/4499] Val Loss:0.5183889865875244\n",
      "Epoch 0[3672/4499] Val Loss:0.4707099199295044\n",
      "Epoch 0[3673/4499] Val Loss:0.46541622281074524\n",
      "Epoch 0[3674/4499] Val Loss:0.3902002274990082\n",
      "Epoch 0[3675/4499] Val Loss:0.5353195071220398\n",
      "Epoch 0[3676/4499] Val Loss:0.4131007790565491\n",
      "Epoch 0[3677/4499] Val Loss:0.34275123476982117\n",
      "Epoch 0[3678/4499] Val Loss:0.36373406648635864\n",
      "Epoch 0[3679/4499] Val Loss:0.46233296394348145\n",
      "Epoch 0[3680/4499] Val Loss:0.3427005708217621\n",
      "Epoch 0[3681/4499] Val Loss:0.31709107756614685\n",
      "Epoch 0[3682/4499] Val Loss:0.416305810213089\n",
      "Epoch 0[3683/4499] Val Loss:0.6125151515007019\n",
      "Epoch 0[3684/4499] Val Loss:0.2447432279586792\n",
      "Epoch 0[3685/4499] Val Loss:0.32450342178344727\n",
      "Epoch 0[3686/4499] Val Loss:0.2747112810611725\n",
      "Epoch 0[3687/4499] Val Loss:0.22830349206924438\n",
      "Epoch 0[3688/4499] Val Loss:0.2369466871023178\n",
      "Epoch 0[3689/4499] Val Loss:0.18166212737560272\n",
      "Epoch 0[3690/4499] Val Loss:0.26134902238845825\n",
      "Epoch 0[3691/4499] Val Loss:0.17520588636398315\n",
      "Epoch 0[3692/4499] Val Loss:0.21169136464595795\n",
      "Epoch 0[3693/4499] Val Loss:0.12407786399126053\n",
      "Epoch 0[3694/4499] Val Loss:0.18431150913238525\n",
      "Epoch 0[3695/4499] Val Loss:0.1652071326971054\n",
      "Epoch 0[3696/4499] Val Loss:0.2789710462093353\n",
      "Epoch 0[3697/4499] Val Loss:0.3646145761013031\n",
      "Epoch 0[3698/4499] Val Loss:0.23832790553569794\n",
      "Epoch 0[3699/4499] Val Loss:0.13133159279823303\n",
      "Epoch 0[3700/4499] Val Loss:0.24194639921188354\n",
      "Epoch 0[3701/4499] Val Loss:0.1937340795993805\n",
      "Epoch 0[3702/4499] Val Loss:0.1646193563938141\n",
      "Epoch 0[3703/4499] Val Loss:0.16694726049900055\n",
      "Epoch 0[3704/4499] Val Loss:0.1856442391872406\n",
      "Epoch 0[3705/4499] Val Loss:0.3474917411804199\n",
      "Epoch 0[3706/4499] Val Loss:0.3177952170372009\n",
      "Epoch 0[3707/4499] Val Loss:0.19616080820560455\n",
      "Epoch 0[3708/4499] Val Loss:0.20403538644313812\n",
      "Epoch 0[3709/4499] Val Loss:0.14715903997421265\n",
      "Epoch 0[3710/4499] Val Loss:0.16143518686294556\n",
      "Epoch 0[3711/4499] Val Loss:0.16434310376644135\n",
      "Epoch 0[3712/4499] Val Loss:0.10242212563753128\n",
      "Epoch 0[3713/4499] Val Loss:0.13924771547317505\n",
      "Epoch 0[3714/4499] Val Loss:0.20497602224349976\n",
      "Epoch 0[3715/4499] Val Loss:0.1528560221195221\n",
      "Epoch 0[3716/4499] Val Loss:0.17551656067371368\n",
      "Epoch 0[3717/4499] Val Loss:0.2752470076084137\n",
      "Epoch 0[3718/4499] Val Loss:0.21761736273765564\n",
      "Epoch 0[3719/4499] Val Loss:0.18539081513881683\n",
      "Epoch 0[3720/4499] Val Loss:0.19143545627593994\n",
      "Epoch 0[3721/4499] Val Loss:0.09934443235397339\n",
      "Epoch 0[3722/4499] Val Loss:0.07184107601642609\n",
      "Epoch 0[3723/4499] Val Loss:0.07450948655605316\n",
      "Epoch 0[3724/4499] Val Loss:0.2376154661178589\n",
      "Epoch 0[3725/4499] Val Loss:0.21449290215969086\n",
      "Epoch 0[3726/4499] Val Loss:0.17350544035434723\n",
      "Epoch 0[3727/4499] Val Loss:0.11385229229927063\n",
      "Epoch 0[3728/4499] Val Loss:0.12707017362117767\n",
      "Epoch 0[3729/4499] Val Loss:0.2407943457365036\n",
      "Epoch 0[3730/4499] Val Loss:0.2840718924999237\n",
      "Epoch 0[3731/4499] Val Loss:0.22952526807785034\n",
      "Epoch 0[3732/4499] Val Loss:0.2249690741300583\n",
      "Epoch 0[3733/4499] Val Loss:0.19028648734092712\n",
      "Epoch 0[3734/4499] Val Loss:0.15445789694786072\n",
      "Epoch 0[3735/4499] Val Loss:0.1409541517496109\n",
      "Epoch 0[3736/4499] Val Loss:0.2175890952348709\n",
      "Epoch 0[3737/4499] Val Loss:0.14379173517227173\n",
      "Epoch 0[3738/4499] Val Loss:0.20830745995044708\n",
      "Epoch 0[3739/4499] Val Loss:0.16004493832588196\n",
      "Epoch 0[3740/4499] Val Loss:0.21937429904937744\n",
      "Epoch 0[3741/4499] Val Loss:0.3361664116382599\n",
      "Epoch 0[3742/4499] Val Loss:0.24831436574459076\n",
      "Epoch 0[3743/4499] Val Loss:0.28293976187705994\n",
      "Epoch 0[3744/4499] Val Loss:0.3232262432575226\n",
      "Epoch 0[3745/4499] Val Loss:0.25129958987236023\n",
      "Epoch 0[3746/4499] Val Loss:0.2500557005405426\n",
      "Epoch 0[3747/4499] Val Loss:0.2556536793708801\n",
      "Epoch 0[3748/4499] Val Loss:0.4289817214012146\n",
      "Epoch 0[3749/4499] Val Loss:0.3939763009548187\n",
      "Epoch 0[3750/4499] Val Loss:0.19283375144004822\n",
      "Epoch 0[3751/4499] Val Loss:0.3326335847377777\n",
      "Epoch 0[3752/4499] Val Loss:0.32170870900154114\n",
      "Epoch 0[3753/4499] Val Loss:0.20106621086597443\n",
      "Epoch 0[3754/4499] Val Loss:0.2782159745693207\n",
      "Epoch 0[3755/4499] Val Loss:0.2789764404296875\n",
      "Epoch 0[3756/4499] Val Loss:0.5160729289054871\n",
      "Epoch 0[3757/4499] Val Loss:0.47785863280296326\n",
      "Epoch 0[3758/4499] Val Loss:0.7190880179405212\n",
      "Epoch 0[3759/4499] Val Loss:0.7721313238143921\n",
      "Epoch 0[3760/4499] Val Loss:0.7162238359451294\n",
      "Epoch 0[3761/4499] Val Loss:0.7193521857261658\n",
      "Epoch 0[3762/4499] Val Loss:0.8214890360832214\n",
      "Epoch 0[3763/4499] Val Loss:0.7749276757240295\n",
      "Epoch 0[3764/4499] Val Loss:0.5910559892654419\n",
      "Epoch 0[3765/4499] Val Loss:0.614112138748169\n",
      "Epoch 0[3766/4499] Val Loss:0.5682796239852905\n",
      "Epoch 0[3767/4499] Val Loss:0.448028028011322\n",
      "Epoch 0[3768/4499] Val Loss:0.6793333888053894\n",
      "Epoch 0[3769/4499] Val Loss:0.5362197160720825\n",
      "Epoch 0[3770/4499] Val Loss:0.7451568245887756\n",
      "Epoch 0[3771/4499] Val Loss:0.7482805848121643\n",
      "Epoch 0[3772/4499] Val Loss:0.5588374137878418\n",
      "Epoch 0[3773/4499] Val Loss:0.5606461763381958\n",
      "Epoch 0[3774/4499] Val Loss:0.5870571732521057\n",
      "Epoch 0[3775/4499] Val Loss:0.7207415699958801\n",
      "Epoch 0[3776/4499] Val Loss:0.5913618803024292\n",
      "Epoch 0[3777/4499] Val Loss:0.2955913245677948\n",
      "Epoch 0[3778/4499] Val Loss:0.2625911831855774\n",
      "Epoch 0[3779/4499] Val Loss:0.200859934091568\n",
      "Epoch 0[3780/4499] Val Loss:0.357511043548584\n",
      "Epoch 0[3781/4499] Val Loss:0.36799702048301697\n",
      "Epoch 0[3782/4499] Val Loss:0.5069472789764404\n",
      "Epoch 0[3783/4499] Val Loss:0.181418314576149\n",
      "Epoch 0[3784/4499] Val Loss:0.14869295060634613\n",
      "Epoch 0[3785/4499] Val Loss:0.14073993265628815\n",
      "Epoch 0[3786/4499] Val Loss:0.2205348014831543\n",
      "Epoch 0[3787/4499] Val Loss:0.25983181595802307\n",
      "Epoch 0[3788/4499] Val Loss:0.3045829236507416\n",
      "Epoch 0[3789/4499] Val Loss:0.276968389749527\n",
      "Epoch 0[3790/4499] Val Loss:0.21800267696380615\n",
      "Epoch 0[3791/4499] Val Loss:0.13140101730823517\n",
      "Epoch 0[3792/4499] Val Loss:0.1511145830154419\n",
      "Epoch 0[3793/4499] Val Loss:0.12909439206123352\n",
      "Epoch 0[3794/4499] Val Loss:0.09497203677892685\n",
      "Epoch 0[3795/4499] Val Loss:0.1273217648267746\n",
      "Epoch 0[3796/4499] Val Loss:0.2414378970861435\n",
      "Epoch 0[3797/4499] Val Loss:0.11535853147506714\n",
      "Epoch 0[3798/4499] Val Loss:0.08665463328361511\n",
      "Epoch 0[3799/4499] Val Loss:0.09631701558828354\n",
      "Epoch 0[3800/4499] Val Loss:0.10466781258583069\n",
      "Epoch 0[3801/4499] Val Loss:0.1911451667547226\n",
      "Epoch 0[3802/4499] Val Loss:0.15194427967071533\n",
      "Epoch 0[3803/4499] Val Loss:0.12552456557750702\n",
      "Epoch 0[3804/4499] Val Loss:0.10114079713821411\n",
      "Epoch 0[3805/4499] Val Loss:0.12374311685562134\n",
      "Epoch 0[3806/4499] Val Loss:0.2400071769952774\n",
      "Epoch 0[3807/4499] Val Loss:0.2259853035211563\n",
      "Epoch 0[3808/4499] Val Loss:0.17310522496700287\n",
      "Epoch 0[3809/4499] Val Loss:0.14295388758182526\n",
      "Epoch 0[3810/4499] Val Loss:0.15787586569786072\n",
      "Epoch 0[3811/4499] Val Loss:0.2142043262720108\n",
      "Epoch 0[3812/4499] Val Loss:0.24380140006542206\n",
      "Epoch 0[3813/4499] Val Loss:0.20959866046905518\n",
      "Epoch 0[3814/4499] Val Loss:0.26367899775505066\n",
      "Epoch 0[3815/4499] Val Loss:0.19902482628822327\n",
      "Epoch 0[3816/4499] Val Loss:0.3404213786125183\n",
      "Epoch 0[3817/4499] Val Loss:0.24996089935302734\n",
      "Epoch 0[3818/4499] Val Loss:0.31817731261253357\n",
      "Epoch 0[3819/4499] Val Loss:0.2878000736236572\n",
      "Epoch 0[3820/4499] Val Loss:0.3320777714252472\n",
      "Epoch 0[3821/4499] Val Loss:0.16026221215724945\n",
      "Epoch 0[3822/4499] Val Loss:0.18248850107192993\n",
      "Epoch 0[3823/4499] Val Loss:0.1740715354681015\n",
      "Epoch 0[3824/4499] Val Loss:0.22262710332870483\n",
      "Epoch 0[3825/4499] Val Loss:0.22730639576911926\n",
      "Epoch 0[3826/4499] Val Loss:0.11038344353437424\n",
      "Epoch 0[3827/4499] Val Loss:0.18739740550518036\n",
      "Epoch 0[3828/4499] Val Loss:0.1936260163784027\n",
      "Epoch 0[3829/4499] Val Loss:0.2948853671550751\n",
      "Epoch 0[3830/4499] Val Loss:0.3176596760749817\n",
      "Epoch 0[3831/4499] Val Loss:0.2873728573322296\n",
      "Epoch 0[3832/4499] Val Loss:0.2942432165145874\n",
      "Epoch 0[3833/4499] Val Loss:0.33630070090293884\n",
      "Epoch 0[3834/4499] Val Loss:0.28587234020233154\n",
      "Epoch 0[3835/4499] Val Loss:0.36048585176467896\n",
      "Epoch 0[3836/4499] Val Loss:0.1802138388156891\n",
      "Epoch 0[3837/4499] Val Loss:0.14959459006786346\n",
      "Epoch 0[3838/4499] Val Loss:0.14776551723480225\n",
      "Epoch 0[3839/4499] Val Loss:0.1596289873123169\n",
      "Epoch 0[3840/4499] Val Loss:0.2212042659521103\n",
      "Epoch 0[3841/4499] Val Loss:0.22938670217990875\n",
      "Epoch 0[3842/4499] Val Loss:0.3023638129234314\n",
      "Epoch 0[3843/4499] Val Loss:0.21493472158908844\n",
      "Epoch 0[3844/4499] Val Loss:0.17079757153987885\n",
      "Epoch 0[3845/4499] Val Loss:0.18319788575172424\n",
      "Epoch 0[3846/4499] Val Loss:0.17268864810466766\n",
      "Epoch 0[3847/4499] Val Loss:0.11759786307811737\n",
      "Epoch 0[3848/4499] Val Loss:0.08974070847034454\n",
      "Epoch 0[3849/4499] Val Loss:0.09262924641370773\n",
      "Epoch 0[3850/4499] Val Loss:0.10579495877027512\n",
      "Epoch 0[3851/4499] Val Loss:0.12044518440961838\n",
      "Epoch 0[3852/4499] Val Loss:0.2545643746852875\n",
      "Epoch 0[3853/4499] Val Loss:0.1426887959241867\n",
      "Epoch 0[3854/4499] Val Loss:0.08172059059143066\n",
      "Epoch 0[3855/4499] Val Loss:0.09654459357261658\n",
      "Epoch 0[3856/4499] Val Loss:0.06933380663394928\n",
      "Epoch 0[3857/4499] Val Loss:0.13393478095531464\n",
      "Epoch 0[3858/4499] Val Loss:0.0776936262845993\n",
      "Epoch 0[3859/4499] Val Loss:0.1641635149717331\n",
      "Epoch 0[3860/4499] Val Loss:0.2323116660118103\n",
      "Epoch 0[3861/4499] Val Loss:0.2854263484477997\n",
      "Epoch 0[3862/4499] Val Loss:0.21094009280204773\n",
      "Epoch 0[3863/4499] Val Loss:0.15695863962173462\n",
      "Epoch 0[3864/4499] Val Loss:0.08632618188858032\n",
      "Epoch 0[3865/4499] Val Loss:0.22340965270996094\n",
      "Epoch 0[3866/4499] Val Loss:0.12294276058673859\n",
      "Epoch 0[3867/4499] Val Loss:0.25508832931518555\n",
      "Epoch 0[3868/4499] Val Loss:0.23574012517929077\n",
      "Epoch 0[3869/4499] Val Loss:0.18739576637744904\n",
      "Epoch 0[3870/4499] Val Loss:0.37394431233406067\n",
      "Epoch 0[3871/4499] Val Loss:0.2222086787223816\n",
      "Epoch 0[3872/4499] Val Loss:0.317979633808136\n",
      "Epoch 0[3873/4499] Val Loss:0.26450714468955994\n",
      "Epoch 0[3874/4499] Val Loss:0.2804628908634186\n",
      "Epoch 0[3875/4499] Val Loss:0.3912383019924164\n",
      "Epoch 0[3876/4499] Val Loss:0.34646984934806824\n",
      "Epoch 0[3877/4499] Val Loss:0.4318171739578247\n",
      "Epoch 0[3878/4499] Val Loss:0.28598836064338684\n",
      "Epoch 0[3879/4499] Val Loss:0.3295322060585022\n",
      "Epoch 0[3880/4499] Val Loss:0.2827461361885071\n",
      "Epoch 0[3881/4499] Val Loss:0.5265226364135742\n",
      "Epoch 0[3882/4499] Val Loss:0.4022051692008972\n",
      "Epoch 0[3883/4499] Val Loss:0.33585813641548157\n",
      "Epoch 0[3884/4499] Val Loss:0.33176088333129883\n",
      "Epoch 0[3885/4499] Val Loss:0.4879625141620636\n",
      "Epoch 0[3886/4499] Val Loss:0.4641052186489105\n",
      "Epoch 0[3887/4499] Val Loss:0.5838941931724548\n",
      "Epoch 0[3888/4499] Val Loss:0.5915611386299133\n",
      "Epoch 0[3889/4499] Val Loss:0.805811882019043\n",
      "Epoch 0[3890/4499] Val Loss:0.4865210950374603\n",
      "Epoch 0[3891/4499] Val Loss:0.5225718021392822\n",
      "Epoch 0[3892/4499] Val Loss:0.586944043636322\n",
      "Epoch 0[3893/4499] Val Loss:0.5894057750701904\n",
      "Epoch 0[3894/4499] Val Loss:0.6252812147140503\n",
      "Epoch 0[3895/4499] Val Loss:0.5622696876525879\n",
      "Epoch 0[3896/4499] Val Loss:0.5726534128189087\n",
      "Epoch 0[3897/4499] Val Loss:0.7342983484268188\n",
      "Epoch 0[3898/4499] Val Loss:0.6742130517959595\n",
      "Epoch 0[3899/4499] Val Loss:0.6609771847724915\n",
      "Epoch 0[3900/4499] Val Loss:0.691982090473175\n",
      "Epoch 0[3901/4499] Val Loss:0.7183738350868225\n",
      "Epoch 0[3902/4499] Val Loss:0.5983322858810425\n",
      "Epoch 0[3903/4499] Val Loss:0.5279988050460815\n",
      "Epoch 0[3904/4499] Val Loss:0.551418662071228\n",
      "Epoch 0[3905/4499] Val Loss:0.5333590507507324\n",
      "Epoch 0[3906/4499] Val Loss:0.5474032759666443\n",
      "Epoch 0[3907/4499] Val Loss:0.5836208462715149\n",
      "Epoch 0[3908/4499] Val Loss:0.8600308895111084\n",
      "Epoch 0[3909/4499] Val Loss:0.4812661409378052\n",
      "Epoch 0[3910/4499] Val Loss:0.4327849745750427\n",
      "Epoch 0[3911/4499] Val Loss:0.36191338300704956\n",
      "Epoch 0[3912/4499] Val Loss:0.389078289270401\n",
      "Epoch 0[3913/4499] Val Loss:0.5340136289596558\n",
      "Epoch 0[3914/4499] Val Loss:0.498088002204895\n",
      "Epoch 0[3915/4499] Val Loss:0.5367513298988342\n",
      "Epoch 0[3916/4499] Val Loss:0.4253852963447571\n",
      "Epoch 0[3917/4499] Val Loss:0.41691285371780396\n",
      "Epoch 0[3918/4499] Val Loss:0.41998162865638733\n",
      "Epoch 0[3919/4499] Val Loss:0.6115795969963074\n",
      "Epoch 0[3920/4499] Val Loss:0.48112571239471436\n",
      "Epoch 0[3921/4499] Val Loss:0.45273205637931824\n",
      "Epoch 0[3922/4499] Val Loss:0.47529178857803345\n",
      "Epoch 0[3923/4499] Val Loss:0.3890891671180725\n",
      "Epoch 0[3924/4499] Val Loss:0.506959080696106\n",
      "Epoch 0[3925/4499] Val Loss:0.5552634596824646\n",
      "Epoch 0[3926/4499] Val Loss:0.4216270446777344\n",
      "Epoch 0[3927/4499] Val Loss:0.444655179977417\n",
      "Epoch 0[3928/4499] Val Loss:0.3843079209327698\n",
      "Epoch 0[3929/4499] Val Loss:0.37620678544044495\n",
      "Epoch 0[3930/4499] Val Loss:0.44601255655288696\n",
      "Epoch 0[3931/4499] Val Loss:0.38823577761650085\n",
      "Epoch 0[3932/4499] Val Loss:0.3231448829174042\n",
      "Epoch 0[3933/4499] Val Loss:0.3918810784816742\n",
      "Epoch 0[3934/4499] Val Loss:0.4922177195549011\n",
      "Epoch 0[3935/4499] Val Loss:0.40721726417541504\n",
      "Epoch 0[3936/4499] Val Loss:0.3431173861026764\n",
      "Epoch 0[3937/4499] Val Loss:0.27519479393959045\n",
      "Epoch 0[3938/4499] Val Loss:0.19040335714817047\n",
      "Epoch 0[3939/4499] Val Loss:0.17317818105220795\n",
      "Epoch 0[3940/4499] Val Loss:0.25373417139053345\n",
      "Epoch 0[3941/4499] Val Loss:0.13648323714733124\n",
      "Epoch 0[3942/4499] Val Loss:0.21078874170780182\n",
      "Epoch 0[3943/4499] Val Loss:0.19151711463928223\n",
      "Epoch 0[3944/4499] Val Loss:0.3139099180698395\n",
      "Epoch 0[3945/4499] Val Loss:0.2004990130662918\n",
      "Epoch 0[3946/4499] Val Loss:0.25768932700157166\n",
      "Epoch 0[3947/4499] Val Loss:0.28496843576431274\n",
      "Epoch 0[3948/4499] Val Loss:0.314151406288147\n",
      "Epoch 0[3949/4499] Val Loss:0.3832966387271881\n",
      "Epoch 0[3950/4499] Val Loss:0.2822689116001129\n",
      "Epoch 0[3951/4499] Val Loss:0.13754531741142273\n",
      "Epoch 0[3952/4499] Val Loss:0.23832166194915771\n",
      "Epoch 0[3953/4499] Val Loss:0.2591242790222168\n",
      "Epoch 0[3954/4499] Val Loss:0.1841295212507248\n",
      "Epoch 0[3955/4499] Val Loss:0.32693541049957275\n",
      "Epoch 0[3956/4499] Val Loss:0.16529256105422974\n",
      "Epoch 0[3957/4499] Val Loss:0.1365332156419754\n",
      "Epoch 0[3958/4499] Val Loss:0.22081857919692993\n",
      "Epoch 0[3959/4499] Val Loss:0.17383433878421783\n",
      "Epoch 0[3960/4499] Val Loss:0.2162715494632721\n",
      "Epoch 0[3961/4499] Val Loss:0.22766700387001038\n",
      "Epoch 0[3962/4499] Val Loss:0.19731831550598145\n",
      "Epoch 0[3963/4499] Val Loss:0.378669410943985\n",
      "Epoch 0[3964/4499] Val Loss:0.2747921049594879\n",
      "Epoch 0[3965/4499] Val Loss:0.19389697909355164\n",
      "Epoch 0[3966/4499] Val Loss:0.20883570611476898\n",
      "Epoch 0[3967/4499] Val Loss:0.2889930009841919\n",
      "Epoch 0[3968/4499] Val Loss:0.2255389541387558\n",
      "Epoch 0[3969/4499] Val Loss:0.2297644317150116\n",
      "Epoch 0[3970/4499] Val Loss:0.38108858466148376\n",
      "Epoch 0[3971/4499] Val Loss:0.29410502314567566\n",
      "Epoch 0[3972/4499] Val Loss:0.28842806816101074\n",
      "Epoch 0[3973/4499] Val Loss:0.2288447618484497\n",
      "Epoch 0[3974/4499] Val Loss:0.1596693992614746\n",
      "Epoch 0[3975/4499] Val Loss:0.11688311398029327\n",
      "Epoch 0[3976/4499] Val Loss:0.20436397194862366\n",
      "Epoch 0[3977/4499] Val Loss:0.2307361513376236\n",
      "Epoch 0[3978/4499] Val Loss:0.24070186913013458\n",
      "Epoch 0[3979/4499] Val Loss:0.17047376930713654\n",
      "Epoch 0[3980/4499] Val Loss:0.1752568930387497\n",
      "Epoch 0[3981/4499] Val Loss:0.18798239529132843\n",
      "Epoch 0[3982/4499] Val Loss:0.1792544275522232\n",
      "Epoch 0[3983/4499] Val Loss:0.19406473636627197\n",
      "Epoch 0[3984/4499] Val Loss:0.15773779153823853\n",
      "Epoch 0[3985/4499] Val Loss:0.16862232983112335\n",
      "Epoch 0[3986/4499] Val Loss:0.18472163379192352\n",
      "Epoch 0[3987/4499] Val Loss:0.13764454424381256\n",
      "Epoch 0[3988/4499] Val Loss:0.1707548052072525\n",
      "Epoch 0[3989/4499] Val Loss:0.15602374076843262\n",
      "Epoch 0[3990/4499] Val Loss:0.21521925926208496\n",
      "Epoch 0[3991/4499] Val Loss:0.17052173614501953\n",
      "Epoch 0[3992/4499] Val Loss:0.1305713653564453\n",
      "Epoch 0[3993/4499] Val Loss:0.0850980207324028\n",
      "Epoch 0[3994/4499] Val Loss:0.09253276139497757\n",
      "Epoch 0[3995/4499] Val Loss:0.10662172734737396\n",
      "Epoch 0[3996/4499] Val Loss:0.15381240844726562\n",
      "Epoch 0[3997/4499] Val Loss:0.13737812638282776\n",
      "Epoch 0[3998/4499] Val Loss:0.10808520764112473\n",
      "Epoch 0[3999/4499] Val Loss:0.12959013879299164\n",
      "Epoch 0[4000/4499] Val Loss:0.09843799471855164\n",
      "Epoch 0[4001/4499] Val Loss:0.12017267942428589\n",
      "Epoch 0[4002/4499] Val Loss:0.2061351239681244\n",
      "Epoch 0[4003/4499] Val Loss:0.16122114658355713\n",
      "Epoch 0[4004/4499] Val Loss:0.2454420030117035\n",
      "Epoch 0[4005/4499] Val Loss:0.14535610377788544\n",
      "Epoch 0[4006/4499] Val Loss:0.21287840604782104\n",
      "Epoch 0[4007/4499] Val Loss:0.25735393166542053\n",
      "Epoch 0[4008/4499] Val Loss:0.23467126488685608\n",
      "Epoch 0[4009/4499] Val Loss:0.36287373304367065\n",
      "Epoch 0[4010/4499] Val Loss:0.280491441488266\n",
      "Epoch 0[4011/4499] Val Loss:0.36520707607269287\n",
      "Epoch 0[4012/4499] Val Loss:0.43381208181381226\n",
      "Epoch 0[4013/4499] Val Loss:0.2820211946964264\n",
      "Epoch 0[4014/4499] Val Loss:0.2830456495285034\n",
      "Epoch 0[4015/4499] Val Loss:0.2229079306125641\n",
      "Epoch 0[4016/4499] Val Loss:0.2950728237628937\n",
      "Epoch 0[4017/4499] Val Loss:0.22386205196380615\n",
      "Epoch 0[4018/4499] Val Loss:0.33288970589637756\n",
      "Epoch 0[4019/4499] Val Loss:0.2893916368484497\n",
      "Epoch 0[4020/4499] Val Loss:0.2736222743988037\n",
      "Epoch 0[4021/4499] Val Loss:0.43529993295669556\n",
      "Epoch 0[4022/4499] Val Loss:0.7075093984603882\n",
      "Epoch 0[4023/4499] Val Loss:0.8447283506393433\n",
      "Epoch 0[4024/4499] Val Loss:0.5738264322280884\n",
      "Epoch 0[4025/4499] Val Loss:0.9572023749351501\n",
      "Epoch 0[4026/4499] Val Loss:0.8185705542564392\n",
      "Epoch 0[4027/4499] Val Loss:1.1015400886535645\n",
      "Epoch 0[4028/4499] Val Loss:0.9188641905784607\n",
      "Epoch 0[4029/4499] Val Loss:0.8283653855323792\n",
      "Epoch 0[4030/4499] Val Loss:0.8328835964202881\n",
      "Epoch 0[4031/4499] Val Loss:0.5688159465789795\n",
      "Epoch 0[4032/4499] Val Loss:0.8133772611618042\n",
      "Epoch 0[4033/4499] Val Loss:0.6058498024940491\n",
      "Epoch 0[4034/4499] Val Loss:0.6253243088722229\n",
      "Epoch 0[4035/4499] Val Loss:0.5517460107803345\n",
      "Epoch 0[4036/4499] Val Loss:0.6739124059677124\n",
      "Epoch 0[4037/4499] Val Loss:0.6246771216392517\n",
      "Epoch 0[4038/4499] Val Loss:0.5803111791610718\n",
      "Epoch 0[4039/4499] Val Loss:0.4774613678455353\n",
      "Epoch 0[4040/4499] Val Loss:0.4213064908981323\n",
      "Epoch 0[4041/4499] Val Loss:0.4669473469257355\n",
      "Epoch 0[4042/4499] Val Loss:0.4843197464942932\n",
      "Epoch 0[4043/4499] Val Loss:0.3875284790992737\n",
      "Epoch 0[4044/4499] Val Loss:0.5717482566833496\n",
      "Epoch 0[4045/4499] Val Loss:0.4714924097061157\n",
      "Epoch 0[4046/4499] Val Loss:0.32945919036865234\n",
      "Epoch 0[4047/4499] Val Loss:0.3734259605407715\n",
      "Epoch 0[4048/4499] Val Loss:0.2853963077068329\n",
      "Epoch 0[4049/4499] Val Loss:0.18269851803779602\n",
      "Epoch 0[4050/4499] Val Loss:0.2168247550725937\n",
      "Epoch 0[4051/4499] Val Loss:0.5150148868560791\n",
      "Epoch 0[4052/4499] Val Loss:0.6295956373214722\n",
      "Epoch 0[4053/4499] Val Loss:0.6311384439468384\n",
      "Epoch 0[4054/4499] Val Loss:0.8440603613853455\n",
      "Epoch 0[4055/4499] Val Loss:1.166404128074646\n",
      "Epoch 0[4056/4499] Val Loss:1.258937120437622\n",
      "Epoch 0[4057/4499] Val Loss:1.0601524114608765\n",
      "Epoch 0[4058/4499] Val Loss:0.9452053904533386\n",
      "Epoch 0[4059/4499] Val Loss:0.5302886962890625\n",
      "Epoch 0[4060/4499] Val Loss:0.48480910062789917\n",
      "Epoch 0[4061/4499] Val Loss:0.4040284752845764\n",
      "Epoch 0[4062/4499] Val Loss:0.2883579730987549\n",
      "Epoch 0[4063/4499] Val Loss:0.30217015743255615\n",
      "Epoch 0[4064/4499] Val Loss:0.2722448706626892\n",
      "Epoch 0[4065/4499] Val Loss:0.2575697898864746\n",
      "Epoch 0[4066/4499] Val Loss:0.4036601185798645\n",
      "Epoch 0[4067/4499] Val Loss:0.37367329001426697\n",
      "Epoch 0[4068/4499] Val Loss:0.3921247124671936\n",
      "Epoch 0[4069/4499] Val Loss:0.3451612591743469\n",
      "Epoch 0[4070/4499] Val Loss:0.31510475277900696\n",
      "Epoch 0[4071/4499] Val Loss:0.2911434471607208\n",
      "Epoch 0[4072/4499] Val Loss:0.3452472388744354\n",
      "Epoch 0[4073/4499] Val Loss:0.3474476635456085\n",
      "Epoch 0[4074/4499] Val Loss:0.34911707043647766\n",
      "Epoch 0[4075/4499] Val Loss:0.28741568326950073\n",
      "Epoch 0[4076/4499] Val Loss:0.32756057381629944\n",
      "Epoch 0[4077/4499] Val Loss:0.3910508155822754\n",
      "Epoch 0[4078/4499] Val Loss:0.3248326778411865\n",
      "Epoch 0[4079/4499] Val Loss:0.7131295800209045\n",
      "Epoch 0[4080/4499] Val Loss:0.5608919262886047\n",
      "Epoch 0[4081/4499] Val Loss:0.5865750312805176\n",
      "Epoch 0[4082/4499] Val Loss:0.6245057582855225\n",
      "Epoch 0[4083/4499] Val Loss:0.6122300028800964\n",
      "Epoch 0[4084/4499] Val Loss:0.7363666892051697\n",
      "Epoch 0[4085/4499] Val Loss:0.8551878333091736\n",
      "Epoch 0[4086/4499] Val Loss:0.8249397873878479\n",
      "Epoch 0[4087/4499] Val Loss:0.9744730591773987\n",
      "Epoch 0[4088/4499] Val Loss:0.6593592166900635\n",
      "Epoch 0[4089/4499] Val Loss:0.40999865531921387\n",
      "Epoch 0[4090/4499] Val Loss:0.3368355929851532\n",
      "Epoch 0[4091/4499] Val Loss:0.3317292630672455\n",
      "Epoch 0[4092/4499] Val Loss:0.38113176822662354\n",
      "Epoch 0[4093/4499] Val Loss:0.3837197422981262\n",
      "Epoch 0[4094/4499] Val Loss:0.3263472616672516\n",
      "Epoch 0[4095/4499] Val Loss:0.3380768597126007\n",
      "Epoch 0[4096/4499] Val Loss:0.3111521601676941\n",
      "Epoch 0[4097/4499] Val Loss:0.2400217205286026\n",
      "Epoch 0[4098/4499] Val Loss:0.35930290818214417\n",
      "Epoch 0[4099/4499] Val Loss:0.3887048363685608\n",
      "Epoch 0[4100/4499] Val Loss:0.33391523361206055\n",
      "Epoch 0[4101/4499] Val Loss:0.3147105574607849\n",
      "Epoch 0[4102/4499] Val Loss:0.17536331713199615\n",
      "Epoch 0[4103/4499] Val Loss:0.1108647808432579\n",
      "Epoch 0[4104/4499] Val Loss:0.25026318430900574\n",
      "Epoch 0[4105/4499] Val Loss:0.7420682907104492\n",
      "Epoch 0[4106/4499] Val Loss:0.6600938439369202\n",
      "Epoch 0[4107/4499] Val Loss:0.7095943093299866\n",
      "Epoch 0[4108/4499] Val Loss:0.7582607865333557\n",
      "Epoch 0[4109/4499] Val Loss:0.8779624104499817\n",
      "Epoch 0[4110/4499] Val Loss:1.0100839138031006\n",
      "Epoch 0[4111/4499] Val Loss:0.7820733189582825\n",
      "Epoch 0[4112/4499] Val Loss:0.3433469533920288\n",
      "Epoch 0[4113/4499] Val Loss:0.2544165551662445\n",
      "Epoch 0[4114/4499] Val Loss:0.26668545603752136\n",
      "Epoch 0[4115/4499] Val Loss:0.35073184967041016\n",
      "Epoch 0[4116/4499] Val Loss:0.2606702446937561\n",
      "Epoch 0[4117/4499] Val Loss:0.23214790225028992\n",
      "Epoch 0[4118/4499] Val Loss:0.2913428544998169\n",
      "Epoch 0[4119/4499] Val Loss:0.2964111268520355\n",
      "Epoch 0[4120/4499] Val Loss:0.27782103419303894\n",
      "Epoch 0[4121/4499] Val Loss:0.25753846764564514\n",
      "Epoch 0[4122/4499] Val Loss:0.28463509678840637\n",
      "Epoch 0[4123/4499] Val Loss:0.261222243309021\n",
      "Epoch 0[4124/4499] Val Loss:0.21942146122455597\n",
      "Epoch 0[4125/4499] Val Loss:0.20522218942642212\n",
      "Epoch 0[4126/4499] Val Loss:0.330664724111557\n",
      "Epoch 0[4127/4499] Val Loss:0.2353779524564743\n",
      "Epoch 0[4128/4499] Val Loss:0.2622736096382141\n",
      "Epoch 0[4129/4499] Val Loss:0.2655368447303772\n",
      "Epoch 0[4130/4499] Val Loss:0.26701420545578003\n",
      "Epoch 0[4131/4499] Val Loss:0.30904287099838257\n",
      "Epoch 0[4132/4499] Val Loss:0.34423136711120605\n",
      "Epoch 0[4133/4499] Val Loss:0.4247748553752899\n",
      "Epoch 0[4134/4499] Val Loss:0.4809572100639343\n",
      "Epoch 0[4135/4499] Val Loss:0.5842852592468262\n",
      "Epoch 0[4136/4499] Val Loss:0.7397446632385254\n",
      "Epoch 0[4137/4499] Val Loss:0.5558144450187683\n",
      "Epoch 0[4138/4499] Val Loss:0.5437978506088257\n",
      "Epoch 0[4139/4499] Val Loss:0.587533712387085\n",
      "Epoch 0[4140/4499] Val Loss:0.48783132433891296\n",
      "Epoch 0[4141/4499] Val Loss:0.5501455068588257\n",
      "Epoch 0[4142/4499] Val Loss:0.5024363398551941\n",
      "Epoch 0[4143/4499] Val Loss:0.4030773937702179\n",
      "Epoch 0[4144/4499] Val Loss:0.27748921513557434\n",
      "Epoch 0[4145/4499] Val Loss:0.1934085488319397\n",
      "Epoch 0[4146/4499] Val Loss:0.22936037182807922\n",
      "Epoch 0[4147/4499] Val Loss:0.21712476015090942\n",
      "Epoch 0[4148/4499] Val Loss:0.20032796263694763\n",
      "Epoch 0[4149/4499] Val Loss:0.24498659372329712\n",
      "Epoch 0[4150/4499] Val Loss:0.3176226019859314\n",
      "Epoch 0[4151/4499] Val Loss:0.33562585711479187\n",
      "Epoch 0[4152/4499] Val Loss:0.3899313509464264\n",
      "Epoch 0[4153/4499] Val Loss:0.42208021879196167\n",
      "Epoch 0[4154/4499] Val Loss:0.3752897381782532\n",
      "Epoch 0[4155/4499] Val Loss:0.44342100620269775\n",
      "Epoch 0[4156/4499] Val Loss:0.287350594997406\n",
      "Epoch 0[4157/4499] Val Loss:0.3700968027114868\n",
      "Epoch 0[4158/4499] Val Loss:0.29601413011550903\n",
      "Epoch 0[4159/4499] Val Loss:0.42782655358314514\n",
      "Epoch 0[4160/4499] Val Loss:0.6051598191261292\n",
      "Epoch 0[4161/4499] Val Loss:0.7446460723876953\n",
      "Epoch 0[4162/4499] Val Loss:0.5454115867614746\n",
      "Epoch 0[4163/4499] Val Loss:0.6162669658660889\n",
      "Epoch 0[4164/4499] Val Loss:0.730459451675415\n",
      "Epoch 0[4165/4499] Val Loss:0.7250025868415833\n",
      "Epoch 0[4166/4499] Val Loss:0.6701634526252747\n",
      "Epoch 0[4167/4499] Val Loss:0.6291142702102661\n",
      "Epoch 0[4168/4499] Val Loss:0.4317764341831207\n",
      "Epoch 0[4169/4499] Val Loss:0.40199485421180725\n",
      "Epoch 0[4170/4499] Val Loss:0.3627387285232544\n",
      "Epoch 0[4171/4499] Val Loss:0.19781698286533356\n",
      "Epoch 0[4172/4499] Val Loss:0.18975037336349487\n",
      "Epoch 0[4173/4499] Val Loss:0.1945209503173828\n",
      "Epoch 0[4174/4499] Val Loss:0.32342687249183655\n",
      "Epoch 0[4175/4499] Val Loss:0.35204288363456726\n",
      "Epoch 0[4176/4499] Val Loss:0.34978070855140686\n",
      "Epoch 0[4177/4499] Val Loss:0.38857683539390564\n",
      "Epoch 0[4178/4499] Val Loss:0.4061277508735657\n",
      "Epoch 0[4179/4499] Val Loss:0.3465823829174042\n",
      "Epoch 0[4180/4499] Val Loss:0.30411654710769653\n",
      "Epoch 0[4181/4499] Val Loss:0.3503449261188507\n",
      "Epoch 0[4182/4499] Val Loss:0.3340209722518921\n",
      "Epoch 0[4183/4499] Val Loss:0.40825656056404114\n",
      "Epoch 0[4184/4499] Val Loss:0.30394572019577026\n",
      "Epoch 0[4185/4499] Val Loss:0.4299260377883911\n",
      "Epoch 0[4186/4499] Val Loss:0.6641184687614441\n",
      "Epoch 0[4187/4499] Val Loss:0.6325318217277527\n",
      "Epoch 0[4188/4499] Val Loss:0.5660465359687805\n",
      "Epoch 0[4189/4499] Val Loss:0.5879696011543274\n",
      "Epoch 0[4190/4499] Val Loss:0.6035229563713074\n",
      "Epoch 0[4191/4499] Val Loss:0.6939972639083862\n",
      "Epoch 0[4192/4499] Val Loss:0.6578360199928284\n",
      "Epoch 0[4193/4499] Val Loss:0.49016866087913513\n",
      "Epoch 0[4194/4499] Val Loss:0.4045552611351013\n",
      "Epoch 0[4195/4499] Val Loss:0.32809001207351685\n",
      "Epoch 0[4196/4499] Val Loss:0.33346882462501526\n",
      "Epoch 0[4197/4499] Val Loss:0.29842284321784973\n",
      "Epoch 0[4198/4499] Val Loss:0.3606775999069214\n",
      "Epoch 0[4199/4499] Val Loss:0.27748069167137146\n",
      "Epoch 0[4200/4499] Val Loss:0.22952111065387726\n",
      "Epoch 0[4201/4499] Val Loss:0.28671976923942566\n",
      "Epoch 0[4202/4499] Val Loss:0.24598489701747894\n",
      "Epoch 0[4203/4499] Val Loss:0.28252485394477844\n",
      "Epoch 0[4204/4499] Val Loss:0.3136482834815979\n",
      "Epoch 0[4205/4499] Val Loss:0.5373247265815735\n",
      "Epoch 0[4206/4499] Val Loss:0.6240440607070923\n",
      "Epoch 0[4207/4499] Val Loss:0.4454803168773651\n",
      "Epoch 0[4208/4499] Val Loss:0.4579629898071289\n",
      "Epoch 0[4209/4499] Val Loss:0.4702073037624359\n",
      "Epoch 0[4210/4499] Val Loss:0.29825830459594727\n",
      "Epoch 0[4211/4499] Val Loss:0.17313475906848907\n",
      "Epoch 0[4212/4499] Val Loss:0.19320610165596008\n",
      "Epoch 0[4213/4499] Val Loss:0.23226068913936615\n",
      "Epoch 0[4214/4499] Val Loss:0.33006444573402405\n",
      "Epoch 0[4215/4499] Val Loss:0.30998167395591736\n",
      "Epoch 0[4216/4499] Val Loss:0.2761789560317993\n",
      "Epoch 0[4217/4499] Val Loss:0.2615566551685333\n",
      "Epoch 0[4218/4499] Val Loss:0.2839563190937042\n",
      "Epoch 0[4219/4499] Val Loss:0.26927363872528076\n",
      "Epoch 0[4220/4499] Val Loss:0.20886024832725525\n",
      "Epoch 0[4221/4499] Val Loss:0.26121214032173157\n",
      "Epoch 0[4222/4499] Val Loss:0.40534666180610657\n",
      "Epoch 0[4223/4499] Val Loss:0.4481735825538635\n",
      "Epoch 0[4224/4499] Val Loss:0.6024099588394165\n",
      "Epoch 0[4225/4499] Val Loss:0.4697154462337494\n",
      "Epoch 0[4226/4499] Val Loss:0.5915666222572327\n",
      "Epoch 0[4227/4499] Val Loss:0.4817441701889038\n",
      "Epoch 0[4228/4499] Val Loss:0.4745524227619171\n",
      "Epoch 0[4229/4499] Val Loss:0.6266247034072876\n",
      "Epoch 0[4230/4499] Val Loss:0.8165987730026245\n",
      "Epoch 0[4231/4499] Val Loss:0.7174003720283508\n",
      "Epoch 0[4232/4499] Val Loss:0.5178192853927612\n",
      "Epoch 0[4233/4499] Val Loss:0.41760513186454773\n",
      "Epoch 0[4234/4499] Val Loss:0.40502649545669556\n",
      "Epoch 0[4235/4499] Val Loss:0.3612710237503052\n",
      "Epoch 0[4236/4499] Val Loss:0.45489072799682617\n",
      "Epoch 0[4237/4499] Val Loss:0.6395536065101624\n",
      "Epoch 0[4238/4499] Val Loss:0.7452139258384705\n",
      "Epoch 0[4239/4499] Val Loss:0.7953349351882935\n",
      "Epoch 0[4240/4499] Val Loss:0.6983213424682617\n",
      "Epoch 0[4241/4499] Val Loss:0.5519832968711853\n",
      "Epoch 0[4242/4499] Val Loss:0.48687174916267395\n",
      "Epoch 0[4243/4499] Val Loss:0.3573392629623413\n",
      "Epoch 0[4244/4499] Val Loss:0.24547012150287628\n",
      "Epoch 0[4245/4499] Val Loss:0.3158303499221802\n",
      "Epoch 0[4246/4499] Val Loss:0.3612198233604431\n",
      "Epoch 0[4247/4499] Val Loss:0.34025144577026367\n",
      "Epoch 0[4248/4499] Val Loss:0.21967706084251404\n",
      "Epoch 0[4249/4499] Val Loss:0.17123930156230927\n",
      "Epoch 0[4250/4499] Val Loss:0.12594398856163025\n",
      "Epoch 0[4251/4499] Val Loss:0.18681283295154572\n",
      "Epoch 0[4252/4499] Val Loss:0.23021948337554932\n",
      "Epoch 0[4253/4499] Val Loss:0.19881805777549744\n",
      "Epoch 0[4254/4499] Val Loss:0.255588173866272\n",
      "Epoch 0[4255/4499] Val Loss:0.15796029567718506\n",
      "Epoch 0[4256/4499] Val Loss:0.19270098209381104\n",
      "Epoch 0[4257/4499] Val Loss:0.23967303335666656\n",
      "Epoch 0[4258/4499] Val Loss:0.2917153239250183\n",
      "Epoch 0[4259/4499] Val Loss:0.28922292590141296\n",
      "Epoch 0[4260/4499] Val Loss:0.336640864610672\n",
      "Epoch 0[4261/4499] Val Loss:0.4896487891674042\n",
      "Epoch 0[4262/4499] Val Loss:0.6287813186645508\n",
      "Epoch 0[4263/4499] Val Loss:0.19545497000217438\n",
      "Epoch 0[4264/4499] Val Loss:0.17512856423854828\n",
      "Epoch 0[4265/4499] Val Loss:0.1836937665939331\n",
      "Epoch 0[4266/4499] Val Loss:0.18400874733924866\n",
      "Epoch 0[4267/4499] Val Loss:0.236619770526886\n",
      "Epoch 0[4268/4499] Val Loss:0.2679552137851715\n",
      "Epoch 0[4269/4499] Val Loss:0.20916856825351715\n",
      "Epoch 0[4270/4499] Val Loss:0.1847425401210785\n",
      "Epoch 0[4271/4499] Val Loss:0.22656545042991638\n",
      "Epoch 0[4272/4499] Val Loss:0.11554871499538422\n",
      "Epoch 0[4273/4499] Val Loss:0.21467168629169464\n",
      "Epoch 0[4274/4499] Val Loss:0.2220955789089203\n",
      "Epoch 0[4275/4499] Val Loss:0.29748085141181946\n",
      "Epoch 0[4276/4499] Val Loss:0.2753054201602936\n",
      "Epoch 0[4277/4499] Val Loss:0.47301822900772095\n",
      "Epoch 0[4278/4499] Val Loss:0.47946596145629883\n",
      "Epoch 0[4279/4499] Val Loss:0.4487568140029907\n",
      "Epoch 0[4280/4499] Val Loss:0.4341135025024414\n",
      "Epoch 0[4281/4499] Val Loss:0.33186015486717224\n",
      "Epoch 0[4282/4499] Val Loss:0.32690271735191345\n",
      "Epoch 0[4283/4499] Val Loss:0.2917100191116333\n",
      "Epoch 0[4284/4499] Val Loss:0.3613360822200775\n",
      "Epoch 0[4285/4499] Val Loss:0.4020508825778961\n",
      "Epoch 0[4286/4499] Val Loss:0.484121173620224\n",
      "Epoch 0[4287/4499] Val Loss:0.39513424038887024\n",
      "Epoch 0[4288/4499] Val Loss:0.4286404252052307\n",
      "Epoch 0[4289/4499] Val Loss:0.4537830054759979\n",
      "Epoch 0[4290/4499] Val Loss:0.45070695877075195\n",
      "Epoch 0[4291/4499] Val Loss:0.37513408064842224\n",
      "Epoch 0[4292/4499] Val Loss:0.13023975491523743\n",
      "Epoch 0[4293/4499] Val Loss:0.30554553866386414\n",
      "Epoch 0[4294/4499] Val Loss:0.24997784197330475\n",
      "Epoch 0[4295/4499] Val Loss:0.2422875016927719\n",
      "Epoch 0[4296/4499] Val Loss:0.20724815130233765\n",
      "Epoch 0[4297/4499] Val Loss:0.18445390462875366\n",
      "Epoch 0[4298/4499] Val Loss:0.23863619565963745\n",
      "Epoch 0[4299/4499] Val Loss:0.254187673330307\n",
      "Epoch 0[4300/4499] Val Loss:0.21896588802337646\n",
      "Epoch 0[4301/4499] Val Loss:0.1847219169139862\n",
      "Epoch 0[4302/4499] Val Loss:0.11413896083831787\n",
      "Epoch 0[4303/4499] Val Loss:0.1146741658449173\n",
      "Epoch 0[4304/4499] Val Loss:0.160671666264534\n",
      "Epoch 0[4305/4499] Val Loss:0.2544395923614502\n",
      "Epoch 0[4306/4499] Val Loss:0.3396468162536621\n",
      "Epoch 0[4307/4499] Val Loss:0.4720239043235779\n",
      "Epoch 0[4308/4499] Val Loss:0.47454091906547546\n",
      "Epoch 0[4309/4499] Val Loss:0.4370560944080353\n",
      "Epoch 0[4310/4499] Val Loss:0.2591833174228668\n",
      "Epoch 0[4311/4499] Val Loss:0.1944422423839569\n",
      "Epoch 0[4312/4499] Val Loss:0.31323280930519104\n",
      "Epoch 0[4313/4499] Val Loss:0.25757619738578796\n",
      "Epoch 0[4314/4499] Val Loss:0.2907690405845642\n",
      "Epoch 0[4315/4499] Val Loss:0.36525094509124756\n",
      "Epoch 0[4316/4499] Val Loss:0.2256556749343872\n",
      "Epoch 0[4317/4499] Val Loss:0.1879742443561554\n",
      "Epoch 0[4318/4499] Val Loss:0.13526134192943573\n",
      "Epoch 0[4319/4499] Val Loss:0.25281280279159546\n",
      "Epoch 0[4320/4499] Val Loss:0.299455851316452\n",
      "Epoch 0[4321/4499] Val Loss:0.31068265438079834\n",
      "Epoch 0[4322/4499] Val Loss:0.46536168456077576\n",
      "Epoch 0[4323/4499] Val Loss:0.404960960149765\n",
      "Epoch 0[4324/4499] Val Loss:0.3759324848651886\n",
      "Epoch 0[4325/4499] Val Loss:0.3260498344898224\n",
      "Epoch 0[4326/4499] Val Loss:0.3248209059238434\n",
      "Epoch 0[4327/4499] Val Loss:0.2530241012573242\n",
      "Epoch 0[4328/4499] Val Loss:0.29139405488967896\n",
      "Epoch 0[4329/4499] Val Loss:0.3178229033946991\n",
      "Epoch 0[4330/4499] Val Loss:0.19010239839553833\n",
      "Epoch 0[4331/4499] Val Loss:0.3645756244659424\n",
      "Epoch 0[4332/4499] Val Loss:0.21304424107074738\n",
      "Epoch 0[4333/4499] Val Loss:0.20478923618793488\n",
      "Epoch 0[4334/4499] Val Loss:0.2244257628917694\n",
      "Epoch 0[4335/4499] Val Loss:0.23158612847328186\n",
      "Epoch 0[4336/4499] Val Loss:0.23322054743766785\n",
      "Epoch 0[4337/4499] Val Loss:0.1790304183959961\n",
      "Epoch 0[4338/4499] Val Loss:0.17750737071037292\n",
      "Epoch 0[4339/4499] Val Loss:0.16516894102096558\n",
      "Epoch 0[4340/4499] Val Loss:0.17576801776885986\n",
      "Epoch 0[4341/4499] Val Loss:0.16783761978149414\n",
      "Epoch 0[4342/4499] Val Loss:0.24942824244499207\n",
      "Epoch 0[4343/4499] Val Loss:0.24393589794635773\n",
      "Epoch 0[4344/4499] Val Loss:0.22371576726436615\n",
      "Epoch 0[4345/4499] Val Loss:0.3328826129436493\n",
      "Epoch 0[4346/4499] Val Loss:0.37887150049209595\n",
      "Epoch 0[4347/4499] Val Loss:0.3885572552680969\n",
      "Epoch 0[4348/4499] Val Loss:0.5272332429885864\n",
      "Epoch 0[4349/4499] Val Loss:0.30670586228370667\n",
      "Epoch 0[4350/4499] Val Loss:0.23911693692207336\n",
      "Epoch 0[4351/4499] Val Loss:0.45022764801979065\n",
      "Epoch 0[4352/4499] Val Loss:0.30572977662086487\n",
      "Epoch 0[4353/4499] Val Loss:0.3613941967487335\n",
      "Epoch 0[4354/4499] Val Loss:0.4942423701286316\n",
      "Epoch 0[4355/4499] Val Loss:0.37924325466156006\n",
      "Epoch 0[4356/4499] Val Loss:0.5567625761032104\n",
      "Epoch 0[4357/4499] Val Loss:0.538602888584137\n",
      "Epoch 0[4358/4499] Val Loss:0.44883987307548523\n",
      "Epoch 0[4359/4499] Val Loss:0.5819527506828308\n",
      "Epoch 0[4360/4499] Val Loss:0.747712254524231\n",
      "Epoch 0[4361/4499] Val Loss:0.657021164894104\n",
      "Epoch 0[4362/4499] Val Loss:0.7334029078483582\n",
      "Epoch 0[4363/4499] Val Loss:0.6919113993644714\n",
      "Epoch 0[4364/4499] Val Loss:0.6878221035003662\n",
      "Epoch 0[4365/4499] Val Loss:0.46347010135650635\n",
      "Epoch 0[4366/4499] Val Loss:0.6223664283752441\n",
      "Epoch 0[4367/4499] Val Loss:0.6554530262947083\n",
      "Epoch 0[4368/4499] Val Loss:0.7589923739433289\n",
      "Epoch 0[4369/4499] Val Loss:0.760737419128418\n",
      "Epoch 0[4370/4499] Val Loss:0.8338183164596558\n",
      "Epoch 0[4371/4499] Val Loss:0.9619439840316772\n",
      "Epoch 0[4372/4499] Val Loss:0.6970608830451965\n",
      "Epoch 0[4373/4499] Val Loss:0.6890084147453308\n",
      "Epoch 0[4374/4499] Val Loss:0.5824034214019775\n",
      "Epoch 0[4375/4499] Val Loss:0.6025447845458984\n",
      "Epoch 0[4376/4499] Val Loss:0.5480145812034607\n",
      "Epoch 0[4377/4499] Val Loss:0.7398568987846375\n",
      "Epoch 0[4378/4499] Val Loss:0.7907702326774597\n",
      "Epoch 0[4379/4499] Val Loss:0.7225369215011597\n",
      "Epoch 0[4380/4499] Val Loss:0.6367297768592834\n",
      "Epoch 0[4381/4499] Val Loss:0.7385793924331665\n",
      "Epoch 0[4382/4499] Val Loss:0.9250251650810242\n",
      "Epoch 0[4383/4499] Val Loss:0.768549382686615\n",
      "Epoch 0[4384/4499] Val Loss:0.6076749563217163\n",
      "Epoch 0[4385/4499] Val Loss:0.4739687442779541\n",
      "Epoch 0[4386/4499] Val Loss:0.5959563851356506\n",
      "Epoch 0[4387/4499] Val Loss:0.5846254229545593\n",
      "Epoch 0[4388/4499] Val Loss:0.6111639142036438\n",
      "Epoch 0[4389/4499] Val Loss:0.47238975763320923\n",
      "Epoch 0[4390/4499] Val Loss:0.3668172061443329\n",
      "Epoch 0[4391/4499] Val Loss:0.35527485609054565\n",
      "Epoch 0[4392/4499] Val Loss:0.47621938586235046\n",
      "Epoch 0[4393/4499] Val Loss:0.5217475295066833\n",
      "Epoch 0[4394/4499] Val Loss:0.5891762375831604\n",
      "Epoch 0[4395/4499] Val Loss:0.40275564789772034\n",
      "Epoch 0[4396/4499] Val Loss:0.39593014121055603\n",
      "Epoch 0[4397/4499] Val Loss:0.36530691385269165\n",
      "Epoch 0[4398/4499] Val Loss:0.27569445967674255\n",
      "Epoch 0[4399/4499] Val Loss:0.16570799052715302\n",
      "Epoch 0[4400/4499] Val Loss:0.2981330156326294\n",
      "Epoch 0[4401/4499] Val Loss:0.2577912509441376\n",
      "Epoch 0[4402/4499] Val Loss:0.2926619350910187\n",
      "Epoch 0[4403/4499] Val Loss:0.3374147117137909\n",
      "Epoch 0[4404/4499] Val Loss:0.41921013593673706\n",
      "Epoch 0[4405/4499] Val Loss:0.37963029742240906\n",
      "Epoch 0[4406/4499] Val Loss:0.37465569376945496\n",
      "Epoch 0[4407/4499] Val Loss:0.4494888186454773\n",
      "Epoch 0[4408/4499] Val Loss:0.5975661277770996\n",
      "Epoch 0[4409/4499] Val Loss:0.5872060060501099\n",
      "Epoch 0[4410/4499] Val Loss:0.5372189283370972\n",
      "Epoch 0[4411/4499] Val Loss:0.7227991223335266\n",
      "Epoch 0[4412/4499] Val Loss:0.6923216581344604\n",
      "Epoch 0[4413/4499] Val Loss:0.6291356086730957\n",
      "Epoch 0[4414/4499] Val Loss:0.535086452960968\n",
      "Epoch 0[4415/4499] Val Loss:0.5741053819656372\n",
      "Epoch 0[4416/4499] Val Loss:0.49108174443244934\n",
      "Epoch 0[4417/4499] Val Loss:0.5886484980583191\n",
      "Epoch 0[4418/4499] Val Loss:0.6682471036911011\n",
      "Epoch 0[4419/4499] Val Loss:0.9914980530738831\n",
      "Epoch 0[4420/4499] Val Loss:0.8062020540237427\n",
      "Epoch 0[4421/4499] Val Loss:0.7756300568580627\n",
      "Epoch 0[4422/4499] Val Loss:0.7475674748420715\n",
      "Epoch 0[4423/4499] Val Loss:0.5287553668022156\n",
      "Epoch 0[4424/4499] Val Loss:0.5471418499946594\n",
      "Epoch 0[4425/4499] Val Loss:0.715919554233551\n",
      "Epoch 0[4426/4499] Val Loss:0.7490988969802856\n",
      "Epoch 0[4427/4499] Val Loss:0.6765623092651367\n",
      "Epoch 0[4428/4499] Val Loss:0.609430730342865\n",
      "Epoch 0[4429/4499] Val Loss:0.6220871210098267\n",
      "Epoch 0[4430/4499] Val Loss:0.6916970610618591\n",
      "Epoch 0[4431/4499] Val Loss:0.5245701670646667\n",
      "Epoch 0[4432/4499] Val Loss:0.5061467885971069\n",
      "Epoch 0[4433/4499] Val Loss:0.45346498489379883\n",
      "Epoch 0[4434/4499] Val Loss:0.4275971055030823\n",
      "Epoch 0[4435/4499] Val Loss:0.4894331693649292\n",
      "Epoch 0[4436/4499] Val Loss:0.5218503475189209\n",
      "Epoch 0[4437/4499] Val Loss:0.5802724957466125\n",
      "Epoch 0[4438/4499] Val Loss:0.39635908603668213\n",
      "Epoch 0[4439/4499] Val Loss:0.28326615691185\n",
      "Epoch 0[4440/4499] Val Loss:0.3013463318347931\n",
      "Epoch 0[4441/4499] Val Loss:0.40252918004989624\n",
      "Epoch 0[4442/4499] Val Loss:0.49558699131011963\n",
      "Epoch 0[4443/4499] Val Loss:0.35785046219825745\n",
      "Epoch 0[4444/4499] Val Loss:0.2872731685638428\n",
      "Epoch 0[4445/4499] Val Loss:0.4623860716819763\n",
      "Epoch 0[4446/4499] Val Loss:0.5571601986885071\n",
      "Epoch 0[4447/4499] Val Loss:0.4039686322212219\n",
      "Epoch 0[4448/4499] Val Loss:0.17456796765327454\n",
      "Epoch 0[4449/4499] Val Loss:0.3286096453666687\n",
      "Epoch 0[4450/4499] Val Loss:0.2179701030254364\n",
      "Epoch 0[4451/4499] Val Loss:0.24855312705039978\n",
      "Epoch 0[4452/4499] Val Loss:0.22927378118038177\n",
      "Epoch 0[4453/4499] Val Loss:0.37528398633003235\n",
      "Epoch 0[4454/4499] Val Loss:0.2342398762702942\n",
      "Epoch 0[4455/4499] Val Loss:0.28452709317207336\n",
      "Epoch 0[4456/4499] Val Loss:0.19408009946346283\n",
      "Epoch 0[4457/4499] Val Loss:0.17942166328430176\n",
      "Epoch 0[4458/4499] Val Loss:0.3132531940937042\n",
      "Epoch 0[4459/4499] Val Loss:0.41066715121269226\n",
      "Epoch 0[4460/4499] Val Loss:0.3829370141029358\n",
      "Epoch 0[4461/4499] Val Loss:0.5336951017379761\n",
      "Epoch 0[4462/4499] Val Loss:0.5232809782028198\n",
      "Epoch 0[4463/4499] Val Loss:0.5044109225273132\n",
      "Epoch 0[4464/4499] Val Loss:0.6276885867118835\n",
      "Epoch 0[4465/4499] Val Loss:0.5273160338401794\n",
      "Epoch 0[4466/4499] Val Loss:0.6791079044342041\n",
      "Epoch 0[4467/4499] Val Loss:0.7249668836593628\n",
      "Epoch 0[4468/4499] Val Loss:0.783037006855011\n",
      "Epoch 0[4469/4499] Val Loss:0.9064511060714722\n",
      "Epoch 0[4470/4499] Val Loss:0.595912516117096\n",
      "Epoch 0[4471/4499] Val Loss:0.49960705637931824\n",
      "Epoch 0[4472/4499] Val Loss:0.5801129341125488\n",
      "Epoch 0[4473/4499] Val Loss:0.5595046281814575\n",
      "Epoch 0[4474/4499] Val Loss:0.5819185972213745\n",
      "Epoch 0[4475/4499] Val Loss:0.7054823637008667\n",
      "Epoch 0[4476/4499] Val Loss:0.9225944876670837\n",
      "Epoch 0[4477/4499] Val Loss:0.8895288705825806\n",
      "Epoch 0[4478/4499] Val Loss:0.7018827199935913\n",
      "Epoch 0[4479/4499] Val Loss:0.7842214107513428\n",
      "Epoch 0[4480/4499] Val Loss:0.7523825764656067\n",
      "Epoch 0[4481/4499] Val Loss:0.5865575671195984\n",
      "Epoch 0[4482/4499] Val Loss:0.48748305439949036\n",
      "Epoch 0[4483/4499] Val Loss:0.37049004435539246\n",
      "Epoch 0[4484/4499] Val Loss:0.424234002828598\n",
      "Epoch 0[4485/4499] Val Loss:0.26104849576950073\n",
      "Epoch 0[4486/4499] Val Loss:0.2784265875816345\n",
      "Epoch 0[4487/4499] Val Loss:0.29458266496658325\n",
      "Epoch 0[4488/4499] Val Loss:0.2777073383331299\n",
      "Epoch 0[4489/4499] Val Loss:0.24791182577610016\n",
      "Epoch 0[4490/4499] Val Loss:0.2480268031358719\n",
      "Epoch 0[4491/4499] Val Loss:0.3327770531177521\n",
      "Epoch 0[4492/4499] Val Loss:0.4094547629356384\n",
      "Epoch 0[4493/4499] Val Loss:0.3453451097011566\n",
      "Epoch 0[4494/4499] Val Loss:0.30695995688438416\n",
      "Epoch 0[4495/4499] Val Loss:0.2975335717201233\n",
      "Epoch 0[4496/4499] Val Loss:0.488921582698822\n",
      "Epoch 0[4497/4499] Val Loss:0.4421144723892212\n",
      "Epoch 0[4498/4499] Val Loss:0.4458196759223938\n",
      "Epoch 0[4499/4499] Val Loss:0.3545871078968048\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.89      0.92    254657\n",
      "           1       0.20      0.52      0.29      5868\n",
      "           2       0.49      0.62      0.55     27470\n",
      "\n",
      "    accuracy                           0.86    287995\n",
      "   macro avg       0.55      0.68      0.58    287995\n",
      "weighted avg       0.89      0.86      0.87    287995\n",
      "\n",
      "Epoch 0: Train Loss 0.6345230060027179, Val Loss 0.38671384692261634, Train Time 4204.753628492355, Val Time 330.0497579574585\n",
      "Training complete in 70m 5s\n",
      "New best model at epoch 0\n",
      "Saving best model at epoch 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#testing \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/cnn_model_input/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpredict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmodel_save_loc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/cnn_model_124.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                \u001b[49m\u001b[43mclass_balance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcheckpoints_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/dartfs-hpc/rc/home/3/f006n33/checkpoints/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpickle_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcustom_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                \u001b[49m\u001b[43msave_predictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                \u001b[49m\u001b[43minclude_test_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                \u001b[49m\u001b[43muse_npy_rotate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlabel_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_true\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_true\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[43msample_frac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                \u001b[49m\u001b[43msample_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnpy_rotate_sets_pkl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/rotating_stack.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(inputs_dir, learning_rate, n_epochs, crop_size, resize, mean, std, num_classes, architecture, batch_size, predict, model_save_loc, pretrained_save_loc, predictions_save_path, predict_set, verbose, class_balance, extract_embeddings, extract_embeddings_df, embedding_out_dir, gpu_id, checkpoints_dir, tensor_dataset, pickle_dataset, label_map, semantic_segmentation, save_metric, custom_dataset, save_predictions, pretrained, save_after_n_batch, include_test_set, use_npy_rotate, sample_frac, sample_every, num_workers, npy_rotate_sets_pkl)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_balance:\n\u001b[1;32m    232\u001b[0m     trainer\u001b[38;5;241m.\u001b[39madd_class_balance_loss(datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtargets \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tensor_dataset \u001b[38;5;28;01melse\u001b[39;00m datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtensors[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[0;32m--> 234\u001b[0m trainer, min_val_loss_f1, best_epoch\u001b[38;5;241m=\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mfitt(dataloaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],verbose\u001b[38;5;241m=\u001b[39mverbose,plot_training_curves\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, plot_save_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/\u001b[39m\u001b[38;5;124m'\u001b[39m, print_val_confusion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, save_val_predictions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    236\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict(), model_save_loc)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mmodel\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "#testing \n",
    "train_model(inputs_dir=\"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/cnn_model_input/\",\n",
    "                learning_rate=5e-4,\n",
    "                n_epochs=1,\n",
    "                num_classes=3,\n",
    "                batch_size=64,\n",
    "                predict=False,\n",
    "                model_save_loc='/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/cnn_model_124.pth',\n",
    "                verbose=2,\n",
    "                class_balance=True,\n",
    "                checkpoints_dir='/dartfs-hpc/rc/home/3/f006n33/checkpoints/',\n",
    "                pickle_dataset=True,\n",
    "                custom_dataset=None,\n",
    "                save_predictions=True,\n",
    "                include_test_set=False,\n",
    "                use_npy_rotate=True,\n",
    "                label_map={'y_true':'y_true'},\n",
    "                sample_frac=1.,\n",
    "                sample_every=2,\n",
    "                num_workers=2,\n",
    "                npy_rotate_sets_pkl=\"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/rotating_stack.pkl\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb297362",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(inputs_dir=\"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/cnn_model_input/\",\n",
    "                learning_rate=5e-4,\n",
    "                n_epochs=10,\n",
    "                num_classes=3,\n",
    "                batch_size=64,\n",
    "                predict=False,\n",
    "                model_save_loc='/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/cnn_model_123.pth',\n",
    "                verbose=2,\n",
    "                class_balance=True,\n",
    "                pickle_dataset=True,\n",
    "                custom_dataset=None,\n",
    "                save_predictions=True,\n",
    "                include_test_set=False,\n",
    "                use_npy_rotate=True,\n",
    "                label_map={'y_true':'y_true'},\n",
    "                sample_frac=1.,\n",
    "                sample_every=2,\n",
    "                num_workers=2,\n",
    "                npy_rotate_sets_pkl=\"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/rotating_stack.pkl\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733520a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hiss",
   "language": "python",
   "name": "hiss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
