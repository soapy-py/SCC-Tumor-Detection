{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "339da00e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import fire\n",
    "from visdom import Visdom\n",
    "import pickle\n",
    "import sys, os\n",
    "import umap, numba\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import os,glob, pandas as pd\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GCNConv, GATConv, DeepGraphInfomax, SAGEConv\n",
    "from torch_geometric.nn import DenseGraphConv\n",
    "from torch_geometric.utils import to_dense_batch, to_dense_adj, dense_to_sparse\n",
    "from torch_geometric.nn import GINEConv\n",
    "from torch_geometric.utils import dropout_adj\n",
    "from torch_geometric.nn import APPNP\n",
    "from torch_cluster import knn_graph\n",
    "from torch_geometric.data import Data \n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "from torch_geometric.data import InMemoryDataset,DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "32ee50a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-15\n",
    "\n",
    "class GCNNet(torch.nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim, hidden_topology=[32,64,128,128], p=0.5, p2=0.1, drop_each=True):\n",
    "        super(GCNNet, self).__init__()\n",
    "        self.out_dim=out_dim\n",
    "        self.convs = nn.ModuleList([GATConv(inp_dim, hidden_topology[0])]+[GATConv(hidden_topology[i],hidden_topology[i+1]) for i in range(len(hidden_topology[:-1]))])\n",
    "        self.drop_edge = lambda edge_index: dropout_adj(edge_index,p=p2)[0]\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.fc = nn.Linear(hidden_topology[-1], out_dim)\n",
    "        self.drop_each=drop_each\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        for conv in self.convs:\n",
    "            if self.drop_each and self.training: edge_index=self.drop_edge(edge_index)\n",
    "            x=conv(x, edge_index, edge_attr)\n",
    "            x = F.relu(x)\n",
    "        if self.training:\n",
    "            x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "class GCNFeatures(torch.nn.Module):\n",
    "    def __init__(self, gcn, bayes=False):\n",
    "        super(GCNFeatures, self).__init__()\n",
    "        self.gcn=gcn\n",
    "        self.drop_each=bayes\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        for conv in self.gcn.convs:\n",
    "            if self.drop_each: edge_index=self.gcn.drop_edge(edge_index)\n",
    "            x = F.relu(conv(x, edge_index, edge_attr))\n",
    "        if self.drop_each:\n",
    "            x = self.gcn.dropout(x)\n",
    "        y = F.softmax(self.gcn.fc(x))\n",
    "        return x,y\n",
    "    \n",
    "def fit_model(graph_data='',\n",
    "              use_weights=False,\n",
    "              use_model=None,\n",
    "              n_batches_backward=1,\n",
    "              f1_metric='weighted',\n",
    "              n_epochs=1500,\n",
    "              out_dir=\"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/gnn/\",\n",
    "              lr=1e-2,\n",
    "              eta_min=1e-4,\n",
    "              T_max=20,\n",
    "              wd=0,\n",
    "              hidden_topology=[32,64,128,128],\n",
    "              p=0.5,\n",
    "              p2=0.3,\n",
    "              burnin=400,\n",
    "              warmup=100,\n",
    "              gpu_id=0,\n",
    "              batch_size=1\n",
    "             ):\n",
    "    print(gpu_id); torch.cuda.set_device(gpu_id)\n",
    "    datasets=pickle.load(open(graph_data,'rb'))\n",
    "    train_data = [dataset for key in datasets for dataset in datasets[key]['train']]\n",
    "    \n",
    "    # dataset splits\n",
    "    train_dataset, test_dataset= train_test_split(train_data, random_state=42)\n",
    "    train_dataset, val_dataset= train_test_split(train_dataset, random_state=42)\n",
    "    # train_dataset=random.sample(train_data,40)  \n",
    "    # val_data = [dataset for key in datasets for dataset in datasets[key]['val']]\n",
    "    # val_dataset=random.sample(val_data,15)\n",
    "    print(len(train_dataset), 'training graphs,', len(val_dataset), 'validation graphs')\n",
    "\n",
    "    # graph sizes\n",
    "    print(\"training graph sizes:\", end='')\n",
    "    for x in train_dataset:\n",
    "        print(x.x.shape[0], end=\", \")\n",
    "    print(\"\\nValidation graph sizes:\", end=\" \")\n",
    "    for x in val_dataset:\n",
    "        print(x.x.shape[0], end=\", \")\n",
    " \n",
    "    y_train=np.hstack([graph.y.numpy() for graph in train_dataset])\n",
    "\n",
    "    # weights\n",
    "    if use_weights: \n",
    "        weights=torch.tensor(compute_class_weight('balanced',classes=np.unique(y_train),y=y_train))\n",
    "    else: \n",
    "        weights=None \n",
    "       \n",
    "    # load model\n",
    "    model=GCNNet(train_dataset[0].x.shape[1],len(np.unique(y_train)),hidden_topology=hidden_topology,p=p,p2=p2)\n",
    "    model=model.cuda()\n",
    "\n",
    "    # load optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr,weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=eta_min, last_epoch=-1)\n",
    "    criterion= torch.nn.CrossEntropyLoss(weight=torch.tensor(weights).float() if use_weights else None)\n",
    "    criterion= criterion.cuda()\n",
    "\n",
    "    # initialize val saving\n",
    "    save_mod=False\n",
    "    past_performance=[0]\n",
    "\n",
    "    # dataloaders\n",
    "    dataloaders={}\n",
    "\n",
    "    dataloaders['train']=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "    dataloaders['val']=DataLoader(val_dataset,shuffle=True)\n",
    "    dataloaders['warmup']=DataLoader(train_dataset,shuffle=False)\n",
    "    train_loader=dataloaders['warmup']\n",
    "\n",
    "    #training\n",
    "    n_total_batches=0\n",
    "    train_val_f1=[]\n",
    "    for epoch in range(n_epochs):\n",
    "        Y,Y_Pred=[],[]\n",
    "        for i,data in enumerate(train_loader):\n",
    "            n_total_batches+=1\n",
    "            model.train(True)\n",
    "            x=data.x.cuda()\n",
    "            edge_index=data.edge_index.cuda()\n",
    "            y=data.y.cuda()\n",
    "            y_out=model(x,edge_index)\n",
    "            loss = criterion(y_out, y) / n_batches_backward\n",
    "            loss.backward()\n",
    "            if n_total_batches%n_batches_backward==0 or (i==len(train_loader.dataset)-1):\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            Y_Pred.append(F.softmax(y_out, dim=1).argmax(1).detach().cpu().numpy().flatten())\n",
    "            Y.append(y.detach().cpu().numpy().flatten())\n",
    "            del x, edge_index, loss, y_out\n",
    "            if epoch <=warmup:\n",
    "                break \n",
    "       \n",
    "        if epoch == warmup:\n",
    "            train_loader=dataloaders['train']\n",
    "        if epoch>=burnin:\n",
    "            save_mod=True\n",
    "        \n",
    "        train_f1=f1_score(np.hstack(Y),np.hstack(Y_Pred),average=f1_metric)\n",
    "        scheduler.step()\n",
    "        Y, Y_Pred, Y_Prob=[],[],[]\n",
    "        \n",
    "        for i,data in enumerate(dataloaders['val']):\n",
    "            model.train(False)\n",
    "            x=data.x.cuda()\n",
    "            edge_index=data.edge_index.cuda()\n",
    "            y=data.y.cuda()\n",
    "            \n",
    "            y_out=model(x,edge_index)\n",
    "            loss = criterion(y_out, y) \n",
    "            y_prob=F.softmax(y_out, dim=1).detach().cpu().numpy()\n",
    "            y_pred=y_prob.argmax(1).flatten()\n",
    "            y_true=y.detach().cpu().numpy().flatten()\n",
    "            Y_Pred.append(y_pred)\n",
    "            Y_Prob.append(y_prob)\n",
    "            Y.append(y_true)\n",
    "            #if vis_every and epoch%vis_every==0 and not i:\n",
    "               # vis.scatter(data.pos.numpy(),opts=dict(markercolor=(y_pred*255).astype(int),webgl=False,markerborderwidth=0,markersize=5),win=\"pred\")\n",
    "               # vis.scatter(data.pos.numpy(),opts=dict(markercolor=y_true*255,webgl=False,markerborderwidth=0,markersize=5),win=\"true\")\n",
    "            del x, edge_index, loss, y_out\n",
    "        val_f1=f1_score(np.hstack(Y),np.hstack(Y_Pred),average=f1_metric)\n",
    "        val_roc=roc_auc_score(np.hstack(Y), np.vstack(Y_Prob)[:,1], average='macro')#multi_class=\"ovr\",average='macro')\n",
    "        \n",
    "        if save_mod and val_roc>=max(past_performance):\n",
    "            best_model_dict=copy.deepcopy(model.state_dict())\n",
    "            past_performance.append(val_roc)\n",
    "        \n",
    "        print(epoch,train_f1,val_f1,val_roc, flush=True)\n",
    "        train_val_f1.append((train_f1,val_f1, val_roc))\n",
    "\n",
    "    model.load_state_dict(best_model_dict)\n",
    "    torch.save(model.state_dict(),os.path.join(out_dir,f\"model.pth\"))\n",
    "    torch.save(train_val_f1,os.path.join(out_dir,f\"f1.log.pth\"))\n",
    "    return Y, Y_Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "602f5a2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "131 training graphs, 44 validation graphs\n",
      "training graph sizes:5719, 5563, 3966, 815, 4659, 2825, 3510, 10168, 256, 2561, 6168, 3216, 2462, 3117, 459, 6239, 8481, 3270, 2825, 2752, 2096, 6622, 6892, 1275, 6236, 2800, 2314, 5083, 7126, 668, 8312, 6222, 2518, 1230, 5075, 20589, 7200, 358, 2955, 2440, 5137, 5176, 826, 844, 22341, 10205, 2647, 2806, 842, 2806, 5831, 2311, 8925, 7890, 8318, 1708, 3460, 4841, 6896, 23840, 5494, 1299, 2059, 6950, 1926, 3291, 18384, 2529, 842, 6353, 1787, 2536, 3424, 7420, 7762, 1686, 2642, 2598, 1665, 5952, 15681, 8040, 1663, 16135, 7910, 12102, 5921, 5019, 6056, 5059, 902, 2955, 3285, 4607, 3797, 2217, 14939, 7163, 2774, 2572, 506, 4642, 4557, 256, 2678, 2435, 4862, 5344, 5193, 8943, 817, 13521, 3746, 10333, 647, 10733, 4710, 212, 4710, 5494, 4557, 8385, 6416, 902, 6601, 2460, 5649, 3544, 6098, 7523, 3236, \n",
      "Validation graph sizes: 2793, 1805, 2198, 1771, 2110, 668, 2053, 703, 2290, 795, 2608, 871, 5101, 15269, 3282, 2706, 3522, 3631, 703, 2365, 10277, 844, 878, 3076, 2573, 3738, 4220, 826, 3317, 11918, 3051, 2950, 2900, 6353, 832, 884, 832, 2460, 2578, 8477, 2178, 6347, 2578, 10713, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ipykernel_196346/3840433938.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  criterion= torch.nn.CrossEntropyLoss(weight=torch.tensor(weights).float() if use_weights else None)\n",
      "/dartfs-hpc/rc/home/3/f006n33/anaconda3/envs/hiss/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7027886634106104 0.686628182897969 0.4774627859582858\n",
      "1 0.7727283732604792 0.686628182897969 0.4797697900966827\n",
      "2 0.8057986626734552 0.686628182897969 0.496874962770547\n",
      "3 0.8052260364608892 0.686628182897969 0.500202471980304\n",
      "4 0.8120064695698704 0.686628182897969 0.5004210444909397\n",
      "5 0.8128752636947686 0.686628182897969 0.5022977650572915\n",
      "6 0.8314341391071112 0.686628182897969 0.506902762024198\n",
      "7 0.8506051132477062 0.686628182897969 0.5106076731598759\n",
      "8 0.8592732326733107 0.686628182897969 0.5134512302361863\n",
      "9 0.8564639205819526 0.686628182897969 0.5139197386177341\n",
      "10 0.8599100111486991 0.686628182897969 0.513070775617314\n",
      "11 0.8629812512360765 0.686628182897969 0.5154179944527795\n",
      "12 0.8632938466718697 0.686628182897969 0.5169797191057953\n",
      "13 0.8652993900140943 0.686628182897969 0.5219232367915752\n",
      "14 0.8614838906200134 0.686628182897969 0.5324840567517884\n",
      "15 0.8548354566325328 0.686628182897969 0.5300558581385584\n",
      "16 0.8493005258014781 0.686628182897969 0.5293159074077883\n",
      "17 0.8504116739983332 0.686628182897969 0.540550766770944\n",
      "18 0.8452512321322564 0.686628182897969 0.550812480462984\n",
      "19 0.8361518936506259 0.686628182897969 0.5573510008876594\n",
      "20 0.7867140737505376 0.686628182897969 0.5619834822481536\n",
      "21 0.7345086510951753 0.686628182897969 0.5755354990897515\n",
      "22 0.7721100332107654 0.686628182897969 0.5963366730326622\n",
      "23 0.8189778624079367 0.686628182897969 0.6067499853560501\n",
      "24 0.8264377410897359 0.686628182897969 0.614848613054956\n",
      "25 0.8299563926778383 0.686628182897969 0.6168565895151372\n",
      "26 0.8861426133185486 0.686628182897969 0.6180997348574676\n",
      "27 0.8786498079175012 0.686628182897969 0.6203553938532378\n",
      "28 0.7630720606557105 0.686628182897969 0.6207159280127802\n",
      "29 0.5584144506760662 0.686628182897969 0.619694392384579\n",
      "30 0.36528579407159245 0.686628182897969 0.6185370332497705\n",
      "31 0.27919699668602954 0.686628182897969 0.6179629331026821\n",
      "32 0.2092442590713316 0.6869081626889836 0.6173840648685948\n",
      "33 0.175679206911201 0.6949910896317718 0.6171833039656036\n",
      "34 0.151860137824596 0.7129170970096843 0.6168533019410526\n",
      "35 0.1741798440824571 0.7136629190189379 0.6166246720742293\n",
      "36 0.15827148748894893 0.7136753670118012 0.61607090920526\n",
      "37 0.18516058052672757 0.7136753670118012 0.6152771075945447\n",
      "38 0.2565629182652493 0.7136753670118012 0.6145909360097528\n",
      "39 0.4038020881053938 0.7136753670118012 0.6138188144354603\n",
      "40 0.7968674278193051 0.7136753670118012 0.6133959831973942\n",
      "41 0.8575466582170697 0.7136753670118012 0.6106497157907947\n",
      "42 0.8173425899029324 0.7136753670118012 0.6091246955680559\n",
      "43 0.6793558205139815 0.7136753670118012 0.6082840498989981\n",
      "44 0.7775639205851319 0.7136753670118012 0.6076737727249981\n",
      "45 0.8630550216241922 0.7136753670118012 0.606113865798026\n",
      "46 0.8979569605573865 0.7136753670118012 0.6047729644517132\n",
      "47 0.904709295467553 0.7136753670118012 0.6030256264434519\n",
      "48 0.8908340357749162 0.7136753670118012 0.6015774291102756\n",
      "49 0.868552139248708 0.7136753670118012 0.5995911017076003\n",
      "50 0.816018796575928 0.7136753670118012 0.5981373849217251\n",
      "51 0.7779522222062005 0.7136753670118012 0.5971332132304252\n",
      "52 0.7843800482482117 0.7136753670118012 0.5956568485708965\n",
      "53 0.8198025124111702 0.7136753670118012 0.5950872399858843\n",
      "54 0.8672167279535546 0.7136753670118012 0.5943783429531838\n",
      "55 0.8946967548665686 0.7136753670118012 0.5935463993126948\n",
      "56 0.8970787427957277 0.7136753670118012 0.5931431708730966\n",
      "57 0.8944145638389505 0.7136753670118012 0.5925814137930266\n",
      "58 0.87563250624156 0.7136753670118012 0.5920084278712477\n",
      "59 0.8366300036402465 0.7136753670118012 0.591215951624936\n",
      "60 0.8268608014048169 0.7136753670118012 0.5906072360930635\n",
      "61 0.8350115431713588 0.7136753670118012 0.5898533379936559\n",
      "62 0.8733575396830308 0.7136753670118012 0.5891159503873791\n",
      "63 0.8933287564536027 0.7136753670118012 0.5882978718436499\n",
      "64 0.8989680521700137 0.7136753670118012 0.5877118187861496\n",
      "65 0.8904161535521561 0.7136753670118012 0.5871397595656624\n",
      "66 0.8748191541539092 0.7136753670118012 0.5865230730568769\n",
      "67 0.8540338319288084 0.7136753670118012 0.5861741181563008\n",
      "68 0.8513147762336626 0.7136753670118012 0.5858407843746197\n",
      "69 0.862525521829876 0.7136753670118012 0.5855805786118354\n",
      "70 0.8863305946382131 0.7136753670118012 0.58541261422492\n",
      "71 0.8914858721245161 0.7136753670118012 0.5853132695357394\n",
      "72 0.8921173362344305 0.7136753670118012 0.5850453606874965\n",
      "73 0.8860753860173312 0.7136753670118012 0.5850871305515905\n",
      "74 0.8657577046800793 0.7136753670118012 0.5851297609693706\n",
      "75 0.8666338745451164 0.7136753670118012 0.5853654531405293\n",
      "76 0.8811422763478468 0.7136753670118012 0.585776149911416\n",
      "77 0.8806414834417343 0.7136753670118012 0.5861882249901834\n",
      "78 0.888494923802654 0.7136753670118012 0.5865739038733038\n",
      "79 0.8896564876618388 0.7136753670118012 0.5866442002414396\n",
      "80 0.8845936937353273 0.7136753670118012 0.5869719956529682\n",
      "81 0.882470035157022 0.7136753670118012 0.5871637113470108\n",
      "82 0.8846316687273867 0.7136753670118012 0.5875471991065676\n",
      "83 0.8783826513741615 0.7136753670118012 0.5880457784943094\n",
      "84 0.8820940688342248 0.7136753670118012 0.5886971098432433\n",
      "85 0.892263846845441 0.7136753670118012 0.5893272751010656\n",
      "86 0.8954619757199836 0.7136753670118012 0.5898677828024445\n",
      "87 0.8982805266610916 0.7136753670118012 0.5901226063592893\n",
      "88 0.8878762003785557 0.7136753670118012 0.5903880354975821\n",
      "89 0.878894821901491 0.7136753670118012 0.5905399129391826\n",
      "90 0.8832904078069845 0.7136753670118012 0.5910401669659212\n",
      "91 0.8876001097368773 0.7136753670118012 0.5919358261130516\n",
      "92 0.896844876743631 0.7136753670118012 0.5925495656888767\n",
      "93 0.9006156419273131 0.7136753670118012 0.5930689125046176\n",
      "94 0.8988831658284758 0.7136753670118012 0.59311734118535\n",
      "95 0.9004923651166082 0.7136753670118012 0.5931312466562718\n",
      "96 0.8925457861194054 0.7136753670118012 0.5934981623457524\n",
      "97 0.8906137697599862 0.7136753670118012 0.5941633798672665\n",
      "98 0.903159709841697 0.7136753670118012 0.5944299324991515\n",
      "99 0.9085968554709781 0.7136753670118012 0.5945786390454573\n",
      "100 0.9060643485066115 0.7136753670118012 0.5946003692321761\n",
      "101 0.5352083629548547 0.0909621021604155 0.4707352517343735\n",
      "102 0.642571820149758 0.6732658981935726 0.47770260937116604\n",
      "103 0.7267543844870281 0.6820215001285508 0.47814201179056925\n",
      "104 0.7285379687421097 0.684735531813743 0.4784995110319482\n",
      "105 0.7285069326613649 0.6861030837089088 0.482747032118626\n",
      "106 0.7284686137585693 0.6864013652406451 0.4833075083982263\n",
      "107 0.7285113706446119 0.6864046063181878 0.4791935831293133\n",
      "108 0.7284630400829635 0.6865536700237056 0.4837773423981033\n",
      "109 0.7284682146962762 0.6865212691759973 0.5796008254208517\n",
      "110 0.7284815798414779 0.6865698695507152 0.47981013620022284\n",
      "111 0.7284922126457721 0.6865601499062539 0.47774205797485286\n",
      "112 0.7285681948758258 0.6865731093843735 0.47757472192670875\n",
      "113 0.7284120551426179 0.6866184645447616 0.4929051585334105\n",
      "114 0.728411011414411 0.6866184645447616 0.49273294660687195\n",
      "115 0.7284210282248202 0.6866217040197404 0.4925648684613106\n",
      "116 0.7284101648204441 0.686628182897969 0.4924345456169115\n",
      "117 0.7284124886247761 0.686628182897969 0.4927707130807583\n",
      "118 0.7284059489908529 0.686628182897969 0.4937580590303583\n",
      "119 0.7284217669034488 0.6866152250458725 0.47925670000647247\n",
      "120 0.7284842521759841 0.6866152250458725 0.37642591990515845\n",
      "121 0.728428414956454 0.686628182897969 0.5\n",
      "122 0.7284306309522626 0.686628182897969 0.39198234060142395\n",
      "123 0.7284192433230432 0.686628182897969 0.5\n",
      "124 0.7284135489352769 0.686628182897969 0.5\n",
      "125 0.7284178732504304 0.686628182897969 0.5\n",
      "126 0.7284272604610013 0.686628182897969 0.5\n",
      "127 0.7284157644684048 0.686628182897969 0.3158289439049802\n",
      "128 0.7284165029770142 0.686628182897969 0.5\n",
      "129 0.728412810421801 0.686628182897969 0.5\n",
      "130 0.7284172414844069 0.686628182897969 0.5\n",
      "131 0.7284194569992849 0.686628182897969 0.5\n",
      "132 0.7284157644684048 0.686628182897969 0.5\n",
      "133 0.7284165029770142 0.686628182897969 0.5\n",
      "134 0.7284179799905829 0.686628182897969 0.5\n",
      "135 0.7284172414844069 0.686628182897969 0.5\n",
      "136 0.7284165029770142 0.686628182897969 0.3617612315243685\n",
      "137 0.7284172414844069 0.686628182897969 0.4870758929225344\n",
      "138 0.7284194569992849 0.686628182897969 0.5\n",
      "139 0.7284194569992849 0.686628182897969 0.5\n",
      "140 0.7284187184955422 0.686628182897969 0.5\n",
      "141 0.7284194569992849 0.686628182897969 0.5\n",
      "142 0.7284194569992849 0.686628182897969 0.6080176593985761\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [54]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y, y_pred\u001b[38;5;241m=\u001b[39m \u001b[43mfit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/graph_dataset/graph_dataset.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                     \u001b[49m\u001b[43muse_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/gnn/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mhidden_topology\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mburnin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mgpu_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [53]\u001b[0m, in \u001b[0;36mfit_model\u001b[0;34m(graph_data, use_weights, use_model, n_batches_backward, f1_metric, n_epochs, out_dir, lr, eta_min, T_max, wd, hidden_topology, p, p2, burnin, warmup, gpu_id, batch_size)\u001b[0m\n\u001b[1;32m    112\u001b[0m n_total_batches\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    113\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 114\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m edge_index\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39medge_index\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    116\u001b[0m y\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mcuda()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y, y_pred= fit_model(graph_data=\"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/graph_dataset/graph_dataset.pkl\",\n",
    "                     use_weights=True,\n",
    "                     n_epochs=800,\n",
    "                     out_dir='/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/gnn/',\n",
    "                     lr=1e-4,\n",
    "                     batch_size=1,\n",
    "                     hidden_topology=[32,64,64,128],\n",
    "                     burnin=0,\n",
    "                     gpu_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "338fc5f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#original approach\u001b[39;00m\n\u001b[1;32m      3\u001b[0m datasets\u001b[38;5;241m=\u001b[39msophie_data\n\u001b[0;32m----> 4\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m)\n\u001b[1;32m      5\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m [dataset \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m datasets \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets[key][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dataset), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining graphs,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(val_dataset), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation graphs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'sample'"
     ]
    }
   ],
   "source": [
    "#original approach\n",
    "\n",
    "datasets=sophie_data\n",
    "train_data = [dataset for key in datasets for dataset in datasets[key]['train']]\n",
    "train_dataset=random.sample(train_data,40)\n",
    "val_data = [dataset for key in datasets for dataset in datasets[key]['val']]\n",
    "val_dataset=random.sample(val_data,15)\n",
    "print(len(train_dataset), 'training graphs,', len(val_dataset), 'validation graphs')\n",
    "\n",
    "#dataloaders\n",
    "\n",
    "\n",
    "dataloaders['train']=DataLoader(train_dataset,batch_size=1,shuffle=True)\n",
    "dataloaders['val']=DataLoader(val_dataset,shuffle=True)\n",
    "dataloaders['warmup']=DataLoader(train_dataset,shuffle=False)\n",
    "train_loader=dataloaders['warmup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7c96da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hiss",
   "language": "python",
   "name": "hiss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
