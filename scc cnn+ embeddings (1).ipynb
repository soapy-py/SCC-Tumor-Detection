{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc486bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "326985d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 95/95 [00:03<00:00, 27.84it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import glob, os, pickle\n",
    "dfs=[]\n",
    "path= \"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/scc_tumor_data/prelim_patch_info_v2/\"\n",
    "\n",
    "for file in tqdm.tqdm(sorted(glob.glob(path + '/*.pkl'))):\n",
    "    basename=os.path.basename(file).replace(\".pkl\",\"\")\n",
    "    df=pd.read_pickle(file)\n",
    "    df['patch_index']=np.arange(df.shape[0])\n",
    "    if 'scc' not in df.columns: df['scc']=0\n",
    "    df['scc']=(df['scc']==1).astype(int)  \n",
    "    df['ID']=basename\n",
    "    dfs.append(df)\n",
    "df=pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9644ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30fe5679",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TUMOR_THRESHOLD=0.2\n",
    "IDs=(df.groupby([\"ID\"])['scc'].mean()).astype(int) #>TUMOR_THRESHOLD).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e4fc47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ID,test_ID=train_test_split(IDs,stratify=IDs.values, random_state=42)\n",
    "train_ID,val_ID=train_test_split(train_ID,stratify=train_ID.values, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "687f1f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_test_slide_ids=dict(train=train_ID.index,val=val_ID.index,test=test_ID.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0a6c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "patch_dict=dict(train=df.loc[df['ID'].isin(train_val_test_slide_ids['train'])].sample(n=160000),\n",
    "               val=df.loc[df['ID'].isin(train_val_test_slide_ids['val'])].sample(n=20000),\n",
    "               test=df.loc[df['ID'].isin(train_val_test_slide_ids['test'])].sample(n=20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3769d9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_test_slide_ids={k:patch_dict[k]['ID'].unique() for k in patch_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92c3ba80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': array(['270_A2f_ASAP_tumor_map', '352_A1d_ASAP_tumor_map',\n",
       "        '364_A4b_ASAP_tumor_map', '270_A1b_ASAP_tumor_map',\n",
       "        '363_A3b_ASAP_tumor_map', '7_A1d_ASAP_tumor_map',\n",
       "        '110_A2b_ASAP_tumor_map', '353_A2b_ASAP_tumor_map',\n",
       "        '362_A1b_ASAP_tumor_map', '365_A1b_ASAP_tumor_map',\n",
       "        '352_A1i_ASAP_tumor_map', '109_A1c_ASAP_tumor_map',\n",
       "        '327_A1d_ASAP_tumor_map', '352_A1h_ASAP_tumor_map',\n",
       "        '364_A1b_ASAP_tumor_map', '346_a_ASAP_tumor_map',\n",
       "        '352_A1g_ASAP_tumor_map', '345_a_ASAP_tumor_map',\n",
       "        '281_A1f_ASAP_tumor_map', '350_A1a_ASAP_tumor_map',\n",
       "        '363_A2b_ASAP_tumor_map', '370_A1b_ASAP_tumor_map',\n",
       "        '343_c_ASAP_tumor_map', '367_A2b_ASAP_tumor_map',\n",
       "        '354_D1b_ASAP_tumor_map', '70_A2b_ASAP_tumor_map',\n",
       "        '369_A1c_ASAP_tumor_map', '341_a_ASAP_tumor_map',\n",
       "        '362_A1c_ASAP_tumor_map', '342_a_ASAP_tumor_map',\n",
       "        '354_A1b_ASAP_tumor_map', '112_a_ASAP_tumor_map',\n",
       "        '351_A2b_ASAP_tumor_map', '343_a_ASAP_tumor_map',\n",
       "        '10_A2b_ASAP_tumor_map', '366_A1c_ASAP_tumor_map',\n",
       "        '368_A1d_ASAP_tumor_map', '361_b_ASAP_tumor_map',\n",
       "        '366_A1a_ASAP_tumor_map', '12_A1c_ASAP_tumor_map',\n",
       "        '368_A1b_ASAP_tumor_map', '112_b_ASAP_tumor_map',\n",
       "        '363_A1c_ASAP_tumor_map', '363_A1b_ASAP_tumor_map',\n",
       "        '354_A3a_ASAP_tumor_map', '345_b_ASAP_tumor_map',\n",
       "        '14_A2b_ASAP_tumor_map', '350_A1d_ASAP_tumor_map',\n",
       "        '350_A1e_ASAP_tumor_map', '350_A1c_ASAP_tumor_map',\n",
       "        '270_A2b_ASAP_tumor_map', '14_A1b_ASAP_tumor_map',\n",
       "        '341_b_ASAP_tumor_map'], dtype=object),\n",
       " 'val': array(['342_b_ASAP_tumor_map', '356_A1b_ASAP_tumor_map',\n",
       "        '352_A1e_ASAP_tumor_map', '281_A2eX_ASAP_tumor_map',\n",
       "        '354_A3c_ASAP_tumor_map', '354_A3b_ASAP_tumor_map',\n",
       "        '281_A1d_ASAP_tumor_map', '350_A1b_ASAP_tumor_map',\n",
       "        '123_A1a_ASAP_tumor_map', '169_A2b_ASAP_tumor_map',\n",
       "        '343_d_ASAP_tumor_map', '270_A1d_ASAP_tumor_map',\n",
       "        '7_A1e_ASAP_tumor_map', '344_b_ASAP_tumor_map',\n",
       "        '361_a_ASAP_tumor_map', '366_A1b_ASAP_tumor_map',\n",
       "        '369_A2b_ASAP_tumor_map', '61_B1a_ASAP_tumor_map'], dtype=object),\n",
       " 'test': array(['327_A1a_ASAP_tumor_map', '365_A2b_ASAP_tumor_map',\n",
       "        '354_A1c_ASAP_tumor_map', '7_A1c_ASAP_tumor_map',\n",
       "        '10_A1b_ASAP_tumor_map', '364_A2b_ASAP_tumor_map',\n",
       "        '10_A1a_ASAP_tumor_map', '270_A1e_ASAP_tumor_map',\n",
       "        '362_A1a_ASAP_tumor_map', '344_a_ASAP_tumor_map',\n",
       "        '327_B1c_ASAP_tumor_map', '343_b_ASAP_tumor_map',\n",
       "        '358_A1b_ASAP_tumor_map', '368_A1c_ASAP_tumor_map',\n",
       "        '37_A2d_ASAP_tumor_map', '369_A1b_ASAP_tumor_map',\n",
       "        '346_b_ASAP_tumor_map', '370_A2b_ASAP_tumor_map',\n",
       "        '61_A1a_ASAP_tumor_map', '355_A1d_ASAP_tumor_map',\n",
       "        '358_A1a_ASAP_tumor_map', '311_A2c_ASAP_tumor_map',\n",
       "        '370_A2a_ASAP_tumor_map', '354_A1d_ASAP_tumor_map'], dtype=object)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_test_slide_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9df0968",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Set']='train'\n",
    "for k in train_val_test_slide_ids:\n",
    "    df.loc[df[\"ID\"].isin(train_val_test_slide_ids[k]),\"Set\"]=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3740d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.to_pickle(\"Master_Dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d62370e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 12 14:28:47 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:18:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    42W / 300W |      0MiB / 32768MiB |      0%   E. Process |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  Off  | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    39W / 300W |      0MiB / 32768MiB |      0%   E. Process |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  Off  | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    55W / 300W |   1240MiB / 32768MiB |     11%   E. Process |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    2   N/A  N/A    213624      C   ...conda3/envs/LP/bin/python     1237MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75311dfb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_val_test:   0%|                                              | 0/3 [00:00<?, ?it/s]\n",
      "case:   0%|                                                       | 0/53 [00:00<?, ?it/s]\u001b[A\n",
      "case:   2%|▉                                              | 1/53 [00:03<03:21,  3.88s/it]\u001b[A\n",
      "case:   4%|█▊                                             | 2/53 [00:05<02:09,  2.54s/it]\u001b[A\n",
      "case:   6%|██▋                                            | 3/53 [00:07<01:44,  2.09s/it]\u001b[A\n",
      "case:   8%|███▌                                           | 4/53 [00:07<01:12,  1.48s/it]\u001b[A\n",
      "case:   9%|████▍                                          | 5/53 [00:08<00:55,  1.16s/it]\u001b[A\n",
      "case:  11%|█████▎                                         | 6/53 [00:09<01:04,  1.38s/it]\u001b[A\n",
      "case:  13%|██████▏                                        | 7/53 [00:11<01:02,  1.36s/it]\u001b[A\n",
      "case:  15%|███████                                        | 8/53 [00:12<01:00,  1.34s/it]\u001b[A\n",
      "case:  17%|███████▉                                       | 9/53 [00:14<01:06,  1.51s/it]\u001b[A\n",
      "case:  19%|████████▋                                     | 10/53 [00:16<01:07,  1.57s/it]\u001b[A\n",
      "case:  21%|█████████▌                                    | 11/53 [00:16<00:54,  1.30s/it]\u001b[A\n",
      "case:  23%|██████████▍                                   | 12/53 [00:18<00:52,  1.29s/it]\u001b[A\n",
      "case:  25%|███████████▎                                  | 13/53 [00:19<00:53,  1.34s/it]\u001b[A\n",
      "case:  26%|████████████▏                                 | 14/53 [00:20<00:46,  1.19s/it]\u001b[A\n",
      "case:  28%|█████████████                                 | 15/53 [00:21<00:38,  1.01s/it]\u001b[A\n",
      "case:  30%|█████████████▉                                | 16/53 [00:27<01:38,  2.65s/it]\u001b[A\n",
      "case:  32%|██████████████▊                               | 17/53 [00:42<03:50,  6.41s/it]\u001b[A\n",
      "case:  34%|███████████████▌                              | 18/53 [01:02<06:09, 10.57s/it]\u001b[A\n",
      "case:  36%|████████████████▍                             | 19/53 [01:16<06:28, 11.43s/it]\u001b[A\n",
      "case:  38%|█████████████████▎                            | 20/53 [01:29<06:32, 11.90s/it]\u001b[A\n",
      "case:  40%|██████████████████▏                           | 21/53 [01:39<06:04, 11.40s/it]\u001b[A\n",
      "case:  42%|███████████████████                           | 22/53 [01:53<06:21, 12.31s/it]\u001b[A\n",
      "case:  43%|███████████████████▉                          | 23/53 [02:09<06:42, 13.41s/it]\u001b[A\n",
      "case:  45%|████████████████████▊                         | 24/53 [02:25<06:44, 13.95s/it]\u001b[A\n",
      "case:  47%|█████████████████████▋                        | 25/53 [02:38<06:21, 13.63s/it]\u001b[A\n",
      "case:  49%|██████████████████████▌                       | 26/53 [02:47<05:33, 12.36s/it]\u001b[A\n",
      "case:  51%|███████████████████████▍                      | 27/53 [03:05<06:08, 14.17s/it]\u001b[A\n",
      "case:  53%|████████████████████████▎                     | 28/53 [03:21<06:05, 14.61s/it]\u001b[A\n",
      "case:  55%|█████████████████████████▏                    | 29/53 [03:34<05:38, 14.11s/it]\u001b[A\n",
      "case:  57%|██████████████████████████                    | 30/53 [03:47<05:14, 13.69s/it]\u001b[A\n",
      "case:  58%|██████████████████████████▉                   | 31/53 [04:03<05:18, 14.49s/it]\u001b[A\n",
      "case:  60%|███████████████████████████▊                  | 32/53 [04:15<04:49, 13.76s/it]\u001b[A\n",
      "case:  62%|████████████████████████████▋                 | 33/53 [04:27<04:23, 13.17s/it]\u001b[A\n",
      "case:  64%|█████████████████████████████▌                | 34/53 [04:40<04:12, 13.30s/it]\u001b[A\n",
      "case:  66%|██████████████████████████████▍               | 35/53 [05:00<04:31, 15.06s/it]\u001b[A\n",
      "case:  68%|███████████████████████████████▏              | 36/53 [05:19<04:35, 16.23s/it]\u001b[A\n",
      "case:  70%|████████████████████████████████              | 37/53 [05:36<04:23, 16.48s/it]\u001b[A\n",
      "case:  72%|████████████████████████████████▉             | 38/53 [05:54<04:16, 17.08s/it]\u001b[A\n",
      "case:  74%|█████████████████████████████████▊            | 39/53 [06:12<04:03, 17.40s/it]\u001b[A\n",
      "case:  75%|██████████████████████████████████▋           | 40/53 [06:34<04:04, 18.81s/it]\u001b[A\n",
      "case:  77%|███████████████████████████████████▌          | 41/53 [06:55<03:53, 19.49s/it]\u001b[A\n",
      "case:  79%|████████████████████████████████████▍         | 42/53 [07:15<03:34, 19.46s/it]\u001b[A\n",
      "case:  81%|█████████████████████████████████████▎        | 43/53 [07:34<03:14, 19.41s/it]\u001b[A\n",
      "case:  83%|██████████████████████████████████████▏       | 44/53 [07:51<02:48, 18.68s/it]\u001b[A\n",
      "case:  85%|███████████████████████████████████████       | 45/53 [08:03<02:13, 16.71s/it]\u001b[A\n",
      "case:  87%|███████████████████████████████████████▉      | 46/53 [08:19<01:54, 16.33s/it]\u001b[A\n",
      "case:  89%|████████████████████████████████████████▊     | 47/53 [08:30<01:29, 14.99s/it]\u001b[A\n",
      "case:  91%|█████████████████████████████████████████▋    | 48/53 [08:55<01:29, 17.89s/it]\u001b[A\n",
      "case:  92%|██████████████████████████████████████████▌   | 49/53 [09:25<01:25, 21.47s/it]\u001b[A\n",
      "case:  94%|███████████████████████████████████████████▍  | 50/53 [09:37<00:56, 18.76s/it]\u001b[A\n",
      "case:  96%|████████████████████████████████████████████▎ | 51/53 [09:47<00:32, 16.02s/it]\u001b[A\n",
      "case:  98%|█████████████████████████████████████████████▏| 52/53 [09:56<00:14, 14.03s/it]\u001b[A\n",
      "case: 100%|██████████████████████████████████████████████| 53/53 [10:12<00:00, 11.55s/it]\u001b[A\n",
      "train_val_test:  33%|████████████▎                        | 1/3 [11:47<23:35, 707.77s/it]\n",
      "case:   0%|                                                       | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "case:   6%|██▌                                            | 1/18 [00:13<03:56, 13.90s/it]\u001b[A\n",
      "case:  11%|█████▏                                         | 2/18 [00:27<03:39, 13.73s/it]\u001b[A\n",
      "case:  17%|███████▊                                       | 3/18 [00:46<04:01, 16.08s/it]\u001b[A\n",
      "case:  22%|██████████▍                                    | 4/18 [00:58<03:21, 14.41s/it]\u001b[A\n",
      "case:  28%|█████████████                                  | 5/18 [01:10<02:55, 13.50s/it]\u001b[A\n",
      "case:  33%|███████████████▋                               | 6/18 [01:16<02:14, 11.23s/it]\u001b[A\n",
      "case:  39%|██████████████████▎                            | 7/18 [01:38<02:40, 14.63s/it]\u001b[A\n",
      "case:  44%|████████████████████▉                          | 8/18 [01:47<02:08, 12.90s/it]\u001b[A\n",
      "case:  50%|███████████████████████▌                       | 9/18 [02:05<02:08, 14.32s/it]\u001b[A\n",
      "case:  56%|█████████████████████████▌                    | 10/18 [02:23<02:03, 15.41s/it]\u001b[A\n",
      "case:  61%|████████████████████████████                  | 11/18 [02:42<01:55, 16.52s/it]\u001b[A\n",
      "case:  67%|██████████████████████████████▋               | 12/18 [03:05<01:51, 18.59s/it]\u001b[A\n",
      "case:  72%|█████████████████████████████████▏            | 13/18 [03:15<01:19, 15.89s/it]\u001b[A\n",
      "case:  78%|███████████████████████████████████▊          | 14/18 [03:30<01:02, 15.71s/it]\u001b[A\n",
      "case:  83%|██████████████████████████████████████▎       | 15/18 [03:45<00:46, 15.40s/it]\u001b[A\n",
      "case:  89%|████████████████████████████████████████▉     | 16/18 [03:57<00:29, 14.65s/it]\u001b[A\n",
      "case:  94%|███████████████████████████████████████████▍  | 17/18 [04:07<00:13, 13.01s/it]\u001b[A\n",
      "case: 100%|██████████████████████████████████████████████| 18/18 [04:19<00:00, 14.43s/it]\u001b[A\n",
      "train_val_test:  67%|████████████████████████▋            | 2/3 [16:30<07:37, 457.46s/it]\n",
      "case:   0%|                                                       | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "case:   4%|█▉                                             | 1/24 [00:14<05:41, 14.86s/it]\u001b[A\n",
      "case:   8%|███▉                                           | 2/24 [00:29<05:22, 14.67s/it]\u001b[A\n",
      "case:  12%|█████▉                                         | 3/24 [00:47<05:39, 16.15s/it]\u001b[A\n",
      "case:  17%|███████▊                                       | 4/24 [00:58<04:40, 14.03s/it]\u001b[A\n",
      "case:  21%|█████████▊                                     | 5/24 [01:20<05:21, 16.94s/it]\u001b[A\n",
      "case:  25%|███████████▊                                   | 6/24 [01:52<06:41, 22.29s/it]\u001b[A\n",
      "case:  29%|█████████████▋                                 | 7/24 [02:12<06:06, 21.55s/it]\u001b[A\n",
      "case:  33%|███████████████▋                               | 8/24 [02:21<04:37, 17.37s/it]\u001b[A\n",
      "case:  38%|█████████████████▋                             | 9/24 [02:30<03:44, 14.96s/it]\u001b[A\n",
      "case:  42%|███████████████████▏                          | 10/24 [02:44<03:23, 14.55s/it]\u001b[A\n",
      "case:  46%|█████████████████████                         | 11/24 [03:00<03:12, 14.82s/it]\u001b[A\n",
      "case:  50%|███████████████████████                       | 12/24 [03:08<02:34, 12.84s/it]\u001b[A\n",
      "case:  54%|████████████████████████▉                     | 13/24 [03:22<02:24, 13.13s/it]\u001b[A\n",
      "case:  58%|██████████████████████████▊                   | 14/24 [03:38<02:20, 14.05s/it]\u001b[A\n",
      "case:  62%|████████████████████████████▊                 | 15/24 [03:54<02:12, 14.71s/it]\u001b[A\n",
      "case:  67%|██████████████████████████████▋               | 16/24 [04:14<02:09, 16.20s/it]\u001b[A\n",
      "case:  71%|████████████████████████████████▌             | 17/24 [04:30<01:52, 16.08s/it]\u001b[A\n",
      "case:  75%|██████████████████████████████████▌           | 18/24 [04:57<01:56, 19.46s/it]\u001b[A\n",
      "case:  79%|████████████████████████████████████▍         | 19/24 [05:10<01:27, 17.41s/it]\u001b[A\n",
      "case:  83%|██████████████████████████████████████▎       | 20/24 [05:18<00:58, 14.60s/it]\u001b[A\n",
      "case:  88%|████████████████████████████████████████▎     | 21/24 [05:26<00:38, 12.73s/it]\u001b[A\n",
      "case:  92%|██████████████████████████████████████████▏   | 22/24 [05:39<00:25, 12.97s/it]\u001b[A\n",
      "case:  96%|████████████████████████████████████████████  | 23/24 [05:49<00:11, 11.89s/it]\u001b[A\n",
      "case: 100%|██████████████████████████████████████████████| 24/24 [06:05<00:00, 15.23s/it]\u001b[A\n",
      "train_val_test: 100%|█████████████████████████████████████| 3/3 [23:01<00:00, 460.38s/it]\n"
     ]
    }
   ],
   "source": [
    "for k in tqdm.tqdm(list(patch_dict.keys()),desc=\"train_val_test\"):\n",
    "    if not os.path.exists(f'cnn_model_input/{k}_data.pkl'):\n",
    "        X,y=[],[]\n",
    "        for name, dff in tqdm.tqdm(patch_dict[k].groupby('ID'),total=patch_dict[k]['ID'].nunique(),desc=\"case\"):\n",
    "            arr=np.load(f\"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/scc_tumor_data/prelim_patch_info/{name}.npy\")\n",
    "            X.append(arr[dff['patch_index'].values])\n",
    "            y.append(dff['scc'].values.flatten().astype(int))\n",
    "            del arr\n",
    "        X=np.concatenate(X,0)\n",
    "        y=np.hstack(y)\n",
    "        with open(f'/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/scc_tumor_data/prelim_patch_info/{k}_data.pkl','wb') as f:\n",
    "            pickle.dump(dict(X=X,y=y,patch_info=patch_dict[k]),f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec41b27c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-12 13:21:52.333085: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "lrr=ReduceLROnPlateau (monitor='val_acc', factor=.01, patience=3, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bea194c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': <pathpretrain.datasets.PickleDataset object at 0x2ab9dd0c4af0>, 'val': <pathpretrain.datasets.PickleDataset object at 0x2ab9dd0c4670>}\n",
      "ResNet(\n",
      "  (features): Sequential(\n",
      "    (init_block): ResInitBlock(\n",
      "      (conv): ConvBlock(\n",
      "        (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (stage1): Sequential(\n",
      "      (unit1): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (identity_conv): ConvBlock(\n",
      "          (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit2): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit3): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (stage2): Sequential(\n",
      "      (unit1): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (identity_conv): ConvBlock(\n",
      "          (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit2): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit3): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit4): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (stage3): Sequential(\n",
      "      (unit1): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (identity_conv): ConvBlock(\n",
      "          (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit2): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit3): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit4): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit5): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit6): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (stage4): Sequential(\n",
      "      (unit1): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (identity_conv): ConvBlock(\n",
      "          (conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit2): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit3): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (final_pool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  )\n",
      "  (output): Linear(in_features=2048, out_features=2, bias=True)\n",
      ")\n",
      "Weights: [0.61241206 2.72396064]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dartfs-hpc/rc/home/3/f006n33/anaconda3/envs/hiss/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 0 1] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0[0/625] Time:0.708, Train Loss:0.7055618166923523\n",
      "Epoch 0[1/625] Time:0.704, Train Loss:3.463738441467285\n",
      "Epoch 0[2/625] Time:0.703, Train Loss:1.5663138628005981\n",
      "Epoch 0[3/625] Time:0.704, Train Loss:0.7297398447990417\n",
      "Epoch 0[4/625] Time:0.703, Train Loss:0.9673916697502136\n",
      "Epoch 0[5/625] Time:0.703, Train Loss:0.7107248902320862\n",
      "Epoch 0[6/625] Time:0.703, Train Loss:0.6107208728790283\n",
      "Epoch 0[7/625] Time:0.741, Train Loss:0.774010419845581\n",
      "Epoch 0[8/625] Time:0.694, Train Loss:0.6371622681617737\n",
      "Epoch 0[9/625] Time:0.695, Train Loss:0.5472089052200317\n",
      "Epoch 0[10/625] Time:0.695, Train Loss:0.5309845209121704\n",
      "Epoch 0[11/625] Time:0.703, Train Loss:0.6355748772621155\n",
      "Epoch 0[12/625] Time:0.708, Train Loss:0.7202491760253906\n",
      "Epoch 0[13/625] Time:0.703, Train Loss:0.5752288103103638\n",
      "Epoch 0[14/625] Time:0.693, Train Loss:0.6069890260696411\n",
      "Epoch 0[15/625] Time:0.693, Train Loss:0.6988580822944641\n",
      "Epoch 0[16/625] Time:0.703, Train Loss:0.5875471234321594\n",
      "Epoch 0[17/625] Time:0.706, Train Loss:0.5827934741973877\n",
      "Epoch 0[18/625] Time:0.704, Train Loss:0.5726590752601624\n",
      "Epoch 0[19/625] Time:0.703, Train Loss:0.5171640515327454\n",
      "Epoch 0[20/625] Time:0.704, Train Loss:0.5587270855903625\n",
      "Epoch 0[21/625] Time:0.704, Train Loss:0.5426642894744873\n",
      "Epoch 0[22/625] Time:0.703, Train Loss:0.5350821614265442\n",
      "Epoch 0[23/625] Time:0.703, Train Loss:0.5746926665306091\n",
      "Epoch 0[24/625] Time:0.703, Train Loss:0.47957149147987366\n",
      "Epoch 0[25/625] Time:0.705, Train Loss:0.5796841979026794\n",
      "Epoch 0[26/625] Time:0.703, Train Loss:0.5628185868263245\n",
      "Epoch 0[27/625] Time:0.703, Train Loss:0.6005640625953674\n",
      "Epoch 0[28/625] Time:0.702, Train Loss:0.6236487627029419\n",
      "Epoch 0[29/625] Time:0.703, Train Loss:0.49987563490867615\n",
      "Epoch 0[30/625] Time:0.703, Train Loss:0.48386919498443604\n",
      "Epoch 0[31/625] Time:0.702, Train Loss:0.4731954038143158\n",
      "Epoch 0[32/625] Time:0.703, Train Loss:0.4918012022972107\n",
      "Epoch 0[33/625] Time:0.703, Train Loss:0.5511437058448792\n",
      "Epoch 0[34/625] Time:0.704, Train Loss:0.5211786031723022\n",
      "Epoch 0[35/625] Time:0.703, Train Loss:0.44229087233543396\n",
      "Epoch 0[36/625] Time:0.705, Train Loss:0.47610488533973694\n",
      "Epoch 0[37/625] Time:0.703, Train Loss:0.9788464307785034\n",
      "Epoch 0[38/625] Time:0.704, Train Loss:0.5035715103149414\n",
      "Epoch 0[39/625] Time:0.734, Train Loss:0.47337788343429565\n",
      "Epoch 0[40/625] Time:0.693, Train Loss:0.5192868709564209\n",
      "Epoch 0[41/625] Time:0.695, Train Loss:0.49264952540397644\n",
      "Epoch 0[42/625] Time:0.703, Train Loss:0.5019650459289551\n",
      "Epoch 0[43/625] Time:0.703, Train Loss:0.5177093744277954\n",
      "Epoch 0[44/625] Time:0.694, Train Loss:0.507499635219574\n",
      "Epoch 0[45/625] Time:0.708, Train Loss:0.5202206969261169\n",
      "Epoch 0[46/625] Time:0.702, Train Loss:0.4337869882583618\n",
      "Epoch 0[47/625] Time:0.695, Train Loss:0.4498349130153656\n",
      "Epoch 0[48/625] Time:0.694, Train Loss:0.41022488474845886\n",
      "Epoch 0[49/625] Time:0.696, Train Loss:0.4833935797214508\n",
      "Epoch 0[50/625] Time:0.695, Train Loss:0.49609464406967163\n",
      "Epoch 0[51/625] Time:0.694, Train Loss:0.49776387214660645\n",
      "Epoch 0[52/625] Time:0.702, Train Loss:0.6461915373802185\n",
      "Epoch 0[53/625] Time:0.715, Train Loss:0.5622808933258057\n",
      "Epoch 0[54/625] Time:0.694, Train Loss:0.4706735908985138\n",
      "Epoch 0[55/625] Time:0.694, Train Loss:0.5176647305488586\n",
      "Epoch 0[56/625] Time:0.694, Train Loss:0.5675526857376099\n",
      "Epoch 0[57/625] Time:0.706, Train Loss:0.5219477415084839\n",
      "Epoch 0[58/625] Time:0.693, Train Loss:0.6096348166465759\n",
      "Epoch 0[59/625] Time:0.695, Train Loss:0.41223347187042236\n",
      "Epoch 0[60/625] Time:0.696, Train Loss:0.48346149921417236\n",
      "Epoch 0[61/625] Time:0.694, Train Loss:0.39922580122947693\n",
      "Epoch 0[62/625] Time:0.694, Train Loss:0.48456504940986633\n",
      "Epoch 0[63/625] Time:0.693, Train Loss:0.40569186210632324\n",
      "Epoch 0[64/625] Time:0.692, Train Loss:0.5991321206092834\n",
      "Epoch 0[65/625] Time:0.694, Train Loss:0.6297317743301392\n",
      "Epoch 0[66/625] Time:0.693, Train Loss:0.4681927263736725\n",
      "Epoch 0[67/625] Time:0.693, Train Loss:0.42461901903152466\n",
      "Epoch 0[68/625] Time:0.699, Train Loss:0.49421796202659607\n",
      "Epoch 0[69/625] Time:0.693, Train Loss:0.47150370478630066\n",
      "Epoch 0[70/625] Time:0.694, Train Loss:0.4598899185657501\n",
      "Epoch 0[71/625] Time:0.695, Train Loss:0.4627884328365326\n",
      "Epoch 0[72/625] Time:0.692, Train Loss:0.557762086391449\n",
      "Epoch 0[73/625] Time:0.694, Train Loss:0.45313721895217896\n",
      "Epoch 0[74/625] Time:0.699, Train Loss:0.45711788535118103\n",
      "Epoch 0[75/625] Time:0.704, Train Loss:0.399617463350296\n",
      "Epoch 0[76/625] Time:0.719, Train Loss:0.5383690595626831\n",
      "Epoch 0[77/625] Time:0.731, Train Loss:0.5587480068206787\n",
      "Epoch 0[78/625] Time:0.702, Train Loss:0.4810348451137543\n",
      "Epoch 0[79/625] Time:0.705, Train Loss:0.4306353032588959\n",
      "Epoch 0[80/625] Time:0.711, Train Loss:0.4273745119571686\n",
      "Epoch 0[81/625] Time:0.691, Train Loss:0.4658442735671997\n",
      "Epoch 0[82/625] Time:0.73, Train Loss:0.5032869577407837\n",
      "Epoch 0[83/625] Time:0.703, Train Loss:0.4031222462654114\n",
      "Epoch 0[84/625] Time:0.705, Train Loss:0.5366837978363037\n",
      "Epoch 0[85/625] Time:0.694, Train Loss:0.49525073170661926\n",
      "Epoch 0[86/625] Time:0.693, Train Loss:0.5262191891670227\n",
      "Epoch 0[87/625] Time:0.694, Train Loss:0.40767940878868103\n",
      "Epoch 0[88/625] Time:0.694, Train Loss:0.5639501214027405\n",
      "Epoch 0[89/625] Time:0.694, Train Loss:0.4707712233066559\n",
      "Epoch 0[90/625] Time:0.693, Train Loss:0.4528678059577942\n",
      "Epoch 0[91/625] Time:0.694, Train Loss:0.47379204630851746\n",
      "Epoch 0[92/625] Time:0.693, Train Loss:0.5717806816101074\n",
      "Epoch 0[93/625] Time:0.693, Train Loss:0.4112965166568756\n",
      "Epoch 0[94/625] Time:0.693, Train Loss:0.4630711078643799\n",
      "Epoch 0[95/625] Time:0.694, Train Loss:0.4420538544654846\n",
      "Epoch 0[96/625] Time:0.694, Train Loss:0.42229950428009033\n",
      "Epoch 0[97/625] Time:0.693, Train Loss:0.5820247530937195\n",
      "Epoch 0[98/625] Time:0.694, Train Loss:0.4121963381767273\n",
      "Epoch 0[99/625] Time:0.694, Train Loss:0.4667227268218994\n",
      "Epoch 0[100/625] Time:0.697, Train Loss:0.4691186249256134\n",
      "Epoch 0[101/625] Time:0.694, Train Loss:0.5290546417236328\n",
      "Epoch 0[102/625] Time:0.693, Train Loss:0.42624661326408386\n",
      "Epoch 0[103/625] Time:0.694, Train Loss:0.41062867641448975\n",
      "Epoch 0[104/625] Time:0.702, Train Loss:0.45861688256263733\n",
      "Epoch 0[105/625] Time:0.693, Train Loss:0.45152992010116577\n",
      "Epoch 0[106/625] Time:0.695, Train Loss:0.5235603451728821\n",
      "Epoch 0[107/625] Time:0.702, Train Loss:0.49967503547668457\n",
      "Epoch 0[108/625] Time:0.702, Train Loss:0.5782933235168457\n",
      "Epoch 0[109/625] Time:0.704, Train Loss:0.4743500351905823\n",
      "Epoch 0[110/625] Time:0.703, Train Loss:0.6935427784919739\n",
      "Epoch 0[111/625] Time:0.704, Train Loss:0.47021764516830444\n",
      "Epoch 0[112/625] Time:0.704, Train Loss:0.47945061326026917\n",
      "Epoch 0[113/625] Time:0.703, Train Loss:0.5250083804130554\n",
      "Epoch 0[114/625] Time:0.724, Train Loss:0.49709242582321167\n",
      "Epoch 0[115/625] Time:0.702, Train Loss:0.5234233140945435\n",
      "Epoch 0[116/625] Time:0.694, Train Loss:0.5061757564544678\n",
      "Epoch 0[117/625] Time:0.693, Train Loss:0.4319325089454651\n",
      "Epoch 0[118/625] Time:0.695, Train Loss:0.5262278318405151\n",
      "Epoch 0[119/625] Time:0.694, Train Loss:0.4513756334781647\n",
      "Epoch 0[120/625] Time:0.694, Train Loss:0.41595566272735596\n",
      "Epoch 0[121/625] Time:0.694, Train Loss:0.4053666889667511\n",
      "Epoch 0[122/625] Time:0.694, Train Loss:0.5609992146492004\n",
      "Epoch 0[123/625] Time:0.703, Train Loss:0.9199697971343994\n",
      "Epoch 0[124/625] Time:0.704, Train Loss:0.4761279821395874\n",
      "Epoch 0[125/625] Time:0.705, Train Loss:0.5095281004905701\n",
      "Epoch 0[126/625] Time:0.704, Train Loss:0.4663161635398865\n",
      "Epoch 0[127/625] Time:0.703, Train Loss:0.4592415392398834\n",
      "Epoch 0[128/625] Time:0.702, Train Loss:0.6172788143157959\n",
      "Epoch 0[129/625] Time:0.703, Train Loss:0.4420308768749237\n",
      "Epoch 0[130/625] Time:0.704, Train Loss:0.5259611010551453\n",
      "Epoch 0[131/625] Time:0.694, Train Loss:0.5369736552238464\n",
      "Epoch 0[132/625] Time:0.693, Train Loss:0.4622550904750824\n",
      "Epoch 0[133/625] Time:0.693, Train Loss:0.474740594625473\n",
      "Epoch 0[134/625] Time:0.691, Train Loss:0.5449991226196289\n",
      "Epoch 0[135/625] Time:0.702, Train Loss:0.5083562731742859\n",
      "Epoch 0[136/625] Time:0.693, Train Loss:0.5128606557846069\n",
      "Epoch 0[137/625] Time:0.693, Train Loss:0.4304109513759613\n",
      "Epoch 0[138/625] Time:0.728, Train Loss:0.4677422046661377\n",
      "Epoch 0[139/625] Time:0.692, Train Loss:0.4251370429992676\n",
      "Epoch 0[140/625] Time:0.692, Train Loss:0.4119516909122467\n",
      "Epoch 0[141/625] Time:0.694, Train Loss:0.5436244010925293\n",
      "Epoch 0[142/625] Time:0.693, Train Loss:0.5191841125488281\n",
      "Epoch 0[143/625] Time:0.697, Train Loss:0.4451677203178406\n",
      "Epoch 0[144/625] Time:0.694, Train Loss:0.5218636393547058\n",
      "Epoch 0[145/625] Time:0.693, Train Loss:0.7532809972763062\n",
      "Epoch 0[146/625] Time:0.693, Train Loss:0.4653477370738983\n",
      "Epoch 0[147/625] Time:0.694, Train Loss:0.4826669991016388\n",
      "Epoch 0[148/625] Time:0.695, Train Loss:0.3752695322036743\n",
      "Epoch 0[149/625] Time:0.703, Train Loss:0.3946295380592346\n",
      "Epoch 0[150/625] Time:0.718, Train Loss:0.5698033571243286\n",
      "Epoch 0[151/625] Time:0.707, Train Loss:0.41569438576698303\n",
      "Epoch 0[152/625] Time:0.694, Train Loss:0.4316107928752899\n",
      "Epoch 0[153/625] Time:0.695, Train Loss:0.5076157450675964\n",
      "Epoch 0[154/625] Time:0.721, Train Loss:0.437582403421402\n",
      "Epoch 0[155/625] Time:0.701, Train Loss:0.42056843638420105\n",
      "Epoch 0[156/625] Time:0.692, Train Loss:0.44773244857788086\n",
      "Epoch 0[157/625] Time:0.694, Train Loss:0.4592055380344391\n",
      "Epoch 0[158/625] Time:0.696, Train Loss:0.3638264834880829\n",
      "Epoch 0[159/625] Time:0.694, Train Loss:0.4838378131389618\n",
      "Epoch 0[160/625] Time:0.695, Train Loss:0.4341098666191101\n",
      "Epoch 0[161/625] Time:0.696, Train Loss:0.42940554022789\n",
      "Epoch 0[162/625] Time:0.694, Train Loss:0.4343791604042053\n",
      "Epoch 0[163/625] Time:0.698, Train Loss:0.43659791350364685\n",
      "Epoch 0[164/625] Time:0.696, Train Loss:0.43136337399482727\n",
      "Epoch 0[165/625] Time:0.694, Train Loss:0.4277481138706207\n",
      "Epoch 0[166/625] Time:0.694, Train Loss:0.4219502806663513\n",
      "Epoch 0[167/625] Time:0.693, Train Loss:0.40700188279151917\n",
      "Epoch 0[168/625] Time:0.694, Train Loss:0.38806644082069397\n",
      "Epoch 0[169/625] Time:0.693, Train Loss:0.5101509690284729\n",
      "Epoch 0[170/625] Time:0.695, Train Loss:0.41791409254074097\n",
      "Epoch 0[171/625] Time:0.693, Train Loss:0.4481344521045685\n",
      "Epoch 0[172/625] Time:0.694, Train Loss:0.45832690596580505\n",
      "Epoch 0[173/625] Time:0.693, Train Loss:0.5239315629005432\n",
      "Epoch 0[174/625] Time:0.694, Train Loss:0.4234256148338318\n",
      "Epoch 0[175/625] Time:0.693, Train Loss:0.4780240058898926\n",
      "Epoch 0[176/625] Time:0.694, Train Loss:0.4069300591945648\n",
      "Epoch 0[177/625] Time:0.693, Train Loss:0.4446810781955719\n",
      "Epoch 0[178/625] Time:0.693, Train Loss:0.3955402076244354\n",
      "Epoch 0[179/625] Time:0.693, Train Loss:0.4784943759441376\n",
      "Epoch 0[180/625] Time:0.691, Train Loss:0.46512579917907715\n",
      "Epoch 0[181/625] Time:0.693, Train Loss:0.44358888268470764\n",
      "Epoch 0[182/625] Time:0.694, Train Loss:0.37607961893081665\n",
      "Epoch 0[183/625] Time:0.691, Train Loss:0.42709964513778687\n",
      "Epoch 0[184/625] Time:0.694, Train Loss:0.44492587447166443\n",
      "Epoch 0[185/625] Time:0.694, Train Loss:0.4247327744960785\n",
      "Epoch 0[186/625] Time:0.695, Train Loss:0.4367009997367859\n",
      "Epoch 0[187/625] Time:0.693, Train Loss:0.39827683568000793\n",
      "Epoch 0[188/625] Time:0.728, Train Loss:0.4858703911304474\n",
      "Epoch 0[189/625] Time:0.693, Train Loss:0.42307189106941223\n",
      "Epoch 0[190/625] Time:0.696, Train Loss:0.5253922343254089\n",
      "Epoch 0[191/625] Time:0.694, Train Loss:0.5061039924621582\n",
      "Epoch 0[192/625] Time:0.693, Train Loss:0.31712421774864197\n",
      "Epoch 0[193/625] Time:0.693, Train Loss:0.3567901849746704\n",
      "Epoch 0[194/625] Time:0.693, Train Loss:0.4815334677696228\n",
      "Epoch 0[195/625] Time:0.697, Train Loss:0.38926631212234497\n",
      "Epoch 0[196/625] Time:0.694, Train Loss:0.38373076915740967\n",
      "Epoch 0[197/625] Time:0.694, Train Loss:0.4314112961292267\n",
      "Epoch 0[198/625] Time:0.695, Train Loss:0.4438764452934265\n",
      "Epoch 0[199/625] Time:0.694, Train Loss:0.35688549280166626\n",
      "Epoch 0[200/625] Time:0.693, Train Loss:0.39218762516975403\n",
      "Epoch 0[201/625] Time:0.694, Train Loss:0.37209993600845337\n",
      "Epoch 0[202/625] Time:0.695, Train Loss:0.3765448331832886\n",
      "Epoch 0[203/625] Time:0.695, Train Loss:0.4847812056541443\n",
      "Epoch 0[204/625] Time:0.73, Train Loss:0.39946457743644714\n",
      "Epoch 0[205/625] Time:0.731, Train Loss:0.38886383175849915\n",
      "Epoch 0[206/625] Time:0.71, Train Loss:0.43518951535224915\n",
      "Epoch 0[207/625] Time:0.692, Train Loss:0.5381887555122375\n",
      "Epoch 0[208/625] Time:0.694, Train Loss:0.4048071503639221\n",
      "Epoch 0[209/625] Time:0.694, Train Loss:0.4593735933303833\n",
      "Epoch 0[210/625] Time:0.693, Train Loss:0.5100800395011902\n",
      "Epoch 0[211/625] Time:0.694, Train Loss:0.388987272977829\n",
      "Epoch 0[212/625] Time:0.694, Train Loss:0.3662491738796234\n",
      "Epoch 0[213/625] Time:0.694, Train Loss:0.5052964091300964\n",
      "Epoch 0[214/625] Time:0.692, Train Loss:0.4971366226673126\n",
      "Epoch 0[215/625] Time:0.703, Train Loss:0.45886948704719543\n",
      "Epoch 0[216/625] Time:0.74, Train Loss:0.3964041471481323\n",
      "Epoch 0[217/625] Time:0.688, Train Loss:0.3925341069698334\n",
      "Epoch 0[218/625] Time:0.693, Train Loss:0.4090711772441864\n",
      "Epoch 0[219/625] Time:0.692, Train Loss:0.3924542963504791\n",
      "Epoch 0[220/625] Time:0.694, Train Loss:0.3151567876338959\n",
      "Epoch 0[221/625] Time:0.692, Train Loss:0.39910265803337097\n",
      "Epoch 0[222/625] Time:0.692, Train Loss:0.5059419870376587\n",
      "Epoch 0[223/625] Time:0.691, Train Loss:0.41563791036605835\n",
      "Epoch 0[224/625] Time:0.692, Train Loss:0.38962605595588684\n",
      "Epoch 0[225/625] Time:0.692, Train Loss:0.34053492546081543\n",
      "Epoch 0[226/625] Time:0.69, Train Loss:0.4030194580554962\n",
      "Epoch 0[227/625] Time:0.693, Train Loss:0.3925491273403168\n",
      "Epoch 0[228/625] Time:0.706, Train Loss:0.35491305589675903\n",
      "Epoch 0[229/625] Time:0.702, Train Loss:0.4458605647087097\n",
      "Epoch 0[230/625] Time:0.693, Train Loss:0.4550710916519165\n",
      "Epoch 0[231/625] Time:0.693, Train Loss:0.381416380405426\n",
      "Epoch 0[232/625] Time:0.693, Train Loss:0.3736894726753235\n",
      "Epoch 0[233/625] Time:0.693, Train Loss:0.4225914180278778\n",
      "Epoch 0[234/625] Time:0.693, Train Loss:0.4845971465110779\n",
      "Epoch 0[235/625] Time:0.693, Train Loss:0.43359875679016113\n",
      "Epoch 0[236/625] Time:0.693, Train Loss:0.353283554315567\n",
      "Epoch 0[237/625] Time:0.695, Train Loss:0.43759530782699585\n",
      "Epoch 0[238/625] Time:0.704, Train Loss:0.4814293682575226\n",
      "Epoch 0[239/625] Time:0.704, Train Loss:0.43660688400268555\n",
      "Epoch 0[240/625] Time:0.71, Train Loss:0.41224467754364014\n",
      "Epoch 0[241/625] Time:0.708, Train Loss:0.4035487771034241\n",
      "Epoch 0[242/625] Time:0.703, Train Loss:0.3967548906803131\n",
      "Epoch 0[243/625] Time:0.692, Train Loss:0.5025188326835632\n",
      "Epoch 0[244/625] Time:0.735, Train Loss:0.4483374059200287\n",
      "Epoch 0[245/625] Time:0.693, Train Loss:0.40837621688842773\n",
      "Epoch 0[246/625] Time:0.722, Train Loss:0.38856926560401917\n",
      "Epoch 0[247/625] Time:0.704, Train Loss:0.3616110682487488\n",
      "Epoch 0[248/625] Time:0.703, Train Loss:0.43854498863220215\n",
      "Epoch 0[249/625] Time:0.704, Train Loss:0.37250301241874695\n",
      "Epoch 0[250/625] Time:0.694, Train Loss:0.4391658902168274\n",
      "Epoch 0[251/625] Time:0.693, Train Loss:0.4640427231788635\n",
      "Epoch 0[252/625] Time:0.696, Train Loss:0.414162814617157\n",
      "Epoch 0[253/625] Time:0.701, Train Loss:0.4541622996330261\n",
      "Epoch 0[254/625] Time:0.693, Train Loss:0.4207758903503418\n",
      "Epoch 0[255/625] Time:0.694, Train Loss:0.30551329255104065\n",
      "Epoch 0[256/625] Time:0.695, Train Loss:0.4344331622123718\n",
      "Epoch 0[257/625] Time:0.697, Train Loss:0.40761274099349976\n",
      "Epoch 0[258/625] Time:0.695, Train Loss:0.39255276322364807\n",
      "Epoch 0[259/625] Time:0.694, Train Loss:0.4515565037727356\n",
      "Epoch 0[260/625] Time:0.695, Train Loss:0.6744383573532104\n",
      "Epoch 0[261/625] Time:0.695, Train Loss:0.47654321789741516\n",
      "Epoch 0[262/625] Time:0.718, Train Loss:0.5009238719940186\n",
      "Epoch 0[263/625] Time:0.702, Train Loss:0.4733888804912567\n",
      "Epoch 0[264/625] Time:0.694, Train Loss:0.39553800225257874\n",
      "Epoch 0[265/625] Time:0.694, Train Loss:0.3784679174423218\n",
      "Epoch 0[266/625] Time:0.695, Train Loss:0.4049171507358551\n",
      "Epoch 0[267/625] Time:0.697, Train Loss:0.43533268570899963\n",
      "Epoch 0[268/625] Time:0.694, Train Loss:0.3844880163669586\n",
      "Epoch 0[269/625] Time:0.696, Train Loss:0.39778512716293335\n",
      "Epoch 0[270/625] Time:0.698, Train Loss:0.4990478456020355\n",
      "Epoch 0[271/625] Time:0.696, Train Loss:0.5619184970855713\n",
      "Epoch 0[272/625] Time:0.72, Train Loss:0.41477373242378235\n",
      "Epoch 0[273/625] Time:0.693, Train Loss:0.46764513850212097\n",
      "Epoch 0[274/625] Time:0.695, Train Loss:0.4323025643825531\n",
      "Epoch 0[275/625] Time:0.697, Train Loss:0.43148717284202576\n",
      "Epoch 0[276/625] Time:0.694, Train Loss:0.4888319969177246\n",
      "Epoch 0[277/625] Time:0.695, Train Loss:0.3665277063846588\n",
      "Epoch 0[278/625] Time:0.694, Train Loss:0.4866430163383484\n",
      "Epoch 0[279/625] Time:0.694, Train Loss:0.4292027950286865\n",
      "Epoch 0[280/625] Time:0.695, Train Loss:0.38850241899490356\n",
      "Epoch 0[281/625] Time:0.694, Train Loss:0.4694853127002716\n",
      "Epoch 0[282/625] Time:0.694, Train Loss:0.4219719171524048\n",
      "Epoch 0[283/625] Time:0.734, Train Loss:0.39860963821411133\n",
      "Epoch 0[284/625] Time:0.69, Train Loss:0.46051499247550964\n",
      "Epoch 0[285/625] Time:0.693, Train Loss:0.39296215772628784\n",
      "Epoch 0[286/625] Time:0.704, Train Loss:0.43405213952064514\n",
      "Epoch 0[287/625] Time:0.719, Train Loss:0.38324210047721863\n",
      "Epoch 0[288/625] Time:0.693, Train Loss:0.4905380308628082\n",
      "Epoch 0[289/625] Time:0.693, Train Loss:0.44274330139160156\n",
      "Epoch 0[290/625] Time:0.705, Train Loss:0.3563196659088135\n",
      "Epoch 0[291/625] Time:0.702, Train Loss:0.4649895131587982\n",
      "Epoch 0[292/625] Time:0.706, Train Loss:0.34479185938835144\n",
      "Epoch 0[293/625] Time:0.704, Train Loss:0.5090579986572266\n",
      "Epoch 0[294/625] Time:0.703, Train Loss:0.43530118465423584\n",
      "Epoch 0[295/625] Time:0.704, Train Loss:0.36169150471687317\n",
      "Epoch 0[296/625] Time:0.701, Train Loss:0.3798310458660126\n",
      "Epoch 0[297/625] Time:0.703, Train Loss:0.3888065218925476\n",
      "Epoch 0[298/625] Time:0.703, Train Loss:0.3947713375091553\n",
      "Epoch 0[299/625] Time:0.703, Train Loss:0.3644656836986542\n",
      "Epoch 0[300/625] Time:0.703, Train Loss:0.33628514409065247\n",
      "Epoch 0[301/625] Time:0.702, Train Loss:0.34795910120010376\n",
      "Epoch 0[302/625] Time:0.703, Train Loss:0.41035163402557373\n",
      "Epoch 0[303/625] Time:0.704, Train Loss:0.3816024363040924\n",
      "Epoch 0[304/625] Time:0.704, Train Loss:0.4802344739437103\n",
      "Epoch 0[305/625] Time:0.702, Train Loss:0.3872377872467041\n",
      "Epoch 0[306/625] Time:0.704, Train Loss:0.40646475553512573\n",
      "Epoch 0[307/625] Time:0.702, Train Loss:0.4444091022014618\n",
      "Epoch 0[308/625] Time:0.705, Train Loss:0.4447520971298218\n",
      "Epoch 0[309/625] Time:0.693, Train Loss:0.41075870394706726\n",
      "Epoch 0[310/625] Time:0.695, Train Loss:0.42587199807167053\n",
      "Epoch 0[311/625] Time:0.694, Train Loss:0.39554473757743835\n",
      "Epoch 0[312/625] Time:0.693, Train Loss:0.4468664526939392\n",
      "Epoch 0[313/625] Time:0.693, Train Loss:0.37935635447502136\n",
      "Epoch 0[314/625] Time:0.696, Train Loss:0.36006006598472595\n",
      "Epoch 0[315/625] Time:0.693, Train Loss:0.31445351243019104\n",
      "Epoch 0[316/625] Time:0.694, Train Loss:0.38027119636535645\n",
      "Epoch 0[317/625] Time:0.693, Train Loss:0.4626564085483551\n",
      "Epoch 0[318/625] Time:0.692, Train Loss:0.3562341034412384\n",
      "Epoch 0[319/625] Time:0.708, Train Loss:0.4278239607810974\n",
      "Epoch 0[320/625] Time:0.703, Train Loss:0.4555925130844116\n",
      "Epoch 0[321/625] Time:0.706, Train Loss:0.5116023421287537\n",
      "Epoch 0[322/625] Time:0.693, Train Loss:0.3276885151863098\n",
      "Epoch 0[323/625] Time:0.693, Train Loss:0.41294336318969727\n",
      "Epoch 0[324/625] Time:0.694, Train Loss:0.3291676938533783\n",
      "Epoch 0[325/625] Time:0.693, Train Loss:0.4571406841278076\n",
      "Epoch 0[326/625] Time:0.693, Train Loss:0.3969394266605377\n",
      "Epoch 0[327/625] Time:0.694, Train Loss:0.397345632314682\n",
      "Epoch 0[328/625] Time:0.694, Train Loss:0.3768635392189026\n",
      "Epoch 0[329/625] Time:0.693, Train Loss:0.3955451548099518\n",
      "Epoch 0[330/625] Time:0.696, Train Loss:0.45432934165000916\n",
      "Epoch 0[331/625] Time:0.695, Train Loss:0.35181325674057007\n",
      "Epoch 0[332/625] Time:0.696, Train Loss:0.3976861536502838\n",
      "Epoch 0[333/625] Time:0.695, Train Loss:0.45085784792900085\n",
      "Epoch 0[334/625] Time:0.696, Train Loss:0.46194595098495483\n",
      "Epoch 0[335/625] Time:0.695, Train Loss:0.3288305997848511\n",
      "Epoch 0[336/625] Time:0.694, Train Loss:0.36307021975517273\n",
      "Epoch 0[337/625] Time:0.693, Train Loss:0.49087512493133545\n",
      "Epoch 0[338/625] Time:0.699, Train Loss:0.37520214915275574\n",
      "Epoch 0[339/625] Time:0.693, Train Loss:0.4545270502567291\n",
      "Epoch 0[340/625] Time:0.691, Train Loss:0.3526167869567871\n",
      "Epoch 0[341/625] Time:0.695, Train Loss:0.36210089921951294\n",
      "Epoch 0[342/625] Time:0.704, Train Loss:0.40725579857826233\n",
      "Epoch 0[343/625] Time:0.702, Train Loss:0.42414993047714233\n",
      "Epoch 0[344/625] Time:0.705, Train Loss:0.3545857071876526\n",
      "Epoch 0[345/625] Time:0.703, Train Loss:0.8743595480918884\n",
      "Epoch 0[346/625] Time:0.71, Train Loss:0.38257452845573425\n",
      "Epoch 0[347/625] Time:0.693, Train Loss:0.4144260585308075\n",
      "Epoch 0[348/625] Time:0.694, Train Loss:0.3599940538406372\n",
      "Epoch 0[349/625] Time:0.695, Train Loss:0.41058018803596497\n",
      "Epoch 0[350/625] Time:0.695, Train Loss:0.4465728998184204\n",
      "Epoch 0[351/625] Time:0.694, Train Loss:0.38900771737098694\n",
      "Epoch 0[352/625] Time:0.694, Train Loss:0.3560744822025299\n",
      "Epoch 0[353/625] Time:0.696, Train Loss:0.4161812663078308\n",
      "Epoch 0[354/625] Time:0.695, Train Loss:0.39184945821762085\n",
      "Epoch 0[355/625] Time:0.694, Train Loss:0.33066120743751526\n",
      "Epoch 0[356/625] Time:0.693, Train Loss:0.41502708196640015\n",
      "Epoch 0[357/625] Time:0.741, Train Loss:0.3566788136959076\n",
      "Epoch 0[358/625] Time:0.693, Train Loss:0.4163999855518341\n",
      "Epoch 0[359/625] Time:0.694, Train Loss:0.4409032166004181\n",
      "Epoch 0[360/625] Time:0.694, Train Loss:0.3849666714668274\n",
      "Epoch 0[361/625] Time:0.695, Train Loss:0.32843631505966187\n",
      "Epoch 0[362/625] Time:0.719, Train Loss:0.4012727439403534\n",
      "Epoch 0[363/625] Time:0.693, Train Loss:0.3934488296508789\n",
      "Epoch 0[364/625] Time:0.732, Train Loss:0.40260112285614014\n",
      "Epoch 0[365/625] Time:0.694, Train Loss:0.34663259983062744\n",
      "Epoch 0[366/625] Time:0.694, Train Loss:0.41338783502578735\n",
      "Epoch 0[367/625] Time:0.695, Train Loss:0.41406530141830444\n",
      "Epoch 0[368/625] Time:0.695, Train Loss:0.3230918347835541\n",
      "Epoch 0[369/625] Time:0.702, Train Loss:0.3727700710296631\n",
      "Epoch 0[370/625] Time:0.695, Train Loss:0.30611810088157654\n",
      "Epoch 0[371/625] Time:0.694, Train Loss:0.347400426864624\n",
      "Epoch 0[372/625] Time:0.696, Train Loss:0.34318169951438904\n",
      "Epoch 0[373/625] Time:0.699, Train Loss:0.440202921628952\n",
      "Epoch 0[374/625] Time:0.721, Train Loss:0.4211852550506592\n",
      "Epoch 0[375/625] Time:0.694, Train Loss:0.3833453357219696\n",
      "Epoch 0[376/625] Time:0.694, Train Loss:0.38754525780677795\n",
      "Epoch 0[377/625] Time:0.7, Train Loss:0.3048724830150604\n",
      "Epoch 0[378/625] Time:0.693, Train Loss:0.6231581568717957\n",
      "Epoch 0[379/625] Time:0.694, Train Loss:0.41008588671684265\n",
      "Epoch 0[380/625] Time:0.693, Train Loss:0.4209713041782379\n",
      "Epoch 0[381/625] Time:0.694, Train Loss:0.4465002417564392\n",
      "Epoch 0[382/625] Time:0.695, Train Loss:0.4154776334762573\n",
      "Epoch 0[383/625] Time:0.694, Train Loss:0.44470569491386414\n",
      "Epoch 0[384/625] Time:0.694, Train Loss:0.5013456344604492\n",
      "Epoch 0[385/625] Time:0.694, Train Loss:0.7908115386962891\n",
      "Epoch 0[386/625] Time:0.693, Train Loss:0.43987029790878296\n",
      "Epoch 0[387/625] Time:0.693, Train Loss:0.46427375078201294\n",
      "Epoch 0[388/625] Time:0.693, Train Loss:0.43233516812324524\n",
      "Epoch 0[389/625] Time:0.693, Train Loss:0.3158448338508606\n",
      "Epoch 0[390/625] Time:0.694, Train Loss:0.38654932379722595\n",
      "Epoch 0[391/625] Time:0.694, Train Loss:0.3874713182449341\n",
      "Epoch 0[392/625] Time:0.695, Train Loss:0.46968790888786316\n",
      "Epoch 0[393/625] Time:0.696, Train Loss:0.3519516587257385\n",
      "Epoch 0[394/625] Time:0.693, Train Loss:0.4574042856693268\n",
      "Epoch 0[395/625] Time:0.692, Train Loss:0.40791794657707214\n",
      "Epoch 0[396/625] Time:0.694, Train Loss:0.3206358850002289\n",
      "Epoch 0[397/625] Time:0.693, Train Loss:0.5732319951057434\n",
      "Epoch 0[398/625] Time:0.693, Train Loss:0.39234113693237305\n",
      "Epoch 0[399/625] Time:0.693, Train Loss:0.31324392557144165\n",
      "Epoch 0[400/625] Time:0.693, Train Loss:0.4510688781738281\n",
      "Epoch 0[401/625] Time:0.693, Train Loss:0.3975736200809479\n",
      "Epoch 0[402/625] Time:0.695, Train Loss:0.6231874823570251\n",
      "Epoch 0[403/625] Time:0.7, Train Loss:0.4433320462703705\n",
      "Epoch 0[404/625] Time:0.692, Train Loss:0.377889484167099\n",
      "Epoch 0[405/625] Time:0.693, Train Loss:0.34889042377471924\n",
      "Epoch 0[406/625] Time:0.693, Train Loss:0.3563791513442993\n",
      "Epoch 0[407/625] Time:0.692, Train Loss:0.4275935888290405\n",
      "Epoch 0[408/625] Time:0.693, Train Loss:0.36871621012687683\n",
      "Epoch 0[409/625] Time:0.695, Train Loss:0.3903673589229584\n",
      "Epoch 0[410/625] Time:0.694, Train Loss:0.4503211975097656\n",
      "Epoch 0[411/625] Time:0.693, Train Loss:0.42801252007484436\n",
      "Epoch 0[412/625] Time:0.725, Train Loss:0.42696189880371094\n",
      "Epoch 0[413/625] Time:0.703, Train Loss:0.39445897936820984\n",
      "Epoch 0[414/625] Time:0.704, Train Loss:0.4700217545032501\n",
      "Epoch 0[415/625] Time:0.705, Train Loss:0.406532883644104\n",
      "Epoch 0[416/625] Time:0.705, Train Loss:0.40508055686950684\n",
      "Epoch 0[417/625] Time:0.704, Train Loss:0.4920913875102997\n",
      "Epoch 0[418/625] Time:0.703, Train Loss:0.3789285719394684\n",
      "Epoch 0[419/625] Time:0.704, Train Loss:0.4190594255924225\n",
      "Epoch 0[420/625] Time:0.695, Train Loss:0.525519073009491\n",
      "Epoch 0[421/625] Time:0.71, Train Loss:0.4877976179122925\n",
      "Epoch 0[422/625] Time:0.702, Train Loss:0.4069657623767853\n",
      "Epoch 0[423/625] Time:0.693, Train Loss:0.38860762119293213\n",
      "Epoch 0[424/625] Time:0.693, Train Loss:0.3944360911846161\n",
      "Epoch 0[425/625] Time:0.694, Train Loss:0.4264521300792694\n",
      "Epoch 0[426/625] Time:0.695, Train Loss:0.36588066816329956\n",
      "Epoch 0[427/625] Time:0.693, Train Loss:0.49192243814468384\n",
      "Epoch 0[428/625] Time:0.694, Train Loss:0.4572475254535675\n",
      "Epoch 0[429/625] Time:0.694, Train Loss:0.4734092652797699\n",
      "Epoch 0[430/625] Time:0.694, Train Loss:0.3677404224872589\n",
      "Epoch 0[431/625] Time:0.694, Train Loss:0.44832220673561096\n",
      "Epoch 0[432/625] Time:0.695, Train Loss:0.4476867616176605\n",
      "Epoch 0[433/625] Time:0.695, Train Loss:0.40238091349601746\n",
      "Epoch 0[434/625] Time:0.704, Train Loss:0.565230131149292\n",
      "Epoch 0[435/625] Time:0.706, Train Loss:0.4341968894004822\n",
      "Epoch 0[436/625] Time:0.707, Train Loss:0.4293253421783447\n",
      "Epoch 0[437/625] Time:0.693, Train Loss:0.4141363501548767\n",
      "Epoch 0[438/625] Time:0.694, Train Loss:0.3981808125972748\n",
      "Epoch 0[439/625] Time:0.694, Train Loss:0.40549591183662415\n",
      "Epoch 0[440/625] Time:0.693, Train Loss:0.3749350905418396\n",
      "Epoch 0[441/625] Time:0.693, Train Loss:0.34653422236442566\n",
      "Epoch 0[442/625] Time:0.693, Train Loss:0.4405900537967682\n",
      "Epoch 0[443/625] Time:0.693, Train Loss:0.423113614320755\n",
      "Epoch 0[444/625] Time:0.693, Train Loss:0.4234105050563812\n",
      "Epoch 0[445/625] Time:0.694, Train Loss:0.46560466289520264\n",
      "Epoch 0[446/625] Time:0.693, Train Loss:0.43611687421798706\n",
      "Epoch 0[447/625] Time:0.693, Train Loss:0.39016780257225037\n",
      "Epoch 0[448/625] Time:0.695, Train Loss:0.559019148349762\n",
      "Epoch 0[449/625] Time:0.694, Train Loss:0.4423905909061432\n",
      "Epoch 0[450/625] Time:0.693, Train Loss:0.34644466638565063\n",
      "Epoch 0[451/625] Time:0.693, Train Loss:0.4102831780910492\n",
      "Epoch 0[452/625] Time:0.693, Train Loss:0.4241756796836853\n",
      "Epoch 0[453/625] Time:0.694, Train Loss:0.44656065106391907\n",
      "Epoch 0[454/625] Time:0.693, Train Loss:0.41870349645614624\n",
      "Epoch 0[455/625] Time:0.695, Train Loss:0.39198049902915955\n",
      "Epoch 0[456/625] Time:0.695, Train Loss:0.37263157963752747\n",
      "Epoch 0[457/625] Time:0.693, Train Loss:0.36449071764945984\n",
      "Epoch 0[458/625] Time:0.695, Train Loss:0.5099721550941467\n",
      "Epoch 0[459/625] Time:0.693, Train Loss:0.39362984895706177\n",
      "Epoch 0[460/625] Time:0.694, Train Loss:0.41493454575538635\n",
      "Epoch 0[461/625] Time:0.694, Train Loss:0.33334413170814514\n",
      "Epoch 0[462/625] Time:0.694, Train Loss:0.41591551899909973\n",
      "Epoch 0[463/625] Time:0.693, Train Loss:0.36886295676231384\n",
      "Epoch 0[464/625] Time:0.696, Train Loss:0.4233446717262268\n",
      "Epoch 0[465/625] Time:0.694, Train Loss:0.4321737587451935\n",
      "Epoch 0[466/625] Time:0.698, Train Loss:0.36120426654815674\n",
      "Epoch 0[467/625] Time:0.711, Train Loss:0.3713005483150482\n",
      "Epoch 0[468/625] Time:0.714, Train Loss:0.45382240414619446\n",
      "Epoch 0[469/625] Time:0.693, Train Loss:0.3506971299648285\n",
      "Epoch 0[470/625] Time:0.693, Train Loss:0.3670376241207123\n",
      "Epoch 0[471/625] Time:0.716, Train Loss:0.5271896719932556\n",
      "Epoch 0[472/625] Time:0.703, Train Loss:0.3776476979255676\n",
      "Epoch 0[473/625] Time:0.712, Train Loss:0.36050719022750854\n",
      "Epoch 0[474/625] Time:0.694, Train Loss:0.4101540446281433\n",
      "Epoch 0[475/625] Time:0.695, Train Loss:0.3390614092350006\n",
      "Epoch 0[476/625] Time:0.694, Train Loss:0.47496503591537476\n",
      "Epoch 0[477/625] Time:0.695, Train Loss:0.40250730514526367\n",
      "Epoch 0[478/625] Time:0.728, Train Loss:0.3703291714191437\n",
      "Epoch 0[479/625] Time:0.694, Train Loss:0.32580846548080444\n",
      "Epoch 0[480/625] Time:0.744, Train Loss:0.39726710319519043\n",
      "Epoch 0[481/625] Time:0.693, Train Loss:0.48479706048965454\n",
      "Epoch 0[482/625] Time:0.695, Train Loss:0.44596296548843384\n",
      "Epoch 0[483/625] Time:0.694, Train Loss:0.4709787368774414\n",
      "Epoch 0[484/625] Time:0.694, Train Loss:0.4630615711212158\n",
      "Epoch 0[485/625] Time:0.695, Train Loss:0.3813670873641968\n",
      "Epoch 0[486/625] Time:0.694, Train Loss:0.636914849281311\n",
      "Epoch 0[487/625] Time:0.694, Train Loss:0.411040723323822\n",
      "Epoch 0[488/625] Time:0.694, Train Loss:0.44956114888191223\n",
      "Epoch 0[489/625] Time:0.694, Train Loss:0.45329177379608154\n",
      "Epoch 0[490/625] Time:0.694, Train Loss:0.4416089355945587\n",
      "Epoch 0[491/625] Time:0.694, Train Loss:0.4273870289325714\n",
      "Epoch 0[492/625] Time:0.719, Train Loss:0.46686461567878723\n",
      "Epoch 0[493/625] Time:0.703, Train Loss:0.4670920968055725\n",
      "Epoch 0[494/625] Time:0.703, Train Loss:0.43948087096214294\n",
      "Epoch 0[495/625] Time:0.704, Train Loss:0.4320606291294098\n",
      "Epoch 0[496/625] Time:0.705, Train Loss:0.39985841512680054\n",
      "Epoch 0[497/625] Time:0.694, Train Loss:0.39722374081611633\n",
      "Epoch 0[498/625] Time:0.697, Train Loss:0.37099146842956543\n",
      "Epoch 0[499/625] Time:0.693, Train Loss:0.4501971900463104\n",
      "Epoch 0[500/625] Time:0.695, Train Loss:0.47891107201576233\n",
      "Epoch 0[501/625] Time:0.694, Train Loss:0.36740750074386597\n",
      "Epoch 0[502/625] Time:0.704, Train Loss:0.3542177975177765\n",
      "Epoch 0[503/625] Time:0.699, Train Loss:0.4477841854095459\n",
      "Epoch 0[504/625] Time:0.692, Train Loss:0.37695175409317017\n",
      "Epoch 0[505/625] Time:0.704, Train Loss:0.3872493803501129\n",
      "Epoch 0[506/625] Time:0.706, Train Loss:0.4934968650341034\n",
      "Epoch 0[507/625] Time:0.702, Train Loss:0.4579083025455475\n",
      "Epoch 0[508/625] Time:0.694, Train Loss:0.4065324068069458\n",
      "Epoch 0[509/625] Time:0.696, Train Loss:0.45578646659851074\n",
      "Epoch 0[510/625] Time:0.694, Train Loss:0.4302060008049011\n",
      "Epoch 0[511/625] Time:0.694, Train Loss:0.45667386054992676\n",
      "Epoch 0[512/625] Time:0.694, Train Loss:0.4023268520832062\n",
      "Epoch 0[513/625] Time:0.694, Train Loss:0.4005366861820221\n",
      "Epoch 0[514/625] Time:0.694, Train Loss:0.40572595596313477\n",
      "Epoch 0[515/625] Time:0.694, Train Loss:0.43109023571014404\n",
      "Epoch 0[516/625] Time:0.696, Train Loss:0.46875280141830444\n",
      "Epoch 0[517/625] Time:0.703, Train Loss:0.30411896109580994\n",
      "Epoch 0[518/625] Time:0.694, Train Loss:0.4044964015483856\n",
      "Epoch 0[519/625] Time:0.694, Train Loss:0.39647412300109863\n",
      "Epoch 0[520/625] Time:0.695, Train Loss:0.37178272008895874\n",
      "Epoch 0[521/625] Time:0.692, Train Loss:0.4469972252845764\n",
      "Epoch 0[522/625] Time:0.693, Train Loss:0.4310877025127411\n",
      "Epoch 0[523/625] Time:0.691, Train Loss:0.4209364652633667\n",
      "Epoch 0[524/625] Time:0.693, Train Loss:0.42333880066871643\n",
      "Epoch 0[525/625] Time:0.695, Train Loss:0.47252359986305237\n",
      "Epoch 0[526/625] Time:0.691, Train Loss:0.5100307464599609\n",
      "Epoch 0[527/625] Time:0.695, Train Loss:0.4236755967140198\n",
      "Epoch 0[528/625] Time:0.72, Train Loss:0.4144541323184967\n",
      "Epoch 0[529/625] Time:0.692, Train Loss:0.43125683069229126\n",
      "Epoch 0[530/625] Time:0.694, Train Loss:0.3403252065181732\n",
      "Epoch 0[531/625] Time:0.702, Train Loss:0.39014139771461487\n",
      "Epoch 0[532/625] Time:0.729, Train Loss:0.42743709683418274\n",
      "Epoch 0[533/625] Time:0.692, Train Loss:0.3862502872943878\n",
      "Epoch 0[534/625] Time:0.698, Train Loss:0.3813571333885193\n",
      "Epoch 0[535/625] Time:0.714, Train Loss:0.42950907349586487\n",
      "Epoch 0[536/625] Time:0.703, Train Loss:0.4177283048629761\n",
      "Epoch 0[537/625] Time:0.706, Train Loss:0.3190423548221588\n",
      "Epoch 0[538/625] Time:0.693, Train Loss:0.3377779424190521\n",
      "Epoch 0[539/625] Time:0.694, Train Loss:0.34418416023254395\n",
      "Epoch 0[540/625] Time:0.693, Train Loss:0.4078870415687561\n",
      "Epoch 0[541/625] Time:0.694, Train Loss:0.41743987798690796\n",
      "Epoch 0[542/625] Time:0.697, Train Loss:0.29309093952178955\n",
      "Epoch 0[543/625] Time:0.705, Train Loss:0.3503493070602417\n",
      "Epoch 0[544/625] Time:0.705, Train Loss:0.38132598996162415\n",
      "Epoch 0[545/625] Time:0.693, Train Loss:0.3687111735343933\n",
      "Epoch 0[546/625] Time:0.694, Train Loss:0.48539161682128906\n",
      "Epoch 0[547/625] Time:0.694, Train Loss:0.3917847275733948\n",
      "Epoch 0[548/625] Time:0.694, Train Loss:0.38047903776168823\n",
      "Epoch 0[549/625] Time:0.694, Train Loss:0.38809987902641296\n",
      "Epoch 0[550/625] Time:0.693, Train Loss:0.4257659614086151\n",
      "Epoch 0[551/625] Time:0.701, Train Loss:0.25864288210868835\n",
      "Epoch 0[552/625] Time:0.704, Train Loss:0.4521958827972412\n",
      "Epoch 0[553/625] Time:0.702, Train Loss:0.4449216425418854\n",
      "Epoch 0[554/625] Time:0.703, Train Loss:0.4399661421775818\n",
      "Epoch 0[555/625] Time:0.703, Train Loss:0.5589234828948975\n",
      "Epoch 0[556/625] Time:0.704, Train Loss:0.3582288324832916\n",
      "Epoch 0[557/625] Time:0.703, Train Loss:0.4106764793395996\n",
      "Epoch 0[558/625] Time:0.705, Train Loss:0.41781941056251526\n",
      "Epoch 0[559/625] Time:0.708, Train Loss:0.3840455412864685\n",
      "Epoch 0[560/625] Time:0.704, Train Loss:0.4103234112262726\n",
      "Epoch 0[561/625] Time:0.711, Train Loss:0.37318429350852966\n",
      "Epoch 0[562/625] Time:0.694, Train Loss:0.6003503799438477\n",
      "Epoch 0[563/625] Time:0.693, Train Loss:0.4639626145362854\n",
      "Epoch 0[564/625] Time:0.71, Train Loss:0.355628103017807\n",
      "Epoch 0[565/625] Time:0.703, Train Loss:0.4047591984272003\n",
      "Epoch 0[566/625] Time:0.704, Train Loss:0.4170863628387451\n",
      "Epoch 0[567/625] Time:0.702, Train Loss:0.3803332448005676\n",
      "Epoch 0[568/625] Time:0.712, Train Loss:0.4736528992652893\n",
      "Epoch 0[569/625] Time:0.693, Train Loss:0.3939019441604614\n",
      "Epoch 0[570/625] Time:0.693, Train Loss:0.46339118480682373\n",
      "Epoch 0[571/625] Time:0.693, Train Loss:0.4082680940628052\n",
      "Epoch 0[572/625] Time:0.696, Train Loss:0.45767468214035034\n",
      "Epoch 0[573/625] Time:0.694, Train Loss:0.44052574038505554\n",
      "Epoch 0[574/625] Time:0.694, Train Loss:0.35086753964424133\n",
      "Epoch 0[575/625] Time:0.694, Train Loss:0.3998227119445801\n",
      "Epoch 0[576/625] Time:0.694, Train Loss:0.36883988976478577\n",
      "Epoch 0[577/625] Time:0.693, Train Loss:0.31733056902885437\n",
      "Epoch 0[578/625] Time:0.694, Train Loss:0.31317248940467834\n",
      "Epoch 0[579/625] Time:0.694, Train Loss:0.37057220935821533\n",
      "Epoch 0[580/625] Time:0.693, Train Loss:0.39859870076179504\n",
      "Epoch 0[581/625] Time:0.73, Train Loss:0.49521780014038086\n",
      "Epoch 0[582/625] Time:0.702, Train Loss:0.4500899910926819\n",
      "Epoch 0[583/625] Time:0.703, Train Loss:0.3748639225959778\n",
      "Epoch 0[584/625] Time:0.702, Train Loss:0.3484278917312622\n",
      "Epoch 0[585/625] Time:0.704, Train Loss:0.49840983748435974\n",
      "Epoch 0[586/625] Time:0.703, Train Loss:0.4866252839565277\n",
      "Epoch 0[587/625] Time:0.704, Train Loss:0.4271734058856964\n",
      "Epoch 0[588/625] Time:0.704, Train Loss:0.4429935812950134\n",
      "Epoch 0[589/625] Time:0.705, Train Loss:0.31112605333328247\n",
      "Epoch 0[590/625] Time:0.693, Train Loss:0.32893842458724976\n",
      "Epoch 0[591/625] Time:0.694, Train Loss:0.37376806139945984\n",
      "Epoch 0[592/625] Time:0.698, Train Loss:0.43741315603256226\n",
      "Epoch 0[593/625] Time:0.695, Train Loss:0.5261113047599792\n",
      "Epoch 0[594/625] Time:0.693, Train Loss:0.36902275681495667\n",
      "Epoch 0[595/625] Time:0.693, Train Loss:0.4544074833393097\n",
      "Epoch 0[596/625] Time:0.692, Train Loss:0.4346316158771515\n",
      "Epoch 0[597/625] Time:0.693, Train Loss:0.35245248675346375\n",
      "Epoch 0[598/625] Time:0.694, Train Loss:0.4408387243747711\n",
      "Epoch 0[599/625] Time:0.697, Train Loss:0.5066525340080261\n",
      "Epoch 0[600/625] Time:0.704, Train Loss:0.40784546732902527\n",
      "Epoch 0[601/625] Time:0.703, Train Loss:0.4225459098815918\n",
      "Epoch 0[602/625] Time:0.703, Train Loss:0.400155246257782\n",
      "Epoch 0[603/625] Time:0.694, Train Loss:0.5340800881385803\n",
      "Epoch 0[604/625] Time:0.691, Train Loss:0.4149441421031952\n",
      "Epoch 0[605/625] Time:0.694, Train Loss:0.41980987787246704\n",
      "Epoch 0[606/625] Time:0.694, Train Loss:0.39072123169898987\n",
      "Epoch 0[607/625] Time:0.694, Train Loss:0.476397842168808\n",
      "Epoch 0[608/625] Time:0.695, Train Loss:0.43966612219810486\n",
      "Epoch 0[609/625] Time:0.694, Train Loss:0.48076850175857544\n",
      "Epoch 0[610/625] Time:0.717, Train Loss:0.37030231952667236\n",
      "Epoch 0[611/625] Time:0.703, Train Loss:0.45714226365089417\n",
      "Epoch 0[612/625] Time:0.705, Train Loss:0.41830912232398987\n",
      "Epoch 0[613/625] Time:0.707, Train Loss:0.5212597250938416\n",
      "Epoch 0[614/625] Time:0.703, Train Loss:0.3477157652378082\n",
      "Epoch 0[615/625] Time:0.693, Train Loss:0.4852369427680969\n",
      "Epoch 0[616/625] Time:0.695, Train Loss:0.39077022671699524\n",
      "Epoch 0[617/625] Time:0.693, Train Loss:0.4073324501514435\n",
      "Epoch 0[618/625] Time:0.693, Train Loss:0.4131900370121002\n",
      "Epoch 0[619/625] Time:0.721, Train Loss:0.42111530900001526\n",
      "Epoch 0[620/625] Time:0.703, Train Loss:0.4601270258426666\n",
      "Epoch 0[621/625] Time:0.703, Train Loss:0.455320805311203\n",
      "Epoch 0[622/625] Time:0.703, Train Loss:0.4301237463951111\n",
      "Epoch 0[623/625] Time:0.704, Train Loss:0.44677838683128357\n",
      "Epoch 0[624/625] Time:0.703, Train Loss:0.4111781120300293\n",
      "Epoch 0[0/78] Val Loss:0.44248273968696594\n",
      "Epoch 0[1/78] Val Loss:0.4262658953666687\n",
      "Epoch 0[2/78] Val Loss:0.4691959023475647\n",
      "Epoch 0[3/78] Val Loss:0.4747241735458374\n",
      "Epoch 0[4/78] Val Loss:0.434908390045166\n",
      "Epoch 0[5/78] Val Loss:0.3982425630092621\n",
      "Epoch 0[6/78] Val Loss:0.39556193351745605\n",
      "Epoch 0[7/78] Val Loss:0.3987739086151123\n",
      "Epoch 0[8/78] Val Loss:0.2983027994632721\n",
      "Epoch 0[9/78] Val Loss:0.23731376230716705\n",
      "Epoch 0[10/78] Val Loss:0.21558839082717896\n",
      "Epoch 0[11/78] Val Loss:0.24660815298557281\n",
      "Epoch 0[12/78] Val Loss:0.2503533661365509\n",
      "Epoch 0[13/78] Val Loss:0.20649899542331696\n",
      "Epoch 0[14/78] Val Loss:0.3139687478542328\n",
      "Epoch 0[15/78] Val Loss:0.2766750156879425\n",
      "Epoch 0[16/78] Val Loss:0.307701975107193\n",
      "Epoch 0[17/78] Val Loss:0.26794683933258057\n",
      "Epoch 0[18/78] Val Loss:0.39264026284217834\n",
      "Epoch 0[19/78] Val Loss:0.4524691104888916\n",
      "Epoch 0[20/78] Val Loss:0.3843744397163391\n",
      "Epoch 0[21/78] Val Loss:0.5611552596092224\n",
      "Epoch 0[22/78] Val Loss:0.7038947939872742\n",
      "Epoch 0[23/78] Val Loss:0.5171299576759338\n",
      "Epoch 0[24/78] Val Loss:0.4125335216522217\n",
      "Epoch 0[25/78] Val Loss:0.46270474791526794\n",
      "Epoch 0[26/78] Val Loss:0.5091695785522461\n",
      "Epoch 0[27/78] Val Loss:0.4616837799549103\n",
      "Epoch 0[28/78] Val Loss:0.4408901631832123\n",
      "Epoch 0[29/78] Val Loss:0.5220916271209717\n",
      "Epoch 0[30/78] Val Loss:1.313122034072876\n",
      "Epoch 0[31/78] Val Loss:1.213902473449707\n",
      "Epoch 0[32/78] Val Loss:1.0437551736831665\n",
      "Epoch 0[33/78] Val Loss:0.6781178712844849\n",
      "Epoch 0[34/78] Val Loss:0.5856419205665588\n",
      "Epoch 0[35/78] Val Loss:0.5841052532196045\n",
      "Epoch 0[36/78] Val Loss:0.6068334579467773\n",
      "Epoch 0[37/78] Val Loss:0.6285404562950134\n",
      "Epoch 0[38/78] Val Loss:0.3462483584880829\n",
      "Epoch 0[39/78] Val Loss:0.3497467637062073\n",
      "Epoch 0[40/78] Val Loss:0.35220053791999817\n",
      "Epoch 0[41/78] Val Loss:0.351165235042572\n",
      "Epoch 0[42/78] Val Loss:0.36123427748680115\n",
      "Epoch 0[43/78] Val Loss:0.3055921494960785\n",
      "Epoch 0[44/78] Val Loss:0.28027647733688354\n",
      "Epoch 0[45/78] Val Loss:0.2945152223110199\n",
      "Epoch 0[46/78] Val Loss:0.27153462171554565\n",
      "Epoch 0[47/78] Val Loss:0.27198243141174316\n",
      "Epoch 0[48/78] Val Loss:0.29989808797836304\n",
      "Epoch 0[49/78] Val Loss:0.2635321617126465\n",
      "Epoch 0[50/78] Val Loss:0.2789901793003082\n",
      "Epoch 0[51/78] Val Loss:0.2877504527568817\n",
      "Epoch 0[52/78] Val Loss:0.2938453257083893\n",
      "Epoch 0[53/78] Val Loss:0.2832185626029968\n",
      "Epoch 0[54/78] Val Loss:0.300424188375473\n",
      "Epoch 0[55/78] Val Loss:0.2996272146701813\n",
      "Epoch 0[56/78] Val Loss:0.5438022613525391\n",
      "Epoch 0[57/78] Val Loss:0.5069987177848816\n",
      "Epoch 0[58/78] Val Loss:0.5057157278060913\n",
      "Epoch 0[59/78] Val Loss:0.45535793900489807\n",
      "Epoch 0[60/78] Val Loss:0.4484013319015503\n",
      "Epoch 0[61/78] Val Loss:0.4413459599018097\n",
      "Epoch 0[62/78] Val Loss:0.4828290343284607\n",
      "Epoch 0[63/78] Val Loss:0.36954498291015625\n",
      "Epoch 0[64/78] Val Loss:0.3251020908355713\n",
      "Epoch 0[65/78] Val Loss:0.29477202892303467\n",
      "Epoch 0[66/78] Val Loss:0.34543576836586\n",
      "Epoch 0[67/78] Val Loss:0.31154096126556396\n",
      "Epoch 0[68/78] Val Loss:0.573451042175293\n",
      "Epoch 0[69/78] Val Loss:0.5120596289634705\n",
      "Epoch 0[70/78] Val Loss:0.49720221757888794\n",
      "Epoch 0[71/78] Val Loss:0.49275103211402893\n",
      "Epoch 0[72/78] Val Loss:0.4363758862018585\n",
      "Epoch 0[73/78] Val Loss:0.4679904580116272\n",
      "Epoch 0[74/78] Val Loss:0.6791959404945374\n",
      "Epoch 0[75/78] Val Loss:0.6642582416534424\n",
      "Epoch 0[76/78] Val Loss:0.6811591386795044\n",
      "Epoch 0[77/78] Val Loss:0.7108645439147949\n",
      "Epoch 0[78/78] Val Loss:0.8874689936637878\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.84      0.89     15691\n",
      "           1       0.58      0.77      0.66      4309\n",
      "\n",
      "    accuracy                           0.83     20000\n",
      "   macro avg       0.75      0.81      0.77     20000\n",
      "weighted avg       0.85      0.83      0.84     20000\n",
      "\n",
      "Epoch 0: Train Loss 0.45047402091026306, Val Loss 0.4571958411580477, Train Time 797.1491730213165, Val Time 38.837316036224365\n",
      "New best model at epoch 0\n",
      "Epoch 1[0/625] Time:0.69, Train Loss:0.40670448541641235\n",
      "Epoch 1[1/625] Time:0.694, Train Loss:0.3611224889755249\n",
      "Epoch 1[2/625] Time:0.703, Train Loss:0.4491806626319885\n",
      "Epoch 1[3/625] Time:0.753, Train Loss:0.422763854265213\n",
      "Epoch 1[4/625] Time:0.695, Train Loss:0.49936145544052124\n",
      "Epoch 1[5/625] Time:0.702, Train Loss:0.39344292879104614\n",
      "Epoch 1[6/625] Time:0.704, Train Loss:0.38945552706718445\n",
      "Epoch 1[7/625] Time:0.705, Train Loss:0.39978381991386414\n",
      "Epoch 1[8/625] Time:0.704, Train Loss:0.5468349456787109\n",
      "Epoch 1[9/625] Time:0.705, Train Loss:0.5009714961051941\n",
      "Epoch 1[10/625] Time:0.703, Train Loss:0.414812833070755\n",
      "Epoch 1[11/625] Time:0.705, Train Loss:0.3910127878189087\n",
      "Epoch 1[12/625] Time:0.704, Train Loss:0.36723592877388\n",
      "Epoch 1[13/625] Time:0.749, Train Loss:0.4027668237686157\n",
      "Epoch 1[14/625] Time:0.716, Train Loss:0.4377381503582001\n",
      "Epoch 1[15/625] Time:0.702, Train Loss:0.3395038843154907\n",
      "Epoch 1[16/625] Time:0.693, Train Loss:0.37685373425483704\n",
      "Epoch 1[17/625] Time:0.694, Train Loss:0.3854440152645111\n",
      "Epoch 1[18/625] Time:0.693, Train Loss:0.4744025468826294\n",
      "Epoch 1[19/625] Time:0.698, Train Loss:0.5274470448493958\n",
      "Epoch 1[20/625] Time:0.693, Train Loss:0.436482310295105\n",
      "Epoch 1[21/625] Time:0.726, Train Loss:0.47265052795410156\n",
      "Epoch 1[22/625] Time:0.694, Train Loss:0.42104071378707886\n",
      "Epoch 1[23/625] Time:0.694, Train Loss:0.47804298996925354\n",
      "Epoch 1[24/625] Time:0.695, Train Loss:0.4029688537120819\n",
      "Epoch 1[25/625] Time:0.724, Train Loss:0.3969348073005676\n",
      "Epoch 1[26/625] Time:0.702, Train Loss:0.3565756678581238\n",
      "Epoch 1[27/625] Time:0.703, Train Loss:0.4906916618347168\n",
      "Epoch 1[28/625] Time:0.704, Train Loss:0.4121656119823456\n",
      "Epoch 1[29/625] Time:0.703, Train Loss:0.3210758864879608\n",
      "Epoch 1[30/625] Time:0.705, Train Loss:0.3769482672214508\n",
      "Epoch 1[31/625] Time:0.747, Train Loss:0.3293498456478119\n",
      "Epoch 1[32/625] Time:0.692, Train Loss:0.3963177800178528\n",
      "Epoch 1[33/625] Time:0.692, Train Loss:0.49078842997550964\n",
      "Epoch 1[34/625] Time:0.696, Train Loss:0.29705023765563965\n",
      "Epoch 1[35/625] Time:0.691, Train Loss:0.4517134726047516\n",
      "Epoch 1[36/625] Time:0.694, Train Loss:0.4054100215435028\n",
      "Epoch 1[37/625] Time:0.694, Train Loss:0.444259375333786\n",
      "Epoch 1[38/625] Time:0.731, Train Loss:0.35059985518455505\n",
      "Epoch 1[39/625] Time:0.702, Train Loss:0.4283563792705536\n",
      "Epoch 1[40/625] Time:0.694, Train Loss:0.40019530057907104\n",
      "Epoch 1[41/625] Time:0.702, Train Loss:0.36256304383277893\n",
      "Epoch 1[42/625] Time:0.705, Train Loss:0.41124778985977173\n",
      "Epoch 1[43/625] Time:0.703, Train Loss:0.3759070634841919\n",
      "Epoch 1[44/625] Time:0.702, Train Loss:0.31671538949012756\n",
      "Epoch 1[45/625] Time:0.749, Train Loss:0.4090515375137329\n",
      "Epoch 1[46/625] Time:0.692, Train Loss:0.406572163105011\n",
      "Epoch 1[47/625] Time:0.694, Train Loss:0.39938974380493164\n",
      "Epoch 1[48/625] Time:0.695, Train Loss:0.3158649504184723\n",
      "Epoch 1[49/625] Time:0.696, Train Loss:0.4188455641269684\n",
      "Epoch 1[50/625] Time:0.694, Train Loss:0.38775408267974854\n",
      "Epoch 1[51/625] Time:0.694, Train Loss:0.3852246403694153\n",
      "Epoch 1[52/625] Time:0.694, Train Loss:0.35060933232307434\n",
      "Epoch 1[53/625] Time:0.694, Train Loss:0.447562575340271\n",
      "Epoch 1[54/625] Time:0.716, Train Loss:0.3940029442310333\n",
      "Epoch 1[55/625] Time:0.701, Train Loss:0.36652666330337524\n",
      "Epoch 1[56/625] Time:0.703, Train Loss:0.44657036662101746\n",
      "Epoch 1[57/625] Time:0.704, Train Loss:0.35870492458343506\n",
      "Epoch 1[58/625] Time:0.694, Train Loss:0.37774986028671265\n",
      "Epoch 1[59/625] Time:0.694, Train Loss:0.41877493262290955\n",
      "Epoch 1[60/625] Time:0.695, Train Loss:0.48680758476257324\n",
      "Epoch 1[61/625] Time:0.694, Train Loss:0.4304502308368683\n",
      "Epoch 1[62/625] Time:0.696, Train Loss:0.3245704472064972\n",
      "Epoch 1[63/625] Time:0.694, Train Loss:0.43494996428489685\n",
      "Epoch 1[64/625] Time:0.697, Train Loss:0.3839063048362732\n",
      "Epoch 1[65/625] Time:0.721, Train Loss:0.33989882469177246\n",
      "Epoch 1[66/625] Time:0.694, Train Loss:0.4508778750896454\n",
      "Epoch 1[67/625] Time:0.694, Train Loss:0.3817494809627533\n",
      "Epoch 1[68/625] Time:0.695, Train Loss:0.4271950423717499\n",
      "Epoch 1[69/625] Time:0.693, Train Loss:0.477542519569397\n",
      "Epoch 1[70/625] Time:0.694, Train Loss:0.3301658630371094\n",
      "Epoch 1[71/625] Time:0.734, Train Loss:0.37468627095222473\n",
      "Epoch 1[72/625] Time:0.693, Train Loss:0.40220746397972107\n",
      "Epoch 1[73/625] Time:0.695, Train Loss:0.40021011233329773\n",
      "Epoch 1[74/625] Time:0.696, Train Loss:0.4724547564983368\n",
      "Epoch 1[75/625] Time:0.695, Train Loss:0.3956899642944336\n",
      "Epoch 1[76/625] Time:0.695, Train Loss:0.4326440095901489\n",
      "Epoch 1[77/625] Time:0.694, Train Loss:0.4450662434101105\n",
      "Epoch 1[78/625] Time:0.695, Train Loss:0.4340302646160126\n",
      "Epoch 1[79/625] Time:0.695, Train Loss:0.3878991901874542\n",
      "Epoch 1[80/625] Time:0.731, Train Loss:0.4533737599849701\n",
      "Epoch 1[81/625] Time:0.693, Train Loss:0.3728839159011841\n",
      "Epoch 1[82/625] Time:0.695, Train Loss:0.41560253500938416\n",
      "Epoch 1[83/625] Time:0.694, Train Loss:0.3646201491355896\n",
      "Epoch 1[84/625] Time:0.695, Train Loss:0.42472562193870544\n",
      "Epoch 1[85/625] Time:0.694, Train Loss:0.3915407359600067\n",
      "Epoch 1[86/625] Time:0.694, Train Loss:0.43812331557273865\n",
      "Epoch 1[87/625] Time:0.695, Train Loss:0.4133770167827606\n",
      "Epoch 1[88/625] Time:0.695, Train Loss:0.4456939101219177\n",
      "Epoch 1[89/625] Time:0.696, Train Loss:0.2994987666606903\n",
      "Epoch 1[90/625] Time:0.695, Train Loss:0.42188140749931335\n",
      "Epoch 1[91/625] Time:0.696, Train Loss:0.39915695786476135\n",
      "Epoch 1[92/625] Time:0.697, Train Loss:0.5051881670951843\n",
      "Epoch 1[93/625] Time:0.695, Train Loss:0.35326144099235535\n",
      "Epoch 1[94/625] Time:0.695, Train Loss:0.32119181752204895\n",
      "Epoch 1[95/625] Time:0.695, Train Loss:0.4377598762512207\n",
      "Epoch 1[96/625] Time:0.695, Train Loss:0.39126312732696533\n",
      "Epoch 1[97/625] Time:0.733, Train Loss:0.4156706631183624\n",
      "Epoch 1[98/625] Time:0.703, Train Loss:0.4485720098018646\n",
      "Epoch 1[99/625] Time:0.693, Train Loss:0.4193933308124542\n",
      "Epoch 1[100/625] Time:0.695, Train Loss:0.38441672921180725\n",
      "Epoch 1[101/625] Time:0.693, Train Loss:0.39244896173477173\n",
      "Epoch 1[102/625] Time:0.706, Train Loss:0.38588714599609375\n",
      "Epoch 1[103/625] Time:0.694, Train Loss:0.4705413281917572\n",
      "Epoch 1[104/625] Time:0.694, Train Loss:0.3405100405216217\n",
      "Epoch 1[105/625] Time:0.695, Train Loss:0.43032193183898926\n",
      "Epoch 1[106/625] Time:0.694, Train Loss:0.41470927000045776\n",
      "Epoch 1[107/625] Time:0.695, Train Loss:0.35800620913505554\n",
      "Epoch 1[108/625] Time:0.694, Train Loss:0.34478121995925903\n",
      "Epoch 1[109/625] Time:0.695, Train Loss:0.30543842911720276\n",
      "Epoch 1[110/625] Time:0.694, Train Loss:0.411417692899704\n",
      "Epoch 1[111/625] Time:0.694, Train Loss:0.42262253165245056\n",
      "Epoch 1[112/625] Time:0.696, Train Loss:0.3738493323326111\n",
      "Epoch 1[113/625] Time:0.696, Train Loss:0.3766452670097351\n",
      "Epoch 1[114/625] Time:0.715, Train Loss:0.484655499458313\n",
      "Epoch 1[115/625] Time:0.693, Train Loss:0.2818440794944763\n",
      "Epoch 1[116/625] Time:0.715, Train Loss:0.3523632884025574\n",
      "Epoch 1[117/625] Time:0.694, Train Loss:0.3951919376850128\n",
      "Epoch 1[118/625] Time:0.694, Train Loss:0.3388504981994629\n",
      "Epoch 1[119/625] Time:0.695, Train Loss:0.3610309362411499\n",
      "Epoch 1[120/625] Time:0.695, Train Loss:0.4171113669872284\n",
      "Epoch 1[121/625] Time:0.739, Train Loss:0.34545570611953735\n",
      "Epoch 1[122/625] Time:0.718, Train Loss:0.34984755516052246\n",
      "Epoch 1[123/625] Time:0.694, Train Loss:0.3399127125740051\n",
      "Epoch 1[124/625] Time:0.694, Train Loss:0.3949093520641327\n",
      "Epoch 1[125/625] Time:0.703, Train Loss:0.41572144627571106\n",
      "Epoch 1[126/625] Time:0.694, Train Loss:0.3600793182849884\n",
      "Epoch 1[127/625] Time:0.695, Train Loss:0.3490697741508484\n",
      "Epoch 1[128/625] Time:0.695, Train Loss:0.36411118507385254\n",
      "Epoch 1[129/625] Time:0.694, Train Loss:0.3978825509548187\n",
      "Epoch 1[130/625] Time:0.732, Train Loss:0.3918286859989166\n",
      "Epoch 1[131/625] Time:0.701, Train Loss:0.4041355550289154\n",
      "Epoch 1[132/625] Time:0.71, Train Loss:0.3579872250556946\n",
      "Epoch 1[133/625] Time:0.703, Train Loss:0.3888000249862671\n",
      "Epoch 1[134/625] Time:0.703, Train Loss:0.39212891459465027\n",
      "Epoch 1[135/625] Time:0.701, Train Loss:0.43095681071281433\n",
      "Epoch 1[136/625] Time:0.702, Train Loss:0.4643545150756836\n",
      "Epoch 1[137/625] Time:0.703, Train Loss:0.35339608788490295\n",
      "Epoch 1[138/625] Time:0.703, Train Loss:0.45222827792167664\n",
      "Epoch 1[139/625] Time:0.705, Train Loss:0.41616126894950867\n",
      "Epoch 1[140/625] Time:0.73, Train Loss:0.5021678805351257\n",
      "Epoch 1[141/625] Time:0.694, Train Loss:0.3389465808868408\n",
      "Epoch 1[142/625] Time:0.694, Train Loss:0.33810606598854065\n",
      "Epoch 1[143/625] Time:0.694, Train Loss:0.4350845217704773\n",
      "Epoch 1[144/625] Time:0.695, Train Loss:0.3210752308368683\n",
      "Epoch 1[145/625] Time:0.694, Train Loss:0.442301869392395\n",
      "Epoch 1[146/625] Time:0.696, Train Loss:0.44727805256843567\n",
      "Epoch 1[147/625] Time:0.694, Train Loss:0.38176512718200684\n",
      "Epoch 1[148/625] Time:0.694, Train Loss:0.27940431237220764\n",
      "Epoch 1[149/625] Time:0.694, Train Loss:0.3790441155433655\n",
      "Epoch 1[150/625] Time:0.695, Train Loss:0.38700270652770996\n",
      "Epoch 1[151/625] Time:0.694, Train Loss:0.2925184965133667\n",
      "Epoch 1[152/625] Time:0.695, Train Loss:0.40497061610221863\n",
      "Epoch 1[153/625] Time:0.694, Train Loss:0.40802648663520813\n",
      "Epoch 1[154/625] Time:0.735, Train Loss:0.4567374587059021\n",
      "Epoch 1[155/625] Time:0.701, Train Loss:0.4432826340198517\n",
      "Epoch 1[156/625] Time:0.726, Train Loss:0.38155433535575867\n",
      "Epoch 1[157/625] Time:0.725, Train Loss:0.4034275412559509\n",
      "Epoch 1[158/625] Time:0.695, Train Loss:0.410403311252594\n",
      "Epoch 1[159/625] Time:0.695, Train Loss:0.39229005575180054\n",
      "Epoch 1[160/625] Time:0.694, Train Loss:0.44845661520957947\n",
      "Epoch 1[161/625] Time:0.694, Train Loss:0.32541629672050476\n",
      "Epoch 1[162/625] Time:0.695, Train Loss:0.44277703762054443\n",
      "Epoch 1[163/625] Time:0.695, Train Loss:0.35396745800971985\n",
      "Epoch 1[164/625] Time:0.694, Train Loss:0.3920680284500122\n",
      "Epoch 1[165/625] Time:0.694, Train Loss:0.3914543688297272\n",
      "Epoch 1[166/625] Time:0.737, Train Loss:0.3860113322734833\n",
      "Epoch 1[167/625] Time:0.702, Train Loss:0.31470921635627747\n",
      "Epoch 1[168/625] Time:0.704, Train Loss:0.37079304456710815\n",
      "Epoch 1[169/625] Time:0.699, Train Loss:0.3394129276275635\n",
      "Epoch 1[170/625] Time:0.694, Train Loss:0.3733110725879669\n",
      "Epoch 1[171/625] Time:0.694, Train Loss:0.29829809069633484\n",
      "Epoch 1[172/625] Time:0.695, Train Loss:0.3465180993080139\n",
      "Epoch 1[173/625] Time:0.695, Train Loss:0.4040263891220093\n",
      "Epoch 1[174/625] Time:0.695, Train Loss:0.35298556089401245\n",
      "Epoch 1[175/625] Time:0.694, Train Loss:0.3907208740711212\n",
      "Epoch 1[176/625] Time:0.695, Train Loss:0.40022948384284973\n",
      "Epoch 1[177/625] Time:0.693, Train Loss:0.3357306122779846\n",
      "Epoch 1[178/625] Time:0.7, Train Loss:0.31477296352386475\n",
      "Epoch 1[179/625] Time:0.731, Train Loss:0.3455275893211365\n",
      "Epoch 1[180/625] Time:0.703, Train Loss:0.3899095952510834\n",
      "Epoch 1[181/625] Time:0.695, Train Loss:0.31808051466941833\n",
      "Epoch 1[182/625] Time:0.696, Train Loss:0.3531440794467926\n",
      "Epoch 1[183/625] Time:0.694, Train Loss:0.4371536374092102\n",
      "Epoch 1[184/625] Time:0.694, Train Loss:0.513495922088623\n",
      "Epoch 1[185/625] Time:0.695, Train Loss:0.41741231083869934\n",
      "Epoch 1[186/625] Time:0.697, Train Loss:0.3886879086494446\n",
      "Epoch 1[187/625] Time:0.694, Train Loss:0.3789612054824829\n",
      "Epoch 1[188/625] Time:0.693, Train Loss:0.37822216749191284\n",
      "Epoch 1[189/625] Time:0.695, Train Loss:0.3977065086364746\n",
      "Epoch 1[190/625] Time:0.696, Train Loss:0.3601963222026825\n",
      "Epoch 1[191/625] Time:0.695, Train Loss:0.36723682284355164\n",
      "Epoch 1[192/625] Time:0.693, Train Loss:0.5322802662849426\n",
      "Epoch 1[193/625] Time:0.697, Train Loss:0.4182414710521698\n",
      "Epoch 1[194/625] Time:0.694, Train Loss:0.37235432863235474\n",
      "Epoch 1[195/625] Time:0.695, Train Loss:0.34378162026405334\n",
      "Epoch 1[196/625] Time:0.697, Train Loss:0.32755276560783386\n",
      "Epoch 1[197/625] Time:0.695, Train Loss:0.33851757645606995\n",
      "Epoch 1[198/625] Time:0.696, Train Loss:0.39273419976234436\n",
      "Epoch 1[199/625] Time:0.696, Train Loss:0.47484758496284485\n",
      "Epoch 1[200/625] Time:0.729, Train Loss:0.32542720437049866\n",
      "Epoch 1[201/625] Time:0.705, Train Loss:0.3317217528820038\n",
      "Epoch 1[202/625] Time:0.704, Train Loss:0.38971441984176636\n",
      "Epoch 1[203/625] Time:0.704, Train Loss:0.3613274395465851\n",
      "Epoch 1[204/625] Time:0.704, Train Loss:0.39867502450942993\n",
      "Epoch 1[205/625] Time:0.703, Train Loss:0.39181751012802124\n",
      "Epoch 1[206/625] Time:0.703, Train Loss:0.3757100999355316\n",
      "Epoch 1[207/625] Time:0.704, Train Loss:0.3740029036998749\n",
      "Epoch 1[208/625] Time:0.704, Train Loss:0.4302453100681305\n",
      "Epoch 1[209/625] Time:0.706, Train Loss:0.32698366045951843\n",
      "Epoch 1[210/625] Time:0.703, Train Loss:0.36279982328414917\n",
      "Epoch 1[211/625] Time:0.703, Train Loss:0.3094037175178528\n",
      "Epoch 1[212/625] Time:0.705, Train Loss:0.42990443110466003\n",
      "Epoch 1[213/625] Time:0.704, Train Loss:0.42520472407341003\n",
      "Epoch 1[214/625] Time:0.706, Train Loss:0.4199809432029724\n",
      "Epoch 1[215/625] Time:0.704, Train Loss:0.3847882151603699\n",
      "Epoch 1[216/625] Time:0.703, Train Loss:0.3939380943775177\n",
      "Epoch 1[217/625] Time:0.699, Train Loss:0.3822302520275116\n",
      "Epoch 1[218/625] Time:0.694, Train Loss:0.4194074273109436\n",
      "Epoch 1[219/625] Time:0.694, Train Loss:0.4219941198825836\n",
      "Epoch 1[220/625] Time:0.694, Train Loss:0.34977424144744873\n",
      "Epoch 1[221/625] Time:0.694, Train Loss:0.38310325145721436\n",
      "Epoch 1[222/625] Time:0.694, Train Loss:0.35632163286209106\n",
      "Epoch 1[223/625] Time:0.695, Train Loss:0.357771098613739\n",
      "Epoch 1[224/625] Time:0.694, Train Loss:0.31798502802848816\n",
      "Epoch 1[225/625] Time:0.696, Train Loss:0.2740413248538971\n",
      "Epoch 1[226/625] Time:0.702, Train Loss:0.4142725467681885\n",
      "Epoch 1[227/625] Time:0.702, Train Loss:0.3186272382736206\n",
      "Epoch 1[228/625] Time:0.702, Train Loss:0.32421472668647766\n",
      "Epoch 1[229/625] Time:0.703, Train Loss:0.34035739302635193\n",
      "Epoch 1[230/625] Time:0.702, Train Loss:0.44336724281311035\n",
      "Epoch 1[231/625] Time:0.704, Train Loss:0.3955147862434387\n",
      "Epoch 1[232/625] Time:0.71, Train Loss:0.3623104393482208\n",
      "Epoch 1[233/625] Time:0.703, Train Loss:0.5636043548583984\n",
      "Epoch 1[234/625] Time:0.694, Train Loss:0.31998246908187866\n",
      "Epoch 1[235/625] Time:0.695, Train Loss:0.2785976529121399\n",
      "Epoch 1[236/625] Time:0.692, Train Loss:0.35022440552711487\n",
      "Epoch 1[237/625] Time:0.693, Train Loss:0.36370912194252014\n",
      "Epoch 1[238/625] Time:0.703, Train Loss:0.4178229868412018\n",
      "Epoch 1[239/625] Time:0.694, Train Loss:0.35830017924308777\n",
      "Epoch 1[240/625] Time:0.697, Train Loss:0.3819445073604584\n",
      "Epoch 1[241/625] Time:0.719, Train Loss:0.4205000698566437\n",
      "Epoch 1[242/625] Time:0.704, Train Loss:0.3806026875972748\n",
      "Epoch 1[243/625] Time:0.703, Train Loss:0.30893170833587646\n",
      "Epoch 1[244/625] Time:0.704, Train Loss:0.3982812464237213\n",
      "Epoch 1[245/625] Time:0.703, Train Loss:0.3403872549533844\n",
      "Epoch 1[246/625] Time:0.703, Train Loss:0.3984302878379822\n",
      "Epoch 1[247/625] Time:0.743, Train Loss:0.32448381185531616\n",
      "Epoch 1[248/625] Time:0.695, Train Loss:0.4336482286453247\n",
      "Epoch 1[249/625] Time:0.694, Train Loss:0.42142292857170105\n",
      "Epoch 1[250/625] Time:0.695, Train Loss:0.42990946769714355\n",
      "Epoch 1[251/625] Time:0.694, Train Loss:0.30467578768730164\n",
      "Epoch 1[252/625] Time:0.695, Train Loss:0.31448590755462646\n",
      "Epoch 1[253/625] Time:0.703, Train Loss:0.448900431394577\n",
      "Epoch 1[254/625] Time:0.703, Train Loss:0.3571966290473938\n",
      "Epoch 1[255/625] Time:0.705, Train Loss:0.38890060782432556\n",
      "Epoch 1[256/625] Time:0.695, Train Loss:0.4405175745487213\n",
      "Epoch 1[257/625] Time:0.697, Train Loss:0.4045429229736328\n",
      "Epoch 1[258/625] Time:0.694, Train Loss:0.34886059165000916\n",
      "Epoch 1[259/625] Time:0.695, Train Loss:0.2927156090736389\n",
      "Epoch 1[260/625] Time:0.696, Train Loss:0.28029492497444153\n",
      "Epoch 1[261/625] Time:0.694, Train Loss:0.34366559982299805\n",
      "Epoch 1[262/625] Time:0.695, Train Loss:0.3685920238494873\n",
      "Epoch 1[263/625] Time:0.707, Train Loss:0.3302147686481476\n",
      "Epoch 1[264/625] Time:0.731, Train Loss:0.382904589176178\n",
      "Epoch 1[265/625] Time:0.693, Train Loss:0.39063283801078796\n",
      "Epoch 1[266/625] Time:0.694, Train Loss:0.33921992778778076\n",
      "Epoch 1[267/625] Time:0.694, Train Loss:0.38608989119529724\n",
      "Epoch 1[268/625] Time:0.694, Train Loss:0.37185195088386536\n",
      "Epoch 1[269/625] Time:0.695, Train Loss:0.31591445207595825\n",
      "Epoch 1[270/625] Time:0.741, Train Loss:0.36613377928733826\n",
      "Epoch 1[271/625] Time:0.702, Train Loss:0.4076630175113678\n",
      "Epoch 1[272/625] Time:0.712, Train Loss:0.3637520968914032\n",
      "Epoch 1[273/625] Time:0.702, Train Loss:0.41697171330451965\n",
      "Epoch 1[274/625] Time:0.703, Train Loss:0.36302104592323303\n",
      "Epoch 1[275/625] Time:0.704, Train Loss:0.3666706383228302\n",
      "Epoch 1[276/625] Time:0.695, Train Loss:0.2965698540210724\n",
      "Epoch 1[277/625] Time:0.699, Train Loss:0.43426743149757385\n",
      "Epoch 1[278/625] Time:0.695, Train Loss:0.3729100823402405\n",
      "Epoch 1[279/625] Time:0.696, Train Loss:0.3388769030570984\n",
      "Epoch 1[280/625] Time:0.697, Train Loss:0.5017184019088745\n",
      "Epoch 1[281/625] Time:0.695, Train Loss:0.33388033509254456\n",
      "Epoch 1[282/625] Time:0.695, Train Loss:0.3996361494064331\n",
      "Epoch 1[283/625] Time:0.695, Train Loss:0.37805548310279846\n",
      "Epoch 1[284/625] Time:0.695, Train Loss:0.33677011728286743\n",
      "Epoch 1[285/625] Time:0.695, Train Loss:0.38293126225471497\n",
      "Epoch 1[286/625] Time:0.695, Train Loss:0.44367820024490356\n",
      "Epoch 1[287/625] Time:0.694, Train Loss:0.4116785526275635\n",
      "Epoch 1[288/625] Time:0.695, Train Loss:0.36983275413513184\n",
      "Epoch 1[289/625] Time:0.695, Train Loss:0.3593607544898987\n",
      "Epoch 1[290/625] Time:0.695, Train Loss:0.34038639068603516\n",
      "Epoch 1[291/625] Time:0.695, Train Loss:0.4161578416824341\n",
      "Epoch 1[292/625] Time:0.696, Train Loss:0.49568358063697815\n",
      "Epoch 1[293/625] Time:0.696, Train Loss:0.43232688307762146\n",
      "Epoch 1[294/625] Time:0.695, Train Loss:0.3747057020664215\n",
      "Epoch 1[295/625] Time:0.696, Train Loss:0.3973388075828552\n",
      "Epoch 1[296/625] Time:0.696, Train Loss:0.35422632098197937\n",
      "Epoch 1[297/625] Time:0.695, Train Loss:0.35312747955322266\n",
      "Epoch 1[298/625] Time:0.696, Train Loss:0.42984598875045776\n",
      "Epoch 1[299/625] Time:0.699, Train Loss:0.41166144609451294\n",
      "Epoch 1[300/625] Time:0.694, Train Loss:0.37650027871131897\n",
      "Epoch 1[301/625] Time:0.695, Train Loss:0.41322627663612366\n",
      "Epoch 1[302/625] Time:0.695, Train Loss:0.5766234397888184\n",
      "Epoch 1[303/625] Time:0.705, Train Loss:0.31810393929481506\n",
      "Epoch 1[304/625] Time:0.693, Train Loss:0.45297983288764954\n",
      "Epoch 1[305/625] Time:0.695, Train Loss:0.360655277967453\n",
      "Epoch 1[306/625] Time:0.714, Train Loss:0.4683801531791687\n",
      "Epoch 1[307/625] Time:0.694, Train Loss:0.31998592615127563\n",
      "Epoch 1[308/625] Time:0.695, Train Loss:0.4123685359954834\n",
      "Epoch 1[309/625] Time:0.696, Train Loss:0.3283843994140625\n",
      "Epoch 1[310/625] Time:0.695, Train Loss:0.3351000249385834\n",
      "Epoch 1[311/625] Time:0.724, Train Loss:0.397355854511261\n",
      "Epoch 1[312/625] Time:0.701, Train Loss:0.3676440715789795\n",
      "Epoch 1[313/625] Time:0.695, Train Loss:0.4607551097869873\n",
      "Epoch 1[314/625] Time:0.696, Train Loss:0.3539268374443054\n",
      "Epoch 1[315/625] Time:0.698, Train Loss:0.4122407138347626\n",
      "Epoch 1[316/625] Time:0.697, Train Loss:0.46884313225746155\n",
      "Epoch 1[317/625] Time:0.698, Train Loss:0.3746214509010315\n",
      "Epoch 1[318/625] Time:0.703, Train Loss:0.37142863869667053\n",
      "Epoch 1[319/625] Time:0.703, Train Loss:0.33089515566825867\n",
      "Epoch 1[320/625] Time:0.703, Train Loss:0.3979325592517853\n",
      "Epoch 1[321/625] Time:0.694, Train Loss:0.3468279540538788\n",
      "Epoch 1[322/625] Time:0.696, Train Loss:0.34394827485084534\n",
      "Epoch 1[323/625] Time:0.696, Train Loss:0.3465674817562103\n",
      "Epoch 1[324/625] Time:0.696, Train Loss:0.3988540470600128\n",
      "Epoch 1[325/625] Time:0.722, Train Loss:0.3661128282546997\n",
      "Epoch 1[326/625] Time:0.701, Train Loss:0.37384501099586487\n",
      "Epoch 1[327/625] Time:0.727, Train Loss:0.3156055212020874\n",
      "Epoch 1[328/625] Time:0.695, Train Loss:0.3714950382709503\n",
      "Epoch 1[329/625] Time:0.697, Train Loss:0.4142138957977295\n",
      "Epoch 1[330/625] Time:0.697, Train Loss:0.3604787588119507\n",
      "Epoch 1[331/625] Time:0.695, Train Loss:0.3282410502433777\n",
      "Epoch 1[332/625] Time:0.695, Train Loss:0.3973694443702698\n",
      "Epoch 1[333/625] Time:0.698, Train Loss:0.4022045135498047\n",
      "Epoch 1[334/625] Time:0.695, Train Loss:0.344087690114975\n",
      "Epoch 1[335/625] Time:0.739, Train Loss:0.39906778931617737\n",
      "Epoch 1[336/625] Time:0.694, Train Loss:0.3851601779460907\n",
      "Epoch 1[337/625] Time:0.737, Train Loss:0.35865941643714905\n",
      "Epoch 1[338/625] Time:0.702, Train Loss:0.330928772687912\n",
      "Epoch 1[339/625] Time:0.693, Train Loss:0.3793628215789795\n",
      "Epoch 1[340/625] Time:0.717, Train Loss:0.31192129850387573\n",
      "Epoch 1[341/625] Time:0.703, Train Loss:0.3487779498100281\n",
      "Epoch 1[342/625] Time:0.695, Train Loss:0.29363468289375305\n",
      "Epoch 1[343/625] Time:0.698, Train Loss:0.40440472960472107\n",
      "Epoch 1[344/625] Time:0.695, Train Loss:0.41555002331733704\n",
      "Epoch 1[345/625] Time:0.696, Train Loss:0.302080363035202\n",
      "Epoch 1[346/625] Time:0.696, Train Loss:0.3721374571323395\n",
      "Epoch 1[347/625] Time:0.696, Train Loss:0.3535478413105011\n",
      "Epoch 1[348/625] Time:0.696, Train Loss:0.42794275283813477\n",
      "Epoch 1[349/625] Time:0.695, Train Loss:0.36444512009620667\n",
      "Epoch 1[350/625] Time:0.696, Train Loss:0.374493807554245\n",
      "Epoch 1[351/625] Time:0.696, Train Loss:0.38335224986076355\n",
      "Epoch 1[352/625] Time:0.695, Train Loss:0.35862788558006287\n",
      "Epoch 1[353/625] Time:0.696, Train Loss:0.3589067757129669\n",
      "Epoch 1[354/625] Time:0.695, Train Loss:0.34707796573638916\n",
      "Epoch 1[355/625] Time:0.695, Train Loss:0.35358667373657227\n",
      "Epoch 1[356/625] Time:0.695, Train Loss:0.31050974130630493\n",
      "Epoch 1[357/625] Time:0.695, Train Loss:0.3434063196182251\n",
      "Epoch 1[358/625] Time:0.695, Train Loss:0.3494246006011963\n",
      "Epoch 1[359/625] Time:0.696, Train Loss:0.34898558259010315\n",
      "Epoch 1[360/625] Time:0.695, Train Loss:0.3951784074306488\n",
      "Epoch 1[361/625] Time:0.697, Train Loss:0.35004255175590515\n",
      "Epoch 1[362/625] Time:0.695, Train Loss:0.3899625539779663\n",
      "Epoch 1[363/625] Time:0.694, Train Loss:0.4012460708618164\n",
      "Epoch 1[364/625] Time:0.694, Train Loss:0.3721286952495575\n",
      "Epoch 1[365/625] Time:0.694, Train Loss:0.3206789493560791\n",
      "Epoch 1[366/625] Time:0.695, Train Loss:0.3112163245677948\n",
      "Epoch 1[367/625] Time:0.737, Train Loss:0.27125194668769836\n",
      "Epoch 1[368/625] Time:0.707, Train Loss:0.4100657105445862\n",
      "Epoch 1[369/625] Time:0.716, Train Loss:0.36290985345840454\n",
      "Epoch 1[370/625] Time:0.704, Train Loss:0.32046186923980713\n",
      "Epoch 1[371/625] Time:0.693, Train Loss:0.396959513425827\n",
      "Epoch 1[372/625] Time:0.693, Train Loss:0.3254948854446411\n",
      "Epoch 1[373/625] Time:0.694, Train Loss:0.38903433084487915\n",
      "Epoch 1[374/625] Time:0.695, Train Loss:0.3795948922634125\n",
      "Epoch 1[375/625] Time:0.693, Train Loss:0.3051695227622986\n",
      "Epoch 1[376/625] Time:0.694, Train Loss:0.3557226359844208\n",
      "Epoch 1[377/625] Time:0.694, Train Loss:0.3838218152523041\n",
      "Epoch 1[378/625] Time:0.693, Train Loss:0.39718344807624817\n",
      "Epoch 1[379/625] Time:0.696, Train Loss:0.29537728428840637\n",
      "Epoch 1[380/625] Time:0.694, Train Loss:0.28440582752227783\n",
      "Epoch 1[381/625] Time:0.694, Train Loss:0.3622884452342987\n",
      "Epoch 1[382/625] Time:0.715, Train Loss:0.36620697379112244\n",
      "Epoch 1[383/625] Time:0.694, Train Loss:0.4437946081161499\n",
      "Epoch 1[384/625] Time:0.694, Train Loss:0.3406330645084381\n",
      "Epoch 1[385/625] Time:0.697, Train Loss:0.3958621025085449\n",
      "Epoch 1[386/625] Time:0.695, Train Loss:0.30061015486717224\n",
      "Epoch 1[387/625] Time:0.694, Train Loss:0.337625116109848\n",
      "Epoch 1[388/625] Time:0.694, Train Loss:0.363941490650177\n",
      "Epoch 1[389/625] Time:0.71, Train Loss:0.3731898367404938\n",
      "Epoch 1[390/625] Time:0.739, Train Loss:0.38032448291778564\n",
      "Epoch 1[391/625] Time:0.702, Train Loss:0.2825694978237152\n",
      "Epoch 1[392/625] Time:0.723, Train Loss:0.380090594291687\n",
      "Epoch 1[393/625] Time:0.693, Train Loss:0.2739758789539337\n",
      "Epoch 1[394/625] Time:0.694, Train Loss:0.3816727101802826\n",
      "Epoch 1[395/625] Time:0.695, Train Loss:0.3088487386703491\n",
      "Epoch 1[396/625] Time:0.694, Train Loss:0.3325198292732239\n",
      "Epoch 1[397/625] Time:0.695, Train Loss:0.30299827456474304\n",
      "Epoch 1[398/625] Time:0.695, Train Loss:0.28838324546813965\n",
      "Epoch 1[399/625] Time:0.696, Train Loss:0.43551772832870483\n",
      "Epoch 1[400/625] Time:0.695, Train Loss:0.36107930541038513\n",
      "Epoch 1[401/625] Time:0.694, Train Loss:0.2702244520187378\n",
      "Epoch 1[402/625] Time:0.695, Train Loss:0.41069796681404114\n",
      "Epoch 1[403/625] Time:0.695, Train Loss:0.35880807042121887\n",
      "Epoch 1[404/625] Time:0.694, Train Loss:0.3900757431983948\n",
      "Epoch 1[405/625] Time:0.696, Train Loss:0.4102681875228882\n",
      "Epoch 1[406/625] Time:0.695, Train Loss:0.4176572561264038\n",
      "Epoch 1[407/625] Time:0.695, Train Loss:0.41209861636161804\n",
      "Epoch 1[408/625] Time:0.694, Train Loss:0.33213716745376587\n",
      "Epoch 1[409/625] Time:0.7, Train Loss:0.39961710572242737\n",
      "Epoch 1[410/625] Time:0.702, Train Loss:0.3325590193271637\n",
      "Epoch 1[411/625] Time:0.704, Train Loss:0.22300122678279877\n",
      "Epoch 1[412/625] Time:0.695, Train Loss:0.36984485387802124\n",
      "Epoch 1[413/625] Time:0.694, Train Loss:0.42462435364723206\n",
      "Epoch 1[414/625] Time:0.696, Train Loss:0.3632020950317383\n",
      "Epoch 1[415/625] Time:0.695, Train Loss:0.3603396415710449\n",
      "Epoch 1[416/625] Time:0.696, Train Loss:0.4423641264438629\n",
      "Epoch 1[417/625] Time:0.695, Train Loss:0.378667414188385\n",
      "Epoch 1[418/625] Time:0.721, Train Loss:0.3790949285030365\n",
      "Epoch 1[419/625] Time:0.703, Train Loss:0.34472548961639404\n",
      "Epoch 1[420/625] Time:0.703, Train Loss:0.24775192141532898\n",
      "Epoch 1[421/625] Time:0.703, Train Loss:0.38921183347702026\n",
      "Epoch 1[422/625] Time:0.705, Train Loss:0.33292609453201294\n",
      "Epoch 1[423/625] Time:0.703, Train Loss:0.33243903517723083\n",
      "Epoch 1[424/625] Time:0.704, Train Loss:0.3581247925758362\n",
      "Epoch 1[425/625] Time:0.705, Train Loss:0.3577097952365875\n",
      "Epoch 1[426/625] Time:0.703, Train Loss:0.35786470770835876\n",
      "Epoch 1[427/625] Time:0.703, Train Loss:0.39304402470588684\n",
      "Epoch 1[428/625] Time:0.696, Train Loss:0.3903138041496277\n",
      "Epoch 1[429/625] Time:0.694, Train Loss:0.49988898634910583\n",
      "Epoch 1[430/625] Time:0.694, Train Loss:0.35563597083091736\n",
      "Epoch 1[431/625] Time:0.696, Train Loss:0.38924190402030945\n",
      "Epoch 1[432/625] Time:0.696, Train Loss:0.3032989799976349\n",
      "Epoch 1[433/625] Time:0.695, Train Loss:0.3423450291156769\n",
      "Epoch 1[434/625] Time:0.696, Train Loss:0.30766746401786804\n",
      "Epoch 1[435/625] Time:0.695, Train Loss:0.4303078353404999\n",
      "Epoch 1[436/625] Time:0.695, Train Loss:0.3589876592159271\n",
      "Epoch 1[437/625] Time:0.696, Train Loss:0.3875221610069275\n",
      "Epoch 1[438/625] Time:0.695, Train Loss:0.3858780264854431\n",
      "Epoch 1[439/625] Time:0.696, Train Loss:0.3237118422985077\n",
      "Epoch 1[440/625] Time:0.696, Train Loss:0.3933597505092621\n",
      "Epoch 1[441/625] Time:0.695, Train Loss:0.2838752567768097\n",
      "Epoch 1[442/625] Time:0.695, Train Loss:0.3047768473625183\n",
      "Epoch 1[443/625] Time:0.695, Train Loss:0.4260694980621338\n",
      "Epoch 1[444/625] Time:0.695, Train Loss:0.4730101525783539\n",
      "Epoch 1[445/625] Time:0.693, Train Loss:0.33184513449668884\n",
      "Epoch 1[446/625] Time:0.695, Train Loss:0.6072748899459839\n",
      "Epoch 1[447/625] Time:0.695, Train Loss:0.3120127320289612\n",
      "Epoch 1[448/625] Time:0.694, Train Loss:0.29010009765625\n",
      "Epoch 1[449/625] Time:0.695, Train Loss:0.4562535583972931\n",
      "Epoch 1[450/625] Time:0.693, Train Loss:0.3048389256000519\n",
      "Epoch 1[451/625] Time:0.715, Train Loss:0.3506964445114136\n",
      "Epoch 1[452/625] Time:0.703, Train Loss:0.4802056849002838\n",
      "Epoch 1[453/625] Time:0.703, Train Loss:0.35040852427482605\n",
      "Epoch 1[454/625] Time:0.703, Train Loss:0.4223591089248657\n",
      "Epoch 1[455/625] Time:0.732, Train Loss:0.3524596095085144\n",
      "Epoch 1[456/625] Time:0.698, Train Loss:0.4503984749317169\n",
      "Epoch 1[457/625] Time:0.704, Train Loss:0.33808836340904236\n",
      "Epoch 1[458/625] Time:0.703, Train Loss:0.36195823550224304\n",
      "Epoch 1[459/625] Time:0.739, Train Loss:0.3862150311470032\n",
      "Epoch 1[460/625] Time:0.702, Train Loss:0.36388686299324036\n",
      "Epoch 1[461/625] Time:0.696, Train Loss:0.35105788707733154\n",
      "Epoch 1[462/625] Time:0.696, Train Loss:0.30238810181617737\n",
      "Epoch 1[463/625] Time:0.695, Train Loss:0.34183254837989807\n",
      "Epoch 1[464/625] Time:0.694, Train Loss:0.46395355463027954\n",
      "Epoch 1[465/625] Time:0.696, Train Loss:0.41457873582839966\n",
      "Epoch 1[466/625] Time:0.694, Train Loss:0.41800493001937866\n",
      "Epoch 1[467/625] Time:0.695, Train Loss:0.32309648394584656\n",
      "Epoch 1[468/625] Time:0.697, Train Loss:0.3156016170978546\n",
      "Epoch 1[469/625] Time:0.694, Train Loss:0.3431210219860077\n",
      "Epoch 1[470/625] Time:0.695, Train Loss:0.3146883547306061\n",
      "Epoch 1[471/625] Time:0.694, Train Loss:0.32863467931747437\n",
      "Epoch 1[472/625] Time:0.695, Train Loss:0.35161110758781433\n",
      "Epoch 1[473/625] Time:0.717, Train Loss:0.4052901566028595\n",
      "Epoch 1[474/625] Time:0.702, Train Loss:0.34354549646377563\n",
      "Epoch 1[475/625] Time:0.729, Train Loss:0.38530233502388\n",
      "Epoch 1[476/625] Time:0.695, Train Loss:0.29070422053337097\n",
      "Epoch 1[477/625] Time:0.695, Train Loss:0.3141896426677704\n",
      "Epoch 1[478/625] Time:0.704, Train Loss:0.3329077661037445\n",
      "Epoch 1[479/625] Time:0.696, Train Loss:0.35119524598121643\n",
      "Epoch 1[480/625] Time:0.704, Train Loss:0.34178903698921204\n",
      "Epoch 1[481/625] Time:0.704, Train Loss:0.3311663866043091\n",
      "Epoch 1[482/625] Time:0.703, Train Loss:0.33457326889038086\n",
      "Epoch 1[483/625] Time:0.702, Train Loss:0.27758800983428955\n",
      "Epoch 1[484/625] Time:0.703, Train Loss:0.3092329502105713\n",
      "Epoch 1[485/625] Time:0.736, Train Loss:0.3979984223842621\n",
      "Epoch 1[486/625] Time:0.693, Train Loss:0.2327507585287094\n",
      "Epoch 1[487/625] Time:0.694, Train Loss:0.2919577360153198\n",
      "Epoch 1[488/625] Time:0.694, Train Loss:0.37602612376213074\n",
      "Epoch 1[489/625] Time:0.693, Train Loss:0.408458411693573\n",
      "Epoch 1[490/625] Time:0.695, Train Loss:0.26767510175704956\n",
      "Epoch 1[491/625] Time:0.696, Train Loss:0.44177037477493286\n",
      "Epoch 1[492/625] Time:0.697, Train Loss:0.3217344880104065\n",
      "Epoch 1[493/625] Time:0.695, Train Loss:0.3579706847667694\n",
      "Epoch 1[494/625] Time:0.696, Train Loss:0.4442218244075775\n",
      "Epoch 1[495/625] Time:0.695, Train Loss:0.34029892086982727\n",
      "Epoch 1[496/625] Time:0.695, Train Loss:0.48870497941970825\n",
      "Epoch 1[497/625] Time:0.695, Train Loss:0.4230026304721832\n",
      "Epoch 1[498/625] Time:0.695, Train Loss:0.4668252766132355\n",
      "Epoch 1[499/625] Time:0.696, Train Loss:0.3910851776599884\n",
      "Epoch 1[500/625] Time:0.696, Train Loss:0.366356760263443\n",
      "Epoch 1[501/625] Time:0.696, Train Loss:0.403753399848938\n",
      "Epoch 1[502/625] Time:0.696, Train Loss:0.3987509310245514\n",
      "Epoch 1[503/625] Time:0.731, Train Loss:0.34389251470565796\n",
      "Epoch 1[504/625] Time:0.703, Train Loss:0.3605195879936218\n",
      "Epoch 1[505/625] Time:0.696, Train Loss:0.30252090096473694\n",
      "Epoch 1[506/625] Time:0.695, Train Loss:0.3940412998199463\n",
      "Epoch 1[507/625] Time:0.695, Train Loss:0.347299188375473\n",
      "Epoch 1[508/625] Time:0.696, Train Loss:0.3695935904979706\n",
      "Epoch 1[509/625] Time:0.695, Train Loss:0.32951217889785767\n",
      "Epoch 1[510/625] Time:0.695, Train Loss:0.3189554512500763\n",
      "Epoch 1[511/625] Time:0.695, Train Loss:0.3538346588611603\n",
      "Epoch 1[512/625] Time:0.695, Train Loss:0.3875700831413269\n",
      "Epoch 1[513/625] Time:0.695, Train Loss:0.415348082780838\n",
      "Epoch 1[514/625] Time:0.695, Train Loss:0.2687487006187439\n",
      "Epoch 1[515/625] Time:0.694, Train Loss:0.3951057195663452\n",
      "Epoch 1[516/625] Time:0.696, Train Loss:0.30054348707199097\n",
      "Epoch 1[517/625] Time:0.697, Train Loss:0.31444501876831055\n",
      "Epoch 1[518/625] Time:0.694, Train Loss:0.28914934396743774\n",
      "Epoch 1[519/625] Time:0.695, Train Loss:0.34127166867256165\n",
      "Epoch 1[520/625] Time:0.694, Train Loss:0.3210175037384033\n",
      "Epoch 1[521/625] Time:0.695, Train Loss:0.4128876030445099\n",
      "Epoch 1[522/625] Time:0.695, Train Loss:0.40711313486099243\n",
      "Epoch 1[523/625] Time:0.695, Train Loss:0.36145853996276855\n",
      "Epoch 1[524/625] Time:0.695, Train Loss:0.34119713306427\n",
      "Epoch 1[525/625] Time:0.694, Train Loss:0.3315655291080475\n",
      "Epoch 1[526/625] Time:0.694, Train Loss:0.3451181948184967\n",
      "Epoch 1[527/625] Time:0.694, Train Loss:0.4595801532268524\n",
      "Epoch 1[528/625] Time:0.694, Train Loss:0.3933578133583069\n",
      "Epoch 1[529/625] Time:0.695, Train Loss:0.3548118472099304\n",
      "Epoch 1[530/625] Time:0.694, Train Loss:0.3632732033729553\n",
      "Epoch 1[531/625] Time:0.695, Train Loss:0.3922288417816162\n",
      "Epoch 1[532/625] Time:0.696, Train Loss:0.3504639267921448\n",
      "Epoch 1[533/625] Time:0.695, Train Loss:0.39206844568252563\n",
      "Epoch 1[534/625] Time:0.695, Train Loss:0.31320953369140625\n",
      "Epoch 1[535/625] Time:0.695, Train Loss:0.35814550518989563\n",
      "Epoch 1[536/625] Time:0.695, Train Loss:0.2816171944141388\n",
      "Epoch 1[537/625] Time:0.696, Train Loss:0.36363089084625244\n",
      "Epoch 1[538/625] Time:0.695, Train Loss:0.3320079445838928\n",
      "Epoch 1[539/625] Time:0.696, Train Loss:0.3617109954357147\n",
      "Epoch 1[540/625] Time:0.695, Train Loss:0.31109559535980225\n",
      "Epoch 1[541/625] Time:0.695, Train Loss:0.3356895446777344\n",
      "Epoch 1[542/625] Time:0.695, Train Loss:0.3266025185585022\n",
      "Epoch 1[543/625] Time:0.695, Train Loss:0.4456319212913513\n",
      "Epoch 1[544/625] Time:0.72, Train Loss:0.3784658908843994\n",
      "Epoch 1[545/625] Time:0.703, Train Loss:0.33425527811050415\n",
      "Epoch 1[546/625] Time:0.703, Train Loss:0.3677732050418854\n",
      "Epoch 1[547/625] Time:0.694, Train Loss:0.3305726945400238\n",
      "Epoch 1[548/625] Time:0.695, Train Loss:0.3378446400165558\n",
      "Epoch 1[549/625] Time:0.694, Train Loss:0.3176521360874176\n",
      "Epoch 1[550/625] Time:0.694, Train Loss:0.30007264018058777\n",
      "Epoch 1[551/625] Time:0.715, Train Loss:0.38149622082710266\n",
      "Epoch 1[552/625] Time:0.702, Train Loss:0.30786585807800293\n",
      "Epoch 1[553/625] Time:0.704, Train Loss:0.32201075553894043\n",
      "Epoch 1[554/625] Time:0.703, Train Loss:0.3499595820903778\n",
      "Epoch 1[555/625] Time:0.705, Train Loss:0.37559589743614197\n",
      "Epoch 1[556/625] Time:0.703, Train Loss:0.3139050602912903\n",
      "Epoch 1[557/625] Time:0.705, Train Loss:0.37225329875946045\n",
      "Epoch 1[558/625] Time:0.704, Train Loss:0.3753368556499481\n",
      "Epoch 1[559/625] Time:0.706, Train Loss:0.4235396385192871\n",
      "Epoch 1[560/625] Time:0.705, Train Loss:0.32763105630874634\n",
      "Epoch 1[561/625] Time:0.694, Train Loss:0.3354508578777313\n",
      "Epoch 1[562/625] Time:0.693, Train Loss:0.28582844138145447\n",
      "Epoch 1[563/625] Time:0.694, Train Loss:0.3396911919116974\n",
      "Epoch 1[564/625] Time:0.694, Train Loss:0.28173115849494934\n",
      "Epoch 1[565/625] Time:0.695, Train Loss:0.3326919376850128\n",
      "Epoch 1[566/625] Time:0.694, Train Loss:0.4783580005168915\n",
      "Epoch 1[567/625] Time:0.694, Train Loss:0.4049818813800812\n",
      "Epoch 1[568/625] Time:0.695, Train Loss:0.36170586943626404\n",
      "Epoch 1[569/625] Time:0.696, Train Loss:0.3340095579624176\n",
      "Epoch 1[570/625] Time:0.702, Train Loss:0.40520867705345154\n",
      "Epoch 1[571/625] Time:0.712, Train Loss:0.305900901556015\n",
      "Epoch 1[572/625] Time:0.701, Train Loss:0.3730071187019348\n",
      "Epoch 1[573/625] Time:0.737, Train Loss:0.3170151710510254\n",
      "Epoch 1[574/625] Time:0.693, Train Loss:0.3159957528114319\n",
      "Epoch 1[575/625] Time:0.695, Train Loss:0.44058385491371155\n",
      "Epoch 1[576/625] Time:0.695, Train Loss:0.3174745738506317\n",
      "Epoch 1[577/625] Time:0.724, Train Loss:0.3473834693431854\n",
      "Epoch 1[578/625] Time:0.694, Train Loss:0.369831383228302\n",
      "Epoch 1[579/625] Time:0.696, Train Loss:0.32942715287208557\n",
      "Epoch 1[580/625] Time:0.695, Train Loss:0.3661400079727173\n",
      "Epoch 1[581/625] Time:0.694, Train Loss:0.45635873079299927\n",
      "Epoch 1[582/625] Time:0.694, Train Loss:0.3353898525238037\n",
      "Epoch 1[583/625] Time:0.695, Train Loss:0.31579405069351196\n",
      "Epoch 1[584/625] Time:0.694, Train Loss:0.3645560145378113\n",
      "Epoch 1[585/625] Time:0.693, Train Loss:0.38899993896484375\n",
      "Epoch 1[586/625] Time:0.695, Train Loss:0.36518287658691406\n",
      "Epoch 1[587/625] Time:0.726, Train Loss:0.3486541509628296\n",
      "Epoch 1[588/625] Time:0.694, Train Loss:0.5191338062286377\n",
      "Epoch 1[589/625] Time:0.695, Train Loss:0.3395662009716034\n",
      "Epoch 1[590/625] Time:0.693, Train Loss:0.44875335693359375\n",
      "Epoch 1[591/625] Time:0.694, Train Loss:0.41509026288986206\n",
      "Epoch 1[592/625] Time:0.695, Train Loss:0.28326448798179626\n",
      "Epoch 1[593/625] Time:0.696, Train Loss:0.3412953317165375\n",
      "Epoch 1[594/625] Time:0.696, Train Loss:0.3269718885421753\n",
      "Epoch 1[595/625] Time:0.724, Train Loss:0.35498565435409546\n",
      "Epoch 1[596/625] Time:0.693, Train Loss:0.33201178908348083\n",
      "Epoch 1[597/625] Time:0.695, Train Loss:0.26026302576065063\n",
      "Epoch 1[598/625] Time:0.695, Train Loss:0.41569268703460693\n",
      "Epoch 1[599/625] Time:0.695, Train Loss:0.4391857385635376\n",
      "Epoch 1[600/625] Time:0.695, Train Loss:0.4263709783554077\n",
      "Epoch 1[601/625] Time:0.715, Train Loss:0.3229454755783081\n",
      "Epoch 1[602/625] Time:0.694, Train Loss:0.40284210443496704\n",
      "Epoch 1[603/625] Time:0.695, Train Loss:0.33265331387519836\n",
      "Epoch 1[604/625] Time:0.695, Train Loss:0.33096426725387573\n",
      "Epoch 1[605/625] Time:0.695, Train Loss:0.3743564188480377\n",
      "Epoch 1[606/625] Time:0.697, Train Loss:0.42219552397727966\n",
      "Epoch 1[607/625] Time:0.694, Train Loss:0.3206256330013275\n",
      "Epoch 1[608/625] Time:0.695, Train Loss:0.3490104675292969\n",
      "Epoch 1[609/625] Time:0.698, Train Loss:0.37700551748275757\n",
      "Epoch 1[610/625] Time:0.696, Train Loss:0.4386524260044098\n",
      "Epoch 1[611/625] Time:0.695, Train Loss:0.32396408915519714\n",
      "Epoch 1[612/625] Time:0.693, Train Loss:0.38014301657676697\n",
      "Epoch 1[613/625] Time:0.74, Train Loss:0.3612721562385559\n",
      "Epoch 1[614/625] Time:0.702, Train Loss:0.3533321022987366\n",
      "Epoch 1[615/625] Time:0.736, Train Loss:0.3010943830013275\n",
      "Epoch 1[616/625] Time:0.703, Train Loss:0.340345561504364\n",
      "Epoch 1[617/625] Time:0.701, Train Loss:0.32652032375335693\n",
      "Epoch 1[618/625] Time:0.695, Train Loss:0.3006995916366577\n",
      "Epoch 1[619/625] Time:0.695, Train Loss:0.318862646818161\n",
      "Epoch 1[620/625] Time:0.695, Train Loss:0.31851842999458313\n",
      "Epoch 1[621/625] Time:0.696, Train Loss:0.3433729410171509\n",
      "Epoch 1[622/625] Time:0.697, Train Loss:0.35849401354789734\n",
      "Epoch 1[623/625] Time:0.695, Train Loss:0.35759490728378296\n",
      "Epoch 1[624/625] Time:0.696, Train Loss:0.30397936701774597\n",
      "Epoch 1[0/78] Val Loss:0.20901179313659668\n",
      "Epoch 1[1/78] Val Loss:0.20249958336353302\n",
      "Epoch 1[2/78] Val Loss:0.21685755252838135\n",
      "Epoch 1[3/78] Val Loss:0.18708285689353943\n",
      "Epoch 1[4/78] Val Loss:0.31565672159194946\n",
      "Epoch 1[5/78] Val Loss:0.28719791769981384\n",
      "Epoch 1[6/78] Val Loss:0.2828311026096344\n",
      "Epoch 1[7/78] Val Loss:0.3561607003211975\n",
      "Epoch 1[8/78] Val Loss:0.2305527925491333\n",
      "Epoch 1[9/78] Val Loss:0.1429642289876938\n",
      "Epoch 1[10/78] Val Loss:0.11652782559394836\n",
      "Epoch 1[11/78] Val Loss:0.16979949176311493\n",
      "Epoch 1[12/78] Val Loss:0.13743315637111664\n",
      "Epoch 1[13/78] Val Loss:0.1236526370048523\n",
      "Epoch 1[14/78] Val Loss:0.22425341606140137\n",
      "Epoch 1[15/78] Val Loss:0.17293182015419006\n",
      "Epoch 1[16/78] Val Loss:0.18259413540363312\n",
      "Epoch 1[17/78] Val Loss:0.18894557654857635\n",
      "Epoch 1[18/78] Val Loss:0.27255845069885254\n",
      "Epoch 1[19/78] Val Loss:0.31060779094696045\n",
      "Epoch 1[20/78] Val Loss:0.2663036286830902\n",
      "Epoch 1[21/78] Val Loss:0.4595136046409607\n",
      "Epoch 1[22/78] Val Loss:0.6278771162033081\n",
      "Epoch 1[23/78] Val Loss:0.49616843461990356\n",
      "Epoch 1[24/78] Val Loss:0.4193538725376129\n",
      "Epoch 1[25/78] Val Loss:0.4829535484313965\n",
      "Epoch 1[26/78] Val Loss:0.46757882833480835\n",
      "Epoch 1[27/78] Val Loss:0.46506306529045105\n",
      "Epoch 1[28/78] Val Loss:0.48224493861198425\n",
      "Epoch 1[29/78] Val Loss:0.6139886975288391\n",
      "Epoch 1[30/78] Val Loss:1.7850309610366821\n",
      "Epoch 1[31/78] Val Loss:1.6497802734375\n",
      "Epoch 1[32/78] Val Loss:1.2849339246749878\n",
      "Epoch 1[33/78] Val Loss:0.43662264943122864\n",
      "Epoch 1[34/78] Val Loss:0.3727530539035797\n",
      "Epoch 1[35/78] Val Loss:0.4120323956012726\n",
      "Epoch 1[36/78] Val Loss:0.4075556993484497\n",
      "Epoch 1[37/78] Val Loss:0.44687044620513916\n",
      "Epoch 1[38/78] Val Loss:0.21727465093135834\n",
      "Epoch 1[39/78] Val Loss:0.19088783860206604\n",
      "Epoch 1[40/78] Val Loss:0.2182224541902542\n",
      "Epoch 1[41/78] Val Loss:0.20469243824481964\n",
      "Epoch 1[42/78] Val Loss:0.2060200572013855\n",
      "Epoch 1[43/78] Val Loss:0.15578526258468628\n",
      "Epoch 1[44/78] Val Loss:0.15598727762699127\n",
      "Epoch 1[45/78] Val Loss:0.13181371986865997\n",
      "Epoch 1[46/78] Val Loss:0.171701580286026\n",
      "Epoch 1[47/78] Val Loss:0.13290156424045563\n",
      "Epoch 1[48/78] Val Loss:0.1699608713388443\n",
      "Epoch 1[49/78] Val Loss:0.15389446914196014\n",
      "Epoch 1[50/78] Val Loss:0.1224190965294838\n",
      "Epoch 1[51/78] Val Loss:0.13700850307941437\n",
      "Epoch 1[52/78] Val Loss:0.17103376984596252\n",
      "Epoch 1[53/78] Val Loss:0.14905260503292084\n",
      "Epoch 1[54/78] Val Loss:0.13368815183639526\n",
      "Epoch 1[55/78] Val Loss:0.13544464111328125\n",
      "Epoch 1[56/78] Val Loss:0.3561103940010071\n",
      "Epoch 1[57/78] Val Loss:0.3298073410987854\n",
      "Epoch 1[58/78] Val Loss:0.33367273211479187\n",
      "Epoch 1[59/78] Val Loss:0.387285977602005\n",
      "Epoch 1[60/78] Val Loss:0.38035669922828674\n",
      "Epoch 1[61/78] Val Loss:0.39691391587257385\n",
      "Epoch 1[62/78] Val Loss:0.456161767244339\n",
      "Epoch 1[63/78] Val Loss:0.26870962977409363\n",
      "Epoch 1[64/78] Val Loss:0.15653325617313385\n",
      "Epoch 1[65/78] Val Loss:0.13428115844726562\n",
      "Epoch 1[66/78] Val Loss:0.17159509658813477\n",
      "Epoch 1[67/78] Val Loss:0.16779500246047974\n",
      "Epoch 1[68/78] Val Loss:0.3977234363555908\n",
      "Epoch 1[69/78] Val Loss:0.3751522898674011\n",
      "Epoch 1[70/78] Val Loss:0.375608891248703\n",
      "Epoch 1[71/78] Val Loss:0.3423287868499756\n",
      "Epoch 1[72/78] Val Loss:0.2171187847852707\n",
      "Epoch 1[73/78] Val Loss:0.22458261251449585\n",
      "Epoch 1[74/78] Val Loss:0.464432030916214\n",
      "Epoch 1[75/78] Val Loss:0.5077540874481201\n",
      "Epoch 1[76/78] Val Loss:0.5332302451133728\n",
      "Epoch 1[77/78] Val Loss:0.5754212141036987\n",
      "Epoch 1[78/78] Val Loss:0.6613733768463135\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91     15691\n",
      "           1       0.68      0.69      0.69      4309\n",
      "\n",
      "    accuracy                           0.86     20000\n",
      "   macro avg       0.80      0.80      0.80     20000\n",
      "weighted avg       0.86      0.86      0.86     20000\n",
      "\n",
      "Epoch 1: Train Loss 0.37500314693450926, Val Loss 0.3471600303474145, Train Time 816.1112596988678, Val Time 38.51836919784546\n",
      "New best model at epoch 1\n",
      "Epoch 2[0/625] Time:0.694, Train Loss:0.40706488490104675\n",
      "Epoch 2[1/625] Time:0.702, Train Loss:0.4020359516143799\n",
      "Epoch 2[2/625] Time:0.727, Train Loss:0.3516223728656769\n",
      "Epoch 2[3/625] Time:0.721, Train Loss:0.378406286239624\n",
      "Epoch 2[4/625] Time:0.694, Train Loss:0.35467663407325745\n",
      "Epoch 2[5/625] Time:0.695, Train Loss:0.31733274459838867\n",
      "Epoch 2[6/625] Time:0.694, Train Loss:0.3435481786727905\n",
      "Epoch 2[7/625] Time:0.695, Train Loss:0.33104029297828674\n",
      "Epoch 2[8/625] Time:0.695, Train Loss:0.3567863404750824\n",
      "Epoch 2[9/625] Time:0.695, Train Loss:0.3877022862434387\n",
      "Epoch 2[10/625] Time:0.694, Train Loss:0.38784047961235046\n",
      "Epoch 2[11/625] Time:0.703, Train Loss:0.3470207154750824\n",
      "Epoch 2[12/625] Time:0.702, Train Loss:0.296190470457077\n",
      "Epoch 2[13/625] Time:0.703, Train Loss:0.27212753891944885\n",
      "Epoch 2[14/625] Time:0.702, Train Loss:0.34957510232925415\n",
      "Epoch 2[15/625] Time:0.706, Train Loss:0.3192478120326996\n",
      "Epoch 2[16/625] Time:0.704, Train Loss:0.26433804631233215\n",
      "Epoch 2[17/625] Time:0.705, Train Loss:0.3030889630317688\n",
      "Epoch 2[18/625] Time:0.705, Train Loss:0.37528038024902344\n",
      "Epoch 2[19/625] Time:0.705, Train Loss:0.4007550776004791\n",
      "Epoch 2[20/625] Time:0.704, Train Loss:0.35266849398612976\n",
      "Epoch 2[21/625] Time:0.695, Train Loss:0.37097012996673584\n",
      "Epoch 2[22/625] Time:0.695, Train Loss:0.41553208231925964\n",
      "Epoch 2[23/625] Time:0.695, Train Loss:0.4062138497829437\n",
      "Epoch 2[24/625] Time:0.73, Train Loss:0.3533798158168793\n",
      "Epoch 2[25/625] Time:0.702, Train Loss:0.3242770731449127\n",
      "Epoch 2[26/625] Time:0.693, Train Loss:0.28811028599739075\n",
      "Epoch 2[27/625] Time:0.693, Train Loss:0.4542286992073059\n",
      "Epoch 2[28/625] Time:0.695, Train Loss:0.39265871047973633\n",
      "Epoch 2[29/625] Time:0.695, Train Loss:0.36465221643447876\n",
      "Epoch 2[30/625] Time:0.695, Train Loss:0.3290073573589325\n",
      "Epoch 2[31/625] Time:0.695, Train Loss:0.35168707370758057\n",
      "Epoch 2[32/625] Time:0.695, Train Loss:0.4175302982330322\n",
      "Epoch 2[33/625] Time:0.694, Train Loss:0.36366018652915955\n",
      "Epoch 2[34/625] Time:0.694, Train Loss:0.3017929196357727\n",
      "Epoch 2[35/625] Time:0.694, Train Loss:0.34448808431625366\n",
      "Epoch 2[36/625] Time:0.694, Train Loss:0.3536558449268341\n",
      "Epoch 2[37/625] Time:0.695, Train Loss:0.38777947425842285\n",
      "Epoch 2[38/625] Time:0.694, Train Loss:0.3166481554508209\n",
      "Epoch 2[39/625] Time:0.696, Train Loss:0.3283398151397705\n",
      "Epoch 2[40/625] Time:0.696, Train Loss:0.340289443731308\n",
      "Epoch 2[41/625] Time:0.695, Train Loss:0.3690098226070404\n",
      "Epoch 2[42/625] Time:0.694, Train Loss:0.3207802474498749\n",
      "Epoch 2[43/625] Time:0.694, Train Loss:0.3841409981250763\n",
      "Epoch 2[44/625] Time:0.694, Train Loss:0.3715044856071472\n",
      "Epoch 2[45/625] Time:0.694, Train Loss:0.32770147919654846\n",
      "Epoch 2[46/625] Time:0.693, Train Loss:0.3676132559776306\n",
      "Epoch 2[47/625] Time:0.696, Train Loss:0.3616744577884674\n",
      "Epoch 2[48/625] Time:0.694, Train Loss:0.3709275722503662\n",
      "Epoch 2[49/625] Time:0.694, Train Loss:0.4457339942455292\n",
      "Epoch 2[50/625] Time:0.696, Train Loss:0.3364335596561432\n",
      "Epoch 2[51/625] Time:0.694, Train Loss:0.32667750120162964\n",
      "Epoch 2[52/625] Time:0.696, Train Loss:0.31359806656837463\n",
      "Epoch 2[53/625] Time:0.695, Train Loss:0.4177296459674835\n",
      "Epoch 2[54/625] Time:0.695, Train Loss:0.31156837940216064\n",
      "Epoch 2[55/625] Time:0.695, Train Loss:0.29459914565086365\n",
      "Epoch 2[56/625] Time:0.695, Train Loss:0.4869312345981598\n",
      "Epoch 2[57/625] Time:0.698, Train Loss:0.2650691270828247\n",
      "Epoch 2[58/625] Time:0.719, Train Loss:0.36812254786491394\n",
      "Epoch 2[59/625] Time:0.702, Train Loss:0.3530862331390381\n",
      "Epoch 2[60/625] Time:0.717, Train Loss:0.3500120937824249\n",
      "Epoch 2[61/625] Time:0.693, Train Loss:0.41082677245140076\n",
      "Epoch 2[62/625] Time:0.695, Train Loss:0.3598743975162506\n",
      "Epoch 2[63/625] Time:0.695, Train Loss:0.35905101895332336\n",
      "Epoch 2[64/625] Time:0.696, Train Loss:0.29053768515586853\n",
      "Epoch 2[65/625] Time:0.694, Train Loss:0.37323519587516785\n",
      "Epoch 2[66/625] Time:0.694, Train Loss:0.3208599388599396\n",
      "Epoch 2[67/625] Time:0.695, Train Loss:0.4055403769016266\n",
      "Epoch 2[68/625] Time:0.694, Train Loss:0.3251539170742035\n",
      "Epoch 2[69/625] Time:0.694, Train Loss:0.3118458390235901\n",
      "Epoch 2[70/625] Time:0.694, Train Loss:0.3796701431274414\n",
      "Epoch 2[71/625] Time:0.694, Train Loss:0.32498258352279663\n",
      "Epoch 2[72/625] Time:0.694, Train Loss:0.3107481598854065\n",
      "Epoch 2[73/625] Time:0.697, Train Loss:0.3329561948776245\n",
      "Epoch 2[74/625] Time:0.695, Train Loss:0.4622531533241272\n",
      "Epoch 2[75/625] Time:0.694, Train Loss:0.38614627718925476\n",
      "Epoch 2[76/625] Time:0.695, Train Loss:0.2664111852645874\n",
      "Epoch 2[77/625] Time:0.695, Train Loss:0.25225770473480225\n",
      "Epoch 2[78/625] Time:0.697, Train Loss:0.3809683918952942\n",
      "Epoch 2[79/625] Time:0.72, Train Loss:0.35778990387916565\n",
      "Epoch 2[80/625] Time:0.706, Train Loss:0.32625940442085266\n",
      "Epoch 2[81/625] Time:0.708, Train Loss:0.3513490557670593\n",
      "Epoch 2[82/625] Time:0.703, Train Loss:0.33090510964393616\n",
      "Epoch 2[83/625] Time:0.694, Train Loss:0.340752512216568\n",
      "Epoch 2[84/625] Time:0.694, Train Loss:0.27871617674827576\n",
      "Epoch 2[85/625] Time:0.694, Train Loss:0.3442286252975464\n",
      "Epoch 2[86/625] Time:0.696, Train Loss:0.2948508560657501\n",
      "Epoch 2[87/625] Time:0.695, Train Loss:0.37470245361328125\n",
      "Epoch 2[88/625] Time:0.694, Train Loss:0.29180362820625305\n",
      "Epoch 2[89/625] Time:0.696, Train Loss:0.33710187673568726\n",
      "Epoch 2[90/625] Time:0.698, Train Loss:0.2939721643924713\n",
      "Epoch 2[91/625] Time:0.694, Train Loss:0.30205675959587097\n",
      "Epoch 2[92/625] Time:0.695, Train Loss:0.31413644552230835\n",
      "Epoch 2[93/625] Time:0.696, Train Loss:0.4395807981491089\n",
      "Epoch 2[94/625] Time:0.696, Train Loss:0.4908723831176758\n",
      "Epoch 2[95/625] Time:0.694, Train Loss:0.32134655117988586\n",
      "Epoch 2[96/625] Time:0.694, Train Loss:0.36071115732192993\n",
      "Epoch 2[97/625] Time:0.697, Train Loss:0.4128006100654602\n",
      "Epoch 2[98/625] Time:0.696, Train Loss:0.36970239877700806\n",
      "Epoch 2[99/625] Time:0.694, Train Loss:0.3664514124393463\n",
      "Epoch 2[100/625] Time:0.695, Train Loss:0.30250364542007446\n",
      "Epoch 2[101/625] Time:0.695, Train Loss:0.3432663083076477\n",
      "Epoch 2[102/625] Time:0.694, Train Loss:0.4238889515399933\n",
      "Epoch 2[103/625] Time:0.695, Train Loss:0.4428468644618988\n",
      "Epoch 2[104/625] Time:0.694, Train Loss:0.419320672750473\n",
      "Epoch 2[105/625] Time:0.694, Train Loss:0.3072234094142914\n",
      "Epoch 2[106/625] Time:0.694, Train Loss:0.34278449416160583\n",
      "Epoch 2[107/625] Time:0.694, Train Loss:0.4483396112918854\n",
      "Epoch 2[108/625] Time:0.694, Train Loss:0.32343488931655884\n",
      "Epoch 2[109/625] Time:0.701, Train Loss:0.39903756976127625\n",
      "Epoch 2[110/625] Time:0.733, Train Loss:0.3086603879928589\n",
      "Epoch 2[111/625] Time:0.702, Train Loss:0.358279824256897\n",
      "Epoch 2[112/625] Time:0.703, Train Loss:0.3693521320819855\n",
      "Epoch 2[113/625] Time:0.702, Train Loss:0.3400706946849823\n",
      "Epoch 2[114/625] Time:0.702, Train Loss:0.3612018823623657\n",
      "Epoch 2[115/625] Time:0.704, Train Loss:0.32600072026252747\n",
      "Epoch 2[116/625] Time:0.706, Train Loss:0.33613675832748413\n",
      "Epoch 2[117/625] Time:0.704, Train Loss:0.29075682163238525\n",
      "Epoch 2[118/625] Time:0.703, Train Loss:0.35476037859916687\n",
      "Epoch 2[119/625] Time:0.699, Train Loss:0.3858100473880768\n",
      "Epoch 2[120/625] Time:0.693, Train Loss:0.38887542486190796\n",
      "Epoch 2[121/625] Time:0.693, Train Loss:0.36582303047180176\n",
      "Epoch 2[122/625] Time:0.694, Train Loss:0.3186221718788147\n",
      "Epoch 2[123/625] Time:0.695, Train Loss:0.3035188913345337\n",
      "Epoch 2[124/625] Time:0.695, Train Loss:0.4188981056213379\n",
      "Epoch 2[125/625] Time:0.701, Train Loss:0.433010995388031\n",
      "Epoch 2[126/625] Time:0.705, Train Loss:0.3168802261352539\n",
      "Epoch 2[127/625] Time:0.704, Train Loss:0.3387928307056427\n",
      "Epoch 2[128/625] Time:0.716, Train Loss:0.368120402097702\n",
      "Epoch 2[129/625] Time:0.695, Train Loss:0.3734431564807892\n",
      "Epoch 2[130/625] Time:0.696, Train Loss:0.26893308758735657\n",
      "Epoch 2[131/625] Time:0.694, Train Loss:0.41265010833740234\n",
      "Epoch 2[132/625] Time:0.694, Train Loss:0.3508884906768799\n",
      "Epoch 2[133/625] Time:0.723, Train Loss:0.3814120590686798\n",
      "Epoch 2[134/625] Time:0.702, Train Loss:0.34432151913642883\n",
      "Epoch 2[135/625] Time:0.703, Train Loss:0.2822301983833313\n",
      "Epoch 2[136/625] Time:0.694, Train Loss:0.2835853099822998\n",
      "Epoch 2[137/625] Time:0.695, Train Loss:0.316992849111557\n",
      "Epoch 2[138/625] Time:0.713, Train Loss:0.3556039333343506\n",
      "Epoch 2[139/625] Time:0.704, Train Loss:0.3001179099082947\n",
      "Epoch 2[140/625] Time:0.704, Train Loss:0.33822473883628845\n",
      "Epoch 2[141/625] Time:0.703, Train Loss:0.3466414511203766\n",
      "Epoch 2[142/625] Time:0.716, Train Loss:0.3265622854232788\n",
      "Epoch 2[143/625] Time:0.692, Train Loss:0.3957156836986542\n",
      "Epoch 2[144/625] Time:0.694, Train Loss:0.34046438336372375\n",
      "Epoch 2[145/625] Time:0.695, Train Loss:0.39364489912986755\n",
      "Epoch 2[146/625] Time:0.695, Train Loss:0.4143049120903015\n",
      "Epoch 2[147/625] Time:0.696, Train Loss:0.3438839018344879\n",
      "Epoch 2[148/625] Time:0.726, Train Loss:0.28497910499572754\n",
      "Epoch 2[149/625] Time:0.702, Train Loss:0.3348173499107361\n",
      "Epoch 2[150/625] Time:0.704, Train Loss:0.3406361937522888\n",
      "Epoch 2[151/625] Time:0.704, Train Loss:0.34441521763801575\n",
      "Epoch 2[152/625] Time:0.694, Train Loss:0.3500325083732605\n",
      "Epoch 2[153/625] Time:0.694, Train Loss:0.35288581252098083\n",
      "Epoch 2[154/625] Time:0.694, Train Loss:0.3694845736026764\n",
      "Epoch 2[155/625] Time:0.696, Train Loss:0.36214834451675415\n",
      "Epoch 2[156/625] Time:0.711, Train Loss:0.4133414626121521\n",
      "Epoch 2[157/625] Time:0.694, Train Loss:0.3173832893371582\n",
      "Epoch 2[158/625] Time:0.694, Train Loss:0.3568337857723236\n",
      "Epoch 2[159/625] Time:0.694, Train Loss:0.3376108705997467\n",
      "Epoch 2[160/625] Time:0.694, Train Loss:0.4012565314769745\n",
      "Epoch 2[161/625] Time:0.695, Train Loss:0.33528387546539307\n",
      "Epoch 2[162/625] Time:0.728, Train Loss:0.377694696187973\n",
      "Epoch 2[163/625] Time:0.702, Train Loss:0.3260675370693207\n",
      "Epoch 2[164/625] Time:0.694, Train Loss:0.33294904232025146\n",
      "Epoch 2[165/625] Time:0.694, Train Loss:0.3454337418079376\n",
      "Epoch 2[166/625] Time:0.694, Train Loss:0.34936100244522095\n",
      "Epoch 2[167/625] Time:0.694, Train Loss:0.3619398772716522\n",
      "Epoch 2[168/625] Time:0.695, Train Loss:0.38548433780670166\n",
      "Epoch 2[169/625] Time:0.693, Train Loss:0.39961934089660645\n",
      "Epoch 2[170/625] Time:0.694, Train Loss:0.3690038323402405\n",
      "Epoch 2[171/625] Time:0.698, Train Loss:0.35612404346466064\n",
      "Epoch 2[172/625] Time:0.693, Train Loss:0.3808108866214752\n",
      "Epoch 2[173/625] Time:0.694, Train Loss:0.3224521577358246\n",
      "Epoch 2[174/625] Time:0.718, Train Loss:0.38416725397109985\n",
      "Epoch 2[175/625] Time:0.704, Train Loss:0.38051050901412964\n",
      "Epoch 2[176/625] Time:0.705, Train Loss:0.3118351399898529\n",
      "Epoch 2[177/625] Time:0.703, Train Loss:0.3174358308315277\n",
      "Epoch 2[178/625] Time:0.704, Train Loss:0.325148344039917\n",
      "Epoch 2[179/625] Time:0.702, Train Loss:0.3094894289970398\n",
      "Epoch 2[180/625] Time:0.704, Train Loss:0.32336413860321045\n",
      "Epoch 2[181/625] Time:0.703, Train Loss:0.33327212929725647\n",
      "Epoch 2[182/625] Time:0.741, Train Loss:0.2917528450489044\n",
      "Epoch 2[183/625] Time:0.699, Train Loss:0.368076354265213\n",
      "Epoch 2[184/625] Time:0.694, Train Loss:0.31331655383110046\n",
      "Epoch 2[185/625] Time:0.698, Train Loss:0.3328453004360199\n",
      "Epoch 2[186/625] Time:0.694, Train Loss:0.35092926025390625\n",
      "Epoch 2[187/625] Time:0.694, Train Loss:0.30373266339302063\n",
      "Epoch 2[188/625] Time:0.694, Train Loss:0.33338794112205505\n",
      "Epoch 2[189/625] Time:0.694, Train Loss:0.4264179468154907\n",
      "Epoch 2[190/625] Time:0.692, Train Loss:0.31241118907928467\n",
      "Epoch 2[191/625] Time:0.694, Train Loss:0.21856079995632172\n",
      "Epoch 2[192/625] Time:0.693, Train Loss:0.37252700328826904\n",
      "Epoch 2[193/625] Time:0.693, Train Loss:0.30154964327812195\n",
      "Epoch 2[194/625] Time:0.699, Train Loss:0.3048042953014374\n",
      "Epoch 2[195/625] Time:0.693, Train Loss:0.5301927924156189\n",
      "Epoch 2[196/625] Time:0.694, Train Loss:0.37904834747314453\n",
      "Epoch 2[197/625] Time:0.694, Train Loss:0.3801105320453644\n",
      "Epoch 2[198/625] Time:0.695, Train Loss:0.42253386974334717\n",
      "Epoch 2[199/625] Time:0.695, Train Loss:0.336952269077301\n",
      "Epoch 2[200/625] Time:0.695, Train Loss:0.4124205708503723\n",
      "Epoch 2[201/625] Time:0.695, Train Loss:0.35833561420440674\n",
      "Epoch 2[202/625] Time:0.695, Train Loss:0.272461473941803\n",
      "Epoch 2[203/625] Time:0.694, Train Loss:0.31623589992523193\n",
      "Epoch 2[204/625] Time:0.694, Train Loss:0.3714883029460907\n",
      "Epoch 2[205/625] Time:0.694, Train Loss:0.35224810242652893\n",
      "Epoch 2[206/625] Time:0.695, Train Loss:0.40795472264289856\n",
      "Epoch 2[207/625] Time:0.722, Train Loss:0.4075232148170471\n",
      "Epoch 2[208/625] Time:0.702, Train Loss:0.4370470345020294\n",
      "Epoch 2[209/625] Time:0.704, Train Loss:0.4423457682132721\n",
      "Epoch 2[210/625] Time:0.703, Train Loss:0.34462931752204895\n",
      "Epoch 2[211/625] Time:0.702, Train Loss:0.2740643322467804\n",
      "Epoch 2[212/625] Time:0.703, Train Loss:0.2850344479084015\n",
      "Epoch 2[213/625] Time:0.72, Train Loss:0.32765495777130127\n",
      "Epoch 2[214/625] Time:0.698, Train Loss:0.34340566396713257\n",
      "Epoch 2[215/625] Time:0.693, Train Loss:0.29781821370124817\n",
      "Epoch 2[216/625] Time:0.692, Train Loss:0.3431105613708496\n",
      "Epoch 2[217/625] Time:0.695, Train Loss:0.3468731641769409\n",
      "Epoch 2[218/625] Time:0.694, Train Loss:0.3492434620857239\n",
      "Epoch 2[219/625] Time:0.694, Train Loss:0.3845864236354828\n",
      "Epoch 2[220/625] Time:0.693, Train Loss:0.33512258529663086\n",
      "Epoch 2[221/625] Time:0.694, Train Loss:0.39333269000053406\n",
      "Epoch 2[222/625] Time:0.694, Train Loss:0.3726712167263031\n",
      "Epoch 2[223/625] Time:0.693, Train Loss:0.28340575098991394\n",
      "Epoch 2[224/625] Time:0.694, Train Loss:0.44665372371673584\n",
      "Epoch 2[225/625] Time:0.7, Train Loss:0.30851104855537415\n",
      "Epoch 2[226/625] Time:0.695, Train Loss:0.3297518491744995\n",
      "Epoch 2[227/625] Time:0.696, Train Loss:0.3298962712287903\n",
      "Epoch 2[228/625] Time:0.694, Train Loss:0.3755617141723633\n",
      "Epoch 2[229/625] Time:0.693, Train Loss:0.3276257812976837\n",
      "Epoch 2[230/625] Time:0.693, Train Loss:0.32267388701438904\n",
      "Epoch 2[231/625] Time:0.694, Train Loss:0.41369718313217163\n",
      "Epoch 2[232/625] Time:0.693, Train Loss:0.2995140850543976\n",
      "Epoch 2[233/625] Time:0.739, Train Loss:0.3034254312515259\n",
      "Epoch 2[234/625] Time:0.692, Train Loss:0.2660216689109802\n",
      "Epoch 2[235/625] Time:0.694, Train Loss:0.3061233162879944\n",
      "Epoch 2[236/625] Time:0.693, Train Loss:0.31257563829421997\n",
      "Epoch 2[237/625] Time:0.693, Train Loss:0.26238635182380676\n",
      "Epoch 2[238/625] Time:0.695, Train Loss:0.3615187406539917\n",
      "Epoch 2[239/625] Time:0.695, Train Loss:0.33900168538093567\n",
      "Epoch 2[240/625] Time:0.695, Train Loss:0.3262192904949188\n",
      "Epoch 2[241/625] Time:0.694, Train Loss:0.303726464509964\n",
      "Epoch 2[242/625] Time:0.694, Train Loss:0.3273748755455017\n",
      "Epoch 2[243/625] Time:0.695, Train Loss:0.35362693667411804\n",
      "Epoch 2[244/625] Time:0.694, Train Loss:0.33124208450317383\n",
      "Epoch 2[245/625] Time:0.694, Train Loss:0.3470457196235657\n",
      "Epoch 2[246/625] Time:0.693, Train Loss:0.36944150924682617\n",
      "Epoch 2[247/625] Time:0.694, Train Loss:0.38405612111091614\n",
      "Epoch 2[248/625] Time:0.694, Train Loss:0.40730851888656616\n",
      "Epoch 2[249/625] Time:0.725, Train Loss:0.3428794741630554\n",
      "Epoch 2[250/625] Time:0.694, Train Loss:0.2858191728591919\n",
      "Epoch 2[251/625] Time:0.695, Train Loss:0.3284964859485626\n",
      "Epoch 2[252/625] Time:0.694, Train Loss:0.3298836052417755\n",
      "Epoch 2[253/625] Time:0.694, Train Loss:0.3411022126674652\n",
      "Epoch 2[254/625] Time:0.694, Train Loss:0.4251863956451416\n",
      "Epoch 2[255/625] Time:0.695, Train Loss:0.28198036551475525\n",
      "Epoch 2[256/625] Time:0.697, Train Loss:0.38641270995140076\n",
      "Epoch 2[257/625] Time:0.694, Train Loss:0.4158833920955658\n",
      "Epoch 2[258/625] Time:0.696, Train Loss:0.25841954350471497\n",
      "Epoch 2[259/625] Time:0.694, Train Loss:0.3733729422092438\n",
      "Epoch 2[260/625] Time:0.694, Train Loss:0.3477709889411926\n",
      "Epoch 2[261/625] Time:0.695, Train Loss:0.4245769679546356\n",
      "Epoch 2[262/625] Time:0.694, Train Loss:0.29832613468170166\n",
      "Epoch 2[263/625] Time:0.694, Train Loss:0.41531896591186523\n",
      "Epoch 2[264/625] Time:0.694, Train Loss:0.31323155760765076\n",
      "Epoch 2[265/625] Time:0.693, Train Loss:0.3756396472454071\n",
      "Epoch 2[266/625] Time:0.694, Train Loss:0.296506404876709\n",
      "Epoch 2[267/625] Time:0.694, Train Loss:0.31228652596473694\n",
      "Epoch 2[268/625] Time:0.693, Train Loss:0.45644861459732056\n",
      "Epoch 2[269/625] Time:0.695, Train Loss:0.3199177384376526\n",
      "Epoch 2[270/625] Time:0.693, Train Loss:0.38774123787879944\n",
      "Epoch 2[271/625] Time:0.695, Train Loss:0.37317949533462524\n",
      "Epoch 2[272/625] Time:0.696, Train Loss:0.3921981453895569\n",
      "Epoch 2[273/625] Time:0.694, Train Loss:0.33729687333106995\n",
      "Epoch 2[274/625] Time:0.694, Train Loss:0.2844540476799011\n",
      "Epoch 2[275/625] Time:0.694, Train Loss:0.3367520272731781\n",
      "Epoch 2[276/625] Time:0.694, Train Loss:0.32740986347198486\n",
      "Epoch 2[277/625] Time:0.695, Train Loss:0.3119584918022156\n",
      "Epoch 2[278/625] Time:0.695, Train Loss:0.3062778413295746\n",
      "Epoch 2[279/625] Time:0.694, Train Loss:0.3708646595478058\n",
      "Epoch 2[280/625] Time:0.733, Train Loss:0.299020916223526\n",
      "Epoch 2[281/625] Time:0.703, Train Loss:0.37111949920654297\n",
      "Epoch 2[282/625] Time:0.703, Train Loss:0.41967320442199707\n",
      "Epoch 2[283/625] Time:0.696, Train Loss:0.33315059542655945\n",
      "Epoch 2[284/625] Time:0.694, Train Loss:0.3487139046192169\n",
      "Epoch 2[285/625] Time:0.695, Train Loss:0.3197234570980072\n",
      "Epoch 2[286/625] Time:0.694, Train Loss:0.2951085865497589\n",
      "Epoch 2[287/625] Time:0.694, Train Loss:0.3303767740726471\n",
      "Epoch 2[288/625] Time:0.694, Train Loss:0.4264709949493408\n",
      "Epoch 2[289/625] Time:0.694, Train Loss:0.31302300095558167\n",
      "Epoch 2[290/625] Time:0.693, Train Loss:0.32635462284088135\n",
      "Epoch 2[291/625] Time:0.694, Train Loss:0.32040345668792725\n",
      "Epoch 2[292/625] Time:0.693, Train Loss:0.27980268001556396\n",
      "Epoch 2[293/625] Time:0.693, Train Loss:0.3003852367401123\n",
      "Epoch 2[294/625] Time:0.694, Train Loss:0.2866367995738983\n",
      "Epoch 2[295/625] Time:0.694, Train Loss:0.3224836587905884\n",
      "Epoch 2[296/625] Time:0.695, Train Loss:0.43054983019828796\n",
      "Epoch 2[297/625] Time:0.694, Train Loss:0.31839239597320557\n",
      "Epoch 2[298/625] Time:0.739, Train Loss:0.30111804604530334\n",
      "Epoch 2[299/625] Time:0.697, Train Loss:0.291583389043808\n",
      "Epoch 2[300/625] Time:0.694, Train Loss:0.2941261827945709\n",
      "Epoch 2[301/625] Time:0.696, Train Loss:0.366260290145874\n",
      "Epoch 2[302/625] Time:0.694, Train Loss:0.4042204022407532\n",
      "Epoch 2[303/625] Time:0.704, Train Loss:0.4172818958759308\n",
      "Epoch 2[304/625] Time:0.694, Train Loss:0.29937005043029785\n",
      "Epoch 2[305/625] Time:0.695, Train Loss:0.3696436583995819\n",
      "Epoch 2[306/625] Time:0.694, Train Loss:0.34606045484542847\n",
      "Epoch 2[307/625] Time:0.7, Train Loss:0.37506943941116333\n",
      "Epoch 2[308/625] Time:0.711, Train Loss:0.335126668214798\n",
      "Epoch 2[309/625] Time:0.703, Train Loss:0.31280556321144104\n",
      "Epoch 2[310/625] Time:0.702, Train Loss:0.30848491191864014\n",
      "Epoch 2[311/625] Time:0.704, Train Loss:0.3434183895587921\n",
      "Epoch 2[312/625] Time:0.703, Train Loss:0.33491843938827515\n",
      "Epoch 2[313/625] Time:0.703, Train Loss:0.3136090636253357\n",
      "Epoch 2[314/625] Time:0.703, Train Loss:0.3060967028141022\n",
      "Epoch 2[315/625] Time:0.704, Train Loss:0.2938235402107239\n",
      "Epoch 2[316/625] Time:0.703, Train Loss:0.399280309677124\n",
      "Epoch 2[317/625] Time:0.703, Train Loss:0.3398284912109375\n",
      "Epoch 2[318/625] Time:0.703, Train Loss:0.4303455054759979\n",
      "Epoch 2[319/625] Time:0.711, Train Loss:0.40499573945999146\n",
      "Epoch 2[320/625] Time:0.694, Train Loss:0.33769288659095764\n",
      "Epoch 2[321/625] Time:0.693, Train Loss:0.35359296202659607\n",
      "Epoch 2[322/625] Time:0.698, Train Loss:0.3474085330963135\n",
      "Epoch 2[323/625] Time:0.694, Train Loss:0.3476603627204895\n",
      "Epoch 2[324/625] Time:0.694, Train Loss:0.32431018352508545\n",
      "Epoch 2[325/625] Time:0.694, Train Loss:0.33155539631843567\n",
      "Epoch 2[326/625] Time:0.739, Train Loss:0.3135758340358734\n",
      "Epoch 2[327/625] Time:0.693, Train Loss:0.31671255826950073\n",
      "Epoch 2[328/625] Time:0.694, Train Loss:0.3040834963321686\n",
      "Epoch 2[329/625] Time:0.695, Train Loss:0.35529839992523193\n",
      "Epoch 2[330/625] Time:0.703, Train Loss:0.28483113646507263\n",
      "Epoch 2[331/625] Time:0.694, Train Loss:0.2961898446083069\n",
      "Epoch 2[332/625] Time:0.694, Train Loss:0.28759369254112244\n",
      "Epoch 2[333/625] Time:0.694, Train Loss:0.44471973180770874\n",
      "Epoch 2[334/625] Time:0.695, Train Loss:0.38336944580078125\n",
      "Epoch 2[335/625] Time:0.694, Train Loss:0.31107988953590393\n",
      "Epoch 2[336/625] Time:0.694, Train Loss:0.31781205534935\n",
      "Epoch 2[337/625] Time:0.698, Train Loss:0.3845791518688202\n",
      "Epoch 2[338/625] Time:0.701, Train Loss:0.3837701678276062\n",
      "Epoch 2[339/625] Time:0.71, Train Loss:0.2871462106704712\n",
      "Epoch 2[340/625] Time:0.694, Train Loss:0.23912478983402252\n",
      "Epoch 2[341/625] Time:0.694, Train Loss:0.3638726472854614\n",
      "Epoch 2[342/625] Time:0.695, Train Loss:0.39040854573249817\n",
      "Epoch 2[343/625] Time:0.695, Train Loss:0.256395548582077\n",
      "Epoch 2[344/625] Time:0.694, Train Loss:0.4197920858860016\n",
      "Epoch 2[345/625] Time:0.694, Train Loss:0.4078100025653839\n",
      "Epoch 2[346/625] Time:0.694, Train Loss:0.33577483892440796\n",
      "Epoch 2[347/625] Time:0.696, Train Loss:0.39397746324539185\n",
      "Epoch 2[348/625] Time:0.695, Train Loss:0.2912585735321045\n",
      "Epoch 2[349/625] Time:0.696, Train Loss:0.40451180934906006\n",
      "Epoch 2[350/625] Time:0.694, Train Loss:0.3908902108669281\n",
      "Epoch 2[351/625] Time:0.715, Train Loss:0.3830444812774658\n",
      "Epoch 2[352/625] Time:0.693, Train Loss:0.38919106125831604\n",
      "Epoch 2[353/625] Time:0.695, Train Loss:0.36869558691978455\n",
      "Epoch 2[354/625] Time:0.695, Train Loss:0.35709431767463684\n",
      "Epoch 2[355/625] Time:0.695, Train Loss:0.4223429262638092\n",
      "Epoch 2[356/625] Time:0.721, Train Loss:0.43339261412620544\n",
      "Epoch 2[357/625] Time:0.703, Train Loss:0.31566035747528076\n",
      "Epoch 2[358/625] Time:0.71, Train Loss:0.3718099594116211\n",
      "Epoch 2[359/625] Time:0.694, Train Loss:0.36865103244781494\n",
      "Epoch 2[360/625] Time:0.694, Train Loss:0.37312257289886475\n",
      "Epoch 2[361/625] Time:0.694, Train Loss:0.32327067852020264\n",
      "Epoch 2[362/625] Time:0.694, Train Loss:0.36503759026527405\n",
      "Epoch 2[363/625] Time:0.694, Train Loss:0.2757410705089569\n",
      "Epoch 2[364/625] Time:0.693, Train Loss:0.33093908429145813\n",
      "Epoch 2[365/625] Time:0.732, Train Loss:0.32196804881095886\n",
      "Epoch 2[366/625] Time:0.702, Train Loss:0.3064647912979126\n",
      "Epoch 2[367/625] Time:0.703, Train Loss:0.29996851086616516\n",
      "Epoch 2[368/625] Time:0.704, Train Loss:0.3505519926548004\n",
      "Epoch 2[369/625] Time:0.703, Train Loss:0.32588180899620056\n",
      "Epoch 2[370/625] Time:0.704, Train Loss:0.3081507384777069\n",
      "Epoch 2[371/625] Time:0.693, Train Loss:0.3601014316082001\n",
      "Epoch 2[372/625] Time:0.694, Train Loss:0.2624186873435974\n",
      "Epoch 2[373/625] Time:0.697, Train Loss:0.39344462752342224\n",
      "Epoch 2[374/625] Time:0.691, Train Loss:0.25799474120140076\n",
      "Epoch 2[375/625] Time:0.723, Train Loss:0.29482024908065796\n",
      "Epoch 2[376/625] Time:0.693, Train Loss:0.3435201942920685\n",
      "Epoch 2[377/625] Time:0.694, Train Loss:0.31003695726394653\n",
      "Epoch 2[378/625] Time:0.694, Train Loss:0.33760228753089905\n",
      "Epoch 2[379/625] Time:0.694, Train Loss:0.30783864855766296\n",
      "Epoch 2[380/625] Time:0.694, Train Loss:0.4028056859970093\n",
      "Epoch 2[381/625] Time:0.697, Train Loss:0.33227232098579407\n",
      "Epoch 2[382/625] Time:0.71, Train Loss:0.3097039461135864\n",
      "Epoch 2[383/625] Time:0.693, Train Loss:0.34118321537971497\n",
      "Epoch 2[384/625] Time:0.693, Train Loss:0.3046610355377197\n",
      "Epoch 2[385/625] Time:0.695, Train Loss:0.33469507098197937\n",
      "Epoch 2[386/625] Time:0.695, Train Loss:0.38361111283302307\n",
      "Epoch 2[387/625] Time:0.703, Train Loss:0.3391745686531067\n",
      "Epoch 2[388/625] Time:0.694, Train Loss:0.2995259463787079\n",
      "Epoch 2[389/625] Time:0.695, Train Loss:0.297843873500824\n",
      "Epoch 2[390/625] Time:0.693, Train Loss:0.32380539178848267\n",
      "Epoch 2[391/625] Time:0.731, Train Loss:0.30910563468933105\n",
      "Epoch 2[392/625] Time:0.703, Train Loss:0.34202587604522705\n",
      "Epoch 2[393/625] Time:0.704, Train Loss:0.4021449089050293\n",
      "Epoch 2[394/625] Time:0.703, Train Loss:0.27987051010131836\n",
      "Epoch 2[395/625] Time:0.711, Train Loss:0.3535710871219635\n",
      "Epoch 2[396/625] Time:0.704, Train Loss:0.4196639358997345\n",
      "Epoch 2[397/625] Time:0.703, Train Loss:0.3179902732372284\n",
      "Epoch 2[398/625] Time:0.702, Train Loss:0.3414209187030792\n",
      "Epoch 2[399/625] Time:0.704, Train Loss:0.2858806550502777\n",
      "Epoch 2[400/625] Time:0.703, Train Loss:0.3957254886627197\n",
      "Epoch 2[401/625] Time:0.739, Train Loss:0.44076552987098694\n",
      "Epoch 2[402/625] Time:0.696, Train Loss:0.36353954672813416\n",
      "Epoch 2[403/625] Time:0.703, Train Loss:0.2709275782108307\n",
      "Epoch 2[404/625] Time:0.719, Train Loss:0.4274672865867615\n",
      "Epoch 2[405/625] Time:0.736, Train Loss:0.3575550615787506\n",
      "Epoch 2[406/625] Time:0.703, Train Loss:0.3554309606552124\n",
      "Epoch 2[407/625] Time:0.708, Train Loss:0.326162725687027\n",
      "Epoch 2[408/625] Time:0.73, Train Loss:0.39285603165626526\n",
      "Epoch 2[409/625] Time:0.693, Train Loss:0.2784970700740814\n",
      "Epoch 2[410/625] Time:0.694, Train Loss:0.30798977613449097\n",
      "Epoch 2[411/625] Time:0.73, Train Loss:0.31437522172927856\n",
      "Epoch 2[412/625] Time:0.706, Train Loss:0.4140545427799225\n",
      "Epoch 2[413/625] Time:0.694, Train Loss:0.2961493730545044\n",
      "Epoch 2[414/625] Time:0.693, Train Loss:0.40699756145477295\n",
      "Epoch 2[415/625] Time:0.694, Train Loss:0.3620394766330719\n",
      "Epoch 2[416/625] Time:0.696, Train Loss:0.3246156573295593\n",
      "Epoch 2[417/625] Time:0.694, Train Loss:0.3661441206932068\n",
      "Epoch 2[418/625] Time:0.695, Train Loss:0.4109646677970886\n",
      "Epoch 2[419/625] Time:0.695, Train Loss:0.2853175103664398\n",
      "Epoch 2[420/625] Time:0.694, Train Loss:0.3805544972419739\n",
      "Epoch 2[421/625] Time:0.721, Train Loss:0.39657795429229736\n",
      "Epoch 2[422/625] Time:0.696, Train Loss:0.3830333650112152\n",
      "Epoch 2[423/625] Time:0.694, Train Loss:0.30324265360832214\n",
      "Epoch 2[424/625] Time:0.694, Train Loss:0.3679962754249573\n",
      "Epoch 2[425/625] Time:0.705, Train Loss:0.3599259555339813\n",
      "Epoch 2[426/625] Time:0.699, Train Loss:0.4028720557689667\n",
      "Epoch 2[427/625] Time:0.695, Train Loss:0.3285895884037018\n",
      "Epoch 2[428/625] Time:0.693, Train Loss:0.35092490911483765\n",
      "Epoch 2[429/625] Time:0.731, Train Loss:0.40795794129371643\n",
      "Epoch 2[430/625] Time:0.692, Train Loss:0.3434028625488281\n",
      "Epoch 2[431/625] Time:0.694, Train Loss:0.3214569687843323\n",
      "Epoch 2[432/625] Time:0.693, Train Loss:0.30185946822166443\n",
      "Epoch 2[433/625] Time:0.693, Train Loss:0.30038735270500183\n",
      "Epoch 2[434/625] Time:0.693, Train Loss:0.35231417417526245\n",
      "Epoch 2[435/625] Time:0.692, Train Loss:0.32189124822616577\n",
      "Epoch 2[436/625] Time:0.694, Train Loss:0.26809561252593994\n",
      "Epoch 2[437/625] Time:0.732, Train Loss:0.31503012776374817\n",
      "Epoch 2[438/625] Time:0.705, Train Loss:0.22365036606788635\n",
      "Epoch 2[439/625] Time:0.695, Train Loss:0.340934693813324\n",
      "Epoch 2[440/625] Time:0.695, Train Loss:0.36540988087654114\n",
      "Epoch 2[441/625] Time:0.695, Train Loss:0.2823929190635681\n",
      "Epoch 2[442/625] Time:0.694, Train Loss:0.41446712613105774\n",
      "Epoch 2[443/625] Time:0.707, Train Loss:0.3055272102355957\n",
      "Epoch 2[444/625] Time:0.703, Train Loss:0.3137071132659912\n",
      "Epoch 2[445/625] Time:0.703, Train Loss:0.23909877240657806\n",
      "Epoch 2[446/625] Time:0.738, Train Loss:0.31589174270629883\n",
      "Epoch 2[447/625] Time:0.703, Train Loss:0.37813931703567505\n",
      "Epoch 2[448/625] Time:0.702, Train Loss:0.34484535455703735\n",
      "Epoch 2[449/625] Time:0.703, Train Loss:0.2659948766231537\n",
      "Epoch 2[450/625] Time:0.703, Train Loss:0.3868446946144104\n",
      "Epoch 2[451/625] Time:0.704, Train Loss:0.31468191742897034\n",
      "Epoch 2[452/625] Time:0.703, Train Loss:0.2494903802871704\n",
      "Epoch 2[453/625] Time:0.704, Train Loss:0.27603670954704285\n",
      "Epoch 2[454/625] Time:0.703, Train Loss:0.3533363938331604\n",
      "Epoch 2[455/625] Time:0.703, Train Loss:0.3411479890346527\n",
      "Epoch 2[456/625] Time:0.702, Train Loss:0.3688134551048279\n",
      "Epoch 2[457/625] Time:0.705, Train Loss:0.2927302122116089\n",
      "Epoch 2[458/625] Time:0.705, Train Loss:0.5280466079711914\n",
      "Epoch 2[459/625] Time:0.693, Train Loss:0.34931251406669617\n",
      "Epoch 2[460/625] Time:0.694, Train Loss:0.37597715854644775\n",
      "Epoch 2[461/625] Time:0.695, Train Loss:0.338640421628952\n",
      "Epoch 2[462/625] Time:0.694, Train Loss:0.327033132314682\n",
      "Epoch 2[463/625] Time:0.694, Train Loss:0.2800716161727905\n",
      "Epoch 2[464/625] Time:0.695, Train Loss:0.2826111912727356\n",
      "Epoch 2[465/625] Time:0.692, Train Loss:0.33331120014190674\n",
      "Epoch 2[466/625] Time:0.694, Train Loss:0.30619075894355774\n",
      "Epoch 2[467/625] Time:0.693, Train Loss:0.4364052712917328\n",
      "Epoch 2[468/625] Time:0.692, Train Loss:0.32186001539230347\n",
      "Epoch 2[469/625] Time:0.696, Train Loss:0.3869108259677887\n",
      "Epoch 2[470/625] Time:0.707, Train Loss:0.3996627926826477\n",
      "Epoch 2[471/625] Time:0.693, Train Loss:0.23268739879131317\n",
      "Epoch 2[472/625] Time:0.694, Train Loss:0.3508395552635193\n",
      "Epoch 2[473/625] Time:0.701, Train Loss:0.4041048288345337\n",
      "Epoch 2[474/625] Time:0.695, Train Loss:0.28692081570625305\n",
      "Epoch 2[475/625] Time:0.695, Train Loss:0.31517162919044495\n",
      "Epoch 2[476/625] Time:0.693, Train Loss:0.3713952898979187\n",
      "Epoch 2[477/625] Time:0.7, Train Loss:0.32315877079963684\n",
      "Epoch 2[478/625] Time:0.702, Train Loss:0.37394651770591736\n",
      "Epoch 2[479/625] Time:0.695, Train Loss:0.3894970417022705\n",
      "Epoch 2[480/625] Time:0.694, Train Loss:0.3291965126991272\n",
      "Epoch 2[481/625] Time:0.694, Train Loss:0.36067894101142883\n",
      "Epoch 2[482/625] Time:0.693, Train Loss:0.30951234698295593\n",
      "Epoch 2[483/625] Time:0.693, Train Loss:0.36633405089378357\n",
      "Epoch 2[484/625] Time:0.694, Train Loss:0.43801364302635193\n",
      "Epoch 2[485/625] Time:0.694, Train Loss:0.3247785270214081\n",
      "Epoch 2[486/625] Time:0.694, Train Loss:0.28761041164398193\n",
      "Epoch 2[487/625] Time:0.695, Train Loss:0.40123602747917175\n",
      "Epoch 2[488/625] Time:0.694, Train Loss:0.4245430529117584\n",
      "Epoch 2[489/625] Time:0.693, Train Loss:0.3603401482105255\n",
      "Epoch 2[490/625] Time:0.694, Train Loss:0.30272385478019714\n",
      "Epoch 2[491/625] Time:0.735, Train Loss:0.30688169598579407\n",
      "Epoch 2[492/625] Time:0.701, Train Loss:0.31853312253952026\n",
      "Epoch 2[493/625] Time:0.706, Train Loss:0.2925729751586914\n",
      "Epoch 2[494/625] Time:0.708, Train Loss:0.29568472504615784\n",
      "Epoch 2[495/625] Time:0.693, Train Loss:0.40317752957344055\n",
      "Epoch 2[496/625] Time:0.696, Train Loss:0.27924513816833496\n",
      "Epoch 2[497/625] Time:0.693, Train Loss:0.3198802173137665\n",
      "Epoch 2[498/625] Time:0.708, Train Loss:0.3979824483394623\n",
      "Epoch 2[499/625] Time:0.693, Train Loss:0.3156628906726837\n",
      "Epoch 2[500/625] Time:0.693, Train Loss:0.23149363696575165\n",
      "Epoch 2[501/625] Time:0.694, Train Loss:0.3772405683994293\n",
      "Epoch 2[502/625] Time:0.694, Train Loss:0.29490214586257935\n",
      "Epoch 2[503/625] Time:0.693, Train Loss:0.3450238108634949\n",
      "Epoch 2[504/625] Time:0.702, Train Loss:0.3062291741371155\n",
      "Epoch 2[505/625] Time:0.693, Train Loss:0.2885802984237671\n",
      "Epoch 2[506/625] Time:0.694, Train Loss:0.33998849987983704\n",
      "Epoch 2[507/625] Time:0.694, Train Loss:0.29278501868247986\n",
      "Epoch 2[508/625] Time:0.709, Train Loss:0.2658099830150604\n",
      "Epoch 2[509/625] Time:0.704, Train Loss:0.3196903169155121\n",
      "Epoch 2[510/625] Time:0.721, Train Loss:0.34312161803245544\n",
      "Epoch 2[511/625] Time:0.713, Train Loss:0.31685709953308105\n",
      "Epoch 2[512/625] Time:0.694, Train Loss:0.37941625714302063\n",
      "Epoch 2[513/625] Time:0.708, Train Loss:0.3552965521812439\n",
      "Epoch 2[514/625] Time:0.703, Train Loss:0.29224756360054016\n",
      "Epoch 2[515/625] Time:0.693, Train Loss:0.38626405596733093\n",
      "Epoch 2[516/625] Time:0.693, Train Loss:0.35428765416145325\n",
      "Epoch 2[517/625] Time:0.693, Train Loss:0.3165636360645294\n",
      "Epoch 2[518/625] Time:0.694, Train Loss:0.3112809360027313\n",
      "Epoch 2[519/625] Time:0.694, Train Loss:0.36342567205429077\n",
      "Epoch 2[520/625] Time:0.694, Train Loss:0.3578163981437683\n",
      "Epoch 2[521/625] Time:0.696, Train Loss:0.3040362596511841\n",
      "Epoch 2[522/625] Time:0.697, Train Loss:0.30062758922576904\n",
      "Epoch 2[523/625] Time:0.695, Train Loss:0.29120928049087524\n",
      "Epoch 2[524/625] Time:0.694, Train Loss:0.3173661231994629\n",
      "Epoch 2[525/625] Time:0.719, Train Loss:0.30920830368995667\n",
      "Epoch 2[526/625] Time:0.72, Train Loss:0.36683499813079834\n",
      "Epoch 2[527/625] Time:0.699, Train Loss:0.3823838233947754\n",
      "Epoch 2[528/625] Time:0.694, Train Loss:0.3300895392894745\n",
      "Epoch 2[529/625] Time:0.694, Train Loss:0.40645188093185425\n",
      "Epoch 2[530/625] Time:0.694, Train Loss:0.4343319535255432\n",
      "Epoch 2[531/625] Time:0.695, Train Loss:0.30513450503349304\n",
      "Epoch 2[532/625] Time:0.724, Train Loss:0.3195127844810486\n",
      "Epoch 2[533/625] Time:0.696, Train Loss:0.329076886177063\n",
      "Epoch 2[534/625] Time:0.694, Train Loss:0.3144916892051697\n",
      "Epoch 2[535/625] Time:0.706, Train Loss:0.3218226432800293\n",
      "Epoch 2[536/625] Time:0.744, Train Loss:0.38070982694625854\n",
      "Epoch 2[537/625] Time:0.707, Train Loss:0.4423394501209259\n",
      "Epoch 2[538/625] Time:0.705, Train Loss:0.303986519575119\n",
      "Epoch 2[539/625] Time:0.703, Train Loss:0.2789399027824402\n",
      "Epoch 2[540/625] Time:0.704, Train Loss:0.3852043151855469\n",
      "Epoch 2[541/625] Time:0.702, Train Loss:0.4624171853065491\n",
      "Epoch 2[542/625] Time:0.703, Train Loss:0.36846160888671875\n",
      "Epoch 2[543/625] Time:0.706, Train Loss:0.3705030679702759\n",
      "Epoch 2[544/625] Time:0.708, Train Loss:0.3669036626815796\n",
      "Epoch 2[545/625] Time:0.702, Train Loss:0.28060635924339294\n",
      "Epoch 2[546/625] Time:0.703, Train Loss:0.38211414217948914\n",
      "Epoch 2[547/625] Time:0.703, Train Loss:0.32244232296943665\n",
      "Epoch 2[548/625] Time:0.693, Train Loss:0.29930558800697327\n",
      "Epoch 2[549/625] Time:0.725, Train Loss:0.3220221698284149\n",
      "Epoch 2[550/625] Time:0.727, Train Loss:0.28890901803970337\n",
      "Epoch 2[551/625] Time:0.692, Train Loss:0.3908439874649048\n",
      "Epoch 2[552/625] Time:0.694, Train Loss:0.34850654006004333\n",
      "Epoch 2[553/625] Time:0.694, Train Loss:0.2869417369365692\n",
      "Epoch 2[554/625] Time:0.693, Train Loss:0.4231667220592499\n",
      "Epoch 2[555/625] Time:0.694, Train Loss:0.40591931343078613\n",
      "Epoch 2[556/625] Time:0.694, Train Loss:0.3131450414657593\n",
      "Epoch 2[557/625] Time:0.694, Train Loss:0.31147336959838867\n",
      "Epoch 2[558/625] Time:0.699, Train Loss:0.280002623796463\n",
      "Epoch 2[559/625] Time:0.693, Train Loss:0.33479219675064087\n",
      "Epoch 2[560/625] Time:0.695, Train Loss:0.4016743004322052\n",
      "Epoch 2[561/625] Time:0.694, Train Loss:0.3828606605529785\n",
      "Epoch 2[562/625] Time:0.704, Train Loss:0.23356516659259796\n",
      "Epoch 2[563/625] Time:0.694, Train Loss:0.3161405026912689\n",
      "Epoch 2[564/625] Time:0.694, Train Loss:0.38819563388824463\n",
      "Epoch 2[565/625] Time:0.693, Train Loss:0.2874578833580017\n",
      "Epoch 2[566/625] Time:0.694, Train Loss:0.4020543098449707\n",
      "Epoch 2[567/625] Time:0.699, Train Loss:0.32529398798942566\n",
      "Epoch 2[568/625] Time:0.694, Train Loss:0.3132418692111969\n",
      "Epoch 2[569/625] Time:0.693, Train Loss:0.28941160440444946\n",
      "Epoch 2[570/625] Time:0.694, Train Loss:0.35634517669677734\n",
      "Epoch 2[571/625] Time:0.695, Train Loss:0.3332551121711731\n",
      "Epoch 2[572/625] Time:0.694, Train Loss:0.4037195146083832\n",
      "Epoch 2[573/625] Time:0.695, Train Loss:0.3912130892276764\n",
      "Epoch 2[574/625] Time:0.694, Train Loss:0.3146001100540161\n",
      "Epoch 2[575/625] Time:0.694, Train Loss:0.34314778447151184\n",
      "Epoch 2[576/625] Time:0.699, Train Loss:0.35707247257232666\n",
      "Epoch 2[577/625] Time:0.704, Train Loss:0.3424237072467804\n",
      "Epoch 2[578/625] Time:0.703, Train Loss:0.37058475613594055\n",
      "Epoch 2[579/625] Time:0.703, Train Loss:0.36700403690338135\n",
      "Epoch 2[580/625] Time:0.704, Train Loss:0.3384558856487274\n",
      "Epoch 2[581/625] Time:0.702, Train Loss:0.27566683292388916\n",
      "Epoch 2[582/625] Time:0.703, Train Loss:0.356910765171051\n",
      "Epoch 2[583/625] Time:0.703, Train Loss:0.3242979943752289\n",
      "Epoch 2[584/625] Time:0.702, Train Loss:0.32777923345565796\n",
      "Epoch 2[585/625] Time:0.702, Train Loss:0.328188955783844\n",
      "Epoch 2[586/625] Time:0.704, Train Loss:0.3130968511104584\n",
      "Epoch 2[587/625] Time:0.702, Train Loss:0.3668089807033539\n",
      "Epoch 2[588/625] Time:0.709, Train Loss:0.29436710476875305\n",
      "Epoch 2[589/625] Time:0.739, Train Loss:0.34242984652519226\n",
      "Epoch 2[590/625] Time:0.693, Train Loss:0.397626131772995\n",
      "Epoch 2[591/625] Time:0.695, Train Loss:0.3988250195980072\n",
      "Epoch 2[592/625] Time:0.728, Train Loss:0.3204328715801239\n",
      "Epoch 2[593/625] Time:0.693, Train Loss:0.3188730478286743\n",
      "Epoch 2[594/625] Time:0.694, Train Loss:0.25349774956703186\n",
      "Epoch 2[595/625] Time:0.694, Train Loss:0.364961177110672\n",
      "Epoch 2[596/625] Time:0.695, Train Loss:0.2728985846042633\n",
      "Epoch 2[597/625] Time:0.696, Train Loss:0.2587992250919342\n",
      "Epoch 2[598/625] Time:0.697, Train Loss:0.3281048536300659\n",
      "Epoch 2[599/625] Time:0.695, Train Loss:0.27922213077545166\n",
      "Epoch 2[600/625] Time:0.695, Train Loss:0.30243101716041565\n",
      "Epoch 2[601/625] Time:0.696, Train Loss:0.34649765491485596\n",
      "Epoch 2[602/625] Time:0.695, Train Loss:0.35169535875320435\n",
      "Epoch 2[603/625] Time:0.694, Train Loss:0.4178447425365448\n",
      "Epoch 2[604/625] Time:0.695, Train Loss:0.4102623462677002\n",
      "Epoch 2[605/625] Time:0.695, Train Loss:0.39962881803512573\n",
      "Epoch 2[606/625] Time:0.695, Train Loss:0.3923092186450958\n",
      "Epoch 2[607/625] Time:0.727, Train Loss:0.26119232177734375\n",
      "Epoch 2[608/625] Time:0.693, Train Loss:0.373879075050354\n",
      "Epoch 2[609/625] Time:0.694, Train Loss:0.369346022605896\n",
      "Epoch 2[610/625] Time:0.694, Train Loss:0.2708453834056854\n",
      "Epoch 2[611/625] Time:0.693, Train Loss:0.35719960927963257\n",
      "Epoch 2[612/625] Time:0.694, Train Loss:0.3295128047466278\n",
      "Epoch 2[613/625] Time:0.693, Train Loss:0.39931759238243103\n",
      "Epoch 2[614/625] Time:0.694, Train Loss:0.36503857374191284\n",
      "Epoch 2[615/625] Time:0.694, Train Loss:0.338515043258667\n",
      "Epoch 2[616/625] Time:0.694, Train Loss:0.3501756191253662\n",
      "Epoch 2[617/625] Time:0.696, Train Loss:0.30500859022140503\n",
      "Epoch 2[618/625] Time:0.694, Train Loss:0.26796773076057434\n",
      "Epoch 2[619/625] Time:0.694, Train Loss:0.3061392307281494\n",
      "Epoch 2[620/625] Time:0.698, Train Loss:0.2929176986217499\n",
      "Epoch 2[621/625] Time:0.694, Train Loss:0.38178834319114685\n",
      "Epoch 2[622/625] Time:0.694, Train Loss:0.3902420401573181\n",
      "Epoch 2[623/625] Time:0.694, Train Loss:0.3744606375694275\n",
      "Epoch 2[624/625] Time:0.694, Train Loss:0.3951859772205353\n",
      "Epoch 2[0/78] Val Loss:0.2019265741109848\n",
      "Epoch 2[1/78] Val Loss:0.19495059549808502\n",
      "Epoch 2[2/78] Val Loss:0.2204175740480423\n",
      "Epoch 2[3/78] Val Loss:0.19275052845478058\n",
      "Epoch 2[4/78] Val Loss:0.3000279664993286\n",
      "Epoch 2[5/78] Val Loss:0.2800047695636749\n",
      "Epoch 2[6/78] Val Loss:0.2559477984905243\n",
      "Epoch 2[7/78] Val Loss:0.35849297046661377\n",
      "Epoch 2[8/78] Val Loss:0.20485304296016693\n",
      "Epoch 2[9/78] Val Loss:0.1282782256603241\n",
      "Epoch 2[10/78] Val Loss:0.10122895985841751\n",
      "Epoch 2[11/78] Val Loss:0.1480236053466797\n",
      "Epoch 2[12/78] Val Loss:0.10722867399454117\n",
      "Epoch 2[13/78] Val Loss:0.10695669800043106\n",
      "Epoch 2[14/78] Val Loss:0.161953404545784\n",
      "Epoch 2[15/78] Val Loss:0.11811921745538712\n",
      "Epoch 2[16/78] Val Loss:0.1344570517539978\n",
      "Epoch 2[17/78] Val Loss:0.13459628820419312\n",
      "Epoch 2[18/78] Val Loss:0.3054850101470947\n",
      "Epoch 2[19/78] Val Loss:0.3815169632434845\n",
      "Epoch 2[20/78] Val Loss:0.2703922390937805\n",
      "Epoch 2[21/78] Val Loss:0.6243279576301575\n",
      "Epoch 2[22/78] Val Loss:0.8210216164588928\n",
      "Epoch 2[23/78] Val Loss:0.6183414459228516\n",
      "Epoch 2[24/78] Val Loss:0.504112720489502\n",
      "Epoch 2[25/78] Val Loss:0.5964720249176025\n",
      "Epoch 2[26/78] Val Loss:0.5814152956008911\n",
      "Epoch 2[27/78] Val Loss:0.5172273516654968\n",
      "Epoch 2[28/78] Val Loss:0.523018479347229\n",
      "Epoch 2[29/78] Val Loss:0.6161460280418396\n",
      "Epoch 2[30/78] Val Loss:1.502907156944275\n",
      "Epoch 2[31/78] Val Loss:1.2427798509597778\n",
      "Epoch 2[32/78] Val Loss:1.0051980018615723\n",
      "Epoch 2[33/78] Val Loss:0.5143920183181763\n",
      "Epoch 2[34/78] Val Loss:0.41782158613204956\n",
      "Epoch 2[35/78] Val Loss:0.4243605434894562\n",
      "Epoch 2[36/78] Val Loss:0.4635592997074127\n",
      "Epoch 2[37/78] Val Loss:0.5172790288925171\n",
      "Epoch 2[38/78] Val Loss:0.2620551884174347\n",
      "Epoch 2[39/78] Val Loss:0.2464887648820877\n",
      "Epoch 2[40/78] Val Loss:0.26024776697158813\n",
      "Epoch 2[41/78] Val Loss:0.26422953605651855\n",
      "Epoch 2[42/78] Val Loss:0.273448646068573\n",
      "Epoch 2[43/78] Val Loss:0.1560065746307373\n",
      "Epoch 2[44/78] Val Loss:0.10806106775999069\n",
      "Epoch 2[45/78] Val Loss:0.09599391371011734\n",
      "Epoch 2[46/78] Val Loss:0.11064571142196655\n",
      "Epoch 2[47/78] Val Loss:0.114525705575943\n",
      "Epoch 2[48/78] Val Loss:0.136005699634552\n",
      "Epoch 2[49/78] Val Loss:0.114944688975811\n",
      "Epoch 2[50/78] Val Loss:0.09325239062309265\n",
      "Epoch 2[51/78] Val Loss:0.1258210688829422\n",
      "Epoch 2[52/78] Val Loss:0.15047408640384674\n",
      "Epoch 2[53/78] Val Loss:0.11714203655719757\n",
      "Epoch 2[54/78] Val Loss:0.10175283998250961\n",
      "Epoch 2[55/78] Val Loss:0.1266373097896576\n",
      "Epoch 2[56/78] Val Loss:0.402358740568161\n",
      "Epoch 2[57/78] Val Loss:0.36402207612991333\n",
      "Epoch 2[58/78] Val Loss:0.33663198351860046\n",
      "Epoch 2[59/78] Val Loss:0.3650285601615906\n",
      "Epoch 2[60/78] Val Loss:0.36908721923828125\n",
      "Epoch 2[61/78] Val Loss:0.40335941314697266\n",
      "Epoch 2[62/78] Val Loss:0.43944913148880005\n",
      "Epoch 2[63/78] Val Loss:0.24171039462089539\n",
      "Epoch 2[64/78] Val Loss:0.14890481531620026\n",
      "Epoch 2[65/78] Val Loss:0.14348828792572021\n",
      "Epoch 2[66/78] Val Loss:0.17476870119571686\n",
      "Epoch 2[67/78] Val Loss:0.15480221807956696\n",
      "Epoch 2[68/78] Val Loss:0.45301753282546997\n",
      "Epoch 2[69/78] Val Loss:0.43491947650909424\n",
      "Epoch 2[70/78] Val Loss:0.4592829942703247\n",
      "Epoch 2[71/78] Val Loss:0.36449432373046875\n",
      "Epoch 2[72/78] Val Loss:0.16805778443813324\n",
      "Epoch 2[73/78] Val Loss:0.13310116529464722\n",
      "Epoch 2[74/78] Val Loss:0.6119247674942017\n",
      "Epoch 2[75/78] Val Loss:0.6539257764816284\n",
      "Epoch 2[76/78] Val Loss:0.6586989164352417\n",
      "Epoch 2[77/78] Val Loss:0.7128614187240601\n",
      "Epoch 2[78/78] Val Loss:0.8343999981880188\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90     15691\n",
      "           1       0.63      0.76      0.69      4309\n",
      "\n",
      "    accuracy                           0.85     20000\n",
      "   macro avg       0.78      0.82      0.80     20000\n",
      "weighted avg       0.87      0.85      0.86     20000\n",
      "\n",
      "Epoch 2: Train Loss 0.3447046779632568, Val Loss 0.3549232981525935, Train Time 805.8926358222961, Val Time 38.392953872680664\n",
      "Epoch 3[0/625] Time:0.68, Train Loss:0.3070506751537323\n",
      "Epoch 3[1/625] Time:0.696, Train Loss:0.3524184823036194\n",
      "Epoch 3[2/625] Time:0.696, Train Loss:0.3258286714553833\n",
      "Epoch 3[3/625] Time:0.695, Train Loss:0.37119629979133606\n",
      "Epoch 3[4/625] Time:0.717, Train Loss:0.3352653980255127\n",
      "Epoch 3[5/625] Time:0.702, Train Loss:0.3352784812450409\n",
      "Epoch 3[6/625] Time:0.702, Train Loss:0.25192195177078247\n",
      "Epoch 3[7/625] Time:0.705, Train Loss:0.3457542657852173\n",
      "Epoch 3[8/625] Time:0.704, Train Loss:0.3347156345844269\n",
      "Epoch 3[9/625] Time:0.704, Train Loss:0.350795179605484\n",
      "Epoch 3[10/625] Time:0.706, Train Loss:0.3158581852912903\n",
      "Epoch 3[11/625] Time:0.705, Train Loss:0.3812423646450043\n",
      "Epoch 3[12/625] Time:0.748, Train Loss:0.3164929449558258\n",
      "Epoch 3[13/625] Time:0.705, Train Loss:0.2905184030532837\n",
      "Epoch 3[14/625] Time:0.704, Train Loss:0.3433636426925659\n",
      "Epoch 3[15/625] Time:0.704, Train Loss:0.3110722601413727\n",
      "Epoch 3[16/625] Time:0.705, Train Loss:0.33709460496902466\n",
      "Epoch 3[17/625] Time:0.705, Train Loss:0.31671833992004395\n",
      "Epoch 3[18/625] Time:0.704, Train Loss:0.2998422384262085\n",
      "Epoch 3[19/625] Time:0.703, Train Loss:0.28168919682502747\n",
      "Epoch 3[20/625] Time:0.747, Train Loss:0.39947205781936646\n",
      "Epoch 3[21/625] Time:0.692, Train Loss:0.34325119853019714\n",
      "Epoch 3[22/625] Time:0.693, Train Loss:0.33393561840057373\n",
      "Epoch 3[23/625] Time:0.693, Train Loss:0.3707171082496643\n",
      "Epoch 3[24/625] Time:0.694, Train Loss:0.35570141673088074\n",
      "Epoch 3[25/625] Time:0.715, Train Loss:0.3197694718837738\n",
      "Epoch 3[26/625] Time:0.695, Train Loss:0.2939881682395935\n",
      "Epoch 3[27/625] Time:0.702, Train Loss:0.2654139995574951\n",
      "Epoch 3[28/625] Time:0.696, Train Loss:0.3968827426433563\n",
      "Epoch 3[29/625] Time:0.693, Train Loss:0.3710182309150696\n",
      "Epoch 3[30/625] Time:0.693, Train Loss:0.3543488085269928\n",
      "Epoch 3[31/625] Time:0.693, Train Loss:0.3209594190120697\n",
      "Epoch 3[32/625] Time:0.693, Train Loss:0.3434028625488281\n",
      "Epoch 3[33/625] Time:0.693, Train Loss:0.3797840476036072\n",
      "Epoch 3[34/625] Time:0.696, Train Loss:0.38131752610206604\n",
      "Epoch 3[35/625] Time:0.695, Train Loss:0.29031455516815186\n",
      "Epoch 3[36/625] Time:0.694, Train Loss:0.3404773771762848\n",
      "Epoch 3[37/625] Time:0.694, Train Loss:0.29645514488220215\n",
      "Epoch 3[38/625] Time:0.694, Train Loss:0.3052957057952881\n",
      "Epoch 3[39/625] Time:0.707, Train Loss:0.3575174808502197\n",
      "Epoch 3[40/625] Time:0.693, Train Loss:0.29707905650138855\n",
      "Epoch 3[41/625] Time:0.694, Train Loss:0.3060535788536072\n",
      "Epoch 3[42/625] Time:0.694, Train Loss:0.3825956881046295\n",
      "Epoch 3[43/625] Time:0.71, Train Loss:0.24250446259975433\n",
      "Epoch 3[44/625] Time:0.694, Train Loss:0.26351988315582275\n",
      "Epoch 3[45/625] Time:0.693, Train Loss:0.30913421511650085\n",
      "Epoch 3[46/625] Time:0.695, Train Loss:0.3229823410511017\n",
      "Epoch 3[47/625] Time:0.696, Train Loss:0.30016911029815674\n",
      "Epoch 3[48/625] Time:0.692, Train Loss:0.3839769661426544\n",
      "Epoch 3[49/625] Time:0.693, Train Loss:0.32434916496276855\n",
      "Epoch 3[50/625] Time:0.704, Train Loss:0.3661772906780243\n",
      "Epoch 3[51/625] Time:0.696, Train Loss:0.32082080841064453\n",
      "Epoch 3[52/625] Time:0.693, Train Loss:0.38371917605400085\n",
      "Epoch 3[53/625] Time:0.694, Train Loss:0.42726966738700867\n",
      "Epoch 3[54/625] Time:0.694, Train Loss:0.26973190903663635\n",
      "Epoch 3[55/625] Time:0.694, Train Loss:0.3272572457790375\n",
      "Epoch 3[56/625] Time:0.693, Train Loss:0.4424348771572113\n",
      "Epoch 3[57/625] Time:0.705, Train Loss:0.3696909546852112\n",
      "Epoch 3[58/625] Time:0.701, Train Loss:0.33134517073631287\n",
      "Epoch 3[59/625] Time:0.695, Train Loss:0.36125901341438293\n",
      "Epoch 3[60/625] Time:0.693, Train Loss:0.2878199815750122\n",
      "Epoch 3[61/625] Time:0.693, Train Loss:0.27855348587036133\n",
      "Epoch 3[62/625] Time:0.693, Train Loss:0.34559765458106995\n",
      "Epoch 3[63/625] Time:0.694, Train Loss:0.37109363079071045\n",
      "Epoch 3[64/625] Time:0.694, Train Loss:0.345721036195755\n",
      "Epoch 3[65/625] Time:0.693, Train Loss:0.34960076212882996\n",
      "Epoch 3[66/625] Time:0.695, Train Loss:0.3167542517185211\n",
      "Epoch 3[67/625] Time:0.695, Train Loss:0.4531438648700714\n",
      "Epoch 3[68/625] Time:0.693, Train Loss:0.35487470030784607\n",
      "Epoch 3[69/625] Time:0.694, Train Loss:0.3429371118545532\n",
      "Epoch 3[70/625] Time:0.721, Train Loss:0.3235126733779907\n",
      "Epoch 3[71/625] Time:0.705, Train Loss:0.3251488208770752\n",
      "Epoch 3[72/625] Time:0.694, Train Loss:0.3067857623100281\n",
      "Epoch 3[73/625] Time:0.703, Train Loss:0.31420090794563293\n",
      "Epoch 3[74/625] Time:0.694, Train Loss:0.33995500206947327\n",
      "Epoch 3[75/625] Time:0.694, Train Loss:0.3061721920967102\n",
      "Epoch 3[76/625] Time:0.693, Train Loss:0.3550981283187866\n",
      "Epoch 3[77/625] Time:0.693, Train Loss:0.3420376777648926\n",
      "Epoch 3[78/625] Time:0.7, Train Loss:0.33381137251853943\n",
      "Epoch 3[79/625] Time:0.704, Train Loss:0.2921482026576996\n",
      "Epoch 3[80/625] Time:0.694, Train Loss:0.38060033321380615\n",
      "Epoch 3[81/625] Time:0.695, Train Loss:0.32032665610313416\n",
      "Epoch 3[82/625] Time:0.694, Train Loss:0.3550778031349182\n",
      "Epoch 3[83/625] Time:0.695, Train Loss:0.3119543194770813\n",
      "Epoch 3[84/625] Time:0.703, Train Loss:0.3587002754211426\n",
      "Epoch 3[85/625] Time:0.702, Train Loss:0.3951253294944763\n",
      "Epoch 3[86/625] Time:0.703, Train Loss:0.29169511795043945\n",
      "Epoch 3[87/625] Time:0.705, Train Loss:0.3886301815509796\n",
      "Epoch 3[88/625] Time:0.705, Train Loss:0.2750607430934906\n",
      "Epoch 3[89/625] Time:0.705, Train Loss:0.3457287549972534\n",
      "Epoch 3[90/625] Time:0.704, Train Loss:0.35914695262908936\n",
      "Epoch 3[91/625] Time:0.702, Train Loss:0.2969313859939575\n",
      "Epoch 3[92/625] Time:0.703, Train Loss:0.28241461515426636\n",
      "Epoch 3[93/625] Time:0.704, Train Loss:0.3954005837440491\n",
      "Epoch 3[94/625] Time:0.703, Train Loss:0.35619688034057617\n",
      "Epoch 3[95/625] Time:0.702, Train Loss:0.3395693302154541\n",
      "Epoch 3[96/625] Time:0.708, Train Loss:0.31826892495155334\n",
      "Epoch 3[97/625] Time:0.693, Train Loss:0.37481656670570374\n",
      "Epoch 3[98/625] Time:0.693, Train Loss:0.3688565790653229\n",
      "Epoch 3[99/625] Time:0.694, Train Loss:0.3471117913722992\n",
      "Epoch 3[100/625] Time:0.693, Train Loss:0.4691798985004425\n",
      "Epoch 3[101/625] Time:0.694, Train Loss:0.31282201409339905\n",
      "Epoch 3[102/625] Time:0.694, Train Loss:0.3307785093784332\n",
      "Epoch 3[103/625] Time:0.694, Train Loss:0.38699454069137573\n",
      "Epoch 3[104/625] Time:0.695, Train Loss:0.31153640151023865\n",
      "Epoch 3[105/625] Time:0.699, Train Loss:0.34200823307037354\n",
      "Epoch 3[106/625] Time:0.694, Train Loss:0.30834999680519104\n",
      "Epoch 3[107/625] Time:0.695, Train Loss:0.33722418546676636\n",
      "Epoch 3[108/625] Time:0.695, Train Loss:0.28448405861854553\n",
      "Epoch 3[109/625] Time:0.736, Train Loss:0.3792235553264618\n",
      "Epoch 3[110/625] Time:0.696, Train Loss:0.3799031376838684\n",
      "Epoch 3[111/625] Time:0.695, Train Loss:0.33703988790512085\n",
      "Epoch 3[112/625] Time:0.696, Train Loss:0.35731905698776245\n",
      "Epoch 3[113/625] Time:0.695, Train Loss:0.3451668620109558\n",
      "Epoch 3[114/625] Time:0.695, Train Loss:0.28748416900634766\n",
      "Epoch 3[115/625] Time:0.695, Train Loss:0.3180243670940399\n",
      "Epoch 3[116/625] Time:0.695, Train Loss:0.3778398633003235\n",
      "Epoch 3[117/625] Time:0.694, Train Loss:0.3063064217567444\n",
      "Epoch 3[118/625] Time:0.694, Train Loss:0.3122221529483795\n",
      "Epoch 3[119/625] Time:0.695, Train Loss:0.3686183989048004\n",
      "Epoch 3[120/625] Time:0.695, Train Loss:0.33659300208091736\n",
      "Epoch 3[121/625] Time:0.695, Train Loss:0.3683266341686249\n",
      "Epoch 3[122/625] Time:0.694, Train Loss:0.357776403427124\n",
      "Epoch 3[123/625] Time:0.7, Train Loss:0.3350122570991516\n",
      "Epoch 3[124/625] Time:0.695, Train Loss:0.3642026484012604\n",
      "Epoch 3[125/625] Time:0.695, Train Loss:0.3506886959075928\n",
      "Epoch 3[126/625] Time:0.694, Train Loss:0.29644036293029785\n",
      "Epoch 3[127/625] Time:0.711, Train Loss:0.30682429671287537\n",
      "Epoch 3[128/625] Time:0.709, Train Loss:0.35840579867362976\n",
      "Epoch 3[129/625] Time:0.716, Train Loss:0.3340664207935333\n",
      "Epoch 3[130/625] Time:0.693, Train Loss:0.3458704650402069\n",
      "Epoch 3[131/625] Time:0.696, Train Loss:0.3249306082725525\n",
      "Epoch 3[132/625] Time:0.694, Train Loss:0.3351801335811615\n",
      "Epoch 3[133/625] Time:0.695, Train Loss:0.3653258979320526\n",
      "Epoch 3[134/625] Time:0.695, Train Loss:0.4166761040687561\n",
      "Epoch 3[135/625] Time:0.694, Train Loss:0.3008766174316406\n",
      "Epoch 3[136/625] Time:0.699, Train Loss:0.3338526487350464\n",
      "Epoch 3[137/625] Time:0.694, Train Loss:0.328056663274765\n",
      "Epoch 3[138/625] Time:0.694, Train Loss:0.2997383177280426\n",
      "Epoch 3[139/625] Time:0.719, Train Loss:0.28781530261039734\n",
      "Epoch 3[140/625] Time:0.704, Train Loss:0.34544122219085693\n",
      "Epoch 3[141/625] Time:0.703, Train Loss:0.30878740549087524\n",
      "Epoch 3[142/625] Time:0.702, Train Loss:0.31905651092529297\n",
      "Epoch 3[143/625] Time:0.703, Train Loss:0.2662069797515869\n",
      "Epoch 3[144/625] Time:0.705, Train Loss:0.2979375123977661\n",
      "Epoch 3[145/625] Time:0.693, Train Loss:0.3748471736907959\n",
      "Epoch 3[146/625] Time:0.694, Train Loss:0.34745463728904724\n",
      "Epoch 3[147/625] Time:0.694, Train Loss:0.4248508810997009\n",
      "Epoch 3[148/625] Time:0.717, Train Loss:0.30645453929901123\n",
      "Epoch 3[149/625] Time:0.692, Train Loss:0.34331101179122925\n",
      "Epoch 3[150/625] Time:0.694, Train Loss:0.30033186078071594\n",
      "Epoch 3[151/625] Time:0.693, Train Loss:0.29155030846595764\n",
      "Epoch 3[152/625] Time:0.694, Train Loss:0.3332010507583618\n",
      "Epoch 3[153/625] Time:0.693, Train Loss:0.3120879530906677\n",
      "Epoch 3[154/625] Time:0.694, Train Loss:0.4636552929878235\n",
      "Epoch 3[155/625] Time:0.734, Train Loss:0.3631910979747772\n",
      "Epoch 3[156/625] Time:0.702, Train Loss:0.2450130134820938\n",
      "Epoch 3[157/625] Time:0.694, Train Loss:0.2923354208469391\n",
      "Epoch 3[158/625] Time:0.693, Train Loss:0.3379570245742798\n",
      "Epoch 3[159/625] Time:0.695, Train Loss:0.33119356632232666\n",
      "Epoch 3[160/625] Time:0.693, Train Loss:0.3630307912826538\n",
      "Epoch 3[161/625] Time:0.694, Train Loss:0.31793326139450073\n",
      "Epoch 3[162/625] Time:0.694, Train Loss:0.29571533203125\n",
      "Epoch 3[163/625] Time:0.694, Train Loss:0.3996710181236267\n",
      "Epoch 3[164/625] Time:0.695, Train Loss:0.3330182731151581\n",
      "Epoch 3[165/625] Time:0.694, Train Loss:0.3968060314655304\n",
      "Epoch 3[166/625] Time:0.693, Train Loss:0.30782604217529297\n",
      "Epoch 3[167/625] Time:0.696, Train Loss:0.3482542634010315\n",
      "Epoch 3[168/625] Time:0.694, Train Loss:0.30503034591674805\n",
      "Epoch 3[169/625] Time:0.694, Train Loss:0.42988041043281555\n",
      "Epoch 3[170/625] Time:0.694, Train Loss:0.46416476368904114\n",
      "Epoch 3[171/625] Time:0.694, Train Loss:0.328041136264801\n",
      "Epoch 3[172/625] Time:0.694, Train Loss:0.3525753915309906\n",
      "Epoch 3[173/625] Time:0.694, Train Loss:0.2848878502845764\n",
      "Epoch 3[174/625] Time:0.694, Train Loss:0.3311483561992645\n",
      "Epoch 3[175/625] Time:0.695, Train Loss:0.283126562833786\n",
      "Epoch 3[176/625] Time:0.694, Train Loss:0.3609446883201599\n",
      "Epoch 3[177/625] Time:0.695, Train Loss:0.3199875056743622\n",
      "Epoch 3[178/625] Time:0.694, Train Loss:0.35257816314697266\n",
      "Epoch 3[179/625] Time:0.694, Train Loss:0.27730152010917664\n",
      "Epoch 3[180/625] Time:0.695, Train Loss:0.3222868740558624\n",
      "Epoch 3[181/625] Time:0.696, Train Loss:0.30391833186149597\n",
      "Epoch 3[182/625] Time:0.694, Train Loss:0.3221919536590576\n",
      "Epoch 3[183/625] Time:0.699, Train Loss:0.3100726008415222\n",
      "Epoch 3[184/625] Time:0.708, Train Loss:0.28417518734931946\n",
      "Epoch 3[185/625] Time:0.702, Train Loss:0.322325199842453\n",
      "Epoch 3[186/625] Time:0.693, Train Loss:0.32694700360298157\n",
      "Epoch 3[187/625] Time:0.715, Train Loss:0.24079680442810059\n",
      "Epoch 3[188/625] Time:0.702, Train Loss:0.2717110514640808\n",
      "Epoch 3[189/625] Time:0.704, Train Loss:0.33534759283065796\n",
      "Epoch 3[190/625] Time:0.704, Train Loss:0.4034844636917114\n",
      "Epoch 3[191/625] Time:0.72, Train Loss:0.30570584535598755\n",
      "Epoch 3[192/625] Time:0.694, Train Loss:0.3042961061000824\n",
      "Epoch 3[193/625] Time:0.695, Train Loss:0.37673649191856384\n",
      "Epoch 3[194/625] Time:0.694, Train Loss:0.3855746388435364\n",
      "Epoch 3[195/625] Time:0.694, Train Loss:0.38329359889030457\n",
      "Epoch 3[196/625] Time:0.696, Train Loss:0.3652392327785492\n",
      "Epoch 3[197/625] Time:0.695, Train Loss:0.3265683352947235\n",
      "Epoch 3[198/625] Time:0.694, Train Loss:0.2778949439525604\n",
      "Epoch 3[199/625] Time:0.695, Train Loss:0.32915329933166504\n",
      "Epoch 3[200/625] Time:0.694, Train Loss:0.3105432093143463\n",
      "Epoch 3[201/625] Time:0.694, Train Loss:0.2871914803981781\n",
      "Epoch 3[202/625] Time:0.694, Train Loss:0.2331559956073761\n",
      "Epoch 3[203/625] Time:0.693, Train Loss:0.38352102041244507\n",
      "Epoch 3[204/625] Time:0.694, Train Loss:0.3273584246635437\n",
      "Epoch 3[205/625] Time:0.694, Train Loss:0.350369930267334\n",
      "Epoch 3[206/625] Time:0.693, Train Loss:0.42341506481170654\n",
      "Epoch 3[207/625] Time:0.728, Train Loss:0.3528512120246887\n",
      "Epoch 3[208/625] Time:0.702, Train Loss:0.3320334553718567\n",
      "Epoch 3[209/625] Time:0.703, Train Loss:0.2993260622024536\n",
      "Epoch 3[210/625] Time:0.703, Train Loss:0.2933048605918884\n",
      "Epoch 3[211/625] Time:0.704, Train Loss:0.28949064016342163\n",
      "Epoch 3[212/625] Time:0.705, Train Loss:0.35612982511520386\n",
      "Epoch 3[213/625] Time:0.736, Train Loss:0.38224637508392334\n",
      "Epoch 3[214/625] Time:0.725, Train Loss:0.2846788763999939\n",
      "Epoch 3[215/625] Time:0.694, Train Loss:0.2587677836418152\n",
      "Epoch 3[216/625] Time:0.692, Train Loss:0.3303837180137634\n",
      "Epoch 3[217/625] Time:0.693, Train Loss:0.34175363183021545\n",
      "Epoch 3[218/625] Time:0.694, Train Loss:0.28538307547569275\n",
      "Epoch 3[219/625] Time:0.693, Train Loss:0.3680334687232971\n",
      "Epoch 3[220/625] Time:0.694, Train Loss:0.2567015290260315\n",
      "Epoch 3[221/625] Time:0.693, Train Loss:0.3410108983516693\n",
      "Epoch 3[222/625] Time:0.693, Train Loss:0.2862427830696106\n",
      "Epoch 3[223/625] Time:0.692, Train Loss:0.26448437571525574\n",
      "Epoch 3[224/625] Time:0.693, Train Loss:0.38817542791366577\n",
      "Epoch 3[225/625] Time:0.693, Train Loss:0.3358477056026459\n",
      "Epoch 3[226/625] Time:0.695, Train Loss:0.26784244179725647\n",
      "Epoch 3[227/625] Time:0.694, Train Loss:0.3099198341369629\n",
      "Epoch 3[228/625] Time:0.702, Train Loss:0.37425047159194946\n",
      "Epoch 3[229/625] Time:0.702, Train Loss:0.38462820649147034\n",
      "Epoch 3[230/625] Time:0.702, Train Loss:0.3753547966480255\n",
      "Epoch 3[231/625] Time:0.703, Train Loss:0.2567431926727295\n",
      "Epoch 3[232/625] Time:0.703, Train Loss:0.3222048878669739\n",
      "Epoch 3[233/625] Time:0.701, Train Loss:0.33657896518707275\n",
      "Epoch 3[234/625] Time:0.703, Train Loss:0.2919667959213257\n",
      "Epoch 3[235/625] Time:0.701, Train Loss:0.28368881344795227\n",
      "Epoch 3[236/625] Time:0.703, Train Loss:0.44408559799194336\n",
      "Epoch 3[237/625] Time:0.705, Train Loss:0.3871430456638336\n",
      "Epoch 3[238/625] Time:0.703, Train Loss:0.32244443893432617\n",
      "Epoch 3[239/625] Time:0.703, Train Loss:0.29269731044769287\n",
      "Epoch 3[240/625] Time:0.704, Train Loss:0.3380451202392578\n",
      "Epoch 3[241/625] Time:0.702, Train Loss:0.3028213083744049\n",
      "Epoch 3[242/625] Time:0.725, Train Loss:0.29921579360961914\n",
      "Epoch 3[243/625] Time:0.693, Train Loss:0.39863821864128113\n",
      "Epoch 3[244/625] Time:0.694, Train Loss:0.3056001365184784\n",
      "Epoch 3[245/625] Time:0.695, Train Loss:0.38668471574783325\n",
      "Epoch 3[246/625] Time:0.693, Train Loss:0.3146841526031494\n",
      "Epoch 3[247/625] Time:0.693, Train Loss:0.3737742304801941\n",
      "Epoch 3[248/625] Time:0.694, Train Loss:0.3106381893157959\n",
      "Epoch 3[249/625] Time:0.695, Train Loss:0.3338661789894104\n",
      "Epoch 3[250/625] Time:0.694, Train Loss:0.366798460483551\n",
      "Epoch 3[251/625] Time:0.693, Train Loss:0.3338180482387543\n",
      "Epoch 3[252/625] Time:0.693, Train Loss:0.32736387848854065\n",
      "Epoch 3[253/625] Time:0.694, Train Loss:0.3211362659931183\n",
      "Epoch 3[254/625] Time:0.694, Train Loss:0.35525020956993103\n",
      "Epoch 3[255/625] Time:0.694, Train Loss:0.2679848372936249\n",
      "Epoch 3[256/625] Time:0.695, Train Loss:0.3110603988170624\n",
      "Epoch 3[257/625] Time:0.694, Train Loss:0.37346166372299194\n",
      "Epoch 3[258/625] Time:0.694, Train Loss:0.3504229485988617\n",
      "Epoch 3[259/625] Time:0.694, Train Loss:0.3104155659675598\n",
      "Epoch 3[260/625] Time:0.695, Train Loss:0.3685910105705261\n",
      "Epoch 3[261/625] Time:0.698, Train Loss:0.44800275564193726\n",
      "Epoch 3[262/625] Time:0.694, Train Loss:0.36141011118888855\n",
      "Epoch 3[263/625] Time:0.694, Train Loss:0.38197872042655945\n",
      "Epoch 3[264/625] Time:0.694, Train Loss:0.3004888892173767\n",
      "Epoch 3[265/625] Time:0.694, Train Loss:0.3760697543621063\n",
      "Epoch 3[266/625] Time:0.694, Train Loss:0.2930530309677124\n",
      "Epoch 3[267/625] Time:0.694, Train Loss:0.3040730953216553\n",
      "Epoch 3[268/625] Time:0.694, Train Loss:0.3285195827484131\n",
      "Epoch 3[269/625] Time:0.695, Train Loss:0.30989351868629456\n",
      "Epoch 3[270/625] Time:0.694, Train Loss:0.24272392690181732\n",
      "Epoch 3[271/625] Time:0.724, Train Loss:0.28535696864128113\n",
      "Epoch 3[272/625] Time:0.702, Train Loss:0.34521031379699707\n",
      "Epoch 3[273/625] Time:0.728, Train Loss:0.3218132257461548\n",
      "Epoch 3[274/625] Time:0.738, Train Loss:0.2348739504814148\n",
      "Epoch 3[275/625] Time:0.694, Train Loss:0.3599695563316345\n",
      "Epoch 3[276/625] Time:0.73, Train Loss:0.28694090247154236\n",
      "Epoch 3[277/625] Time:0.694, Train Loss:0.32831382751464844\n",
      "Epoch 3[278/625] Time:0.694, Train Loss:0.3083408772945404\n",
      "Epoch 3[279/625] Time:0.698, Train Loss:0.3458702266216278\n",
      "Epoch 3[280/625] Time:0.694, Train Loss:0.2840413451194763\n",
      "Epoch 3[281/625] Time:0.694, Train Loss:0.29374435544013977\n",
      "Epoch 3[282/625] Time:0.694, Train Loss:0.4300693869590759\n",
      "Epoch 3[283/625] Time:0.694, Train Loss:0.3685453534126282\n",
      "Epoch 3[284/625] Time:0.7, Train Loss:0.30773037672042847\n",
      "Epoch 3[285/625] Time:0.694, Train Loss:0.28422293066978455\n",
      "Epoch 3[286/625] Time:0.692, Train Loss:0.29264721274375916\n",
      "Epoch 3[287/625] Time:0.694, Train Loss:0.3328555226325989\n",
      "Epoch 3[288/625] Time:0.694, Train Loss:0.29724282026290894\n",
      "Epoch 3[289/625] Time:0.694, Train Loss:0.37071773409843445\n",
      "Epoch 3[290/625] Time:0.696, Train Loss:0.22417603433132172\n",
      "Epoch 3[291/625] Time:0.694, Train Loss:0.26939404010772705\n",
      "Epoch 3[292/625] Time:0.702, Train Loss:0.32228803634643555\n",
      "Epoch 3[293/625] Time:0.694, Train Loss:0.294399231672287\n",
      "Epoch 3[294/625] Time:0.693, Train Loss:0.3313121199607849\n",
      "Epoch 3[295/625] Time:0.694, Train Loss:0.4077642560005188\n",
      "Epoch 3[296/625] Time:0.708, Train Loss:0.35415875911712646\n",
      "Epoch 3[297/625] Time:0.705, Train Loss:0.30110979080200195\n",
      "Epoch 3[298/625] Time:0.694, Train Loss:0.36447054147720337\n",
      "Epoch 3[299/625] Time:0.694, Train Loss:0.36357709765434265\n",
      "Epoch 3[300/625] Time:0.694, Train Loss:0.28046929836273193\n",
      "Epoch 3[301/625] Time:0.695, Train Loss:0.2881551682949066\n",
      "Epoch 3[302/625] Time:0.695, Train Loss:0.2962391972541809\n",
      "Epoch 3[303/625] Time:0.694, Train Loss:0.2865007221698761\n",
      "Epoch 3[304/625] Time:0.694, Train Loss:0.2952028512954712\n",
      "Epoch 3[305/625] Time:0.694, Train Loss:0.3987092077732086\n",
      "Epoch 3[306/625] Time:0.693, Train Loss:0.2741428017616272\n",
      "Epoch 3[307/625] Time:0.694, Train Loss:0.38156217336654663\n",
      "Epoch 3[308/625] Time:0.696, Train Loss:0.36417514085769653\n",
      "Epoch 3[309/625] Time:0.734, Train Loss:0.39473703503608704\n",
      "Epoch 3[310/625] Time:0.701, Train Loss:0.26549994945526123\n",
      "Epoch 3[311/625] Time:0.724, Train Loss:0.3219068944454193\n",
      "Epoch 3[312/625] Time:0.72, Train Loss:0.3569307029247284\n",
      "Epoch 3[313/625] Time:0.703, Train Loss:0.29799288511276245\n",
      "Epoch 3[314/625] Time:0.703, Train Loss:0.24857231974601746\n",
      "Epoch 3[315/625] Time:0.704, Train Loss:0.4100458323955536\n",
      "Epoch 3[316/625] Time:0.703, Train Loss:0.27875447273254395\n",
      "Epoch 3[317/625] Time:0.704, Train Loss:0.31470757722854614\n",
      "Epoch 3[318/625] Time:0.703, Train Loss:0.2571513056755066\n",
      "Epoch 3[319/625] Time:0.703, Train Loss:0.32974618673324585\n",
      "Epoch 3[320/625] Time:0.703, Train Loss:0.2658417224884033\n",
      "Epoch 3[321/625] Time:0.703, Train Loss:0.4229872226715088\n",
      "Epoch 3[322/625] Time:0.694, Train Loss:0.2902301251888275\n",
      "Epoch 3[323/625] Time:0.694, Train Loss:0.28782927989959717\n",
      "Epoch 3[324/625] Time:0.695, Train Loss:0.4200536608695984\n",
      "Epoch 3[325/625] Time:0.695, Train Loss:0.3301984667778015\n",
      "Epoch 3[326/625] Time:0.695, Train Loss:0.3396608829498291\n",
      "Epoch 3[327/625] Time:0.694, Train Loss:0.3639638125896454\n",
      "Epoch 3[328/625] Time:0.695, Train Loss:0.2701563537120819\n",
      "Epoch 3[329/625] Time:0.695, Train Loss:0.38469406962394714\n",
      "Epoch 3[330/625] Time:0.709, Train Loss:0.35266539454460144\n",
      "Epoch 3[331/625] Time:0.703, Train Loss:0.31509819626808167\n",
      "Epoch 3[332/625] Time:0.693, Train Loss:0.3044024109840393\n",
      "Epoch 3[333/625] Time:0.696, Train Loss:0.39539214968681335\n",
      "Epoch 3[334/625] Time:0.703, Train Loss:0.3623761534690857\n",
      "Epoch 3[335/625] Time:0.714, Train Loss:0.42123061418533325\n",
      "Epoch 3[336/625] Time:0.704, Train Loss:0.283887654542923\n",
      "Epoch 3[337/625] Time:0.702, Train Loss:0.31252560019493103\n",
      "Epoch 3[338/625] Time:0.704, Train Loss:0.380502313375473\n",
      "Epoch 3[339/625] Time:0.702, Train Loss:0.35345974564552307\n",
      "Epoch 3[340/625] Time:0.707, Train Loss:0.27320635318756104\n",
      "Epoch 3[341/625] Time:0.703, Train Loss:0.3840390741825104\n",
      "Epoch 3[342/625] Time:0.702, Train Loss:0.33264070749282837\n",
      "Epoch 3[343/625] Time:0.702, Train Loss:0.3472982943058014\n",
      "Epoch 3[344/625] Time:0.704, Train Loss:0.3000718653202057\n",
      "Epoch 3[345/625] Time:0.704, Train Loss:0.28958848118782043\n",
      "Epoch 3[346/625] Time:0.703, Train Loss:0.268658846616745\n",
      "Epoch 3[347/625] Time:0.704, Train Loss:0.33275169134140015\n",
      "Epoch 3[348/625] Time:0.703, Train Loss:0.27181944251060486\n",
      "Epoch 3[349/625] Time:0.694, Train Loss:0.33434244990348816\n",
      "Epoch 3[350/625] Time:0.694, Train Loss:0.3193487823009491\n",
      "Epoch 3[351/625] Time:0.707, Train Loss:0.34782665967941284\n",
      "Epoch 3[352/625] Time:0.694, Train Loss:0.3006240427494049\n",
      "Epoch 3[353/625] Time:0.693, Train Loss:0.308979332447052\n",
      "Epoch 3[354/625] Time:0.708, Train Loss:0.30598488450050354\n",
      "Epoch 3[355/625] Time:0.694, Train Loss:0.3715466856956482\n",
      "Epoch 3[356/625] Time:0.694, Train Loss:0.35752639174461365\n",
      "Epoch 3[357/625] Time:0.694, Train Loss:0.38551393151283264\n",
      "Epoch 3[358/625] Time:0.694, Train Loss:0.3787819743156433\n",
      "Epoch 3[359/625] Time:0.694, Train Loss:0.29123765230178833\n",
      "Epoch 3[360/625] Time:0.692, Train Loss:0.27982693910598755\n",
      "Epoch 3[361/625] Time:0.701, Train Loss:0.3289656937122345\n",
      "Epoch 3[362/625] Time:0.694, Train Loss:0.3382185995578766\n",
      "Epoch 3[363/625] Time:0.696, Train Loss:0.31916287541389465\n",
      "Epoch 3[364/625] Time:0.696, Train Loss:0.3567975163459778\n",
      "Epoch 3[365/625] Time:0.695, Train Loss:0.37663549184799194\n",
      "Epoch 3[366/625] Time:0.694, Train Loss:0.3506582975387573\n",
      "Epoch 3[367/625] Time:0.695, Train Loss:0.291238397359848\n",
      "Epoch 3[368/625] Time:0.695, Train Loss:0.2960745692253113\n",
      "Epoch 3[369/625] Time:0.702, Train Loss:0.2849807143211365\n",
      "Epoch 3[370/625] Time:0.695, Train Loss:0.2914544641971588\n",
      "Epoch 3[371/625] Time:0.695, Train Loss:0.25367066264152527\n",
      "Epoch 3[372/625] Time:0.695, Train Loss:0.2864881157875061\n",
      "Epoch 3[373/625] Time:0.694, Train Loss:0.34116438031196594\n",
      "Epoch 3[374/625] Time:0.695, Train Loss:0.31704339385032654\n",
      "Epoch 3[375/625] Time:0.694, Train Loss:0.2929658591747284\n",
      "Epoch 3[376/625] Time:0.694, Train Loss:0.29182127118110657\n",
      "Epoch 3[377/625] Time:0.697, Train Loss:0.30891984701156616\n",
      "Epoch 3[378/625] Time:0.696, Train Loss:0.32510998845100403\n",
      "Epoch 3[379/625] Time:0.694, Train Loss:0.3199218511581421\n",
      "Epoch 3[380/625] Time:0.695, Train Loss:0.36050945520401\n",
      "Epoch 3[381/625] Time:0.696, Train Loss:0.4215361475944519\n",
      "Epoch 3[382/625] Time:0.695, Train Loss:0.3289113938808441\n",
      "Epoch 3[383/625] Time:0.722, Train Loss:0.2651820778846741\n",
      "Epoch 3[384/625] Time:0.695, Train Loss:0.33979177474975586\n",
      "Epoch 3[385/625] Time:0.695, Train Loss:0.4026040732860565\n",
      "Epoch 3[386/625] Time:0.698, Train Loss:0.3031246066093445\n",
      "Epoch 3[387/625] Time:0.695, Train Loss:0.3853759467601776\n",
      "Epoch 3[388/625] Time:0.695, Train Loss:0.31500744819641113\n",
      "Epoch 3[389/625] Time:0.695, Train Loss:0.28261470794677734\n",
      "Epoch 3[390/625] Time:0.696, Train Loss:0.31588244438171387\n",
      "Epoch 3[391/625] Time:0.694, Train Loss:0.4041072726249695\n",
      "Epoch 3[392/625] Time:0.736, Train Loss:0.2791531980037689\n",
      "Epoch 3[393/625] Time:0.702, Train Loss:0.29362723231315613\n",
      "Epoch 3[394/625] Time:0.727, Train Loss:0.34921327233314514\n",
      "Epoch 3[395/625] Time:0.694, Train Loss:0.31814703345298767\n",
      "Epoch 3[396/625] Time:0.698, Train Loss:0.29513469338417053\n",
      "Epoch 3[397/625] Time:0.692, Train Loss:0.3362677991390228\n",
      "Epoch 3[398/625] Time:0.74, Train Loss:0.28137120604515076\n",
      "Epoch 3[399/625] Time:0.702, Train Loss:0.32629168033599854\n",
      "Epoch 3[400/625] Time:0.711, Train Loss:0.27859005331993103\n",
      "Epoch 3[401/625] Time:0.716, Train Loss:0.3570462465286255\n",
      "Epoch 3[402/625] Time:0.702, Train Loss:0.35008692741394043\n",
      "Epoch 3[403/625] Time:0.703, Train Loss:0.40969717502593994\n",
      "Epoch 3[404/625] Time:0.703, Train Loss:0.3835226893424988\n",
      "Epoch 3[405/625] Time:0.704, Train Loss:0.328859806060791\n",
      "Epoch 3[406/625] Time:0.704, Train Loss:0.3122408092021942\n",
      "Epoch 3[407/625] Time:0.703, Train Loss:0.31562939286231995\n",
      "Epoch 3[408/625] Time:0.72, Train Loss:0.2737835645675659\n",
      "Epoch 3[409/625] Time:0.694, Train Loss:0.3532392680644989\n",
      "Epoch 3[410/625] Time:0.697, Train Loss:0.30234935879707336\n",
      "Epoch 3[411/625] Time:0.694, Train Loss:0.266427606344223\n",
      "Epoch 3[412/625] Time:0.696, Train Loss:0.36695799231529236\n",
      "Epoch 3[413/625] Time:0.696, Train Loss:0.4895828366279602\n",
      "Epoch 3[414/625] Time:0.694, Train Loss:0.2984165847301483\n",
      "Epoch 3[415/625] Time:0.709, Train Loss:0.3690703213214874\n",
      "Epoch 3[416/625] Time:0.703, Train Loss:0.27237600088119507\n",
      "Epoch 3[417/625] Time:0.693, Train Loss:0.3213401734828949\n",
      "Epoch 3[418/625] Time:0.693, Train Loss:0.28342920541763306\n",
      "Epoch 3[419/625] Time:0.694, Train Loss:0.3698503375053406\n",
      "Epoch 3[420/625] Time:0.695, Train Loss:0.3160221576690674\n",
      "Epoch 3[421/625] Time:0.694, Train Loss:0.3255726993083954\n",
      "Epoch 3[422/625] Time:0.694, Train Loss:0.3726220726966858\n",
      "Epoch 3[423/625] Time:0.694, Train Loss:0.30258908867836\n",
      "Epoch 3[424/625] Time:0.721, Train Loss:0.3213622570037842\n",
      "Epoch 3[425/625] Time:0.702, Train Loss:0.30290716886520386\n",
      "Epoch 3[426/625] Time:0.694, Train Loss:0.3575069308280945\n",
      "Epoch 3[427/625] Time:0.701, Train Loss:0.3854199945926666\n",
      "Epoch 3[428/625] Time:0.702, Train Loss:0.31754714250564575\n",
      "Epoch 3[429/625] Time:0.693, Train Loss:0.41200652718544006\n",
      "Epoch 3[430/625] Time:0.695, Train Loss:0.29842129349708557\n",
      "Epoch 3[431/625] Time:0.701, Train Loss:0.3048075735569\n",
      "Epoch 3[432/625] Time:0.695, Train Loss:0.27624931931495667\n",
      "Epoch 3[433/625] Time:0.694, Train Loss:0.27677756547927856\n",
      "Epoch 3[434/625] Time:0.694, Train Loss:0.31552189588546753\n",
      "Epoch 3[435/625] Time:0.695, Train Loss:0.3370579481124878\n",
      "Epoch 3[436/625] Time:0.694, Train Loss:0.28756216168403625\n",
      "Epoch 3[437/625] Time:0.704, Train Loss:0.32692480087280273\n",
      "Epoch 3[438/625] Time:0.695, Train Loss:0.3764877915382385\n",
      "Epoch 3[439/625] Time:0.694, Train Loss:0.3184290826320648\n",
      "Epoch 3[440/625] Time:0.696, Train Loss:0.40034934878349304\n",
      "Epoch 3[441/625] Time:0.694, Train Loss:0.3023483157157898\n",
      "Epoch 3[442/625] Time:0.694, Train Loss:0.2923152446746826\n",
      "Epoch 3[443/625] Time:0.694, Train Loss:0.325679212808609\n",
      "Epoch 3[444/625] Time:0.695, Train Loss:0.3068629503250122\n",
      "Epoch 3[445/625] Time:0.695, Train Loss:0.3617269992828369\n",
      "Epoch 3[446/625] Time:0.694, Train Loss:0.2891194522380829\n",
      "Epoch 3[447/625] Time:0.695, Train Loss:0.31258299946784973\n",
      "Epoch 3[448/625] Time:0.695, Train Loss:0.296832412481308\n",
      "Epoch 3[449/625] Time:0.707, Train Loss:0.2730005979537964\n",
      "Epoch 3[450/625] Time:0.703, Train Loss:0.3119490146636963\n",
      "Epoch 3[451/625] Time:0.694, Train Loss:0.3268258273601532\n",
      "Epoch 3[452/625] Time:0.722, Train Loss:0.3222796320915222\n",
      "Epoch 3[453/625] Time:0.703, Train Loss:0.3469705879688263\n",
      "Epoch 3[454/625] Time:0.695, Train Loss:0.3192870318889618\n",
      "Epoch 3[455/625] Time:0.695, Train Loss:0.4377954304218292\n",
      "Epoch 3[456/625] Time:0.694, Train Loss:0.4481751322746277\n",
      "Epoch 3[457/625] Time:0.694, Train Loss:0.33854761719703674\n",
      "Epoch 3[458/625] Time:0.716, Train Loss:0.25563615560531616\n",
      "Epoch 3[459/625] Time:0.703, Train Loss:0.27853748202323914\n",
      "Epoch 3[460/625] Time:0.702, Train Loss:0.3599121570587158\n",
      "Epoch 3[461/625] Time:0.694, Train Loss:0.2643907368183136\n",
      "Epoch 3[462/625] Time:0.701, Train Loss:0.2978757619857788\n",
      "Epoch 3[463/625] Time:0.696, Train Loss:0.3167968690395355\n",
      "Epoch 3[464/625] Time:0.696, Train Loss:0.2921161651611328\n",
      "Epoch 3[465/625] Time:0.695, Train Loss:0.3649803400039673\n",
      "Epoch 3[466/625] Time:0.695, Train Loss:0.30437445640563965\n",
      "Epoch 3[467/625] Time:0.696, Train Loss:0.299810528755188\n",
      "Epoch 3[468/625] Time:0.696, Train Loss:0.33304738998413086\n",
      "Epoch 3[469/625] Time:0.712, Train Loss:0.32518982887268066\n",
      "Epoch 3[470/625] Time:0.708, Train Loss:0.3159474730491638\n",
      "Epoch 3[471/625] Time:0.695, Train Loss:0.4027712941169739\n",
      "Epoch 3[472/625] Time:0.719, Train Loss:0.4067346155643463\n",
      "Epoch 3[473/625] Time:0.693, Train Loss:0.31287938356399536\n",
      "Epoch 3[474/625] Time:0.697, Train Loss:0.3334403336048126\n",
      "Epoch 3[475/625] Time:0.729, Train Loss:0.3290388584136963\n",
      "Epoch 3[476/625] Time:0.695, Train Loss:0.35111793875694275\n",
      "Epoch 3[477/625] Time:0.693, Train Loss:0.33101820945739746\n",
      "Epoch 3[478/625] Time:0.695, Train Loss:0.25267693400382996\n",
      "Epoch 3[479/625] Time:0.694, Train Loss:0.3584018647670746\n",
      "Epoch 3[480/625] Time:0.694, Train Loss:0.3455405831336975\n",
      "Epoch 3[481/625] Time:0.694, Train Loss:0.3969900906085968\n",
      "Epoch 3[482/625] Time:0.719, Train Loss:0.33038511872291565\n",
      "Epoch 3[483/625] Time:0.693, Train Loss:0.30601781606674194\n",
      "Epoch 3[484/625] Time:0.695, Train Loss:0.3940889537334442\n",
      "Epoch 3[485/625] Time:0.698, Train Loss:0.37468963861465454\n",
      "Epoch 3[486/625] Time:0.694, Train Loss:0.40886738896369934\n",
      "Epoch 3[487/625] Time:0.721, Train Loss:0.30174028873443604\n",
      "Epoch 3[488/625] Time:0.702, Train Loss:0.32643410563468933\n",
      "Epoch 3[489/625] Time:0.703, Train Loss:0.35647550225257874\n",
      "Epoch 3[490/625] Time:0.703, Train Loss:0.3409885764122009\n",
      "Epoch 3[491/625] Time:0.704, Train Loss:0.33138877153396606\n",
      "Epoch 3[492/625] Time:0.703, Train Loss:0.35293781757354736\n",
      "Epoch 3[493/625] Time:0.704, Train Loss:0.3257465958595276\n",
      "Epoch 3[494/625] Time:0.703, Train Loss:0.3705928325653076\n",
      "Epoch 3[495/625] Time:0.704, Train Loss:0.31437981128692627\n",
      "Epoch 3[496/625] Time:0.703, Train Loss:0.31280919909477234\n",
      "Epoch 3[497/625] Time:0.731, Train Loss:0.32079046964645386\n",
      "Epoch 3[498/625] Time:0.692, Train Loss:0.3241388499736786\n",
      "Epoch 3[499/625] Time:0.71, Train Loss:0.3818817436695099\n",
      "Epoch 3[500/625] Time:0.694, Train Loss:0.3672347664833069\n",
      "Epoch 3[501/625] Time:0.694, Train Loss:0.35650625824928284\n",
      "Epoch 3[502/625] Time:0.694, Train Loss:0.42943286895751953\n",
      "Epoch 3[503/625] Time:0.694, Train Loss:0.34991562366485596\n",
      "Epoch 3[504/625] Time:0.693, Train Loss:0.290255069732666\n",
      "Epoch 3[505/625] Time:0.694, Train Loss:0.2929510772228241\n",
      "Epoch 3[506/625] Time:0.694, Train Loss:0.31212693452835083\n",
      "Epoch 3[507/625] Time:0.695, Train Loss:0.28568094968795776\n",
      "Epoch 3[508/625] Time:0.694, Train Loss:0.31794896721839905\n",
      "Epoch 3[509/625] Time:0.695, Train Loss:0.3261072635650635\n",
      "Epoch 3[510/625] Time:0.694, Train Loss:0.30097752809524536\n",
      "Epoch 3[511/625] Time:0.711, Train Loss:0.380704402923584\n",
      "Epoch 3[512/625] Time:0.701, Train Loss:0.2962192893028259\n",
      "Epoch 3[513/625] Time:0.695, Train Loss:0.27732935547828674\n",
      "Epoch 3[514/625] Time:0.695, Train Loss:0.2823937237262726\n",
      "Epoch 3[515/625] Time:0.696, Train Loss:0.2577262222766876\n",
      "Epoch 3[516/625] Time:0.7, Train Loss:0.3644675314426422\n",
      "Epoch 3[517/625] Time:0.694, Train Loss:0.37299850583076477\n",
      "Epoch 3[518/625] Time:0.695, Train Loss:0.29315948486328125\n",
      "Epoch 3[519/625] Time:0.694, Train Loss:0.3773057758808136\n",
      "Epoch 3[520/625] Time:0.695, Train Loss:0.3446964621543884\n",
      "Epoch 3[521/625] Time:0.695, Train Loss:0.35223791003227234\n",
      "Epoch 3[522/625] Time:0.694, Train Loss:0.28135260939598083\n",
      "Epoch 3[523/625] Time:0.695, Train Loss:0.30286678671836853\n",
      "Epoch 3[524/625] Time:0.695, Train Loss:0.4043446183204651\n",
      "Epoch 3[525/625] Time:0.694, Train Loss:0.40618452429771423\n",
      "Epoch 3[526/625] Time:0.694, Train Loss:0.2725440263748169\n",
      "Epoch 3[527/625] Time:0.695, Train Loss:0.3043736219406128\n",
      "Epoch 3[528/625] Time:0.696, Train Loss:0.3355291485786438\n",
      "Epoch 3[529/625] Time:0.694, Train Loss:0.2645382583141327\n",
      "Epoch 3[530/625] Time:0.695, Train Loss:0.38241925835609436\n",
      "Epoch 3[531/625] Time:0.695, Train Loss:0.3760097026824951\n",
      "Epoch 3[532/625] Time:0.703, Train Loss:0.2669883370399475\n",
      "Epoch 3[533/625] Time:0.694, Train Loss:0.27091723680496216\n",
      "Epoch 3[534/625] Time:0.695, Train Loss:0.28618699312210083\n",
      "Epoch 3[535/625] Time:0.736, Train Loss:0.2929885983467102\n",
      "Epoch 3[536/625] Time:0.693, Train Loss:0.3656235337257385\n",
      "Epoch 3[537/625] Time:0.695, Train Loss:0.3341444134712219\n",
      "Epoch 3[538/625] Time:0.695, Train Loss:0.3075488209724426\n",
      "Epoch 3[539/625] Time:0.696, Train Loss:0.2948671579360962\n",
      "Epoch 3[540/625] Time:0.696, Train Loss:0.2737685739994049\n",
      "Epoch 3[541/625] Time:0.695, Train Loss:0.28464311361312866\n",
      "Epoch 3[542/625] Time:0.694, Train Loss:0.32536545395851135\n",
      "Epoch 3[543/625] Time:0.694, Train Loss:0.2848120927810669\n",
      "Epoch 3[544/625] Time:0.693, Train Loss:0.33126863837242126\n",
      "Epoch 3[545/625] Time:0.694, Train Loss:0.24855521321296692\n",
      "Epoch 3[546/625] Time:0.693, Train Loss:0.3382892310619354\n",
      "Epoch 3[547/625] Time:0.694, Train Loss:0.3237581253051758\n",
      "Epoch 3[548/625] Time:0.695, Train Loss:0.26652806997299194\n",
      "Epoch 3[549/625] Time:0.694, Train Loss:0.29826122522354126\n",
      "Epoch 3[550/625] Time:0.694, Train Loss:0.24203398823738098\n",
      "Epoch 3[551/625] Time:0.694, Train Loss:0.3385748565196991\n",
      "Epoch 3[552/625] Time:0.696, Train Loss:0.3169591724872589\n",
      "Epoch 3[553/625] Time:0.695, Train Loss:0.26536378264427185\n",
      "Epoch 3[554/625] Time:0.694, Train Loss:0.29828280210494995\n",
      "Epoch 3[555/625] Time:0.694, Train Loss:0.3775257170200348\n",
      "Epoch 3[556/625] Time:0.708, Train Loss:0.3964836001396179\n",
      "Epoch 3[557/625] Time:0.693, Train Loss:0.3592045307159424\n",
      "Epoch 3[558/625] Time:0.694, Train Loss:0.28624966740608215\n",
      "Epoch 3[559/625] Time:0.694, Train Loss:0.2593232989311218\n",
      "Epoch 3[560/625] Time:0.695, Train Loss:0.32912760972976685\n",
      "Epoch 3[561/625] Time:0.742, Train Loss:0.35281991958618164\n",
      "Epoch 3[562/625] Time:0.701, Train Loss:0.29831525683403015\n",
      "Epoch 3[563/625] Time:0.698, Train Loss:0.3269890546798706\n",
      "Epoch 3[564/625] Time:0.693, Train Loss:0.38034018874168396\n",
      "Epoch 3[565/625] Time:0.694, Train Loss:0.2720678746700287\n",
      "Epoch 3[566/625] Time:0.694, Train Loss:0.3172244131565094\n",
      "Epoch 3[567/625] Time:0.695, Train Loss:0.32658296823501587\n",
      "Epoch 3[568/625] Time:0.694, Train Loss:0.29821592569351196\n",
      "Epoch 3[569/625] Time:0.694, Train Loss:0.22999337315559387\n",
      "Epoch 3[570/625] Time:0.694, Train Loss:0.3393056094646454\n",
      "Epoch 3[571/625] Time:0.696, Train Loss:0.23642732203006744\n",
      "Epoch 3[572/625] Time:0.695, Train Loss:0.2873285710811615\n",
      "Epoch 3[573/625] Time:0.696, Train Loss:0.3356688618659973\n",
      "Epoch 3[574/625] Time:0.694, Train Loss:0.33308523893356323\n",
      "Epoch 3[575/625] Time:0.697, Train Loss:0.2584058344364166\n",
      "Epoch 3[576/625] Time:0.694, Train Loss:0.36352428793907166\n",
      "Epoch 3[577/625] Time:0.7, Train Loss:0.32079339027404785\n",
      "Epoch 3[578/625] Time:0.695, Train Loss:0.33958834409713745\n",
      "Epoch 3[579/625] Time:0.694, Train Loss:0.38337618112564087\n",
      "Epoch 3[580/625] Time:0.729, Train Loss:0.3123342990875244\n",
      "Epoch 3[581/625] Time:0.694, Train Loss:0.34654122591018677\n",
      "Epoch 3[582/625] Time:0.695, Train Loss:0.3006596267223358\n",
      "Epoch 3[583/625] Time:0.695, Train Loss:0.34337660670280457\n",
      "Epoch 3[584/625] Time:0.695, Train Loss:0.3416438400745392\n",
      "Epoch 3[585/625] Time:0.71, Train Loss:0.3101356029510498\n",
      "Epoch 3[586/625] Time:0.702, Train Loss:0.29251113533973694\n",
      "Epoch 3[587/625] Time:0.703, Train Loss:0.33875906467437744\n",
      "Epoch 3[588/625] Time:0.702, Train Loss:0.34254008531570435\n",
      "Epoch 3[589/625] Time:0.702, Train Loss:0.30413204431533813\n",
      "Epoch 3[590/625] Time:0.702, Train Loss:0.34543076157569885\n",
      "Epoch 3[591/625] Time:0.705, Train Loss:0.38434794545173645\n",
      "Epoch 3[592/625] Time:0.705, Train Loss:0.28105536103248596\n",
      "Epoch 3[593/625] Time:0.706, Train Loss:0.38182929158210754\n",
      "Epoch 3[594/625] Time:0.703, Train Loss:0.3649168610572815\n",
      "Epoch 3[595/625] Time:0.703, Train Loss:0.3050988018512726\n",
      "Epoch 3[596/625] Time:0.703, Train Loss:0.33369648456573486\n",
      "Epoch 3[597/625] Time:0.703, Train Loss:0.3593175411224365\n",
      "Epoch 3[598/625] Time:0.703, Train Loss:0.3297387957572937\n",
      "Epoch 3[599/625] Time:0.704, Train Loss:0.3684106767177582\n",
      "Epoch 3[600/625] Time:0.703, Train Loss:0.30566534399986267\n",
      "Epoch 3[601/625] Time:0.693, Train Loss:0.3246336281299591\n",
      "Epoch 3[602/625] Time:0.695, Train Loss:0.2617560625076294\n",
      "Epoch 3[603/625] Time:0.694, Train Loss:0.29324379563331604\n",
      "Epoch 3[604/625] Time:0.695, Train Loss:0.2947607934474945\n",
      "Epoch 3[605/625] Time:0.695, Train Loss:0.32930606603622437\n",
      "Epoch 3[606/625] Time:0.695, Train Loss:0.30413225293159485\n",
      "Epoch 3[607/625] Time:0.694, Train Loss:0.42608779668807983\n",
      "Epoch 3[608/625] Time:0.695, Train Loss:0.31883710622787476\n",
      "Epoch 3[609/625] Time:0.695, Train Loss:0.2540208697319031\n",
      "Epoch 3[610/625] Time:0.695, Train Loss:0.30837714672088623\n",
      "Epoch 3[611/625] Time:0.695, Train Loss:0.3310130834579468\n",
      "Epoch 3[612/625] Time:0.732, Train Loss:0.2788033187389374\n",
      "Epoch 3[613/625] Time:0.696, Train Loss:0.352276474237442\n",
      "Epoch 3[614/625] Time:0.696, Train Loss:0.30758044123649597\n",
      "Epoch 3[615/625] Time:0.695, Train Loss:0.3099004030227661\n",
      "Epoch 3[616/625] Time:0.698, Train Loss:0.4187120795249939\n",
      "Epoch 3[617/625] Time:0.695, Train Loss:0.32205870747566223\n",
      "Epoch 3[618/625] Time:0.696, Train Loss:0.34794336557388306\n",
      "Epoch 3[619/625] Time:0.732, Train Loss:0.2726139724254608\n",
      "Epoch 3[620/625] Time:0.718, Train Loss:0.31300073862075806\n",
      "Epoch 3[621/625] Time:0.709, Train Loss:0.39041340351104736\n",
      "Epoch 3[622/625] Time:0.702, Train Loss:0.3635254502296448\n",
      "Epoch 3[623/625] Time:0.703, Train Loss:0.3661057949066162\n",
      "Epoch 3[624/625] Time:0.704, Train Loss:0.38562440872192383\n",
      "Epoch 3[0/78] Val Loss:0.22455304861068726\n",
      "Epoch 3[1/78] Val Loss:0.21035847067832947\n",
      "Epoch 3[2/78] Val Loss:0.23562520742416382\n",
      "Epoch 3[3/78] Val Loss:0.20678114891052246\n",
      "Epoch 3[4/78] Val Loss:0.2608115077018738\n",
      "Epoch 3[5/78] Val Loss:0.21771660447120667\n",
      "Epoch 3[6/78] Val Loss:0.23458868265151978\n",
      "Epoch 3[7/78] Val Loss:0.27229538559913635\n",
      "Epoch 3[8/78] Val Loss:0.18013808131217957\n",
      "Epoch 3[9/78] Val Loss:0.11808795481920242\n",
      "Epoch 3[10/78] Val Loss:0.08535346388816833\n",
      "Epoch 3[11/78] Val Loss:0.15423235297203064\n",
      "Epoch 3[12/78] Val Loss:0.12119321525096893\n",
      "Epoch 3[13/78] Val Loss:0.09483887255191803\n",
      "Epoch 3[14/78] Val Loss:0.1269693225622177\n",
      "Epoch 3[15/78] Val Loss:0.09949392825365067\n",
      "Epoch 3[16/78] Val Loss:0.13261185586452484\n",
      "Epoch 3[17/78] Val Loss:0.13310173153877258\n",
      "Epoch 3[18/78] Val Loss:0.2766839563846588\n",
      "Epoch 3[19/78] Val Loss:0.3459630608558655\n",
      "Epoch 3[20/78] Val Loss:0.2534402012825012\n",
      "Epoch 3[21/78] Val Loss:0.6161122918128967\n",
      "Epoch 3[22/78] Val Loss:0.8871241807937622\n",
      "Epoch 3[23/78] Val Loss:0.6262072324752808\n",
      "Epoch 3[24/78] Val Loss:0.5076555609703064\n",
      "Epoch 3[25/78] Val Loss:0.6047057509422302\n",
      "Epoch 3[26/78] Val Loss:0.5673668384552002\n",
      "Epoch 3[27/78] Val Loss:0.5237412452697754\n",
      "Epoch 3[28/78] Val Loss:0.5050836205482483\n",
      "Epoch 3[29/78] Val Loss:0.701849102973938\n",
      "Epoch 3[30/78] Val Loss:2.454446792602539\n",
      "Epoch 3[31/78] Val Loss:2.0050175189971924\n",
      "Epoch 3[32/78] Val Loss:1.6406917572021484\n",
      "Epoch 3[33/78] Val Loss:0.46586430072784424\n",
      "Epoch 3[34/78] Val Loss:0.4176899492740631\n",
      "Epoch 3[35/78] Val Loss:0.3978469967842102\n",
      "Epoch 3[36/78] Val Loss:0.46912360191345215\n",
      "Epoch 3[37/78] Val Loss:0.5037811398506165\n",
      "Epoch 3[38/78] Val Loss:0.304287314414978\n",
      "Epoch 3[39/78] Val Loss:0.30895721912384033\n",
      "Epoch 3[40/78] Val Loss:0.3315632939338684\n",
      "Epoch 3[41/78] Val Loss:0.32343438267707825\n",
      "Epoch 3[42/78] Val Loss:0.35472938418388367\n",
      "Epoch 3[43/78] Val Loss:0.19332952797412872\n",
      "Epoch 3[44/78] Val Loss:0.11897168308496475\n",
      "Epoch 3[45/78] Val Loss:0.11359245330095291\n",
      "Epoch 3[46/78] Val Loss:0.1279968023300171\n",
      "Epoch 3[47/78] Val Loss:0.12705931067466736\n",
      "Epoch 3[48/78] Val Loss:0.17076696455478668\n",
      "Epoch 3[49/78] Val Loss:0.1266203671693802\n",
      "Epoch 3[50/78] Val Loss:0.09996892511844635\n",
      "Epoch 3[51/78] Val Loss:0.14125961065292358\n",
      "Epoch 3[52/78] Val Loss:0.13670313358306885\n",
      "Epoch 3[53/78] Val Loss:0.12553848326206207\n",
      "Epoch 3[54/78] Val Loss:0.09931299090385437\n",
      "Epoch 3[55/78] Val Loss:0.11491231620311737\n",
      "Epoch 3[56/78] Val Loss:0.37203875184059143\n",
      "Epoch 3[57/78] Val Loss:0.32544809579849243\n",
      "Epoch 3[58/78] Val Loss:0.2913838028907776\n",
      "Epoch 3[59/78] Val Loss:0.3893447816371918\n",
      "Epoch 3[60/78] Val Loss:0.38804399967193604\n",
      "Epoch 3[61/78] Val Loss:0.47888484597206116\n",
      "Epoch 3[62/78] Val Loss:0.4228246808052063\n",
      "Epoch 3[63/78] Val Loss:0.2678842842578888\n",
      "Epoch 3[64/78] Val Loss:0.1570815145969391\n",
      "Epoch 3[65/78] Val Loss:0.12969759106636047\n",
      "Epoch 3[66/78] Val Loss:0.16688449680805206\n",
      "Epoch 3[67/78] Val Loss:0.13996408879756927\n",
      "Epoch 3[68/78] Val Loss:0.37538620829582214\n",
      "Epoch 3[69/78] Val Loss:0.38315099477767944\n",
      "Epoch 3[70/78] Val Loss:0.3832641541957855\n",
      "Epoch 3[71/78] Val Loss:0.30778276920318604\n",
      "Epoch 3[72/78] Val Loss:0.1472773551940918\n",
      "Epoch 3[73/78] Val Loss:0.11446305364370346\n",
      "Epoch 3[74/78] Val Loss:0.5567013621330261\n",
      "Epoch 3[75/78] Val Loss:0.6295274496078491\n",
      "Epoch 3[76/78] Val Loss:0.5865611433982849\n",
      "Epoch 3[77/78] Val Loss:0.647712767124176\n",
      "Epoch 3[78/78] Val Loss:0.692732572555542\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90     15691\n",
      "           1       0.63      0.75      0.68      4309\n",
      "\n",
      "    accuracy                           0.85     20000\n",
      "   macro avg       0.78      0.82      0.79     20000\n",
      "weighted avg       0.86      0.85      0.85     20000\n",
      "\n",
      "Epoch 3: Train Loss 0.3295533000707626, Val Loss 0.37887424182815427, Train Time 804.3777918815613, Val Time 39.44765877723694\n",
      "Epoch 4[0/625] Time:0.692, Train Loss:0.379212886095047\n",
      "Epoch 4[1/625] Time:0.705, Train Loss:0.31884872913360596\n",
      "Epoch 4[2/625] Time:0.703, Train Loss:0.3619626760482788\n",
      "Epoch 4[3/625] Time:0.702, Train Loss:0.37630584836006165\n",
      "Epoch 4[4/625] Time:0.7, Train Loss:0.38298937678337097\n",
      "Epoch 4[5/625] Time:0.694, Train Loss:0.32754743099212646\n",
      "Epoch 4[6/625] Time:0.703, Train Loss:0.27258768677711487\n",
      "Epoch 4[7/625] Time:0.703, Train Loss:0.28124773502349854\n",
      "Epoch 4[8/625] Time:0.702, Train Loss:0.3956301510334015\n",
      "Epoch 4[9/625] Time:0.746, Train Loss:0.3134261667728424\n",
      "Epoch 4[10/625] Time:0.702, Train Loss:0.29033249616622925\n",
      "Epoch 4[11/625] Time:0.703, Train Loss:0.42661038041114807\n",
      "Epoch 4[12/625] Time:0.703, Train Loss:0.32397791743278503\n",
      "Epoch 4[13/625] Time:0.702, Train Loss:0.2992193400859833\n",
      "Epoch 4[14/625] Time:0.7, Train Loss:0.23958922922611237\n",
      "Epoch 4[15/625] Time:0.7, Train Loss:0.3904366195201874\n",
      "Epoch 4[16/625] Time:0.702, Train Loss:0.257828950881958\n",
      "Epoch 4[17/625] Time:0.708, Train Loss:0.24455998837947845\n",
      "Epoch 4[18/625] Time:0.703, Train Loss:0.20058074593544006\n",
      "Epoch 4[19/625] Time:0.703, Train Loss:0.3013778328895569\n",
      "Epoch 4[20/625] Time:0.694, Train Loss:0.224437415599823\n",
      "Epoch 4[21/625] Time:0.693, Train Loss:0.3006165623664856\n",
      "Epoch 4[22/625] Time:0.693, Train Loss:0.2504310607910156\n",
      "Epoch 4[23/625] Time:0.694, Train Loss:0.3161415755748749\n",
      "Epoch 4[24/625] Time:0.693, Train Loss:0.3150039613246918\n",
      "Epoch 4[25/625] Time:0.694, Train Loss:0.336836040019989\n",
      "Epoch 4[26/625] Time:0.695, Train Loss:0.2687850892543793\n",
      "Epoch 4[27/625] Time:0.694, Train Loss:0.36504247784614563\n",
      "Epoch 4[28/625] Time:0.724, Train Loss:0.3389063775539398\n",
      "Epoch 4[29/625] Time:0.702, Train Loss:0.34633922576904297\n",
      "Epoch 4[30/625] Time:0.702, Train Loss:0.3124934434890747\n",
      "Epoch 4[31/625] Time:0.703, Train Loss:0.29009050130844116\n",
      "Epoch 4[32/625] Time:0.702, Train Loss:0.2672627866268158\n",
      "Epoch 4[33/625] Time:0.704, Train Loss:0.37413278222084045\n",
      "Epoch 4[34/625] Time:0.705, Train Loss:0.28832754492759705\n",
      "Epoch 4[35/625] Time:0.704, Train Loss:0.41750380396842957\n",
      "Epoch 4[36/625] Time:0.744, Train Loss:0.28497207164764404\n",
      "Epoch 4[37/625] Time:0.694, Train Loss:0.32410532236099243\n",
      "Epoch 4[38/625] Time:0.695, Train Loss:0.29589030146598816\n",
      "Epoch 4[39/625] Time:0.694, Train Loss:0.3500228226184845\n",
      "Epoch 4[40/625] Time:0.711, Train Loss:0.35631895065307617\n",
      "Epoch 4[41/625] Time:0.694, Train Loss:0.322933554649353\n",
      "Epoch 4[42/625] Time:0.696, Train Loss:0.38503316044807434\n",
      "Epoch 4[43/625] Time:0.693, Train Loss:0.2999066710472107\n",
      "Epoch 4[44/625] Time:0.694, Train Loss:0.31611964106559753\n",
      "Epoch 4[45/625] Time:0.694, Train Loss:0.33610865473747253\n",
      "Epoch 4[46/625] Time:0.695, Train Loss:0.33153051137924194\n",
      "Epoch 4[47/625] Time:0.697, Train Loss:0.30094462633132935\n",
      "Epoch 4[48/625] Time:0.694, Train Loss:0.290182501077652\n",
      "Epoch 4[49/625] Time:0.703, Train Loss:0.2732096016407013\n",
      "Epoch 4[50/625] Time:0.696, Train Loss:0.25903183221817017\n",
      "Epoch 4[51/625] Time:0.694, Train Loss:0.45092907547950745\n",
      "Epoch 4[52/625] Time:0.695, Train Loss:0.3637996315956116\n",
      "Epoch 4[53/625] Time:0.694, Train Loss:0.37181898951530457\n",
      "Epoch 4[54/625] Time:0.693, Train Loss:0.39955049753189087\n",
      "Epoch 4[55/625] Time:0.694, Train Loss:0.34910088777542114\n",
      "Epoch 4[56/625] Time:0.717, Train Loss:0.4455764889717102\n",
      "Epoch 4[57/625] Time:0.702, Train Loss:0.3295585811138153\n",
      "Epoch 4[58/625] Time:0.694, Train Loss:0.3374437689781189\n",
      "Epoch 4[59/625] Time:0.695, Train Loss:0.3244115710258484\n",
      "Epoch 4[60/625] Time:0.694, Train Loss:0.31207722425460815\n",
      "Epoch 4[61/625] Time:0.695, Train Loss:0.31023073196411133\n",
      "Epoch 4[62/625] Time:0.695, Train Loss:0.3014102280139923\n",
      "Epoch 4[63/625] Time:0.699, Train Loss:0.35722973942756653\n",
      "Epoch 4[64/625] Time:0.694, Train Loss:0.26765862107276917\n",
      "Epoch 4[65/625] Time:0.695, Train Loss:0.3242306709289551\n",
      "Epoch 4[66/625] Time:0.694, Train Loss:0.3607492446899414\n",
      "Epoch 4[67/625] Time:0.696, Train Loss:0.3998970091342926\n",
      "Epoch 4[68/625] Time:0.695, Train Loss:0.27162447571754456\n",
      "Epoch 4[69/625] Time:0.695, Train Loss:0.2520948350429535\n",
      "Epoch 4[70/625] Time:0.719, Train Loss:0.2865665555000305\n",
      "Epoch 4[71/625] Time:0.694, Train Loss:0.3247089684009552\n",
      "Epoch 4[72/625] Time:0.695, Train Loss:0.3524833023548126\n",
      "Epoch 4[73/625] Time:0.694, Train Loss:0.35954010486602783\n",
      "Epoch 4[74/625] Time:0.695, Train Loss:0.36400529742240906\n",
      "Epoch 4[75/625] Time:0.695, Train Loss:0.36967310309410095\n",
      "Epoch 4[76/625] Time:0.695, Train Loss:0.37864089012145996\n",
      "Epoch 4[77/625] Time:0.695, Train Loss:0.32634565234184265\n",
      "Epoch 4[78/625] Time:0.694, Train Loss:0.3279474973678589\n",
      "Epoch 4[79/625] Time:0.696, Train Loss:0.3187755048274994\n",
      "Epoch 4[80/625] Time:0.734, Train Loss:0.39770054817199707\n",
      "Epoch 4[81/625] Time:0.694, Train Loss:0.36179956793785095\n",
      "Epoch 4[82/625] Time:0.696, Train Loss:0.368628591299057\n",
      "Epoch 4[83/625] Time:0.697, Train Loss:0.3566876947879791\n",
      "Epoch 4[84/625] Time:0.695, Train Loss:0.26310864090919495\n",
      "Epoch 4[85/625] Time:0.696, Train Loss:0.31666699051856995\n",
      "Epoch 4[86/625] Time:0.695, Train Loss:0.31459906697273254\n",
      "Epoch 4[87/625] Time:0.696, Train Loss:0.3328050374984741\n",
      "Epoch 4[88/625] Time:0.696, Train Loss:0.26514095067977905\n",
      "Epoch 4[89/625] Time:0.695, Train Loss:0.25124281644821167\n",
      "Epoch 4[90/625] Time:0.695, Train Loss:0.2722892165184021\n",
      "Epoch 4[91/625] Time:0.693, Train Loss:0.31265562772750854\n",
      "Epoch 4[92/625] Time:0.695, Train Loss:0.3504747152328491\n",
      "Epoch 4[93/625] Time:0.695, Train Loss:0.2877981662750244\n",
      "Epoch 4[94/625] Time:0.697, Train Loss:0.29565510153770447\n",
      "Epoch 4[95/625] Time:0.694, Train Loss:0.31276461482048035\n",
      "Epoch 4[96/625] Time:0.695, Train Loss:0.34386441111564636\n",
      "Epoch 4[97/625] Time:0.696, Train Loss:0.27180027961730957\n",
      "Epoch 4[98/625] Time:0.735, Train Loss:0.28785327076911926\n",
      "Epoch 4[99/625] Time:0.694, Train Loss:0.5306969881057739\n",
      "Epoch 4[100/625] Time:0.694, Train Loss:0.3553858697414398\n",
      "Epoch 4[101/625] Time:0.694, Train Loss:0.2725800573825836\n",
      "Epoch 4[102/625] Time:0.701, Train Loss:0.30729180574417114\n",
      "Epoch 4[103/625] Time:0.724, Train Loss:0.41186606884002686\n",
      "Epoch 4[104/625] Time:0.709, Train Loss:0.2839450240135193\n",
      "Epoch 4[105/625] Time:0.694, Train Loss:0.26726454496383667\n",
      "Epoch 4[106/625] Time:0.695, Train Loss:0.3120427131652832\n",
      "Epoch 4[107/625] Time:0.695, Train Loss:0.4245453178882599\n",
      "Epoch 4[108/625] Time:0.724, Train Loss:0.3163401186466217\n",
      "Epoch 4[109/625] Time:0.702, Train Loss:0.36316290497779846\n",
      "Epoch 4[110/625] Time:0.693, Train Loss:0.3241981267929077\n",
      "Epoch 4[111/625] Time:0.734, Train Loss:0.3658605217933655\n",
      "Epoch 4[112/625] Time:0.702, Train Loss:0.33516576886177063\n",
      "Epoch 4[113/625] Time:0.704, Train Loss:0.4120914340019226\n",
      "Epoch 4[114/625] Time:0.704, Train Loss:0.29854220151901245\n",
      "Epoch 4[115/625] Time:0.703, Train Loss:0.25983062386512756\n",
      "Epoch 4[116/625] Time:0.703, Train Loss:0.30500155687332153\n",
      "Epoch 4[117/625] Time:0.704, Train Loss:0.3269282877445221\n",
      "Epoch 4[118/625] Time:0.695, Train Loss:0.2172948271036148\n",
      "Epoch 4[119/625] Time:0.698, Train Loss:0.42690154910087585\n",
      "Epoch 4[120/625] Time:0.695, Train Loss:0.2874129116535187\n",
      "Epoch 4[121/625] Time:0.694, Train Loss:0.31615564227104187\n",
      "Epoch 4[122/625] Time:0.703, Train Loss:0.3589822053909302\n",
      "Epoch 4[123/625] Time:0.702, Train Loss:0.28854188323020935\n",
      "Epoch 4[124/625] Time:0.703, Train Loss:0.26909759640693665\n",
      "Epoch 4[125/625] Time:0.704, Train Loss:0.3354099690914154\n",
      "Epoch 4[126/625] Time:0.702, Train Loss:0.3877987265586853\n",
      "Epoch 4[127/625] Time:0.701, Train Loss:0.278982549905777\n",
      "Epoch 4[128/625] Time:0.703, Train Loss:0.34738096594810486\n",
      "Epoch 4[129/625] Time:0.704, Train Loss:0.31422847509384155\n",
      "Epoch 4[130/625] Time:0.704, Train Loss:0.31233876943588257\n",
      "Epoch 4[131/625] Time:0.703, Train Loss:0.2695552408695221\n",
      "Epoch 4[132/625] Time:0.704, Train Loss:0.3466465473175049\n",
      "Epoch 4[133/625] Time:0.704, Train Loss:0.29798099398612976\n",
      "Epoch 4[134/625] Time:0.693, Train Loss:0.3752403259277344\n",
      "Epoch 4[135/625] Time:0.731, Train Loss:0.4135385751724243\n",
      "Epoch 4[136/625] Time:0.702, Train Loss:0.32519426941871643\n",
      "Epoch 4[137/625] Time:0.702, Train Loss:0.27332445979118347\n",
      "Epoch 4[138/625] Time:0.703, Train Loss:0.2798496186733246\n",
      "Epoch 4[139/625] Time:0.694, Train Loss:0.32939887046813965\n",
      "Epoch 4[140/625] Time:0.694, Train Loss:0.23825426399707794\n",
      "Epoch 4[141/625] Time:0.694, Train Loss:0.320925772190094\n",
      "Epoch 4[142/625] Time:0.694, Train Loss:0.276213139295578\n",
      "Epoch 4[143/625] Time:0.693, Train Loss:0.31232407689094543\n",
      "Epoch 4[144/625] Time:0.694, Train Loss:0.35193997621536255\n",
      "Epoch 4[145/625] Time:0.694, Train Loss:0.35023272037506104\n",
      "Epoch 4[146/625] Time:0.694, Train Loss:0.2464163452386856\n",
      "Epoch 4[147/625] Time:0.695, Train Loss:0.27183079719543457\n",
      "Epoch 4[148/625] Time:0.694, Train Loss:0.3134145736694336\n",
      "Epoch 4[149/625] Time:0.718, Train Loss:0.3163720369338989\n",
      "Epoch 4[150/625] Time:0.702, Train Loss:0.2686867415904999\n",
      "Epoch 4[151/625] Time:0.73, Train Loss:0.4085223972797394\n",
      "Epoch 4[152/625] Time:0.693, Train Loss:0.3245696723461151\n",
      "Epoch 4[153/625] Time:0.695, Train Loss:0.3913000524044037\n",
      "Epoch 4[154/625] Time:0.695, Train Loss:0.27874332666397095\n",
      "Epoch 4[155/625] Time:0.695, Train Loss:0.2474096566438675\n",
      "Epoch 4[156/625] Time:0.697, Train Loss:0.24481578171253204\n",
      "Epoch 4[157/625] Time:0.716, Train Loss:0.317394882440567\n",
      "Epoch 4[158/625] Time:0.693, Train Loss:0.40087196230888367\n",
      "Epoch 4[159/625] Time:0.695, Train Loss:0.27514973282814026\n",
      "Epoch 4[160/625] Time:0.695, Train Loss:0.2625131905078888\n",
      "Epoch 4[161/625] Time:0.694, Train Loss:0.2339419573545456\n",
      "Epoch 4[162/625] Time:0.694, Train Loss:0.29560449719429016\n",
      "Epoch 4[163/625] Time:0.695, Train Loss:0.3564991056919098\n",
      "Epoch 4[164/625] Time:0.701, Train Loss:0.3189443349838257\n",
      "Epoch 4[165/625] Time:0.694, Train Loss:0.4135777950286865\n",
      "Epoch 4[166/625] Time:0.695, Train Loss:0.3584797978401184\n",
      "Epoch 4[167/625] Time:0.694, Train Loss:0.33598917722702026\n",
      "Epoch 4[168/625] Time:0.738, Train Loss:0.33171892166137695\n",
      "Epoch 4[169/625] Time:0.702, Train Loss:0.33420678973197937\n",
      "Epoch 4[170/625] Time:0.705, Train Loss:0.3345773220062256\n",
      "Epoch 4[171/625] Time:0.704, Train Loss:0.22277015447616577\n",
      "Epoch 4[172/625] Time:0.705, Train Loss:0.2858606278896332\n",
      "Epoch 4[173/625] Time:0.694, Train Loss:0.2508623003959656\n",
      "Epoch 4[174/625] Time:0.694, Train Loss:0.20212779939174652\n",
      "Epoch 4[175/625] Time:0.694, Train Loss:0.33021044731140137\n",
      "Epoch 4[176/625] Time:0.694, Train Loss:0.23208743333816528\n",
      "Epoch 4[177/625] Time:0.694, Train Loss:0.29406794905662537\n",
      "Epoch 4[178/625] Time:0.695, Train Loss:0.25964394211769104\n",
      "Epoch 4[179/625] Time:0.695, Train Loss:0.318647563457489\n",
      "Epoch 4[180/625] Time:0.697, Train Loss:0.2550796866416931\n",
      "Epoch 4[181/625] Time:0.696, Train Loss:0.42352527379989624\n",
      "Epoch 4[182/625] Time:0.707, Train Loss:0.2988409101963043\n",
      "Epoch 4[183/625] Time:0.698, Train Loss:0.31529146432876587\n",
      "Epoch 4[184/625] Time:0.694, Train Loss:0.29013848304748535\n",
      "Epoch 4[185/625] Time:0.696, Train Loss:0.2761884331703186\n",
      "Epoch 4[186/625] Time:0.696, Train Loss:0.32801172137260437\n",
      "Epoch 4[187/625] Time:0.7, Train Loss:0.3495514988899231\n",
      "Epoch 4[188/625] Time:0.695, Train Loss:0.3345142602920532\n",
      "Epoch 4[189/625] Time:0.696, Train Loss:0.41577962040901184\n",
      "Epoch 4[190/625] Time:0.696, Train Loss:0.3434167504310608\n",
      "Epoch 4[191/625] Time:0.727, Train Loss:0.2997851073741913\n",
      "Epoch 4[192/625] Time:0.692, Train Loss:0.34273087978363037\n",
      "Epoch 4[193/625] Time:0.695, Train Loss:0.28634098172187805\n",
      "Epoch 4[194/625] Time:0.694, Train Loss:0.2890501320362091\n",
      "Epoch 4[195/625] Time:0.695, Train Loss:0.304421603679657\n",
      "Epoch 4[196/625] Time:0.695, Train Loss:0.2992406487464905\n",
      "Epoch 4[197/625] Time:0.695, Train Loss:0.38774585723876953\n",
      "Epoch 4[198/625] Time:0.733, Train Loss:0.3093571364879608\n",
      "Epoch 4[199/625] Time:0.701, Train Loss:0.34660300612449646\n",
      "Epoch 4[200/625] Time:0.742, Train Loss:0.3004569411277771\n",
      "Epoch 4[201/625] Time:0.702, Train Loss:0.33270230889320374\n",
      "Epoch 4[202/625] Time:0.722, Train Loss:0.292520135641098\n",
      "Epoch 4[203/625] Time:0.694, Train Loss:0.3333176076412201\n",
      "Epoch 4[204/625] Time:0.703, Train Loss:0.42237526178359985\n",
      "Epoch 4[205/625] Time:0.703, Train Loss:0.2920498549938202\n",
      "Epoch 4[206/625] Time:0.694, Train Loss:0.27373766899108887\n",
      "Epoch 4[207/625] Time:0.696, Train Loss:0.3275025188922882\n",
      "Epoch 4[208/625] Time:0.695, Train Loss:0.3661002218723297\n",
      "Epoch 4[209/625] Time:0.729, Train Loss:0.47942405939102173\n",
      "Epoch 4[210/625] Time:0.702, Train Loss:0.44064775109291077\n",
      "Epoch 4[211/625] Time:0.697, Train Loss:0.3686644434928894\n",
      "Epoch 4[212/625] Time:0.695, Train Loss:0.3534213602542877\n",
      "Epoch 4[213/625] Time:0.694, Train Loss:0.3631691038608551\n",
      "Epoch 4[214/625] Time:0.695, Train Loss:0.34800586104393005\n",
      "Epoch 4[215/625] Time:0.695, Train Loss:0.33486807346343994\n",
      "Epoch 4[216/625] Time:0.695, Train Loss:0.3388407826423645\n",
      "Epoch 4[217/625] Time:0.695, Train Loss:0.400386780500412\n",
      "Epoch 4[218/625] Time:0.694, Train Loss:0.31325873732566833\n",
      "Epoch 4[219/625] Time:0.694, Train Loss:0.317840039730072\n",
      "Epoch 4[220/625] Time:0.695, Train Loss:0.37237557768821716\n",
      "Epoch 4[221/625] Time:0.695, Train Loss:0.28343647718429565\n",
      "Epoch 4[222/625] Time:0.695, Train Loss:0.3263271152973175\n",
      "Epoch 4[223/625] Time:0.696, Train Loss:0.3660254180431366\n",
      "Epoch 4[224/625] Time:0.696, Train Loss:0.3064955770969391\n",
      "Epoch 4[225/625] Time:0.696, Train Loss:0.36113643646240234\n",
      "Epoch 4[226/625] Time:0.695, Train Loss:0.3258526921272278\n",
      "Epoch 4[227/625] Time:0.694, Train Loss:0.28623780608177185\n",
      "Epoch 4[228/625] Time:0.694, Train Loss:0.28843027353286743\n",
      "Epoch 4[229/625] Time:0.695, Train Loss:0.3187738358974457\n",
      "Epoch 4[230/625] Time:0.693, Train Loss:0.2735089957714081\n",
      "Epoch 4[231/625] Time:0.694, Train Loss:0.3118690252304077\n",
      "Epoch 4[232/625] Time:0.695, Train Loss:0.3686561584472656\n",
      "Epoch 4[233/625] Time:0.696, Train Loss:0.31070539355278015\n",
      "Epoch 4[234/625] Time:0.695, Train Loss:0.3305498957633972\n",
      "Epoch 4[235/625] Time:0.695, Train Loss:0.3073672652244568\n",
      "Epoch 4[236/625] Time:0.694, Train Loss:0.322470486164093\n",
      "Epoch 4[237/625] Time:0.695, Train Loss:0.2611311972141266\n",
      "Epoch 4[238/625] Time:0.695, Train Loss:0.2624378800392151\n",
      "Epoch 4[239/625] Time:0.694, Train Loss:0.33699727058410645\n",
      "Epoch 4[240/625] Time:0.721, Train Loss:0.418745219707489\n",
      "Epoch 4[241/625] Time:0.717, Train Loss:0.2863907516002655\n",
      "Epoch 4[242/625] Time:0.696, Train Loss:0.3378974199295044\n",
      "Epoch 4[243/625] Time:0.695, Train Loss:0.3250964879989624\n",
      "Epoch 4[244/625] Time:0.695, Train Loss:0.35094884037971497\n",
      "Epoch 4[245/625] Time:0.695, Train Loss:0.3055647611618042\n",
      "Epoch 4[246/625] Time:0.695, Train Loss:0.37944984436035156\n",
      "Epoch 4[247/625] Time:0.694, Train Loss:0.2727201581001282\n",
      "Epoch 4[248/625] Time:0.695, Train Loss:0.33359014987945557\n",
      "Epoch 4[249/625] Time:0.695, Train Loss:0.3081749975681305\n",
      "Epoch 4[250/625] Time:0.695, Train Loss:0.21866728365421295\n",
      "Epoch 4[251/625] Time:0.695, Train Loss:0.27960583567619324\n",
      "Epoch 4[252/625] Time:0.694, Train Loss:0.33077505230903625\n",
      "Epoch 4[253/625] Time:0.694, Train Loss:0.2876491844654083\n",
      "Epoch 4[254/625] Time:0.697, Train Loss:0.260925829410553\n",
      "Epoch 4[255/625] Time:0.696, Train Loss:0.3273843824863434\n",
      "Epoch 4[256/625] Time:0.694, Train Loss:0.3003142178058624\n",
      "Epoch 4[257/625] Time:0.702, Train Loss:0.28353822231292725\n",
      "Epoch 4[258/625] Time:0.696, Train Loss:0.3463827073574066\n",
      "Epoch 4[259/625] Time:0.697, Train Loss:0.29335346817970276\n",
      "Epoch 4[260/625] Time:0.695, Train Loss:0.3142129182815552\n",
      "Epoch 4[261/625] Time:0.695, Train Loss:0.303552508354187\n",
      "Epoch 4[262/625] Time:0.698, Train Loss:0.35411545634269714\n",
      "Epoch 4[263/625] Time:0.696, Train Loss:0.33281657099723816\n",
      "Epoch 4[264/625] Time:0.719, Train Loss:0.307570219039917\n",
      "Epoch 4[265/625] Time:0.693, Train Loss:0.3315529227256775\n",
      "Epoch 4[266/625] Time:0.694, Train Loss:0.3003860414028168\n",
      "Epoch 4[267/625] Time:0.694, Train Loss:0.2623785734176636\n",
      "Epoch 4[268/625] Time:0.694, Train Loss:0.3522917926311493\n",
      "Epoch 4[269/625] Time:0.702, Train Loss:0.2821817994117737\n",
      "Epoch 4[270/625] Time:0.704, Train Loss:0.2605684697628021\n",
      "Epoch 4[271/625] Time:0.703, Train Loss:0.3747860789299011\n",
      "Epoch 4[272/625] Time:0.705, Train Loss:0.32335972785949707\n",
      "Epoch 4[273/625] Time:0.695, Train Loss:0.32066646218299866\n",
      "Epoch 4[274/625] Time:0.695, Train Loss:0.2651083767414093\n",
      "Epoch 4[275/625] Time:0.697, Train Loss:0.30357441306114197\n",
      "Epoch 4[276/625] Time:0.695, Train Loss:0.30144304037094116\n",
      "Epoch 4[277/625] Time:0.695, Train Loss:0.3462487757205963\n",
      "Epoch 4[278/625] Time:0.729, Train Loss:0.29475441575050354\n",
      "Epoch 4[279/625] Time:0.717, Train Loss:0.2875997722148895\n",
      "Epoch 4[280/625] Time:0.695, Train Loss:0.2917979955673218\n",
      "Epoch 4[281/625] Time:0.695, Train Loss:0.3791317045688629\n",
      "Epoch 4[282/625] Time:0.695, Train Loss:0.29020363092422485\n",
      "Epoch 4[283/625] Time:0.694, Train Loss:0.25159066915512085\n",
      "Epoch 4[284/625] Time:0.695, Train Loss:0.3301938474178314\n",
      "Epoch 4[285/625] Time:0.695, Train Loss:0.24076110124588013\n",
      "Epoch 4[286/625] Time:0.695, Train Loss:0.2887818217277527\n",
      "Epoch 4[287/625] Time:0.721, Train Loss:0.2934187054634094\n",
      "Epoch 4[288/625] Time:0.708, Train Loss:0.3030959963798523\n",
      "Epoch 4[289/625] Time:0.694, Train Loss:0.27568480372428894\n",
      "Epoch 4[290/625] Time:0.725, Train Loss:0.30678942799568176\n",
      "Epoch 4[291/625] Time:0.701, Train Loss:0.3138045370578766\n",
      "Epoch 4[292/625] Time:0.701, Train Loss:0.24546794593334198\n",
      "Epoch 4[293/625] Time:0.694, Train Loss:0.27323344349861145\n",
      "Epoch 4[294/625] Time:0.695, Train Loss:0.31756967306137085\n",
      "Epoch 4[295/625] Time:0.695, Train Loss:0.29755258560180664\n",
      "Epoch 4[296/625] Time:0.695, Train Loss:0.31721192598342896\n",
      "Epoch 4[297/625] Time:0.695, Train Loss:0.41409799456596375\n",
      "Epoch 4[298/625] Time:0.694, Train Loss:0.34675052762031555\n",
      "Epoch 4[299/625] Time:0.695, Train Loss:0.24125388264656067\n",
      "Epoch 4[300/625] Time:0.694, Train Loss:0.30014100670814514\n",
      "Epoch 4[301/625] Time:0.695, Train Loss:0.28483065962791443\n",
      "Epoch 4[302/625] Time:0.694, Train Loss:0.3046804964542389\n",
      "Epoch 4[303/625] Time:0.695, Train Loss:0.3974481225013733\n",
      "Epoch 4[304/625] Time:0.695, Train Loss:0.24427540600299835\n",
      "Epoch 4[305/625] Time:0.695, Train Loss:0.29323020577430725\n",
      "Epoch 4[306/625] Time:0.695, Train Loss:0.28697720170021057\n",
      "Epoch 4[307/625] Time:0.694, Train Loss:0.3127869665622711\n",
      "Epoch 4[308/625] Time:0.696, Train Loss:0.29989340901374817\n",
      "Epoch 4[309/625] Time:0.695, Train Loss:0.35490062832832336\n",
      "Epoch 4[310/625] Time:0.695, Train Loss:0.3126378357410431\n",
      "Epoch 4[311/625] Time:0.695, Train Loss:0.30800962448120117\n",
      "Epoch 4[312/625] Time:0.741, Train Loss:0.33519595861434937\n",
      "Epoch 4[313/625] Time:0.716, Train Loss:0.3390793800354004\n",
      "Epoch 4[314/625] Time:0.696, Train Loss:0.31172654032707214\n",
      "Epoch 4[315/625] Time:0.696, Train Loss:0.279069721698761\n",
      "Epoch 4[316/625] Time:0.695, Train Loss:0.36862045526504517\n",
      "Epoch 4[317/625] Time:0.736, Train Loss:0.31197425723075867\n",
      "Epoch 4[318/625] Time:0.697, Train Loss:0.3247857689857483\n",
      "Epoch 4[319/625] Time:0.695, Train Loss:0.30404531955718994\n",
      "Epoch 4[320/625] Time:0.701, Train Loss:0.33210527896881104\n",
      "Epoch 4[321/625] Time:0.694, Train Loss:0.33280661702156067\n",
      "Epoch 4[322/625] Time:0.726, Train Loss:0.3138619363307953\n",
      "Epoch 4[323/625] Time:0.702, Train Loss:0.33936476707458496\n",
      "Epoch 4[324/625] Time:0.694, Train Loss:0.3103557229042053\n",
      "Epoch 4[325/625] Time:0.692, Train Loss:0.37144985795021057\n",
      "Epoch 4[326/625] Time:0.694, Train Loss:0.3270036280155182\n",
      "Epoch 4[327/625] Time:0.705, Train Loss:0.2686409652233124\n",
      "Epoch 4[328/625] Time:0.704, Train Loss:0.3886335790157318\n",
      "Epoch 4[329/625] Time:0.723, Train Loss:0.33156004548072815\n",
      "Epoch 4[330/625] Time:0.716, Train Loss:0.291272908449173\n",
      "Epoch 4[331/625] Time:0.702, Train Loss:0.2890787720680237\n",
      "Epoch 4[332/625] Time:0.696, Train Loss:0.3574882745742798\n",
      "Epoch 4[333/625] Time:0.723, Train Loss:0.28953438997268677\n",
      "Epoch 4[334/625] Time:0.704, Train Loss:0.3205665946006775\n",
      "Epoch 4[335/625] Time:0.695, Train Loss:0.2834417521953583\n",
      "Epoch 4[336/625] Time:0.695, Train Loss:0.4108486473560333\n",
      "Epoch 4[337/625] Time:0.695, Train Loss:0.27788931131362915\n",
      "Epoch 4[338/625] Time:0.722, Train Loss:0.2934633195400238\n",
      "Epoch 4[339/625] Time:0.694, Train Loss:0.33241984248161316\n",
      "Epoch 4[340/625] Time:0.694, Train Loss:0.30855947732925415\n",
      "Epoch 4[341/625] Time:0.694, Train Loss:0.3172009289264679\n",
      "Epoch 4[342/625] Time:0.694, Train Loss:0.2748485207557678\n",
      "Epoch 4[343/625] Time:0.695, Train Loss:0.2846895158290863\n",
      "Epoch 4[344/625] Time:0.716, Train Loss:0.2756986916065216\n",
      "Epoch 4[345/625] Time:0.695, Train Loss:0.2804695665836334\n",
      "Epoch 4[346/625] Time:0.695, Train Loss:0.3843330442905426\n",
      "Epoch 4[347/625] Time:0.694, Train Loss:0.3618282079696655\n",
      "Epoch 4[348/625] Time:0.697, Train Loss:0.38075464963912964\n",
      "Epoch 4[349/625] Time:0.694, Train Loss:0.2649441063404083\n",
      "Epoch 4[350/625] Time:0.729, Train Loss:0.35675227642059326\n",
      "Epoch 4[351/625] Time:0.694, Train Loss:0.4382156729698181\n",
      "Epoch 4[352/625] Time:0.694, Train Loss:0.2889365255832672\n",
      "Epoch 4[353/625] Time:0.727, Train Loss:0.3775688409805298\n",
      "Epoch 4[354/625] Time:0.703, Train Loss:0.3212035000324249\n",
      "Epoch 4[355/625] Time:0.706, Train Loss:0.30305245518684387\n",
      "Epoch 4[356/625] Time:0.694, Train Loss:0.3373148441314697\n",
      "Epoch 4[357/625] Time:0.694, Train Loss:0.3029240667819977\n",
      "Epoch 4[358/625] Time:0.695, Train Loss:0.32907330989837646\n",
      "Epoch 4[359/625] Time:0.695, Train Loss:0.319436639547348\n",
      "Epoch 4[360/625] Time:0.697, Train Loss:0.28895196318626404\n",
      "Epoch 4[361/625] Time:0.694, Train Loss:0.39509689807891846\n",
      "Epoch 4[362/625] Time:0.694, Train Loss:0.3299477696418762\n",
      "Epoch 4[363/625] Time:0.694, Train Loss:0.33496078848838806\n",
      "Epoch 4[364/625] Time:0.694, Train Loss:0.2871931493282318\n",
      "Epoch 4[365/625] Time:0.695, Train Loss:0.25986552238464355\n",
      "Epoch 4[366/625] Time:0.695, Train Loss:0.30310603976249695\n",
      "Epoch 4[367/625] Time:0.733, Train Loss:0.2786799967288971\n",
      "Epoch 4[368/625] Time:0.716, Train Loss:0.35456621646881104\n",
      "Epoch 4[369/625] Time:0.694, Train Loss:0.27280011773109436\n",
      "Epoch 4[370/625] Time:0.694, Train Loss:0.2543567717075348\n",
      "Epoch 4[371/625] Time:0.694, Train Loss:0.29643514752388\n",
      "Epoch 4[372/625] Time:0.724, Train Loss:0.3240399956703186\n",
      "Epoch 4[373/625] Time:0.703, Train Loss:0.2616965174674988\n",
      "Epoch 4[374/625] Time:0.723, Train Loss:0.3034422695636749\n",
      "Epoch 4[375/625] Time:0.725, Train Loss:0.2838749289512634\n",
      "Epoch 4[376/625] Time:0.693, Train Loss:0.2578023374080658\n",
      "Epoch 4[377/625] Time:0.694, Train Loss:0.33128300309181213\n",
      "Epoch 4[378/625] Time:0.694, Train Loss:0.3207288384437561\n",
      "Epoch 4[379/625] Time:0.693, Train Loss:0.34825217723846436\n",
      "Epoch 4[380/625] Time:0.694, Train Loss:0.25448963046073914\n",
      "Epoch 4[381/625] Time:0.692, Train Loss:0.3214070796966553\n",
      "Epoch 4[382/625] Time:0.692, Train Loss:0.20475080609321594\n",
      "Epoch 4[383/625] Time:0.694, Train Loss:0.31218451261520386\n",
      "Epoch 4[384/625] Time:0.695, Train Loss:0.25938737392425537\n",
      "Epoch 4[385/625] Time:0.696, Train Loss:0.2827977240085602\n",
      "Epoch 4[386/625] Time:0.694, Train Loss:0.29670223593711853\n",
      "Epoch 4[387/625] Time:0.696, Train Loss:0.3028908371925354\n",
      "Epoch 4[388/625] Time:0.695, Train Loss:0.359948068857193\n",
      "Epoch 4[389/625] Time:0.695, Train Loss:0.39661288261413574\n",
      "Epoch 4[390/625] Time:0.695, Train Loss:0.3462749421596527\n",
      "Epoch 4[391/625] Time:0.703, Train Loss:0.25735440850257874\n",
      "Epoch 4[392/625] Time:0.694, Train Loss:0.20771002769470215\n",
      "Epoch 4[393/625] Time:0.695, Train Loss:0.3332494795322418\n",
      "Epoch 4[394/625] Time:0.696, Train Loss:0.27999943494796753\n",
      "Epoch 4[395/625] Time:0.695, Train Loss:0.3975379467010498\n",
      "Epoch 4[396/625] Time:0.695, Train Loss:0.33658191561698914\n",
      "Epoch 4[397/625] Time:0.7, Train Loss:0.3527974784374237\n",
      "Epoch 4[398/625] Time:0.695, Train Loss:0.33131393790245056\n",
      "Epoch 4[399/625] Time:0.695, Train Loss:0.29823094606399536\n",
      "Epoch 4[400/625] Time:0.695, Train Loss:0.2715796232223511\n",
      "Epoch 4[401/625] Time:0.695, Train Loss:0.348639577627182\n",
      "Epoch 4[402/625] Time:0.697, Train Loss:0.3453325629234314\n",
      "Epoch 4[403/625] Time:0.695, Train Loss:0.3080836832523346\n",
      "Epoch 4[404/625] Time:0.695, Train Loss:0.2929944396018982\n",
      "Epoch 4[405/625] Time:0.72, Train Loss:0.2400384247303009\n",
      "Epoch 4[406/625] Time:0.696, Train Loss:0.24916613101959229\n",
      "Epoch 4[407/625] Time:0.695, Train Loss:0.22241614758968353\n",
      "Epoch 4[408/625] Time:0.695, Train Loss:0.33312708139419556\n",
      "Epoch 4[409/625] Time:0.694, Train Loss:0.28291788697242737\n",
      "Epoch 4[410/625] Time:0.731, Train Loss:0.38232097029685974\n",
      "Epoch 4[411/625] Time:0.714, Train Loss:0.3181478679180145\n",
      "Epoch 4[412/625] Time:0.695, Train Loss:0.30802223086357117\n",
      "Epoch 4[413/625] Time:0.695, Train Loss:0.3993549644947052\n",
      "Epoch 4[414/625] Time:0.695, Train Loss:0.38342300057411194\n",
      "Epoch 4[415/625] Time:0.695, Train Loss:0.3216366767883301\n",
      "Epoch 4[416/625] Time:0.695, Train Loss:0.334840327501297\n",
      "Epoch 4[417/625] Time:0.697, Train Loss:0.38468047976493835\n",
      "Epoch 4[418/625] Time:0.695, Train Loss:0.2911042869091034\n",
      "Epoch 4[419/625] Time:0.694, Train Loss:0.3967985212802887\n",
      "Epoch 4[420/625] Time:0.695, Train Loss:0.3156236410140991\n",
      "Epoch 4[421/625] Time:0.695, Train Loss:0.3705892562866211\n",
      "Epoch 4[422/625] Time:0.696, Train Loss:0.49462661147117615\n",
      "Epoch 4[423/625] Time:0.695, Train Loss:0.34347057342529297\n",
      "Epoch 4[424/625] Time:0.696, Train Loss:0.3023391664028168\n",
      "Epoch 4[425/625] Time:0.695, Train Loss:0.3048318028450012\n",
      "Epoch 4[426/625] Time:0.695, Train Loss:0.32414180040359497\n",
      "Epoch 4[427/625] Time:0.694, Train Loss:0.29443293809890747\n",
      "Epoch 4[428/625] Time:0.696, Train Loss:0.28641241788864136\n",
      "Epoch 4[429/625] Time:0.695, Train Loss:0.3374541103839874\n",
      "Epoch 4[430/625] Time:0.694, Train Loss:0.3219833970069885\n",
      "Epoch 4[431/625] Time:0.695, Train Loss:0.2892921268939972\n",
      "Epoch 4[432/625] Time:0.695, Train Loss:0.27513381838798523\n",
      "Epoch 4[433/625] Time:0.695, Train Loss:0.3148171007633209\n",
      "Epoch 4[434/625] Time:0.694, Train Loss:0.34415900707244873\n",
      "Epoch 4[435/625] Time:0.694, Train Loss:0.2593995928764343\n",
      "Epoch 4[436/625] Time:0.698, Train Loss:0.3168351352214813\n",
      "Epoch 4[437/625] Time:0.694, Train Loss:0.30481672286987305\n",
      "Epoch 4[438/625] Time:0.694, Train Loss:0.3979859948158264\n",
      "Epoch 4[439/625] Time:0.694, Train Loss:0.23284074664115906\n",
      "Epoch 4[440/625] Time:0.693, Train Loss:0.27793923020362854\n",
      "Epoch 4[441/625] Time:0.695, Train Loss:0.36324283480644226\n",
      "Epoch 4[442/625] Time:0.699, Train Loss:0.36583203077316284\n",
      "Epoch 4[443/625] Time:0.694, Train Loss:0.29272332787513733\n",
      "Epoch 4[444/625] Time:0.711, Train Loss:0.33195456862449646\n",
      "Epoch 4[445/625] Time:0.695, Train Loss:0.2334330826997757\n",
      "Epoch 4[446/625] Time:0.697, Train Loss:0.2757262587547302\n",
      "Epoch 4[447/625] Time:0.695, Train Loss:0.2721680998802185\n",
      "Epoch 4[448/625] Time:0.696, Train Loss:0.3309694826602936\n",
      "Epoch 4[449/625] Time:0.695, Train Loss:0.32945865392684937\n",
      "Epoch 4[450/625] Time:0.695, Train Loss:0.30709728598594666\n",
      "Epoch 4[451/625] Time:0.695, Train Loss:0.33775386214256287\n",
      "Epoch 4[452/625] Time:0.746, Train Loss:0.3084709346294403\n",
      "Epoch 4[453/625] Time:0.697, Train Loss:0.37898197770118713\n",
      "Epoch 4[454/625] Time:0.695, Train Loss:0.27236637473106384\n",
      "Epoch 4[455/625] Time:0.694, Train Loss:0.3182319104671478\n",
      "Epoch 4[456/625] Time:0.695, Train Loss:0.2724361717700958\n",
      "Epoch 4[457/625] Time:0.695, Train Loss:0.29641014337539673\n",
      "Epoch 4[458/625] Time:0.694, Train Loss:0.2745600640773773\n",
      "Epoch 4[459/625] Time:0.694, Train Loss:0.32012641429901123\n",
      "Epoch 4[460/625] Time:0.694, Train Loss:0.3304750323295593\n",
      "Epoch 4[461/625] Time:0.694, Train Loss:0.32177263498306274\n",
      "Epoch 4[462/625] Time:0.728, Train Loss:0.29848840832710266\n",
      "Epoch 4[463/625] Time:0.711, Train Loss:0.2645559310913086\n",
      "Epoch 4[464/625] Time:0.697, Train Loss:0.26027345657348633\n",
      "Epoch 4[465/625] Time:0.694, Train Loss:0.3275381624698639\n",
      "Epoch 4[466/625] Time:0.695, Train Loss:0.34844285249710083\n",
      "Epoch 4[467/625] Time:0.695, Train Loss:0.3641764223575592\n",
      "Epoch 4[468/625] Time:0.695, Train Loss:0.40818729996681213\n",
      "Epoch 4[469/625] Time:0.694, Train Loss:0.3269902467727661\n",
      "Epoch 4[470/625] Time:0.695, Train Loss:0.3521348834037781\n",
      "Epoch 4[471/625] Time:0.696, Train Loss:0.3283728063106537\n",
      "Epoch 4[472/625] Time:0.695, Train Loss:0.47362446784973145\n",
      "Epoch 4[473/625] Time:0.695, Train Loss:0.29325008392333984\n",
      "Epoch 4[474/625] Time:0.695, Train Loss:0.3146967887878418\n",
      "Epoch 4[475/625] Time:0.699, Train Loss:0.3008025586605072\n",
      "Epoch 4[476/625] Time:0.702, Train Loss:0.24993574619293213\n",
      "Epoch 4[477/625] Time:0.695, Train Loss:0.3179587721824646\n",
      "Epoch 4[478/625] Time:0.695, Train Loss:0.32424044609069824\n",
      "Epoch 4[479/625] Time:0.695, Train Loss:0.3309217393398285\n",
      "Epoch 4[480/625] Time:0.694, Train Loss:0.23223042488098145\n",
      "Epoch 4[481/625] Time:0.694, Train Loss:0.2737923264503479\n",
      "Epoch 4[482/625] Time:0.695, Train Loss:0.3498111665248871\n",
      "Epoch 4[483/625] Time:0.695, Train Loss:0.3557387590408325\n",
      "Epoch 4[484/625] Time:0.695, Train Loss:0.3586796224117279\n",
      "Epoch 4[485/625] Time:0.695, Train Loss:0.40038764476776123\n",
      "Epoch 4[486/625] Time:0.694, Train Loss:0.30053648352622986\n",
      "Epoch 4[487/625] Time:0.695, Train Loss:0.2896571159362793\n",
      "Epoch 4[488/625] Time:0.694, Train Loss:0.3128998875617981\n",
      "Epoch 4[489/625] Time:0.695, Train Loss:0.3286295235157013\n",
      "Epoch 4[490/625] Time:0.699, Train Loss:0.31214791536331177\n",
      "Epoch 4[491/625] Time:0.705, Train Loss:0.35530543327331543\n",
      "Epoch 4[492/625] Time:0.696, Train Loss:0.3389258086681366\n",
      "Epoch 4[493/625] Time:0.741, Train Loss:0.36308562755584717\n",
      "Epoch 4[494/625] Time:0.719, Train Loss:0.2866930067539215\n",
      "Epoch 4[495/625] Time:0.703, Train Loss:0.26857829093933105\n",
      "Epoch 4[496/625] Time:0.712, Train Loss:0.3299297094345093\n",
      "Epoch 4[497/625] Time:0.721, Train Loss:0.2892664670944214\n",
      "Epoch 4[498/625] Time:0.697, Train Loss:0.34979507327079773\n",
      "Epoch 4[499/625] Time:0.695, Train Loss:0.3769606649875641\n",
      "Epoch 4[500/625] Time:0.696, Train Loss:0.3272683918476105\n",
      "Epoch 4[501/625] Time:0.694, Train Loss:0.33560195565223694\n",
      "Epoch 4[502/625] Time:0.703, Train Loss:0.30766138434410095\n",
      "Epoch 4[503/625] Time:0.7, Train Loss:0.32485106587409973\n",
      "Epoch 4[504/625] Time:0.701, Train Loss:0.2503483295440674\n",
      "Epoch 4[505/625] Time:0.694, Train Loss:0.21766579151153564\n",
      "Epoch 4[506/625] Time:0.695, Train Loss:0.3392569124698639\n",
      "Epoch 4[507/625] Time:0.741, Train Loss:0.38899269700050354\n",
      "Epoch 4[508/625] Time:0.695, Train Loss:0.3504710793495178\n",
      "Epoch 4[509/625] Time:0.695, Train Loss:0.2774057984352112\n",
      "Epoch 4[510/625] Time:0.695, Train Loss:0.33944085240364075\n",
      "Epoch 4[511/625] Time:0.695, Train Loss:0.3305525779724121\n",
      "Epoch 4[512/625] Time:0.694, Train Loss:0.2602652609348297\n",
      "Epoch 4[513/625] Time:0.695, Train Loss:0.28591451048851013\n",
      "Epoch 4[514/625] Time:0.694, Train Loss:0.36644378304481506\n",
      "Epoch 4[515/625] Time:0.695, Train Loss:0.2779393792152405\n",
      "Epoch 4[516/625] Time:0.694, Train Loss:0.3618641793727875\n",
      "Epoch 4[517/625] Time:0.695, Train Loss:0.2663595974445343\n",
      "Epoch 4[518/625] Time:0.695, Train Loss:0.322444349527359\n",
      "Epoch 4[519/625] Time:0.694, Train Loss:0.32304054498672485\n",
      "Epoch 4[520/625] Time:0.695, Train Loss:0.3009372055530548\n",
      "Epoch 4[521/625] Time:0.694, Train Loss:0.3057738244533539\n",
      "Epoch 4[522/625] Time:0.696, Train Loss:0.30043095350265503\n",
      "Epoch 4[523/625] Time:0.695, Train Loss:0.3622690737247467\n",
      "Epoch 4[524/625] Time:0.694, Train Loss:0.26374587416648865\n",
      "Epoch 4[525/625] Time:0.696, Train Loss:0.3645159900188446\n",
      "Epoch 4[526/625] Time:0.695, Train Loss:0.3269878625869751\n",
      "Epoch 4[527/625] Time:0.695, Train Loss:0.27806171774864197\n",
      "Epoch 4[528/625] Time:0.695, Train Loss:0.3096587359905243\n",
      "Epoch 4[529/625] Time:0.757, Train Loss:0.33809342980384827\n",
      "Epoch 4[530/625] Time:0.717, Train Loss:0.3153863549232483\n",
      "Epoch 4[531/625] Time:0.696, Train Loss:0.27671727538108826\n",
      "Epoch 4[532/625] Time:0.695, Train Loss:0.27642956376075745\n",
      "Epoch 4[533/625] Time:0.694, Train Loss:0.34499749541282654\n",
      "Epoch 4[534/625] Time:0.724, Train Loss:0.32803741097450256\n",
      "Epoch 4[535/625] Time:0.694, Train Loss:0.2859572172164917\n",
      "Epoch 4[536/625] Time:0.695, Train Loss:0.29630500078201294\n",
      "Epoch 4[537/625] Time:0.695, Train Loss:0.32075437903404236\n",
      "Epoch 4[538/625] Time:0.694, Train Loss:0.2774902284145355\n",
      "Epoch 4[539/625] Time:0.694, Train Loss:0.37339094281196594\n",
      "Epoch 4[540/625] Time:0.694, Train Loss:0.2623481750488281\n",
      "Epoch 4[541/625] Time:0.695, Train Loss:0.3293081223964691\n",
      "Epoch 4[542/625] Time:0.695, Train Loss:0.2645116150379181\n",
      "Epoch 4[543/625] Time:0.702, Train Loss:0.3488948941230774\n",
      "Epoch 4[544/625] Time:0.703, Train Loss:0.29588544368743896\n",
      "Epoch 4[545/625] Time:0.705, Train Loss:0.36762282252311707\n",
      "Epoch 4[546/625] Time:0.694, Train Loss:0.2551444172859192\n",
      "Epoch 4[547/625] Time:0.695, Train Loss:0.31349876523017883\n",
      "Epoch 4[548/625] Time:0.695, Train Loss:0.33056944608688354\n",
      "Epoch 4[549/625] Time:0.695, Train Loss:0.28370803594589233\n",
      "Epoch 4[550/625] Time:0.695, Train Loss:0.3417116105556488\n",
      "Epoch 4[551/625] Time:0.694, Train Loss:0.26987507939338684\n",
      "Epoch 4[552/625] Time:0.728, Train Loss:0.31456756591796875\n",
      "Epoch 4[553/625] Time:0.742, Train Loss:0.3115593492984772\n",
      "Epoch 4[554/625] Time:0.694, Train Loss:0.33197301626205444\n",
      "Epoch 4[555/625] Time:0.694, Train Loss:0.3249611258506775\n",
      "Epoch 4[556/625] Time:0.694, Train Loss:0.34186404943466187\n",
      "Epoch 4[557/625] Time:0.695, Train Loss:0.23535224795341492\n",
      "Epoch 4[558/625] Time:0.694, Train Loss:0.2713121175765991\n",
      "Epoch 4[559/625] Time:0.694, Train Loss:0.4038366675376892\n",
      "Epoch 4[560/625] Time:0.696, Train Loss:0.29420819878578186\n",
      "Epoch 4[561/625] Time:0.695, Train Loss:0.25077366828918457\n",
      "Epoch 4[562/625] Time:0.695, Train Loss:0.26283273100852966\n",
      "Epoch 4[563/625] Time:0.695, Train Loss:0.31111645698547363\n",
      "Epoch 4[564/625] Time:0.695, Train Loss:0.3579745292663574\n",
      "Epoch 4[565/625] Time:0.694, Train Loss:0.24797503650188446\n",
      "Epoch 4[566/625] Time:0.695, Train Loss:0.3384914994239807\n",
      "Epoch 4[567/625] Time:0.696, Train Loss:0.3152327239513397\n",
      "Epoch 4[568/625] Time:0.696, Train Loss:0.3380369246006012\n",
      "Epoch 4[569/625] Time:0.695, Train Loss:0.2927810847759247\n",
      "Epoch 4[570/625] Time:0.695, Train Loss:0.36204108595848083\n",
      "Epoch 4[571/625] Time:0.694, Train Loss:0.36968886852264404\n",
      "Epoch 4[572/625] Time:0.695, Train Loss:0.2714698314666748\n",
      "Epoch 4[573/625] Time:0.694, Train Loss:0.3381690979003906\n",
      "Epoch 4[574/625] Time:0.694, Train Loss:0.2787777781486511\n",
      "Epoch 4[575/625] Time:0.708, Train Loss:0.31702691316604614\n",
      "Epoch 4[576/625] Time:0.723, Train Loss:0.3678409159183502\n",
      "Epoch 4[577/625] Time:0.694, Train Loss:0.3087697923183441\n",
      "Epoch 4[578/625] Time:0.694, Train Loss:0.26978838443756104\n",
      "Epoch 4[579/625] Time:0.695, Train Loss:0.3019554316997528\n",
      "Epoch 4[580/625] Time:0.733, Train Loss:0.29343917965888977\n",
      "Epoch 4[581/625] Time:0.702, Train Loss:0.24863795936107635\n",
      "Epoch 4[582/625] Time:0.704, Train Loss:0.3436645269393921\n",
      "Epoch 4[583/625] Time:0.704, Train Loss:0.2847306728363037\n",
      "Epoch 4[584/625] Time:0.705, Train Loss:0.26749277114868164\n",
      "Epoch 4[585/625] Time:0.698, Train Loss:0.35236266255378723\n",
      "Epoch 4[586/625] Time:0.695, Train Loss:0.2950518727302551\n",
      "Epoch 4[587/625] Time:0.695, Train Loss:0.29201453924179077\n",
      "Epoch 4[588/625] Time:0.695, Train Loss:0.29044973850250244\n",
      "Epoch 4[589/625] Time:0.697, Train Loss:0.2576040029525757\n",
      "Epoch 4[590/625] Time:0.694, Train Loss:0.2583470046520233\n",
      "Epoch 4[591/625] Time:0.694, Train Loss:0.2945146858692169\n",
      "Epoch 4[592/625] Time:0.697, Train Loss:0.37915438413619995\n",
      "Epoch 4[593/625] Time:0.697, Train Loss:0.31866708397865295\n",
      "Epoch 4[594/625] Time:0.695, Train Loss:0.3236481249332428\n",
      "Epoch 4[595/625] Time:0.695, Train Loss:0.3665764033794403\n",
      "Epoch 4[596/625] Time:0.734, Train Loss:0.23768384754657745\n",
      "Epoch 4[597/625] Time:0.694, Train Loss:0.2857653498649597\n",
      "Epoch 4[598/625] Time:0.697, Train Loss:0.3085188567638397\n",
      "Epoch 4[599/625] Time:0.695, Train Loss:0.2449130117893219\n",
      "Epoch 4[600/625] Time:0.696, Train Loss:0.3051947355270386\n",
      "Epoch 4[601/625] Time:0.694, Train Loss:0.27282434701919556\n",
      "Epoch 4[602/625] Time:0.697, Train Loss:0.3549017608165741\n",
      "Epoch 4[603/625] Time:0.697, Train Loss:0.2637278437614441\n",
      "Epoch 4[604/625] Time:0.695, Train Loss:0.2631056606769562\n",
      "Epoch 4[605/625] Time:0.697, Train Loss:0.2557477056980133\n",
      "Epoch 4[606/625] Time:0.695, Train Loss:0.2782326638698578\n",
      "Epoch 4[607/625] Time:0.695, Train Loss:0.38487744331359863\n",
      "Epoch 4[608/625] Time:0.696, Train Loss:0.2978549599647522\n",
      "Epoch 4[609/625] Time:0.694, Train Loss:0.24782030284404755\n",
      "Epoch 4[610/625] Time:0.694, Train Loss:0.3044958710670471\n",
      "Epoch 4[611/625] Time:0.695, Train Loss:0.3017362356185913\n",
      "Epoch 4[612/625] Time:0.694, Train Loss:0.2823677361011505\n",
      "Epoch 4[613/625] Time:0.696, Train Loss:0.21248860657215118\n",
      "Epoch 4[614/625] Time:0.696, Train Loss:0.39442962408065796\n",
      "Epoch 4[615/625] Time:0.693, Train Loss:0.3109181821346283\n",
      "Epoch 4[616/625] Time:0.692, Train Loss:0.3265487849712372\n",
      "Epoch 4[617/625] Time:0.694, Train Loss:0.308090478181839\n",
      "Epoch 4[618/625] Time:0.697, Train Loss:0.27017176151275635\n",
      "Epoch 4[619/625] Time:0.718, Train Loss:0.34428179264068604\n",
      "Epoch 4[620/625] Time:0.701, Train Loss:0.36417996883392334\n",
      "Epoch 4[621/625] Time:0.704, Train Loss:0.2765858471393585\n",
      "Epoch 4[622/625] Time:0.704, Train Loss:0.2610781490802765\n",
      "Epoch 4[623/625] Time:0.737, Train Loss:0.29835599660873413\n",
      "Epoch 4[624/625] Time:0.693, Train Loss:0.35899657011032104\n",
      "Epoch 4[0/78] Val Loss:0.41064661741256714\n",
      "Epoch 4[1/78] Val Loss:0.40044140815734863\n",
      "Epoch 4[2/78] Val Loss:0.44413554668426514\n",
      "Epoch 4[3/78] Val Loss:0.4253740906715393\n",
      "Epoch 4[4/78] Val Loss:0.3217252790927887\n",
      "Epoch 4[5/78] Val Loss:0.18031597137451172\n",
      "Epoch 4[6/78] Val Loss:0.20310473442077637\n",
      "Epoch 4[7/78] Val Loss:0.26617303490638733\n",
      "Epoch 4[8/78] Val Loss:0.16799481213092804\n",
      "Epoch 4[9/78] Val Loss:0.13574765622615814\n",
      "Epoch 4[10/78] Val Loss:0.10808680206537247\n",
      "Epoch 4[11/78] Val Loss:0.1453646421432495\n",
      "Epoch 4[12/78] Val Loss:0.11828309297561646\n",
      "Epoch 4[13/78] Val Loss:0.10791225731372833\n",
      "Epoch 4[14/78] Val Loss:0.18214596807956696\n",
      "Epoch 4[15/78] Val Loss:0.15470318496227264\n",
      "Epoch 4[16/78] Val Loss:0.18003396689891815\n",
      "Epoch 4[17/78] Val Loss:0.1453700065612793\n",
      "Epoch 4[18/78] Val Loss:0.24001166224479675\n",
      "Epoch 4[19/78] Val Loss:0.33007025718688965\n",
      "Epoch 4[20/78] Val Loss:0.2536029517650604\n",
      "Epoch 4[21/78] Val Loss:0.4532994031906128\n",
      "Epoch 4[22/78] Val Loss:0.6508349776268005\n",
      "Epoch 4[23/78] Val Loss:0.49977728724479675\n",
      "Epoch 4[24/78] Val Loss:0.3998009264469147\n",
      "Epoch 4[25/78] Val Loss:0.4598962068557739\n",
      "Epoch 4[26/78] Val Loss:0.4616983234882355\n",
      "Epoch 4[27/78] Val Loss:0.40936970710754395\n",
      "Epoch 4[28/78] Val Loss:0.3739088177680969\n",
      "Epoch 4[29/78] Val Loss:0.5579286217689514\n",
      "Epoch 4[30/78] Val Loss:1.4016953706741333\n",
      "Epoch 4[31/78] Val Loss:1.209427833557129\n",
      "Epoch 4[32/78] Val Loss:0.9862028360366821\n",
      "Epoch 4[33/78] Val Loss:0.4759911596775055\n",
      "Epoch 4[34/78] Val Loss:0.4168652892112732\n",
      "Epoch 4[35/78] Val Loss:0.40697547793388367\n",
      "Epoch 4[36/78] Val Loss:0.46668320894241333\n",
      "Epoch 4[37/78] Val Loss:0.5131257176399231\n",
      "Epoch 4[38/78] Val Loss:0.418180376291275\n",
      "Epoch 4[39/78] Val Loss:0.4071563184261322\n",
      "Epoch 4[40/78] Val Loss:0.435305655002594\n",
      "Epoch 4[41/78] Val Loss:0.42940133810043335\n",
      "Epoch 4[42/78] Val Loss:0.44047290086746216\n",
      "Epoch 4[43/78] Val Loss:0.2650555372238159\n",
      "Epoch 4[44/78] Val Loss:0.12894901633262634\n",
      "Epoch 4[45/78] Val Loss:0.13084985315799713\n",
      "Epoch 4[46/78] Val Loss:0.11726916581392288\n",
      "Epoch 4[47/78] Val Loss:0.12573176622390747\n",
      "Epoch 4[48/78] Val Loss:0.15734288096427917\n",
      "Epoch 4[49/78] Val Loss:0.10712343454360962\n",
      "Epoch 4[50/78] Val Loss:0.09993374347686768\n",
      "Epoch 4[51/78] Val Loss:0.10649415850639343\n",
      "Epoch 4[52/78] Val Loss:0.15019948780536652\n",
      "Epoch 4[53/78] Val Loss:0.10810789465904236\n",
      "Epoch 4[54/78] Val Loss:0.10448478162288666\n",
      "Epoch 4[55/78] Val Loss:0.14867092669010162\n",
      "Epoch 4[56/78] Val Loss:0.44533973932266235\n",
      "Epoch 4[57/78] Val Loss:0.398100882768631\n",
      "Epoch 4[58/78] Val Loss:0.3481125235557556\n",
      "Epoch 4[59/78] Val Loss:0.3601108491420746\n",
      "Epoch 4[60/78] Val Loss:0.36195582151412964\n",
      "Epoch 4[61/78] Val Loss:0.4040946960449219\n",
      "Epoch 4[62/78] Val Loss:0.4110468924045563\n",
      "Epoch 4[63/78] Val Loss:0.287751168012619\n",
      "Epoch 4[64/78] Val Loss:0.1893988698720932\n",
      "Epoch 4[65/78] Val Loss:0.18016871809959412\n",
      "Epoch 4[66/78] Val Loss:0.18999196588993073\n",
      "Epoch 4[67/78] Val Loss:0.19275891780853271\n",
      "Epoch 4[68/78] Val Loss:0.4327768385410309\n",
      "Epoch 4[69/78] Val Loss:0.45684003829956055\n",
      "Epoch 4[70/78] Val Loss:0.47766435146331787\n",
      "Epoch 4[71/78] Val Loss:0.431946337223053\n",
      "Epoch 4[72/78] Val Loss:0.32908615469932556\n",
      "Epoch 4[73/78] Val Loss:0.31550270318984985\n",
      "Epoch 4[74/78] Val Loss:0.5151399374008179\n",
      "Epoch 4[75/78] Val Loss:0.6033231019973755\n",
      "Epoch 4[76/78] Val Loss:0.507166862487793\n",
      "Epoch 4[77/78] Val Loss:0.6251534819602966\n",
      "Epoch 4[78/78] Val Loss:0.6738535761833191\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.84      0.89     15691\n",
      "           1       0.59      0.83      0.69      4309\n",
      "\n",
      "    accuracy                           0.84     20000\n",
      "   macro avg       0.77      0.84      0.79     20000\n",
      "weighted avg       0.87      0.84      0.85     20000\n",
      "\n",
      "Epoch 4: Train Loss 0.31652944662570953, Val Loss 0.3600351124619826, Train Time 804.3840403556824, Val Time 38.06605768203735\n",
      "Epoch 5[0/625] Time:0.689, Train Loss:0.3387296795845032\n",
      "Epoch 5[1/625] Time:0.704, Train Loss:0.32938694953918457\n",
      "Epoch 5[2/625] Time:0.703, Train Loss:0.29889383912086487\n",
      "Epoch 5[3/625] Time:0.704, Train Loss:0.3481520712375641\n",
      "Epoch 5[4/625] Time:0.703, Train Loss:0.2967856228351593\n",
      "Epoch 5[5/625] Time:0.703, Train Loss:0.2421783059835434\n",
      "Epoch 5[6/625] Time:0.702, Train Loss:0.2599658668041229\n",
      "Epoch 5[7/625] Time:0.703, Train Loss:0.29207512736320496\n",
      "Epoch 5[8/625] Time:0.703, Train Loss:0.3861061930656433\n",
      "Epoch 5[9/625] Time:0.703, Train Loss:0.3101924657821655\n",
      "Epoch 5[10/625] Time:0.705, Train Loss:0.3106188178062439\n",
      "Epoch 5[11/625] Time:0.745, Train Loss:0.3602598011493683\n",
      "Epoch 5[12/625] Time:0.694, Train Loss:0.2538672685623169\n",
      "Epoch 5[13/625] Time:0.703, Train Loss:0.3604060113430023\n",
      "Epoch 5[14/625] Time:0.703, Train Loss:0.22700008749961853\n",
      "Epoch 5[15/625] Time:0.708, Train Loss:0.28645163774490356\n",
      "Epoch 5[16/625] Time:0.704, Train Loss:0.3475612998008728\n",
      "Epoch 5[17/625] Time:0.703, Train Loss:0.3274568021297455\n",
      "Epoch 5[18/625] Time:0.703, Train Loss:0.37235909700393677\n",
      "Epoch 5[19/625] Time:0.702, Train Loss:0.4098353683948517\n",
      "Epoch 5[20/625] Time:0.703, Train Loss:0.2479238361120224\n",
      "Epoch 5[21/625] Time:0.702, Train Loss:0.3668838441371918\n",
      "Epoch 5[22/625] Time:0.702, Train Loss:0.3187214136123657\n",
      "Epoch 5[23/625] Time:0.706, Train Loss:0.3264387249946594\n",
      "Epoch 5[24/625] Time:0.703, Train Loss:0.2726556956768036\n",
      "Epoch 5[25/625] Time:0.703, Train Loss:0.30129167437553406\n",
      "Epoch 5[26/625] Time:0.703, Train Loss:0.2562558054924011\n",
      "Epoch 5[27/625] Time:0.703, Train Loss:0.29223138093948364\n",
      "Epoch 5[28/625] Time:0.703, Train Loss:0.3112172484397888\n",
      "Epoch 5[29/625] Time:0.702, Train Loss:0.2227744162082672\n",
      "Epoch 5[30/625] Time:0.702, Train Loss:0.34789299964904785\n",
      "Epoch 5[31/625] Time:0.736, Train Loss:0.2832719087600708\n",
      "Epoch 5[32/625] Time:0.704, Train Loss:0.32965585589408875\n",
      "Epoch 5[33/625] Time:0.705, Train Loss:0.32793480157852173\n",
      "Epoch 5[34/625] Time:0.702, Train Loss:0.34329503774642944\n",
      "Epoch 5[35/625] Time:0.701, Train Loss:0.23351654410362244\n",
      "Epoch 5[36/625] Time:0.702, Train Loss:0.3016238510608673\n",
      "Epoch 5[37/625] Time:0.703, Train Loss:0.336851567029953\n",
      "Epoch 5[38/625] Time:0.702, Train Loss:0.296739786863327\n",
      "Epoch 5[39/625] Time:0.703, Train Loss:0.30726292729377747\n",
      "Epoch 5[40/625] Time:0.702, Train Loss:0.301148384809494\n",
      "Epoch 5[41/625] Time:0.702, Train Loss:0.3790138065814972\n",
      "Epoch 5[42/625] Time:0.703, Train Loss:0.29898491501808167\n",
      "Epoch 5[43/625] Time:0.703, Train Loss:0.3168652057647705\n",
      "Epoch 5[44/625] Time:0.705, Train Loss:0.3183768689632416\n",
      "Epoch 5[45/625] Time:0.704, Train Loss:0.24748191237449646\n",
      "Epoch 5[46/625] Time:0.704, Train Loss:0.3061569035053253\n",
      "Epoch 5[47/625] Time:0.704, Train Loss:0.2392370104789734\n",
      "Epoch 5[48/625] Time:0.703, Train Loss:0.3327258229255676\n",
      "Epoch 5[49/625] Time:0.703, Train Loss:0.3139551877975464\n",
      "Epoch 5[50/625] Time:0.703, Train Loss:0.312000572681427\n",
      "Epoch 5[51/625] Time:0.702, Train Loss:0.3018222153186798\n",
      "Epoch 5[52/625] Time:0.703, Train Loss:0.20943190157413483\n",
      "Epoch 5[53/625] Time:0.702, Train Loss:0.2619919180870056\n",
      "Epoch 5[54/625] Time:0.702, Train Loss:0.2764451205730438\n",
      "Epoch 5[55/625] Time:0.704, Train Loss:0.32011649012565613\n",
      "Epoch 5[56/625] Time:0.703, Train Loss:0.2203679084777832\n",
      "Epoch 5[57/625] Time:0.704, Train Loss:0.27488672733306885\n",
      "Epoch 5[58/625] Time:0.704, Train Loss:0.20923781394958496\n",
      "Epoch 5[59/625] Time:0.702, Train Loss:0.24043624103069305\n",
      "Epoch 5[60/625] Time:0.704, Train Loss:0.34479281306266785\n",
      "Epoch 5[61/625] Time:0.702, Train Loss:0.31351345777511597\n",
      "Epoch 5[62/625] Time:0.703, Train Loss:0.28777259588241577\n",
      "Epoch 5[63/625] Time:0.727, Train Loss:0.33116403222084045\n",
      "Epoch 5[64/625] Time:0.692, Train Loss:0.32605814933776855\n",
      "Epoch 5[65/625] Time:0.695, Train Loss:0.2858186960220337\n",
      "Epoch 5[66/625] Time:0.703, Train Loss:0.31325170397758484\n",
      "Epoch 5[67/625] Time:0.695, Train Loss:0.39975476264953613\n",
      "Epoch 5[68/625] Time:0.694, Train Loss:0.2919269800186157\n",
      "Epoch 5[69/625] Time:0.696, Train Loss:0.27174875140190125\n",
      "Epoch 5[70/625] Time:0.716, Train Loss:0.3539300560951233\n",
      "Epoch 5[71/625] Time:0.702, Train Loss:0.32218286395072937\n",
      "Epoch 5[72/625] Time:0.702, Train Loss:0.331485390663147\n",
      "Epoch 5[73/625] Time:0.703, Train Loss:0.3028290569782257\n",
      "Epoch 5[74/625] Time:0.704, Train Loss:0.35247623920440674\n",
      "Epoch 5[75/625] Time:0.702, Train Loss:0.36207297444343567\n",
      "Epoch 5[76/625] Time:0.702, Train Loss:0.34862595796585083\n",
      "Epoch 5[77/625] Time:0.702, Train Loss:0.3019515872001648\n",
      "Epoch 5[78/625] Time:0.702, Train Loss:0.25532257556915283\n",
      "Epoch 5[79/625] Time:0.701, Train Loss:0.2838975191116333\n",
      "Epoch 5[80/625] Time:0.702, Train Loss:0.3946281373500824\n",
      "Epoch 5[81/625] Time:0.703, Train Loss:0.303410142660141\n",
      "Epoch 5[82/625] Time:0.702, Train Loss:0.3032284379005432\n",
      "Epoch 5[83/625] Time:0.703, Train Loss:0.3311525583267212\n",
      "Epoch 5[84/625] Time:0.703, Train Loss:0.3008526563644409\n",
      "Epoch 5[85/625] Time:0.703, Train Loss:0.27197518944740295\n",
      "Epoch 5[86/625] Time:0.703, Train Loss:0.27920132875442505\n",
      "Epoch 5[87/625] Time:0.704, Train Loss:0.21231333911418915\n",
      "Epoch 5[88/625] Time:0.724, Train Loss:0.2922876179218292\n",
      "Epoch 5[89/625] Time:0.703, Train Loss:0.38337334990501404\n",
      "Epoch 5[90/625] Time:0.725, Train Loss:0.3630228638648987\n",
      "Epoch 5[91/625] Time:0.702, Train Loss:0.3595587909221649\n",
      "Epoch 5[92/625] Time:0.703, Train Loss:0.330274373292923\n",
      "Epoch 5[93/625] Time:0.703, Train Loss:0.2943706512451172\n",
      "Epoch 5[94/625] Time:0.704, Train Loss:0.31815609335899353\n",
      "Epoch 5[95/625] Time:0.705, Train Loss:0.346338152885437\n",
      "Epoch 5[96/625] Time:0.703, Train Loss:0.3078577220439911\n",
      "Epoch 5[97/625] Time:0.704, Train Loss:0.3260171413421631\n",
      "Epoch 5[98/625] Time:0.706, Train Loss:0.3788391053676605\n",
      "Epoch 5[99/625] Time:0.706, Train Loss:0.3019625246524811\n",
      "Epoch 5[100/625] Time:0.705, Train Loss:0.3903793394565582\n",
      "Epoch 5[101/625] Time:0.704, Train Loss:0.2595529854297638\n",
      "Epoch 5[102/625] Time:0.705, Train Loss:0.24369148910045624\n",
      "Epoch 5[103/625] Time:0.704, Train Loss:0.2519990801811218\n",
      "Epoch 5[104/625] Time:0.746, Train Loss:0.3003137707710266\n",
      "Epoch 5[105/625] Time:0.726, Train Loss:0.29875603318214417\n",
      "Epoch 5[106/625] Time:0.702, Train Loss:0.326103150844574\n",
      "Epoch 5[107/625] Time:0.702, Train Loss:0.3169216513633728\n",
      "Epoch 5[108/625] Time:0.703, Train Loss:0.29331454634666443\n",
      "Epoch 5[109/625] Time:0.704, Train Loss:0.29538071155548096\n",
      "Epoch 5[110/625] Time:0.702, Train Loss:0.2912155091762543\n",
      "Epoch 5[111/625] Time:0.703, Train Loss:0.26671674847602844\n",
      "Epoch 5[112/625] Time:0.703, Train Loss:0.28334835171699524\n",
      "Epoch 5[113/625] Time:0.702, Train Loss:0.29246410727500916\n",
      "Epoch 5[114/625] Time:0.703, Train Loss:0.30038270354270935\n",
      "Epoch 5[115/625] Time:0.703, Train Loss:0.22323621809482574\n",
      "Epoch 5[116/625] Time:0.702, Train Loss:0.3004177510738373\n",
      "Epoch 5[117/625] Time:0.704, Train Loss:0.33203262090682983\n",
      "Epoch 5[118/625] Time:0.702, Train Loss:0.33524590730667114\n",
      "Epoch 5[119/625] Time:0.701, Train Loss:0.29218149185180664\n",
      "Epoch 5[120/625] Time:0.703, Train Loss:0.26518651843070984\n",
      "Epoch 5[121/625] Time:0.703, Train Loss:0.3830266296863556\n",
      "Epoch 5[122/625] Time:0.706, Train Loss:0.3521474301815033\n",
      "Epoch 5[123/625] Time:0.703, Train Loss:0.3342178463935852\n",
      "Epoch 5[124/625] Time:0.703, Train Loss:0.2665970027446747\n",
      "Epoch 5[125/625] Time:0.703, Train Loss:0.27423837780952454\n",
      "Epoch 5[126/625] Time:0.703, Train Loss:0.34321218729019165\n",
      "Epoch 5[127/625] Time:0.703, Train Loss:0.3085327744483948\n",
      "Epoch 5[128/625] Time:0.703, Train Loss:0.357964426279068\n",
      "Epoch 5[129/625] Time:0.703, Train Loss:0.35111573338508606\n",
      "Epoch 5[130/625] Time:0.704, Train Loss:0.3012971878051758\n",
      "Epoch 5[131/625] Time:0.702, Train Loss:0.302112877368927\n",
      "Epoch 5[132/625] Time:0.704, Train Loss:0.365572065114975\n",
      "Epoch 5[133/625] Time:0.703, Train Loss:0.2745252251625061\n",
      "Epoch 5[134/625] Time:0.703, Train Loss:0.32485973834991455\n",
      "Epoch 5[135/625] Time:0.704, Train Loss:0.2731762230396271\n",
      "Epoch 5[136/625] Time:0.702, Train Loss:0.3240601122379303\n",
      "Epoch 5[137/625] Time:0.703, Train Loss:0.26739221811294556\n",
      "Epoch 5[138/625] Time:0.703, Train Loss:0.30722111463546753\n",
      "Epoch 5[139/625] Time:0.705, Train Loss:0.356092244386673\n",
      "Epoch 5[140/625] Time:0.702, Train Loss:0.3196388781070709\n",
      "Epoch 5[141/625] Time:0.705, Train Loss:0.28617095947265625\n",
      "Epoch 5[142/625] Time:0.703, Train Loss:0.25197508931159973\n",
      "Epoch 5[143/625] Time:0.705, Train Loss:0.2868790626525879\n",
      "Epoch 5[144/625] Time:0.704, Train Loss:0.3372823894023895\n",
      "Epoch 5[145/625] Time:0.704, Train Loss:0.4560750722885132\n",
      "Epoch 5[146/625] Time:0.706, Train Loss:0.26362526416778564\n",
      "Epoch 5[147/625] Time:0.705, Train Loss:0.3150280714035034\n",
      "Epoch 5[148/625] Time:0.703, Train Loss:0.2927769720554352\n",
      "Epoch 5[149/625] Time:0.704, Train Loss:0.2861272096633911\n",
      "Epoch 5[150/625] Time:0.712, Train Loss:0.2686596214771271\n",
      "Epoch 5[151/625] Time:0.703, Train Loss:0.2443143129348755\n",
      "Epoch 5[152/625] Time:0.703, Train Loss:0.29146215319633484\n",
      "Epoch 5[153/625] Time:0.704, Train Loss:0.3138534426689148\n",
      "Epoch 5[154/625] Time:0.702, Train Loss:0.3314649164676666\n",
      "Epoch 5[155/625] Time:0.702, Train Loss:0.32696929574012756\n",
      "Epoch 5[156/625] Time:0.706, Train Loss:0.4523530602455139\n",
      "Epoch 5[157/625] Time:0.703, Train Loss:0.2674771249294281\n",
      "Epoch 5[158/625] Time:0.702, Train Loss:0.23791266977787018\n",
      "Epoch 5[159/625] Time:0.703, Train Loss:0.3549935221672058\n",
      "Epoch 5[160/625] Time:0.704, Train Loss:0.2198614776134491\n",
      "Epoch 5[161/625] Time:0.703, Train Loss:0.2159687578678131\n",
      "Epoch 5[162/625] Time:0.704, Train Loss:0.21988317370414734\n",
      "Epoch 5[163/625] Time:0.734, Train Loss:0.3672533631324768\n",
      "Epoch 5[164/625] Time:0.722, Train Loss:0.26293644309043884\n",
      "Epoch 5[165/625] Time:0.702, Train Loss:0.24265047907829285\n",
      "Epoch 5[166/625] Time:0.705, Train Loss:0.34830906987190247\n",
      "Epoch 5[167/625] Time:0.706, Train Loss:0.29270750284194946\n",
      "Epoch 5[168/625] Time:0.704, Train Loss:0.21271488070487976\n",
      "Epoch 5[169/625] Time:0.705, Train Loss:0.27469560503959656\n",
      "Epoch 5[170/625] Time:0.702, Train Loss:0.3108595907688141\n",
      "Epoch 5[171/625] Time:0.704, Train Loss:0.28415340185165405\n",
      "Epoch 5[172/625] Time:0.704, Train Loss:0.3022558391094208\n",
      "Epoch 5[173/625] Time:0.704, Train Loss:0.28744107484817505\n",
      "Epoch 5[174/625] Time:0.702, Train Loss:0.3541473150253296\n",
      "Epoch 5[175/625] Time:0.703, Train Loss:0.23353669047355652\n",
      "Epoch 5[176/625] Time:0.705, Train Loss:0.35710078477859497\n",
      "Epoch 5[177/625] Time:0.705, Train Loss:0.258248507976532\n",
      "Epoch 5[178/625] Time:0.704, Train Loss:0.3021237850189209\n",
      "Epoch 5[179/625] Time:0.747, Train Loss:0.27127522230148315\n",
      "Epoch 5[180/625] Time:0.702, Train Loss:0.2691902220249176\n",
      "Epoch 5[181/625] Time:0.703, Train Loss:0.378628134727478\n",
      "Epoch 5[182/625] Time:0.705, Train Loss:0.30573976039886475\n",
      "Epoch 5[183/625] Time:0.706, Train Loss:0.3250144422054291\n",
      "Epoch 5[184/625] Time:0.702, Train Loss:0.2656846344470978\n",
      "Epoch 5[185/625] Time:0.705, Train Loss:0.27844420075416565\n",
      "Epoch 5[186/625] Time:0.705, Train Loss:0.21301332116127014\n",
      "Epoch 5[187/625] Time:0.708, Train Loss:0.32113635540008545\n",
      "Epoch 5[188/625] Time:0.704, Train Loss:0.2595008909702301\n",
      "Epoch 5[189/625] Time:0.703, Train Loss:0.2924313545227051\n",
      "Epoch 5[190/625] Time:0.703, Train Loss:0.29375123977661133\n",
      "Epoch 5[191/625] Time:0.703, Train Loss:0.261416494846344\n",
      "Epoch 5[192/625] Time:0.703, Train Loss:0.29084306955337524\n",
      "Epoch 5[193/625] Time:0.703, Train Loss:0.30063697695732117\n",
      "Epoch 5[194/625] Time:0.703, Train Loss:0.37935683131217957\n",
      "Epoch 5[195/625] Time:0.707, Train Loss:0.2535902261734009\n",
      "Epoch 5[196/625] Time:0.703, Train Loss:0.2914186716079712\n",
      "Epoch 5[197/625] Time:0.704, Train Loss:0.3068161606788635\n",
      "Epoch 5[198/625] Time:0.703, Train Loss:0.2870020568370819\n",
      "Epoch 5[199/625] Time:0.704, Train Loss:0.26708877086639404\n",
      "Epoch 5[200/625] Time:0.704, Train Loss:0.3319123387336731\n",
      "Epoch 5[201/625] Time:0.705, Train Loss:0.3003304898738861\n",
      "Epoch 5[202/625] Time:0.733, Train Loss:0.28891900181770325\n",
      "Epoch 5[203/625] Time:0.691, Train Loss:0.25870293378829956\n",
      "Epoch 5[204/625] Time:0.694, Train Loss:0.269634485244751\n",
      "Epoch 5[205/625] Time:0.692, Train Loss:0.26602017879486084\n",
      "Epoch 5[206/625] Time:0.693, Train Loss:0.24078895151615143\n",
      "Epoch 5[207/625] Time:0.695, Train Loss:0.3319130539894104\n",
      "Epoch 5[208/625] Time:0.696, Train Loss:0.38665804266929626\n",
      "Epoch 5[209/625] Time:0.703, Train Loss:0.39490625262260437\n",
      "Epoch 5[210/625] Time:0.703, Train Loss:0.3449755609035492\n",
      "Epoch 5[211/625] Time:0.705, Train Loss:0.30082130432128906\n",
      "Epoch 5[212/625] Time:0.702, Train Loss:0.4140584468841553\n",
      "Epoch 5[213/625] Time:0.702, Train Loss:0.2799031138420105\n",
      "Epoch 5[214/625] Time:0.702, Train Loss:0.24126900732517242\n",
      "Epoch 5[215/625] Time:0.703, Train Loss:0.3006320297718048\n",
      "Epoch 5[216/625] Time:0.703, Train Loss:0.30566340684890747\n",
      "Epoch 5[217/625] Time:0.702, Train Loss:0.29497644305229187\n",
      "Epoch 5[218/625] Time:0.703, Train Loss:0.3272443413734436\n",
      "Epoch 5[219/625] Time:0.702, Train Loss:0.2507075369358063\n",
      "Epoch 5[220/625] Time:0.703, Train Loss:0.317877858877182\n",
      "Epoch 5[221/625] Time:0.702, Train Loss:0.2710135877132416\n",
      "Epoch 5[222/625] Time:0.702, Train Loss:0.33231768012046814\n",
      "Epoch 5[223/625] Time:0.703, Train Loss:0.2482047826051712\n",
      "Epoch 5[224/625] Time:0.704, Train Loss:0.3392663598060608\n",
      "Epoch 5[225/625] Time:0.703, Train Loss:0.2621414363384247\n",
      "Epoch 5[226/625] Time:0.702, Train Loss:0.3186081349849701\n",
      "Epoch 5[227/625] Time:0.703, Train Loss:0.2406686246395111\n",
      "Epoch 5[228/625] Time:0.703, Train Loss:0.24220213294029236\n",
      "Epoch 5[229/625] Time:0.703, Train Loss:0.19857069849967957\n",
      "Epoch 5[230/625] Time:0.707, Train Loss:0.28256991505622864\n",
      "Epoch 5[231/625] Time:0.694, Train Loss:0.36801066994667053\n",
      "Epoch 5[232/625] Time:0.708, Train Loss:0.3183915615081787\n",
      "Epoch 5[233/625] Time:0.695, Train Loss:0.2908685803413391\n",
      "Epoch 5[234/625] Time:0.7, Train Loss:0.2966667711734772\n",
      "Epoch 5[235/625] Time:0.702, Train Loss:0.2512856423854828\n",
      "Epoch 5[236/625] Time:0.703, Train Loss:0.31559860706329346\n",
      "Epoch 5[237/625] Time:0.702, Train Loss:0.2604338228702545\n",
      "Epoch 5[238/625] Time:0.704, Train Loss:0.3577878177165985\n",
      "Epoch 5[239/625] Time:0.703, Train Loss:0.32109585404396057\n",
      "Epoch 5[240/625] Time:0.703, Train Loss:0.3145729899406433\n",
      "Epoch 5[241/625] Time:0.708, Train Loss:0.3870583772659302\n",
      "Epoch 5[242/625] Time:0.703, Train Loss:0.3442918360233307\n",
      "Epoch 5[243/625] Time:0.703, Train Loss:0.34877437353134155\n",
      "Epoch 5[244/625] Time:0.702, Train Loss:0.36291006207466125\n",
      "Epoch 5[245/625] Time:0.702, Train Loss:0.3232550323009491\n",
      "Epoch 5[246/625] Time:0.704, Train Loss:0.2996494770050049\n",
      "Epoch 5[247/625] Time:0.704, Train Loss:0.25177961587905884\n",
      "Epoch 5[248/625] Time:0.703, Train Loss:0.2796911299228668\n",
      "Epoch 5[249/625] Time:0.705, Train Loss:0.2815759479999542\n",
      "Epoch 5[250/625] Time:0.705, Train Loss:0.28789660334587097\n",
      "Epoch 5[251/625] Time:0.704, Train Loss:0.3261116147041321\n",
      "Epoch 5[252/625] Time:0.702, Train Loss:0.2636398375034332\n",
      "Epoch 5[253/625] Time:0.706, Train Loss:0.3008708655834198\n",
      "Epoch 5[254/625] Time:0.722, Train Loss:0.26998093724250793\n",
      "Epoch 5[255/625] Time:0.702, Train Loss:0.37057822942733765\n",
      "Epoch 5[256/625] Time:0.703, Train Loss:0.24109992384910583\n",
      "Epoch 5[257/625] Time:0.703, Train Loss:0.2989863455295563\n",
      "Epoch 5[258/625] Time:0.703, Train Loss:0.324593186378479\n",
      "Epoch 5[259/625] Time:0.709, Train Loss:0.32711225748062134\n",
      "Epoch 5[260/625] Time:0.702, Train Loss:0.2979040741920471\n",
      "Epoch 5[261/625] Time:0.703, Train Loss:0.27948957681655884\n",
      "Epoch 5[262/625] Time:0.702, Train Loss:0.2937832772731781\n",
      "Epoch 5[263/625] Time:0.703, Train Loss:0.2478884607553482\n",
      "Epoch 5[264/625] Time:0.702, Train Loss:0.2737525403499603\n",
      "Epoch 5[265/625] Time:0.703, Train Loss:0.2628227174282074\n",
      "Epoch 5[266/625] Time:0.702, Train Loss:0.26905351877212524\n",
      "Epoch 5[267/625] Time:0.703, Train Loss:0.3017708361148834\n",
      "Epoch 5[268/625] Time:0.704, Train Loss:0.34836408495903015\n",
      "Epoch 5[269/625] Time:0.701, Train Loss:0.3758966624736786\n",
      "Epoch 5[270/625] Time:0.702, Train Loss:0.3028799295425415\n",
      "Epoch 5[271/625] Time:0.704, Train Loss:0.25473281741142273\n",
      "Epoch 5[272/625] Time:0.702, Train Loss:0.31418439745903015\n",
      "Epoch 5[273/625] Time:0.704, Train Loss:0.3214702308177948\n",
      "Epoch 5[274/625] Time:0.703, Train Loss:0.2586207091808319\n",
      "Epoch 5[275/625] Time:0.704, Train Loss:0.34686192870140076\n",
      "Epoch 5[276/625] Time:0.704, Train Loss:0.26399505138397217\n",
      "Epoch 5[277/625] Time:0.703, Train Loss:0.26715290546417236\n",
      "Epoch 5[278/625] Time:0.703, Train Loss:0.3733367323875427\n",
      "Epoch 5[279/625] Time:0.703, Train Loss:0.32323694229125977\n",
      "Epoch 5[280/625] Time:0.702, Train Loss:0.2925267517566681\n",
      "Epoch 5[281/625] Time:0.702, Train Loss:0.35167834162712097\n",
      "Epoch 5[282/625] Time:0.704, Train Loss:0.3988436758518219\n",
      "Epoch 5[283/625] Time:0.703, Train Loss:0.30920475721359253\n",
      "Epoch 5[284/625] Time:0.702, Train Loss:0.3331807255744934\n",
      "Epoch 5[285/625] Time:0.704, Train Loss:0.3370315432548523\n",
      "Epoch 5[286/625] Time:0.718, Train Loss:0.27423393726348877\n",
      "Epoch 5[287/625] Time:0.695, Train Loss:0.32719162106513977\n",
      "Epoch 5[288/625] Time:0.726, Train Loss:0.3056512176990509\n",
      "Epoch 5[289/625] Time:0.702, Train Loss:0.3185170888900757\n",
      "Epoch 5[290/625] Time:0.701, Train Loss:0.3165953755378723\n",
      "Epoch 5[291/625] Time:0.701, Train Loss:0.3474205732345581\n",
      "Epoch 5[292/625] Time:0.703, Train Loss:0.2861151099205017\n",
      "Epoch 5[293/625] Time:0.703, Train Loss:0.3984115719795227\n",
      "Epoch 5[294/625] Time:0.703, Train Loss:0.30625808238983154\n",
      "Epoch 5[295/625] Time:0.704, Train Loss:0.30062705278396606\n",
      "Epoch 5[296/625] Time:0.703, Train Loss:0.34068888425827026\n",
      "Epoch 5[297/625] Time:0.704, Train Loss:0.3267195224761963\n",
      "Epoch 5[298/625] Time:0.703, Train Loss:0.27034252882003784\n",
      "Epoch 5[299/625] Time:0.703, Train Loss:0.3428705930709839\n",
      "Epoch 5[300/625] Time:0.703, Train Loss:0.22900506854057312\n",
      "Epoch 5[301/625] Time:0.702, Train Loss:0.2968390882015228\n",
      "Epoch 5[302/625] Time:0.702, Train Loss:0.3312496840953827\n",
      "Epoch 5[303/625] Time:0.703, Train Loss:0.4478662610054016\n",
      "Epoch 5[304/625] Time:0.702, Train Loss:0.28262415528297424\n",
      "Epoch 5[305/625] Time:0.704, Train Loss:0.3516336977481842\n",
      "Epoch 5[306/625] Time:0.703, Train Loss:0.3105107247829437\n",
      "Epoch 5[307/625] Time:0.704, Train Loss:0.3605709969997406\n",
      "Epoch 5[308/625] Time:0.702, Train Loss:0.2659294903278351\n",
      "Epoch 5[309/625] Time:0.704, Train Loss:0.2807440459728241\n",
      "Epoch 5[310/625] Time:0.701, Train Loss:0.28072747588157654\n",
      "Epoch 5[311/625] Time:0.703, Train Loss:0.3472961187362671\n",
      "Epoch 5[312/625] Time:0.702, Train Loss:0.28735148906707764\n",
      "Epoch 5[313/625] Time:0.702, Train Loss:0.28822246193885803\n",
      "Epoch 5[314/625] Time:0.705, Train Loss:0.3068409264087677\n",
      "Epoch 5[315/625] Time:0.695, Train Loss:0.27075323462486267\n",
      "Epoch 5[316/625] Time:0.696, Train Loss:0.3332855701446533\n",
      "Epoch 5[317/625] Time:0.695, Train Loss:0.22031483054161072\n",
      "Epoch 5[318/625] Time:0.694, Train Loss:0.32298189401626587\n",
      "Epoch 5[319/625] Time:0.702, Train Loss:0.35501861572265625\n",
      "Epoch 5[320/625] Time:0.702, Train Loss:0.2603551745414734\n",
      "Epoch 5[321/625] Time:0.704, Train Loss:0.2609236538410187\n",
      "Epoch 5[322/625] Time:0.703, Train Loss:0.23960047960281372\n",
      "Epoch 5[323/625] Time:0.703, Train Loss:0.30734536051750183\n",
      "Epoch 5[324/625] Time:0.703, Train Loss:0.33900153636932373\n",
      "Epoch 5[325/625] Time:0.702, Train Loss:0.21093037724494934\n",
      "Epoch 5[326/625] Time:0.704, Train Loss:0.26343172788619995\n",
      "Epoch 5[327/625] Time:0.706, Train Loss:0.2689460217952728\n",
      "Epoch 5[328/625] Time:0.702, Train Loss:0.3170401155948639\n",
      "Epoch 5[329/625] Time:0.704, Train Loss:0.24987415969371796\n",
      "Epoch 5[330/625] Time:0.703, Train Loss:0.2523999512195587\n",
      "Epoch 5[331/625] Time:0.704, Train Loss:0.21482254564762115\n",
      "Epoch 5[332/625] Time:0.706, Train Loss:0.27093032002449036\n",
      "Epoch 5[333/625] Time:0.704, Train Loss:0.23732341825962067\n",
      "Epoch 5[334/625] Time:0.704, Train Loss:0.3042478561401367\n",
      "Epoch 5[335/625] Time:0.704, Train Loss:0.24030543863773346\n",
      "Epoch 5[336/625] Time:0.704, Train Loss:0.34991294145584106\n",
      "Epoch 5[337/625] Time:0.705, Train Loss:0.24885264039039612\n",
      "Epoch 5[338/625] Time:0.703, Train Loss:0.287101686000824\n",
      "Epoch 5[339/625] Time:0.704, Train Loss:0.3660525679588318\n",
      "Epoch 5[340/625] Time:0.703, Train Loss:0.3882268965244293\n",
      "Epoch 5[341/625] Time:0.704, Train Loss:0.23212304711341858\n",
      "Epoch 5[342/625] Time:0.703, Train Loss:0.2893334627151489\n",
      "Epoch 5[343/625] Time:0.704, Train Loss:0.2511151134967804\n",
      "Epoch 5[344/625] Time:0.704, Train Loss:0.36477452516555786\n",
      "Epoch 5[345/625] Time:0.704, Train Loss:0.40485334396362305\n",
      "Epoch 5[346/625] Time:0.695, Train Loss:0.3197653889656067\n",
      "Epoch 5[347/625] Time:0.705, Train Loss:0.2800934314727783\n",
      "Epoch 5[348/625] Time:0.749, Train Loss:0.37717199325561523\n",
      "Epoch 5[349/625] Time:0.69, Train Loss:0.38459286093711853\n",
      "Epoch 5[350/625] Time:0.694, Train Loss:0.2755257487297058\n",
      "Epoch 5[351/625] Time:0.692, Train Loss:0.3415103852748871\n",
      "Epoch 5[352/625] Time:0.702, Train Loss:0.3131360411643982\n",
      "Epoch 5[353/625] Time:0.702, Train Loss:0.28360939025878906\n",
      "Epoch 5[354/625] Time:0.703, Train Loss:0.28920355439186096\n",
      "Epoch 5[355/625] Time:0.703, Train Loss:0.2841816544532776\n",
      "Epoch 5[356/625] Time:0.703, Train Loss:0.2521800100803375\n",
      "Epoch 5[357/625] Time:0.703, Train Loss:0.3227872848510742\n",
      "Epoch 5[358/625] Time:0.703, Train Loss:0.3021286129951477\n",
      "Epoch 5[359/625] Time:0.702, Train Loss:0.3086845874786377\n",
      "Epoch 5[360/625] Time:0.703, Train Loss:0.34214356541633606\n",
      "Epoch 5[361/625] Time:0.706, Train Loss:0.343513548374176\n",
      "Epoch 5[362/625] Time:0.702, Train Loss:0.3254011571407318\n",
      "Epoch 5[363/625] Time:0.705, Train Loss:0.299207478761673\n",
      "Epoch 5[364/625] Time:0.703, Train Loss:0.24356338381767273\n",
      "Epoch 5[365/625] Time:0.705, Train Loss:0.22703717648983002\n",
      "Epoch 5[366/625] Time:0.704, Train Loss:0.2628381550312042\n",
      "Epoch 5[367/625] Time:0.703, Train Loss:0.2860677242279053\n",
      "Epoch 5[368/625] Time:0.703, Train Loss:0.2763060927391052\n",
      "Epoch 5[369/625] Time:0.706, Train Loss:0.43919840455055237\n",
      "Epoch 5[370/625] Time:0.702, Train Loss:0.2646925151348114\n",
      "Epoch 5[371/625] Time:0.702, Train Loss:0.30897247791290283\n",
      "Epoch 5[372/625] Time:0.702, Train Loss:0.31220608949661255\n",
      "Epoch 5[373/625] Time:0.703, Train Loss:0.30318135023117065\n",
      "Epoch 5[374/625] Time:0.701, Train Loss:0.24745020270347595\n",
      "Epoch 5[375/625] Time:0.702, Train Loss:0.2931060194969177\n",
      "Epoch 5[376/625] Time:0.702, Train Loss:0.28458091616630554\n",
      "Epoch 5[377/625] Time:0.704, Train Loss:0.4033922255039215\n",
      "Epoch 5[378/625] Time:0.703, Train Loss:0.3380849063396454\n",
      "Epoch 5[379/625] Time:0.704, Train Loss:0.26116305589675903\n",
      "Epoch 5[380/625] Time:0.706, Train Loss:0.24120987951755524\n",
      "Epoch 5[381/625] Time:0.702, Train Loss:0.29618287086486816\n",
      "Epoch 5[382/625] Time:0.703, Train Loss:0.28065258264541626\n",
      "Epoch 5[383/625] Time:0.702, Train Loss:0.21915528178215027\n",
      "Epoch 5[384/625] Time:0.702, Train Loss:0.30901041626930237\n",
      "Epoch 5[385/625] Time:0.709, Train Loss:0.2982403039932251\n",
      "Epoch 5[386/625] Time:0.702, Train Loss:0.356916218996048\n",
      "Epoch 5[387/625] Time:0.702, Train Loss:0.29608088731765747\n",
      "Epoch 5[388/625] Time:0.703, Train Loss:0.3148539960384369\n",
      "Epoch 5[389/625] Time:0.702, Train Loss:0.3468710482120514\n",
      "Epoch 5[390/625] Time:0.702, Train Loss:0.3720490038394928\n",
      "Epoch 5[391/625] Time:0.747, Train Loss:0.32347601652145386\n",
      "Epoch 5[392/625] Time:0.736, Train Loss:0.35669147968292236\n",
      "Epoch 5[393/625] Time:0.701, Train Loss:0.38155123591423035\n",
      "Epoch 5[394/625] Time:0.702, Train Loss:0.2666926980018616\n",
      "Epoch 5[395/625] Time:0.703, Train Loss:0.25727489590644836\n",
      "Epoch 5[396/625] Time:0.701, Train Loss:0.26864880323410034\n",
      "Epoch 5[397/625] Time:0.702, Train Loss:0.29578670859336853\n",
      "Epoch 5[398/625] Time:0.738, Train Loss:0.3310503363609314\n",
      "Epoch 5[399/625] Time:0.701, Train Loss:0.23388899862766266\n",
      "Epoch 5[400/625] Time:0.703, Train Loss:0.2697713375091553\n",
      "Epoch 5[401/625] Time:0.704, Train Loss:0.3156066834926605\n",
      "Epoch 5[402/625] Time:0.702, Train Loss:0.2886742353439331\n",
      "Epoch 5[403/625] Time:0.703, Train Loss:0.3325590491294861\n",
      "Epoch 5[404/625] Time:0.703, Train Loss:0.2409513294696808\n",
      "Epoch 5[405/625] Time:0.702, Train Loss:0.24364349246025085\n",
      "Epoch 5[406/625] Time:0.702, Train Loss:0.3059389591217041\n",
      "Epoch 5[407/625] Time:0.703, Train Loss:0.2804468274116516\n",
      "Epoch 5[408/625] Time:0.704, Train Loss:0.3682674169540405\n",
      "Epoch 5[409/625] Time:0.702, Train Loss:0.2038457840681076\n",
      "Epoch 5[410/625] Time:0.702, Train Loss:0.33405110239982605\n",
      "Epoch 5[411/625] Time:0.702, Train Loss:0.4072108864784241\n",
      "Epoch 5[412/625] Time:0.702, Train Loss:0.2366337925195694\n",
      "Epoch 5[413/625] Time:0.703, Train Loss:0.3460431694984436\n",
      "Epoch 5[414/625] Time:0.704, Train Loss:0.2931651771068573\n",
      "Epoch 5[415/625] Time:0.704, Train Loss:0.29914915561676025\n",
      "Epoch 5[416/625] Time:0.703, Train Loss:0.3106885850429535\n",
      "Epoch 5[417/625] Time:0.703, Train Loss:0.3774208724498749\n",
      "Epoch 5[418/625] Time:0.702, Train Loss:0.2403523325920105\n",
      "Epoch 5[419/625] Time:0.702, Train Loss:0.3503291606903076\n",
      "Epoch 5[420/625] Time:0.702, Train Loss:0.28254634141921997\n",
      "Epoch 5[421/625] Time:0.703, Train Loss:0.30169883370399475\n",
      "Epoch 5[422/625] Time:0.703, Train Loss:0.3219221532344818\n",
      "Epoch 5[423/625] Time:0.703, Train Loss:0.28653472661972046\n",
      "Epoch 5[424/625] Time:0.703, Train Loss:0.26109007000923157\n",
      "Epoch 5[425/625] Time:0.703, Train Loss:0.32287126779556274\n",
      "Epoch 5[426/625] Time:0.703, Train Loss:0.29739588499069214\n",
      "Epoch 5[427/625] Time:0.702, Train Loss:0.3993459641933441\n",
      "Epoch 5[428/625] Time:0.704, Train Loss:0.3941916823387146\n",
      "Epoch 5[429/625] Time:0.702, Train Loss:0.3082558512687683\n",
      "Epoch 5[430/625] Time:0.702, Train Loss:0.23514586687088013\n",
      "Epoch 5[431/625] Time:0.703, Train Loss:0.2979884743690491\n",
      "Epoch 5[432/625] Time:0.703, Train Loss:0.22986282408237457\n",
      "Epoch 5[433/625] Time:0.702, Train Loss:0.30349504947662354\n",
      "Epoch 5[434/625] Time:0.702, Train Loss:0.4019705355167389\n",
      "Epoch 5[435/625] Time:0.704, Train Loss:0.3610495328903198\n",
      "Epoch 5[436/625] Time:0.702, Train Loss:0.2942011058330536\n",
      "Epoch 5[437/625] Time:0.702, Train Loss:0.27924394607543945\n",
      "Epoch 5[438/625] Time:0.704, Train Loss:0.32180726528167725\n",
      "Epoch 5[439/625] Time:0.704, Train Loss:0.2863602340221405\n",
      "Epoch 5[440/625] Time:0.702, Train Loss:0.3129478693008423\n",
      "Epoch 5[441/625] Time:0.702, Train Loss:0.2797192931175232\n",
      "Epoch 5[442/625] Time:0.703, Train Loss:0.2945289611816406\n",
      "Epoch 5[443/625] Time:0.703, Train Loss:0.2729654610157013\n",
      "Epoch 5[444/625] Time:0.704, Train Loss:0.25245431065559387\n",
      "Epoch 5[445/625] Time:0.702, Train Loss:0.3196616470813751\n",
      "Epoch 5[446/625] Time:0.702, Train Loss:0.30645516514778137\n",
      "Epoch 5[447/625] Time:0.702, Train Loss:0.38439643383026123\n",
      "Epoch 5[448/625] Time:0.702, Train Loss:0.31624794006347656\n",
      "Epoch 5[449/625] Time:0.703, Train Loss:0.23506984114646912\n",
      "Epoch 5[450/625] Time:0.702, Train Loss:0.31930816173553467\n",
      "Epoch 5[451/625] Time:0.702, Train Loss:0.2828506529331207\n",
      "Epoch 5[452/625] Time:0.702, Train Loss:0.3150442838668823\n",
      "Epoch 5[453/625] Time:0.702, Train Loss:0.33817705512046814\n",
      "Epoch 5[454/625] Time:0.702, Train Loss:0.34923624992370605\n",
      "Epoch 5[455/625] Time:0.703, Train Loss:0.2874944806098938\n",
      "Epoch 5[456/625] Time:0.702, Train Loss:0.23816204071044922\n",
      "Epoch 5[457/625] Time:0.703, Train Loss:0.30814093351364136\n",
      "Epoch 5[458/625] Time:0.705, Train Loss:0.2597590684890747\n",
      "Epoch 5[459/625] Time:0.703, Train Loss:0.2597281038761139\n",
      "Epoch 5[460/625] Time:0.706, Train Loss:0.25516414642333984\n",
      "Epoch 5[461/625] Time:0.703, Train Loss:0.3367757201194763\n",
      "Epoch 5[462/625] Time:0.702, Train Loss:0.2891267240047455\n",
      "Epoch 5[463/625] Time:0.703, Train Loss:0.3380512297153473\n",
      "Epoch 5[464/625] Time:0.709, Train Loss:0.29695120453834534\n",
      "Epoch 5[465/625] Time:0.703, Train Loss:0.345717191696167\n",
      "Epoch 5[466/625] Time:0.703, Train Loss:0.26593855023384094\n",
      "Epoch 5[467/625] Time:0.702, Train Loss:0.23058512806892395\n",
      "Epoch 5[468/625] Time:0.702, Train Loss:0.2879669964313507\n",
      "Epoch 5[469/625] Time:0.702, Train Loss:0.26787424087524414\n",
      "Epoch 5[470/625] Time:0.702, Train Loss:0.28397732973098755\n",
      "Epoch 5[471/625] Time:0.703, Train Loss:0.24465228617191315\n",
      "Epoch 5[472/625] Time:0.702, Train Loss:0.26781633496284485\n",
      "Epoch 5[473/625] Time:0.704, Train Loss:0.3416517674922943\n",
      "Epoch 5[474/625] Time:0.704, Train Loss:0.3637014329433441\n",
      "Epoch 5[475/625] Time:0.702, Train Loss:0.27307185530662537\n",
      "Epoch 5[476/625] Time:0.702, Train Loss:0.3158010244369507\n",
      "Epoch 5[477/625] Time:0.702, Train Loss:0.27474603056907654\n",
      "Epoch 5[478/625] Time:0.748, Train Loss:0.36783909797668457\n",
      "Epoch 5[479/625] Time:0.711, Train Loss:0.3995075225830078\n",
      "Epoch 5[480/625] Time:0.702, Train Loss:0.3084017038345337\n",
      "Epoch 5[481/625] Time:0.703, Train Loss:0.3228684663772583\n",
      "Epoch 5[482/625] Time:0.702, Train Loss:0.3419778347015381\n",
      "Epoch 5[483/625] Time:0.711, Train Loss:0.367204874753952\n",
      "Epoch 5[484/625] Time:0.728, Train Loss:0.3192926049232483\n",
      "Epoch 5[485/625] Time:0.702, Train Loss:0.31676721572875977\n",
      "Epoch 5[486/625] Time:0.704, Train Loss:0.30313554406166077\n",
      "Epoch 5[487/625] Time:0.727, Train Loss:0.2713754177093506\n",
      "Epoch 5[488/625] Time:0.701, Train Loss:0.3395332098007202\n",
      "Epoch 5[489/625] Time:0.702, Train Loss:0.25518208742141724\n",
      "Epoch 5[490/625] Time:0.702, Train Loss:0.25836846232414246\n",
      "Epoch 5[491/625] Time:0.703, Train Loss:0.35134628415107727\n",
      "Epoch 5[492/625] Time:0.702, Train Loss:0.3238387405872345\n",
      "Epoch 5[493/625] Time:0.702, Train Loss:0.29419922828674316\n",
      "Epoch 5[494/625] Time:0.704, Train Loss:0.2610762119293213\n",
      "Epoch 5[495/625] Time:0.703, Train Loss:0.3480443060398102\n",
      "Epoch 5[496/625] Time:0.716, Train Loss:0.26591160893440247\n",
      "Epoch 5[497/625] Time:0.703, Train Loss:0.3300379812717438\n",
      "Epoch 5[498/625] Time:0.703, Train Loss:0.3466070294380188\n",
      "Epoch 5[499/625] Time:0.704, Train Loss:0.36817169189453125\n",
      "Epoch 5[500/625] Time:0.702, Train Loss:0.30318376421928406\n",
      "Epoch 5[501/625] Time:0.702, Train Loss:0.28112825751304626\n",
      "Epoch 5[502/625] Time:0.701, Train Loss:0.41578537225723267\n",
      "Epoch 5[503/625] Time:0.702, Train Loss:0.28429490327835083\n",
      "Epoch 5[504/625] Time:0.706, Train Loss:0.3682869076728821\n",
      "Epoch 5[505/625] Time:0.703, Train Loss:0.36349359154701233\n",
      "Epoch 5[506/625] Time:0.704, Train Loss:0.3037205636501312\n",
      "Epoch 5[507/625] Time:0.704, Train Loss:0.34037140011787415\n",
      "Epoch 5[508/625] Time:0.702, Train Loss:0.3174748718738556\n",
      "Epoch 5[509/625] Time:0.702, Train Loss:0.24300514161586761\n",
      "Epoch 5[510/625] Time:0.727, Train Loss:0.31777194142341614\n",
      "Epoch 5[511/625] Time:0.732, Train Loss:0.2883181869983673\n",
      "Epoch 5[512/625] Time:0.7, Train Loss:0.26124823093414307\n",
      "Epoch 5[513/625] Time:0.703, Train Loss:0.2801224887371063\n",
      "Epoch 5[514/625] Time:0.703, Train Loss:0.3159729540348053\n",
      "Epoch 5[515/625] Time:0.7, Train Loss:0.30989870429039\n",
      "Epoch 5[516/625] Time:0.702, Train Loss:0.3710315525531769\n",
      "Epoch 5[517/625] Time:0.703, Train Loss:0.2925673723220825\n",
      "Epoch 5[518/625] Time:0.704, Train Loss:0.28745925426483154\n",
      "Epoch 5[519/625] Time:0.704, Train Loss:0.2843754291534424\n",
      "Epoch 5[520/625] Time:0.704, Train Loss:0.327620267868042\n",
      "Epoch 5[521/625] Time:0.704, Train Loss:0.2941565215587616\n",
      "Epoch 5[522/625] Time:0.703, Train Loss:0.31611624360084534\n",
      "Epoch 5[523/625] Time:0.705, Train Loss:0.267543762922287\n",
      "Epoch 5[524/625] Time:0.703, Train Loss:0.3206541836261749\n",
      "Epoch 5[525/625] Time:0.704, Train Loss:0.33125048875808716\n",
      "Epoch 5[526/625] Time:0.703, Train Loss:0.3089742660522461\n",
      "Epoch 5[527/625] Time:0.705, Train Loss:0.319381445646286\n",
      "Epoch 5[528/625] Time:0.704, Train Loss:0.2836756706237793\n",
      "Epoch 5[529/625] Time:0.724, Train Loss:0.3422793745994568\n",
      "Epoch 5[530/625] Time:0.702, Train Loss:0.29767510294914246\n",
      "Epoch 5[531/625] Time:0.703, Train Loss:0.32977238297462463\n",
      "Epoch 5[532/625] Time:0.703, Train Loss:0.24745510518550873\n",
      "Epoch 5[533/625] Time:0.702, Train Loss:0.3496944308280945\n",
      "Epoch 5[534/625] Time:0.701, Train Loss:0.22766901552677155\n",
      "Epoch 5[535/625] Time:0.737, Train Loss:0.3123267889022827\n",
      "Epoch 5[536/625] Time:0.694, Train Loss:0.24280418455600739\n",
      "Epoch 5[537/625] Time:0.698, Train Loss:0.3058963716030121\n",
      "Epoch 5[538/625] Time:0.695, Train Loss:0.2952701449394226\n",
      "Epoch 5[539/625] Time:0.694, Train Loss:0.243740051984787\n",
      "Epoch 5[540/625] Time:0.695, Train Loss:0.28618812561035156\n",
      "Epoch 5[541/625] Time:0.694, Train Loss:0.2395273894071579\n",
      "Epoch 5[542/625] Time:0.696, Train Loss:0.2758759558200836\n",
      "Epoch 5[543/625] Time:0.698, Train Loss:0.29413166642189026\n",
      "Epoch 5[544/625] Time:0.703, Train Loss:0.38294175267219543\n",
      "Epoch 5[545/625] Time:0.744, Train Loss:0.2988560199737549\n",
      "Epoch 5[546/625] Time:0.693, Train Loss:0.2962217926979065\n",
      "Epoch 5[547/625] Time:0.693, Train Loss:0.2771715223789215\n",
      "Epoch 5[548/625] Time:0.692, Train Loss:0.28371328115463257\n",
      "Epoch 5[549/625] Time:0.693, Train Loss:0.3276267647743225\n",
      "Epoch 5[550/625] Time:0.694, Train Loss:0.2545866072177887\n",
      "Epoch 5[551/625] Time:0.695, Train Loss:0.23914922773838043\n",
      "Epoch 5[552/625] Time:0.696, Train Loss:0.29373040795326233\n",
      "Epoch 5[553/625] Time:0.695, Train Loss:0.28599968552589417\n",
      "Epoch 5[554/625] Time:0.694, Train Loss:0.32154855132102966\n",
      "Epoch 5[555/625] Time:0.696, Train Loss:0.2737296521663666\n",
      "Epoch 5[556/625] Time:0.695, Train Loss:0.24253219366073608\n",
      "Epoch 5[557/625] Time:0.694, Train Loss:0.3229459822177887\n",
      "Epoch 5[558/625] Time:0.7, Train Loss:0.23602637648582458\n",
      "Epoch 5[559/625] Time:0.695, Train Loss:0.2263103723526001\n",
      "Epoch 5[560/625] Time:0.704, Train Loss:0.2675304710865021\n",
      "Epoch 5[561/625] Time:0.704, Train Loss:0.26210421323776245\n",
      "Epoch 5[562/625] Time:0.735, Train Loss:0.3000181317329407\n",
      "Epoch 5[563/625] Time:0.694, Train Loss:0.29107892513275146\n",
      "Epoch 5[564/625] Time:0.693, Train Loss:0.351072758436203\n",
      "Epoch 5[565/625] Time:0.704, Train Loss:0.22991947829723358\n",
      "Epoch 5[566/625] Time:0.705, Train Loss:0.3192715048789978\n",
      "Epoch 5[567/625] Time:0.701, Train Loss:0.26360392570495605\n",
      "Epoch 5[568/625] Time:0.703, Train Loss:0.30576780438423157\n",
      "Epoch 5[569/625] Time:0.705, Train Loss:0.25399869680404663\n",
      "Epoch 5[570/625] Time:0.702, Train Loss:0.3238108456134796\n",
      "Epoch 5[571/625] Time:0.703, Train Loss:0.2974662482738495\n",
      "Epoch 5[572/625] Time:0.703, Train Loss:0.3794303834438324\n",
      "Epoch 5[573/625] Time:0.703, Train Loss:0.27331656217575073\n",
      "Epoch 5[574/625] Time:0.707, Train Loss:0.31632518768310547\n",
      "Epoch 5[575/625] Time:0.702, Train Loss:0.29029470682144165\n",
      "Epoch 5[576/625] Time:0.703, Train Loss:0.268128901720047\n",
      "Epoch 5[577/625] Time:0.702, Train Loss:0.2636593282222748\n",
      "Epoch 5[578/625] Time:0.703, Train Loss:0.35040438175201416\n",
      "Epoch 5[579/625] Time:0.703, Train Loss:0.2544567883014679\n",
      "Epoch 5[580/625] Time:0.702, Train Loss:0.28502506017684937\n",
      "Epoch 5[581/625] Time:0.702, Train Loss:0.24074192345142365\n",
      "Epoch 5[582/625] Time:0.703, Train Loss:0.34640827775001526\n",
      "Epoch 5[583/625] Time:0.702, Train Loss:0.34458112716674805\n",
      "Epoch 5[584/625] Time:0.704, Train Loss:0.38531509041786194\n",
      "Epoch 5[585/625] Time:0.702, Train Loss:0.2663273513317108\n",
      "Epoch 5[586/625] Time:0.703, Train Loss:0.23004166781902313\n",
      "Epoch 5[587/625] Time:0.704, Train Loss:0.29500091075897217\n",
      "Epoch 5[588/625] Time:0.702, Train Loss:0.19828404486179352\n",
      "Epoch 5[589/625] Time:0.704, Train Loss:0.3143996596336365\n",
      "Epoch 5[590/625] Time:0.704, Train Loss:0.2999102771282196\n",
      "Epoch 5[591/625] Time:0.707, Train Loss:0.3637041449546814\n",
      "Epoch 5[592/625] Time:0.704, Train Loss:0.3620944619178772\n",
      "Epoch 5[593/625] Time:0.702, Train Loss:0.4319453537464142\n",
      "Epoch 5[594/625] Time:0.703, Train Loss:0.34338366985321045\n",
      "Epoch 5[595/625] Time:0.722, Train Loss:0.28232061862945557\n",
      "Epoch 5[596/625] Time:0.712, Train Loss:0.24814261496067047\n",
      "Epoch 5[597/625] Time:0.703, Train Loss:0.22237661480903625\n",
      "Epoch 5[598/625] Time:0.725, Train Loss:0.24491867423057556\n",
      "Epoch 5[599/625] Time:0.695, Train Loss:0.30280953645706177\n",
      "Epoch 5[600/625] Time:0.694, Train Loss:0.2717151641845703\n",
      "Epoch 5[601/625] Time:0.694, Train Loss:0.30462437868118286\n",
      "Epoch 5[602/625] Time:0.694, Train Loss:0.35836610198020935\n",
      "Epoch 5[603/625] Time:0.694, Train Loss:0.2620016634464264\n",
      "Epoch 5[604/625] Time:0.693, Train Loss:0.3047640323638916\n",
      "Epoch 5[605/625] Time:0.703, Train Loss:0.2642277777194977\n",
      "Epoch 5[606/625] Time:0.702, Train Loss:0.35948312282562256\n",
      "Epoch 5[607/625] Time:0.704, Train Loss:0.31956374645233154\n",
      "Epoch 5[608/625] Time:0.703, Train Loss:0.26987117528915405\n",
      "Epoch 5[609/625] Time:0.744, Train Loss:0.2280845046043396\n",
      "Epoch 5[610/625] Time:0.695, Train Loss:0.287751704454422\n",
      "Epoch 5[611/625] Time:0.694, Train Loss:0.2805264890193939\n",
      "Epoch 5[612/625] Time:0.694, Train Loss:0.21603140234947205\n",
      "Epoch 5[613/625] Time:0.693, Train Loss:0.30281373858451843\n",
      "Epoch 5[614/625] Time:0.695, Train Loss:0.3534178137779236\n",
      "Epoch 5[615/625] Time:0.702, Train Loss:0.2534440755844116\n",
      "Epoch 5[616/625] Time:0.702, Train Loss:0.28297197818756104\n",
      "Epoch 5[617/625] Time:0.703, Train Loss:0.27658480405807495\n",
      "Epoch 5[618/625] Time:0.703, Train Loss:0.30745697021484375\n",
      "Epoch 5[619/625] Time:0.702, Train Loss:0.27997496724128723\n",
      "Epoch 5[620/625] Time:0.703, Train Loss:0.38206958770751953\n",
      "Epoch 5[621/625] Time:0.704, Train Loss:0.3472561538219452\n",
      "Epoch 5[622/625] Time:0.708, Train Loss:0.38099178671836853\n",
      "Epoch 5[623/625] Time:0.702, Train Loss:0.326797753572464\n",
      "Epoch 5[624/625] Time:0.702, Train Loss:0.28356409072875977\n",
      "Epoch 5[0/78] Val Loss:0.22512613236904144\n",
      "Epoch 5[1/78] Val Loss:0.17295505106449127\n",
      "Epoch 5[2/78] Val Loss:0.212458536028862\n",
      "Epoch 5[3/78] Val Loss:0.21451883018016815\n",
      "Epoch 5[4/78] Val Loss:0.3658711612224579\n",
      "Epoch 5[5/78] Val Loss:0.26508215069770813\n",
      "Epoch 5[6/78] Val Loss:0.24936269223690033\n",
      "Epoch 5[7/78] Val Loss:0.3557385802268982\n",
      "Epoch 5[8/78] Val Loss:0.19713948667049408\n",
      "Epoch 5[9/78] Val Loss:0.15465854108333588\n",
      "Epoch 5[10/78] Val Loss:0.10283055901527405\n",
      "Epoch 5[11/78] Val Loss:0.15566380321979523\n",
      "Epoch 5[12/78] Val Loss:0.11224447935819626\n",
      "Epoch 5[13/78] Val Loss:0.08883611857891083\n",
      "Epoch 5[14/78] Val Loss:0.21385204792022705\n",
      "Epoch 5[15/78] Val Loss:0.18847563862800598\n",
      "Epoch 5[16/78] Val Loss:0.2190045416355133\n",
      "Epoch 5[17/78] Val Loss:0.1365562081336975\n",
      "Epoch 5[18/78] Val Loss:0.2476174384355545\n",
      "Epoch 5[19/78] Val Loss:0.3231774866580963\n",
      "Epoch 5[20/78] Val Loss:0.1935456395149231\n",
      "Epoch 5[21/78] Val Loss:0.5307180881500244\n",
      "Epoch 5[22/78] Val Loss:0.7518311142921448\n",
      "Epoch 5[23/78] Val Loss:0.5232453346252441\n",
      "Epoch 5[24/78] Val Loss:0.4146958589553833\n",
      "Epoch 5[25/78] Val Loss:0.4447747468948364\n",
      "Epoch 5[26/78] Val Loss:0.46054551005363464\n",
      "Epoch 5[27/78] Val Loss:0.43606919050216675\n",
      "Epoch 5[28/78] Val Loss:0.41132909059524536\n",
      "Epoch 5[29/78] Val Loss:0.5212109684944153\n",
      "Epoch 5[30/78] Val Loss:1.6499271392822266\n",
      "Epoch 5[31/78] Val Loss:1.379116177558899\n",
      "Epoch 5[32/78] Val Loss:1.0835176706314087\n",
      "Epoch 5[33/78] Val Loss:0.42835113406181335\n",
      "Epoch 5[34/78] Val Loss:0.35251349210739136\n",
      "Epoch 5[35/78] Val Loss:0.37933051586151123\n",
      "Epoch 5[36/78] Val Loss:0.3290386497974396\n",
      "Epoch 5[37/78] Val Loss:0.4514084458351135\n",
      "Epoch 5[38/78] Val Loss:0.21707051992416382\n",
      "Epoch 5[39/78] Val Loss:0.21327361464500427\n",
      "Epoch 5[40/78] Val Loss:0.21288087964057922\n",
      "Epoch 5[41/78] Val Loss:0.23314201831817627\n",
      "Epoch 5[42/78] Val Loss:0.23932945728302002\n",
      "Epoch 5[43/78] Val Loss:0.1335463523864746\n",
      "Epoch 5[44/78] Val Loss:0.10174702852964401\n",
      "Epoch 5[45/78] Val Loss:0.10591480135917664\n",
      "Epoch 5[46/78] Val Loss:0.11271588504314423\n",
      "Epoch 5[47/78] Val Loss:0.10317790508270264\n",
      "Epoch 5[48/78] Val Loss:0.16303382813930511\n",
      "Epoch 5[49/78] Val Loss:0.11429441720247269\n",
      "Epoch 5[50/78] Val Loss:0.0840904638171196\n",
      "Epoch 5[51/78] Val Loss:0.10989557951688766\n",
      "Epoch 5[52/78] Val Loss:0.12040376663208008\n",
      "Epoch 5[53/78] Val Loss:0.10429330915212631\n",
      "Epoch 5[54/78] Val Loss:0.09373323619365692\n",
      "Epoch 5[55/78] Val Loss:0.11128906905651093\n",
      "Epoch 5[56/78] Val Loss:0.5115227699279785\n",
      "Epoch 5[57/78] Val Loss:0.46028098464012146\n",
      "Epoch 5[58/78] Val Loss:0.4184544086456299\n",
      "Epoch 5[59/78] Val Loss:0.4318263530731201\n",
      "Epoch 5[60/78] Val Loss:0.43943724036216736\n",
      "Epoch 5[61/78] Val Loss:0.4840274155139923\n",
      "Epoch 5[62/78] Val Loss:0.5063984394073486\n",
      "Epoch 5[63/78] Val Loss:0.2769269347190857\n",
      "Epoch 5[64/78] Val Loss:0.18150854110717773\n",
      "Epoch 5[65/78] Val Loss:0.17268255352973938\n",
      "Epoch 5[66/78] Val Loss:0.19790400564670563\n",
      "Epoch 5[67/78] Val Loss:0.21530155837535858\n",
      "Epoch 5[68/78] Val Loss:0.4995116889476776\n",
      "Epoch 5[69/78] Val Loss:0.4762336313724518\n",
      "Epoch 5[70/78] Val Loss:0.48903971910476685\n",
      "Epoch 5[71/78] Val Loss:0.40326979756355286\n",
      "Epoch 5[72/78] Val Loss:0.2826680839061737\n",
      "Epoch 5[73/78] Val Loss:0.2709007263183594\n",
      "Epoch 5[74/78] Val Loss:0.5390563011169434\n",
      "Epoch 5[75/78] Val Loss:0.6370540261268616\n",
      "Epoch 5[76/78] Val Loss:0.5400192737579346\n",
      "Epoch 5[77/78] Val Loss:0.582699716091156\n",
      "Epoch 5[78/78] Val Loss:0.6916590332984924\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91     15691\n",
      "           1       0.65      0.79      0.71      4309\n",
      "\n",
      "    accuracy                           0.86     20000\n",
      "   macro avg       0.79      0.83      0.81     20000\n",
      "weighted avg       0.87      0.86      0.87     20000\n",
      "\n",
      "Epoch 5: Train Loss 0.3025682065963745, Val Loss 0.3490327382699037, Train Time 794.7874209880829, Val Time 37.106266021728516\n",
      "Epoch 6[0/625] Time:0.688, Train Loss:0.2655434310436249\n",
      "Epoch 6[1/625] Time:0.704, Train Loss:0.28759023547172546\n",
      "Epoch 6[2/625] Time:0.703, Train Loss:0.326431006193161\n",
      "Epoch 6[3/625] Time:0.704, Train Loss:0.27714693546295166\n",
      "Epoch 6[4/625] Time:0.707, Train Loss:0.3540519177913666\n",
      "Epoch 6[5/625] Time:0.691, Train Loss:0.31823819875717163\n",
      "Epoch 6[6/625] Time:0.69, Train Loss:0.2590130865573883\n",
      "Epoch 6[7/625] Time:0.69, Train Loss:0.4621480107307434\n",
      "Epoch 6[8/625] Time:0.69, Train Loss:0.28449884057044983\n",
      "Epoch 6[9/625] Time:0.69, Train Loss:0.3064112663269043\n",
      "Epoch 6[10/625] Time:0.691, Train Loss:0.3231441378593445\n",
      "Epoch 6[11/625] Time:0.691, Train Loss:0.23469310998916626\n",
      "Epoch 6[12/625] Time:0.692, Train Loss:0.24293705821037292\n",
      "Epoch 6[13/625] Time:0.725, Train Loss:0.35006895661354065\n",
      "Epoch 6[14/625] Time:0.703, Train Loss:0.3603878617286682\n",
      "Epoch 6[15/625] Time:0.746, Train Loss:0.3464667797088623\n",
      "Epoch 6[16/625] Time:0.703, Train Loss:0.3138895332813263\n",
      "Epoch 6[17/625] Time:0.705, Train Loss:0.30902814865112305\n",
      "Epoch 6[18/625] Time:0.704, Train Loss:0.23999883234500885\n",
      "Epoch 6[19/625] Time:0.704, Train Loss:0.3365834057331085\n",
      "Epoch 6[20/625] Time:0.729, Train Loss:0.2843277156352997\n",
      "Epoch 6[21/625] Time:0.709, Train Loss:0.26555678248405457\n",
      "Epoch 6[22/625] Time:0.702, Train Loss:0.2502688467502594\n",
      "Epoch 6[23/625] Time:0.703, Train Loss:0.322308748960495\n",
      "Epoch 6[24/625] Time:0.702, Train Loss:0.2918225824832916\n",
      "Epoch 6[25/625] Time:0.703, Train Loss:0.3906385600566864\n",
      "Epoch 6[26/625] Time:0.702, Train Loss:0.3364916145801544\n",
      "Epoch 6[27/625] Time:0.702, Train Loss:0.25676244497299194\n",
      "Epoch 6[28/625] Time:0.704, Train Loss:0.3408276438713074\n",
      "Epoch 6[29/625] Time:0.71, Train Loss:0.2248949408531189\n",
      "Epoch 6[30/625] Time:0.702, Train Loss:0.2689940631389618\n",
      "Epoch 6[31/625] Time:0.748, Train Loss:0.2527105510234833\n",
      "Epoch 6[32/625] Time:0.724, Train Loss:0.26285919547080994\n",
      "Epoch 6[33/625] Time:0.702, Train Loss:0.3530702292919159\n",
      "Epoch 6[34/625] Time:0.704, Train Loss:0.24727097153663635\n",
      "Epoch 6[35/625] Time:0.703, Train Loss:0.29044729471206665\n",
      "Epoch 6[36/625] Time:0.702, Train Loss:0.37362781167030334\n",
      "Epoch 6[37/625] Time:0.704, Train Loss:0.268727570772171\n",
      "Epoch 6[38/625] Time:0.703, Train Loss:0.23903656005859375\n",
      "Epoch 6[39/625] Time:0.703, Train Loss:0.3611932694911957\n",
      "Epoch 6[40/625] Time:0.703, Train Loss:0.2770889699459076\n",
      "Epoch 6[41/625] Time:0.702, Train Loss:0.30579671263694763\n",
      "Epoch 6[42/625] Time:0.703, Train Loss:0.2837618589401245\n",
      "Epoch 6[43/625] Time:0.702, Train Loss:0.3093807101249695\n",
      "Epoch 6[44/625] Time:0.703, Train Loss:0.31898295879364014\n",
      "Epoch 6[45/625] Time:0.702, Train Loss:0.3566395342350006\n",
      "Epoch 6[46/625] Time:0.702, Train Loss:0.25716906785964966\n",
      "Epoch 6[47/625] Time:0.702, Train Loss:0.28650379180908203\n",
      "Epoch 6[48/625] Time:0.703, Train Loss:0.2791163921356201\n",
      "Epoch 6[49/625] Time:0.703, Train Loss:0.2945752739906311\n",
      "Epoch 6[50/625] Time:0.704, Train Loss:0.2678861618041992\n",
      "Epoch 6[51/625] Time:0.703, Train Loss:0.3312585949897766\n",
      "Epoch 6[52/625] Time:0.704, Train Loss:0.3287634253501892\n",
      "Epoch 6[53/625] Time:0.705, Train Loss:0.3121093213558197\n",
      "Epoch 6[54/625] Time:0.703, Train Loss:0.30283576250076294\n",
      "Epoch 6[55/625] Time:0.703, Train Loss:0.3211574852466583\n",
      "Epoch 6[56/625] Time:0.703, Train Loss:0.34969595074653625\n",
      "Epoch 6[57/625] Time:0.702, Train Loss:0.29655328392982483\n",
      "Epoch 6[58/625] Time:0.704, Train Loss:0.31905341148376465\n",
      "Epoch 6[59/625] Time:0.703, Train Loss:0.27306511998176575\n",
      "Epoch 6[60/625] Time:0.703, Train Loss:0.2646305561065674\n",
      "Epoch 6[61/625] Time:0.702, Train Loss:0.2838841378688812\n",
      "Epoch 6[62/625] Time:0.715, Train Loss:0.2965594232082367\n",
      "Epoch 6[63/625] Time:0.704, Train Loss:0.2455935925245285\n",
      "Epoch 6[64/625] Time:0.703, Train Loss:0.2873286306858063\n",
      "Epoch 6[65/625] Time:0.703, Train Loss:0.2421984225511551\n",
      "Epoch 6[66/625] Time:0.706, Train Loss:0.32072094082832336\n",
      "Epoch 6[67/625] Time:0.703, Train Loss:0.2742369771003723\n",
      "Epoch 6[68/625] Time:0.708, Train Loss:0.2662743926048279\n",
      "Epoch 6[69/625] Time:0.703, Train Loss:0.2942672073841095\n",
      "Epoch 6[70/625] Time:0.703, Train Loss:0.29329362511634827\n",
      "Epoch 6[71/625] Time:0.704, Train Loss:0.29037731885910034\n",
      "Epoch 6[72/625] Time:0.703, Train Loss:0.25838640332221985\n",
      "Epoch 6[73/625] Time:0.703, Train Loss:0.3379789888858795\n",
      "Epoch 6[74/625] Time:0.703, Train Loss:0.25789737701416016\n",
      "Epoch 6[75/625] Time:0.702, Train Loss:0.30298808217048645\n",
      "Epoch 6[76/625] Time:0.704, Train Loss:0.2955125868320465\n",
      "Epoch 6[77/625] Time:0.702, Train Loss:0.317350298166275\n",
      "Epoch 6[78/625] Time:0.703, Train Loss:0.2616315484046936\n",
      "Epoch 6[79/625] Time:0.703, Train Loss:0.24194033443927765\n",
      "Epoch 6[80/625] Time:0.703, Train Loss:0.2954411506652832\n",
      "Epoch 6[81/625] Time:0.704, Train Loss:0.28139251470565796\n",
      "Epoch 6[82/625] Time:0.703, Train Loss:0.2681366801261902\n",
      "Epoch 6[83/625] Time:0.704, Train Loss:0.23891209065914154\n",
      "Epoch 6[84/625] Time:0.71, Train Loss:0.3389877378940582\n",
      "Epoch 6[85/625] Time:0.704, Train Loss:0.22309371829032898\n",
      "Epoch 6[86/625] Time:0.703, Train Loss:0.30866849422454834\n",
      "Epoch 6[87/625] Time:0.702, Train Loss:0.24069899320602417\n",
      "Epoch 6[88/625] Time:0.704, Train Loss:0.3886636793613434\n",
      "Epoch 6[89/625] Time:0.703, Train Loss:0.30201685428619385\n",
      "Epoch 6[90/625] Time:0.704, Train Loss:0.25439926981925964\n",
      "Epoch 6[91/625] Time:0.703, Train Loss:0.26790863275527954\n",
      "Epoch 6[92/625] Time:0.703, Train Loss:0.2511065602302551\n",
      "Epoch 6[93/625] Time:0.702, Train Loss:0.26290518045425415\n",
      "Epoch 6[94/625] Time:0.702, Train Loss:0.28190645575523376\n",
      "Epoch 6[95/625] Time:0.694, Train Loss:0.3298172652721405\n",
      "Epoch 6[96/625] Time:0.695, Train Loss:0.2447105199098587\n",
      "Epoch 6[97/625] Time:0.691, Train Loss:0.22221799194812775\n",
      "Epoch 6[98/625] Time:0.694, Train Loss:0.23970690369606018\n",
      "Epoch 6[99/625] Time:0.694, Train Loss:0.38053587079048157\n",
      "Epoch 6[100/625] Time:0.694, Train Loss:0.3304390013217926\n",
      "Epoch 6[101/625] Time:0.693, Train Loss:0.24052637815475464\n",
      "Epoch 6[102/625] Time:0.691, Train Loss:0.2505318522453308\n",
      "Epoch 6[103/625] Time:0.693, Train Loss:0.26570355892181396\n",
      "Epoch 6[104/625] Time:0.693, Train Loss:0.331686794757843\n",
      "Epoch 6[105/625] Time:0.694, Train Loss:0.2809228301048279\n",
      "Epoch 6[106/625] Time:0.695, Train Loss:0.3112756311893463\n",
      "Epoch 6[107/625] Time:0.694, Train Loss:0.27466481924057007\n",
      "Epoch 6[108/625] Time:0.694, Train Loss:0.27718135714530945\n",
      "Epoch 6[109/625] Time:0.694, Train Loss:0.30643749237060547\n",
      "Epoch 6[110/625] Time:0.694, Train Loss:0.3852459788322449\n",
      "Epoch 6[111/625] Time:0.709, Train Loss:0.30519911646842957\n",
      "Epoch 6[112/625] Time:0.704, Train Loss:0.2860206961631775\n",
      "Epoch 6[113/625] Time:0.704, Train Loss:0.22940073907375336\n",
      "Epoch 6[114/625] Time:0.704, Train Loss:0.27795377373695374\n",
      "Epoch 6[115/625] Time:0.704, Train Loss:0.29500141739845276\n",
      "Epoch 6[116/625] Time:0.703, Train Loss:0.27081841230392456\n",
      "Epoch 6[117/625] Time:0.703, Train Loss:0.2384943962097168\n",
      "Epoch 6[118/625] Time:0.703, Train Loss:0.3017656207084656\n",
      "Epoch 6[119/625] Time:0.704, Train Loss:0.2700852155685425\n",
      "Epoch 6[120/625] Time:0.703, Train Loss:0.2865976393222809\n",
      "Epoch 6[121/625] Time:0.722, Train Loss:0.3514990508556366\n",
      "Epoch 6[122/625] Time:0.707, Train Loss:0.24568913877010345\n",
      "Epoch 6[123/625] Time:0.704, Train Loss:0.2628260850906372\n",
      "Epoch 6[124/625] Time:0.702, Train Loss:0.2797269821166992\n",
      "Epoch 6[125/625] Time:0.701, Train Loss:0.334842711687088\n",
      "Epoch 6[126/625] Time:0.694, Train Loss:0.19498883187770844\n",
      "Epoch 6[127/625] Time:0.707, Train Loss:0.32708677649497986\n",
      "Epoch 6[128/625] Time:0.701, Train Loss:0.3202661871910095\n",
      "Epoch 6[129/625] Time:0.701, Train Loss:0.20857197046279907\n",
      "Epoch 6[130/625] Time:0.704, Train Loss:0.2649335265159607\n",
      "Epoch 6[131/625] Time:0.702, Train Loss:0.2446039468050003\n",
      "Epoch 6[132/625] Time:0.702, Train Loss:0.2593979835510254\n",
      "Epoch 6[133/625] Time:0.702, Train Loss:0.30634090304374695\n",
      "Epoch 6[134/625] Time:0.703, Train Loss:0.1913292109966278\n",
      "Epoch 6[135/625] Time:0.705, Train Loss:0.321727991104126\n",
      "Epoch 6[136/625] Time:0.703, Train Loss:0.24840828776359558\n",
      "Epoch 6[137/625] Time:0.705, Train Loss:0.3121812641620636\n",
      "Epoch 6[138/625] Time:0.701, Train Loss:0.2857899069786072\n",
      "Epoch 6[139/625] Time:0.707, Train Loss:0.2227090746164322\n",
      "Epoch 6[140/625] Time:0.704, Train Loss:0.26575779914855957\n",
      "Epoch 6[141/625] Time:0.703, Train Loss:0.30433154106140137\n",
      "Epoch 6[142/625] Time:0.702, Train Loss:0.24809159338474274\n",
      "Epoch 6[143/625] Time:0.703, Train Loss:0.3494740426540375\n",
      "Epoch 6[144/625] Time:0.703, Train Loss:0.2833746373653412\n",
      "Epoch 6[145/625] Time:0.701, Train Loss:0.3117426037788391\n",
      "Epoch 6[146/625] Time:0.702, Train Loss:0.2732650935649872\n",
      "Epoch 6[147/625] Time:0.703, Train Loss:0.29046693444252014\n",
      "Epoch 6[148/625] Time:0.701, Train Loss:0.2510887384414673\n",
      "Epoch 6[149/625] Time:0.702, Train Loss:0.30224737524986267\n",
      "Epoch 6[150/625] Time:0.702, Train Loss:0.284131795167923\n",
      "Epoch 6[151/625] Time:0.703, Train Loss:0.29471081495285034\n",
      "Epoch 6[152/625] Time:0.703, Train Loss:0.2986980974674225\n",
      "Epoch 6[153/625] Time:0.706, Train Loss:0.2769758999347687\n",
      "Epoch 6[154/625] Time:0.703, Train Loss:0.3992583155632019\n",
      "Epoch 6[155/625] Time:0.702, Train Loss:0.38671573996543884\n",
      "Epoch 6[156/625] Time:0.702, Train Loss:0.33157962560653687\n",
      "Epoch 6[157/625] Time:0.729, Train Loss:0.2520075738430023\n",
      "Epoch 6[158/625] Time:0.712, Train Loss:0.297781765460968\n",
      "Epoch 6[159/625] Time:0.703, Train Loss:0.2666521966457367\n",
      "Epoch 6[160/625] Time:0.703, Train Loss:0.40998512506484985\n",
      "Epoch 6[161/625] Time:0.704, Train Loss:0.2421969324350357\n",
      "Epoch 6[162/625] Time:0.705, Train Loss:0.26151373982429504\n",
      "Epoch 6[163/625] Time:0.702, Train Loss:0.18726764619350433\n",
      "Epoch 6[164/625] Time:0.702, Train Loss:0.23543787002563477\n",
      "Epoch 6[165/625] Time:0.703, Train Loss:0.33966130018234253\n",
      "Epoch 6[166/625] Time:0.702, Train Loss:0.23818011581897736\n",
      "Epoch 6[167/625] Time:0.702, Train Loss:0.26199716329574585\n",
      "Epoch 6[168/625] Time:0.703, Train Loss:0.25704604387283325\n",
      "Epoch 6[169/625] Time:0.701, Train Loss:0.2635156214237213\n",
      "Epoch 6[170/625] Time:0.703, Train Loss:0.31584155559539795\n",
      "Epoch 6[171/625] Time:0.701, Train Loss:0.26511096954345703\n",
      "Epoch 6[172/625] Time:0.701, Train Loss:0.32186028361320496\n",
      "Epoch 6[173/625] Time:0.704, Train Loss:0.2923957109451294\n",
      "Epoch 6[174/625] Time:0.703, Train Loss:0.243303120136261\n",
      "Epoch 6[175/625] Time:0.703, Train Loss:0.3095399737358093\n",
      "Epoch 6[176/625] Time:0.702, Train Loss:0.2637418508529663\n",
      "Epoch 6[177/625] Time:0.702, Train Loss:0.2936267852783203\n",
      "Epoch 6[178/625] Time:0.704, Train Loss:0.2827913463115692\n",
      "Epoch 6[179/625] Time:0.703, Train Loss:0.3115399181842804\n",
      "Epoch 6[180/625] Time:0.703, Train Loss:0.22473645210266113\n",
      "Epoch 6[181/625] Time:0.703, Train Loss:0.3153974115848541\n",
      "Epoch 6[182/625] Time:0.702, Train Loss:0.26611289381980896\n",
      "Epoch 6[183/625] Time:0.704, Train Loss:0.34173107147216797\n",
      "Epoch 6[184/625] Time:0.704, Train Loss:0.30985724925994873\n",
      "Epoch 6[185/625] Time:0.702, Train Loss:0.32161077857017517\n",
      "Epoch 6[186/625] Time:0.708, Train Loss:0.22297652065753937\n",
      "Epoch 6[187/625] Time:0.702, Train Loss:0.2813864052295685\n",
      "Epoch 6[188/625] Time:0.702, Train Loss:0.31926074624061584\n",
      "Epoch 6[189/625] Time:0.703, Train Loss:0.3663399815559387\n",
      "Epoch 6[190/625] Time:0.702, Train Loss:0.308322936296463\n",
      "Epoch 6[191/625] Time:0.703, Train Loss:0.25147226452827454\n",
      "Epoch 6[192/625] Time:0.703, Train Loss:0.23160146176815033\n",
      "Epoch 6[193/625] Time:0.702, Train Loss:0.28508514165878296\n",
      "Epoch 6[194/625] Time:0.703, Train Loss:0.3320538401603699\n",
      "Epoch 6[195/625] Time:0.702, Train Loss:0.25671935081481934\n",
      "Epoch 6[196/625] Time:0.702, Train Loss:0.2996691167354584\n",
      "Epoch 6[197/625] Time:0.702, Train Loss:0.4021705090999603\n",
      "Epoch 6[198/625] Time:0.702, Train Loss:0.3019252419471741\n",
      "Epoch 6[199/625] Time:0.703, Train Loss:0.32828661799430847\n",
      "Epoch 6[200/625] Time:0.702, Train Loss:0.31231164932250977\n",
      "Epoch 6[201/625] Time:0.732, Train Loss:0.2961846590042114\n",
      "Epoch 6[202/625] Time:0.728, Train Loss:0.29761430621147156\n",
      "Epoch 6[203/625] Time:0.702, Train Loss:0.29433125257492065\n",
      "Epoch 6[204/625] Time:0.702, Train Loss:0.3452804684638977\n",
      "Epoch 6[205/625] Time:0.702, Train Loss:0.2514789402484894\n",
      "Epoch 6[206/625] Time:0.704, Train Loss:0.3584112226963043\n",
      "Epoch 6[207/625] Time:0.703, Train Loss:0.2915898263454437\n",
      "Epoch 6[208/625] Time:0.702, Train Loss:0.27851271629333496\n",
      "Epoch 6[209/625] Time:0.703, Train Loss:0.36286604404449463\n",
      "Epoch 6[210/625] Time:0.703, Train Loss:0.28675034642219543\n",
      "Epoch 6[211/625] Time:0.703, Train Loss:0.2838108241558075\n",
      "Epoch 6[212/625] Time:0.703, Train Loss:0.32931822538375854\n",
      "Epoch 6[213/625] Time:0.703, Train Loss:0.23005278408527374\n",
      "Epoch 6[214/625] Time:0.704, Train Loss:0.2732860743999481\n",
      "Epoch 6[215/625] Time:0.704, Train Loss:0.36171039938926697\n",
      "Epoch 6[216/625] Time:0.704, Train Loss:0.2888059616088867\n",
      "Epoch 6[217/625] Time:0.704, Train Loss:0.3270707130432129\n",
      "Epoch 6[218/625] Time:0.703, Train Loss:0.3980649411678314\n",
      "Epoch 6[219/625] Time:0.703, Train Loss:0.2328648567199707\n",
      "Epoch 6[220/625] Time:0.705, Train Loss:0.19303935766220093\n",
      "Epoch 6[221/625] Time:0.704, Train Loss:0.2986568808555603\n",
      "Epoch 6[222/625] Time:0.703, Train Loss:0.3175095319747925\n",
      "Epoch 6[223/625] Time:0.703, Train Loss:0.2756219208240509\n",
      "Epoch 6[224/625] Time:0.703, Train Loss:0.2468886822462082\n",
      "Epoch 6[225/625] Time:0.706, Train Loss:0.2403717339038849\n",
      "Epoch 6[226/625] Time:0.702, Train Loss:0.2817770838737488\n",
      "Epoch 6[227/625] Time:0.702, Train Loss:0.262893408536911\n",
      "Epoch 6[228/625] Time:0.704, Train Loss:0.23146533966064453\n",
      "Epoch 6[229/625] Time:0.703, Train Loss:0.2969657778739929\n",
      "Epoch 6[230/625] Time:0.703, Train Loss:0.2377813458442688\n",
      "Epoch 6[231/625] Time:0.703, Train Loss:0.21605250239372253\n",
      "Epoch 6[232/625] Time:0.703, Train Loss:0.27221572399139404\n",
      "Epoch 6[233/625] Time:0.71, Train Loss:0.357231467962265\n",
      "Epoch 6[234/625] Time:0.703, Train Loss:0.3050728440284729\n",
      "Epoch 6[235/625] Time:0.703, Train Loss:0.271366685628891\n",
      "Epoch 6[236/625] Time:0.703, Train Loss:0.2691371440887451\n",
      "Epoch 6[237/625] Time:0.703, Train Loss:0.3404102325439453\n",
      "Epoch 6[238/625] Time:0.703, Train Loss:0.27341020107269287\n",
      "Epoch 6[239/625] Time:0.704, Train Loss:0.2941038906574249\n",
      "Epoch 6[240/625] Time:0.747, Train Loss:0.32880356907844543\n",
      "Epoch 6[241/625] Time:0.7, Train Loss:0.2872028648853302\n",
      "Epoch 6[242/625] Time:0.747, Train Loss:0.2759035527706146\n",
      "Epoch 6[243/625] Time:0.702, Train Loss:0.2912660539150238\n",
      "Epoch 6[244/625] Time:0.703, Train Loss:0.20943573117256165\n",
      "Epoch 6[245/625] Time:0.703, Train Loss:0.34009018540382385\n",
      "Epoch 6[246/625] Time:0.703, Train Loss:0.22862687706947327\n",
      "Epoch 6[247/625] Time:0.702, Train Loss:0.2404801994562149\n",
      "Epoch 6[248/625] Time:0.703, Train Loss:0.285467267036438\n",
      "Epoch 6[249/625] Time:0.703, Train Loss:0.2559591829776764\n",
      "Epoch 6[250/625] Time:0.703, Train Loss:0.2818591296672821\n",
      "Epoch 6[251/625] Time:0.703, Train Loss:0.284167617559433\n",
      "Epoch 6[252/625] Time:0.703, Train Loss:0.29969897866249084\n",
      "Epoch 6[253/625] Time:0.702, Train Loss:0.25993090867996216\n",
      "Epoch 6[254/625] Time:0.703, Train Loss:0.24654099345207214\n",
      "Epoch 6[255/625] Time:0.703, Train Loss:0.2528466284275055\n",
      "Epoch 6[256/625] Time:0.702, Train Loss:0.3073628842830658\n",
      "Epoch 6[257/625] Time:0.703, Train Loss:0.27107298374176025\n",
      "Epoch 6[258/625] Time:0.702, Train Loss:0.3050404191017151\n",
      "Epoch 6[259/625] Time:0.703, Train Loss:0.29944750666618347\n",
      "Epoch 6[260/625] Time:0.703, Train Loss:0.2651236057281494\n",
      "Epoch 6[261/625] Time:0.705, Train Loss:0.2721095681190491\n",
      "Epoch 6[262/625] Time:0.703, Train Loss:0.24514345824718475\n",
      "Epoch 6[263/625] Time:0.703, Train Loss:0.2952900230884552\n",
      "Epoch 6[264/625] Time:0.703, Train Loss:0.26048797369003296\n",
      "Epoch 6[265/625] Time:0.702, Train Loss:0.2633110284805298\n",
      "Epoch 6[266/625] Time:0.703, Train Loss:0.33134955167770386\n",
      "Epoch 6[267/625] Time:0.703, Train Loss:0.29034289717674255\n",
      "Epoch 6[268/625] Time:0.702, Train Loss:0.25401100516319275\n",
      "Epoch 6[269/625] Time:0.702, Train Loss:0.2793973982334137\n",
      "Epoch 6[270/625] Time:0.702, Train Loss:0.28591835498809814\n",
      "Epoch 6[271/625] Time:0.702, Train Loss:0.24545514583587646\n",
      "Epoch 6[272/625] Time:0.707, Train Loss:0.24571728706359863\n",
      "Epoch 6[273/625] Time:0.703, Train Loss:0.24399413168430328\n",
      "Epoch 6[274/625] Time:0.702, Train Loss:0.21242693066596985\n",
      "Epoch 6[275/625] Time:0.702, Train Loss:0.39217397570610046\n",
      "Epoch 6[276/625] Time:0.704, Train Loss:0.29781782627105713\n",
      "Epoch 6[277/625] Time:0.703, Train Loss:0.340587854385376\n",
      "Epoch 6[278/625] Time:0.703, Train Loss:0.26154381036758423\n",
      "Epoch 6[279/625] Time:0.703, Train Loss:0.28231632709503174\n",
      "Epoch 6[280/625] Time:0.705, Train Loss:0.2762075364589691\n",
      "Epoch 6[281/625] Time:0.715, Train Loss:0.28272315859794617\n",
      "Epoch 6[282/625] Time:0.704, Train Loss:0.21502996981143951\n",
      "Epoch 6[283/625] Time:0.704, Train Loss:0.23091550171375275\n",
      "Epoch 6[284/625] Time:0.703, Train Loss:0.24123239517211914\n",
      "Epoch 6[285/625] Time:0.703, Train Loss:0.3782246708869934\n",
      "Epoch 6[286/625] Time:0.702, Train Loss:0.31030645966529846\n",
      "Epoch 6[287/625] Time:0.703, Train Loss:0.26006099581718445\n",
      "Epoch 6[288/625] Time:0.704, Train Loss:0.32160332798957825\n",
      "Epoch 6[289/625] Time:0.703, Train Loss:0.284294068813324\n",
      "Epoch 6[290/625] Time:0.702, Train Loss:0.23945632576942444\n",
      "Epoch 6[291/625] Time:0.703, Train Loss:0.2985633611679077\n",
      "Epoch 6[292/625] Time:0.703, Train Loss:0.3486815392971039\n",
      "Epoch 6[293/625] Time:0.702, Train Loss:0.263054758310318\n",
      "Epoch 6[294/625] Time:0.703, Train Loss:0.27297553420066833\n",
      "Epoch 6[295/625] Time:0.702, Train Loss:0.23801715672016144\n",
      "Epoch 6[296/625] Time:0.703, Train Loss:0.2688056230545044\n",
      "Epoch 6[297/625] Time:0.702, Train Loss:0.4897705316543579\n",
      "Epoch 6[298/625] Time:0.703, Train Loss:0.2565130889415741\n",
      "Epoch 6[299/625] Time:0.703, Train Loss:0.30266210436820984\n",
      "Epoch 6[300/625] Time:0.702, Train Loss:0.3678288459777832\n",
      "Epoch 6[301/625] Time:0.702, Train Loss:0.26039692759513855\n",
      "Epoch 6[302/625] Time:0.702, Train Loss:0.298456609249115\n",
      "Epoch 6[303/625] Time:0.702, Train Loss:0.2489861100912094\n",
      "Epoch 6[304/625] Time:0.702, Train Loss:0.3298438489437103\n",
      "Epoch 6[305/625] Time:0.72, Train Loss:0.2677241563796997\n",
      "Epoch 6[306/625] Time:0.694, Train Loss:0.23611976206302643\n",
      "Epoch 6[307/625] Time:0.696, Train Loss:0.21754056215286255\n",
      "Epoch 6[308/625] Time:0.694, Train Loss:0.20510385930538177\n",
      "Epoch 6[309/625] Time:0.695, Train Loss:0.27503716945648193\n",
      "Epoch 6[310/625] Time:0.696, Train Loss:0.25779226422309875\n",
      "Epoch 6[311/625] Time:0.694, Train Loss:0.28652751445770264\n",
      "Epoch 6[312/625] Time:0.695, Train Loss:0.26748865842819214\n",
      "Epoch 6[313/625] Time:0.693, Train Loss:0.2742801606655121\n",
      "Epoch 6[314/625] Time:0.702, Train Loss:0.36415693163871765\n",
      "Epoch 6[315/625] Time:0.704, Train Loss:0.2777532935142517\n",
      "Epoch 6[316/625] Time:0.703, Train Loss:0.2979757785797119\n",
      "Epoch 6[317/625] Time:0.704, Train Loss:0.2503642737865448\n",
      "Epoch 6[318/625] Time:0.703, Train Loss:0.2419823855161667\n",
      "Epoch 6[319/625] Time:0.703, Train Loss:0.26850515604019165\n",
      "Epoch 6[320/625] Time:0.744, Train Loss:0.2723913788795471\n",
      "Epoch 6[321/625] Time:0.694, Train Loss:0.2821982800960541\n",
      "Epoch 6[322/625] Time:0.703, Train Loss:0.3018640875816345\n",
      "Epoch 6[323/625] Time:0.71, Train Loss:0.22915537655353546\n",
      "Epoch 6[324/625] Time:0.694, Train Loss:0.2673152983188629\n",
      "Epoch 6[325/625] Time:0.705, Train Loss:0.3986874520778656\n",
      "Epoch 6[326/625] Time:0.737, Train Loss:0.23518671095371246\n",
      "Epoch 6[327/625] Time:0.704, Train Loss:0.31949493288993835\n",
      "Epoch 6[328/625] Time:0.697, Train Loss:0.29567623138427734\n",
      "Epoch 6[329/625] Time:0.695, Train Loss:0.2974441349506378\n",
      "Epoch 6[330/625] Time:0.702, Train Loss:0.31106868386268616\n",
      "Epoch 6[331/625] Time:0.703, Train Loss:0.257615327835083\n",
      "Epoch 6[332/625] Time:0.703, Train Loss:0.26353156566619873\n",
      "Epoch 6[333/625] Time:0.7, Train Loss:0.24075564742088318\n",
      "Epoch 6[334/625] Time:0.703, Train Loss:0.2629629671573639\n",
      "Epoch 6[335/625] Time:0.704, Train Loss:0.289011687040329\n",
      "Epoch 6[336/625] Time:0.694, Train Loss:0.20685610175132751\n",
      "Epoch 6[337/625] Time:0.693, Train Loss:0.346340537071228\n",
      "Epoch 6[338/625] Time:0.693, Train Loss:0.22804339230060577\n",
      "Epoch 6[339/625] Time:0.696, Train Loss:0.26385411620140076\n",
      "Epoch 6[340/625] Time:0.695, Train Loss:0.21035456657409668\n",
      "Epoch 6[341/625] Time:0.694, Train Loss:0.3613469898700714\n",
      "Epoch 6[342/625] Time:0.694, Train Loss:0.2763864994049072\n",
      "Epoch 6[343/625] Time:0.693, Train Loss:0.3989214301109314\n",
      "Epoch 6[344/625] Time:0.695, Train Loss:0.23148681223392487\n",
      "Epoch 6[345/625] Time:0.694, Train Loss:0.27927717566490173\n",
      "Epoch 6[346/625] Time:0.706, Train Loss:0.2675994336605072\n",
      "Epoch 6[347/625] Time:0.704, Train Loss:0.28381776809692383\n",
      "Epoch 6[348/625] Time:0.703, Train Loss:0.3593323230743408\n",
      "Epoch 6[349/625] Time:0.702, Train Loss:0.3354715406894684\n",
      "Epoch 6[350/625] Time:0.703, Train Loss:0.3113274574279785\n",
      "Epoch 6[351/625] Time:0.704, Train Loss:0.2862960696220398\n",
      "Epoch 6[352/625] Time:0.704, Train Loss:0.23384849727153778\n",
      "Epoch 6[353/625] Time:0.702, Train Loss:0.3164689838886261\n",
      "Epoch 6[354/625] Time:0.706, Train Loss:0.3444385826587677\n",
      "Epoch 6[355/625] Time:0.702, Train Loss:0.287664532661438\n",
      "Epoch 6[356/625] Time:0.703, Train Loss:0.27915358543395996\n",
      "Epoch 6[357/625] Time:0.705, Train Loss:0.4315597712993622\n",
      "Epoch 6[358/625] Time:0.705, Train Loss:0.28123536705970764\n",
      "Epoch 6[359/625] Time:0.705, Train Loss:0.264920711517334\n",
      "Epoch 6[360/625] Time:0.728, Train Loss:0.2620183825492859\n",
      "Epoch 6[361/625] Time:0.705, Train Loss:0.34368252754211426\n",
      "Epoch 6[362/625] Time:0.702, Train Loss:0.3842194974422455\n",
      "Epoch 6[363/625] Time:0.711, Train Loss:0.20333801209926605\n",
      "Epoch 6[364/625] Time:0.693, Train Loss:0.2489679455757141\n",
      "Epoch 6[365/625] Time:0.694, Train Loss:0.2546992599964142\n",
      "Epoch 6[366/625] Time:0.719, Train Loss:0.3054361939430237\n",
      "Epoch 6[367/625] Time:0.705, Train Loss:0.27622440457344055\n",
      "Epoch 6[368/625] Time:0.694, Train Loss:0.3705950975418091\n",
      "Epoch 6[369/625] Time:0.695, Train Loss:0.2554292678833008\n",
      "Epoch 6[370/625] Time:0.696, Train Loss:0.2923491299152374\n",
      "Epoch 6[371/625] Time:0.694, Train Loss:0.3117753267288208\n",
      "Epoch 6[372/625] Time:0.705, Train Loss:0.2879871428012848\n",
      "Epoch 6[373/625] Time:0.704, Train Loss:0.2934649586677551\n",
      "Epoch 6[374/625] Time:0.705, Train Loss:0.34994155168533325\n",
      "Epoch 6[375/625] Time:0.704, Train Loss:0.1931212693452835\n",
      "Epoch 6[376/625] Time:0.705, Train Loss:0.2670173645019531\n",
      "Epoch 6[377/625] Time:0.706, Train Loss:0.2976183295249939\n",
      "Epoch 6[378/625] Time:0.734, Train Loss:0.2705211937427521\n",
      "Epoch 6[379/625] Time:0.692, Train Loss:0.33535605669021606\n",
      "Epoch 6[380/625] Time:0.702, Train Loss:0.2693966031074524\n",
      "Epoch 6[381/625] Time:0.729, Train Loss:0.30629584193229675\n",
      "Epoch 6[382/625] Time:0.691, Train Loss:0.3438403308391571\n",
      "Epoch 6[383/625] Time:0.693, Train Loss:0.2961888909339905\n",
      "Epoch 6[384/625] Time:0.7, Train Loss:0.271260142326355\n",
      "Epoch 6[385/625] Time:0.701, Train Loss:0.22258830070495605\n",
      "Epoch 6[386/625] Time:0.701, Train Loss:0.28180235624313354\n",
      "Epoch 6[387/625] Time:0.704, Train Loss:0.23260948061943054\n",
      "Epoch 6[388/625] Time:0.738, Train Loss:0.2968820631504059\n",
      "Epoch 6[389/625] Time:0.692, Train Loss:0.3372170925140381\n",
      "Epoch 6[390/625] Time:0.694, Train Loss:0.2253301739692688\n",
      "Epoch 6[391/625] Time:0.694, Train Loss:0.30354663729667664\n",
      "Epoch 6[392/625] Time:0.693, Train Loss:0.2165592759847641\n",
      "Epoch 6[393/625] Time:0.692, Train Loss:0.2245941013097763\n",
      "Epoch 6[394/625] Time:0.694, Train Loss:0.31920674443244934\n",
      "Epoch 6[395/625] Time:0.694, Train Loss:0.35089635848999023\n",
      "Epoch 6[396/625] Time:0.703, Train Loss:0.2740710973739624\n",
      "Epoch 6[397/625] Time:0.702, Train Loss:0.339682012796402\n",
      "Epoch 6[398/625] Time:0.702, Train Loss:0.2657877802848816\n",
      "Epoch 6[399/625] Time:0.702, Train Loss:0.2938687801361084\n",
      "Epoch 6[400/625] Time:0.702, Train Loss:0.28097835183143616\n",
      "Epoch 6[401/625] Time:0.704, Train Loss:0.29427045583724976\n",
      "Epoch 6[402/625] Time:0.705, Train Loss:0.2542307376861572\n",
      "Epoch 6[403/625] Time:0.703, Train Loss:0.3012181222438812\n",
      "Epoch 6[404/625] Time:0.707, Train Loss:0.23285825550556183\n",
      "Epoch 6[405/625] Time:0.703, Train Loss:0.2612262964248657\n",
      "Epoch 6[406/625] Time:0.703, Train Loss:0.23226970434188843\n",
      "Epoch 6[407/625] Time:0.702, Train Loss:0.2893839180469513\n",
      "Epoch 6[408/625] Time:0.703, Train Loss:0.27964848279953003\n",
      "Epoch 6[409/625] Time:0.703, Train Loss:0.33314424753189087\n",
      "Epoch 6[410/625] Time:0.704, Train Loss:0.3087213635444641\n",
      "Epoch 6[411/625] Time:0.702, Train Loss:0.3755423426628113\n",
      "Epoch 6[412/625] Time:0.703, Train Loss:0.28984376788139343\n",
      "Epoch 6[413/625] Time:0.702, Train Loss:0.27504247426986694\n",
      "Epoch 6[414/625] Time:0.703, Train Loss:0.30343571305274963\n",
      "Epoch 6[415/625] Time:0.703, Train Loss:0.2651631832122803\n",
      "Epoch 6[416/625] Time:0.704, Train Loss:0.3183436095714569\n",
      "Epoch 6[417/625] Time:0.705, Train Loss:0.24046114087104797\n",
      "Epoch 6[418/625] Time:0.704, Train Loss:0.2111247181892395\n",
      "Epoch 6[419/625] Time:0.703, Train Loss:0.3248167335987091\n",
      "Epoch 6[420/625] Time:0.705, Train Loss:0.3870168924331665\n",
      "Epoch 6[421/625] Time:0.703, Train Loss:0.2809496819972992\n",
      "Epoch 6[422/625] Time:0.702, Train Loss:0.2780260443687439\n",
      "Epoch 6[423/625] Time:0.744, Train Loss:0.22274711728096008\n",
      "Epoch 6[424/625] Time:0.694, Train Loss:0.3868125081062317\n",
      "Epoch 6[425/625] Time:0.694, Train Loss:0.34347274899482727\n",
      "Epoch 6[426/625] Time:0.697, Train Loss:0.27039089798927307\n",
      "Epoch 6[427/625] Time:0.703, Train Loss:0.31438544392585754\n",
      "Epoch 6[428/625] Time:0.705, Train Loss:0.24981796741485596\n",
      "Epoch 6[429/625] Time:0.703, Train Loss:0.32839834690093994\n",
      "Epoch 6[430/625] Time:0.704, Train Loss:0.30397605895996094\n",
      "Epoch 6[431/625] Time:0.713, Train Loss:0.31783437728881836\n",
      "Epoch 6[432/625] Time:0.703, Train Loss:0.25337085127830505\n",
      "Epoch 6[433/625] Time:0.705, Train Loss:0.32134726643562317\n",
      "Epoch 6[434/625] Time:0.704, Train Loss:0.3546872138977051\n",
      "Epoch 6[435/625] Time:0.703, Train Loss:0.27377429604530334\n",
      "Epoch 6[436/625] Time:0.734, Train Loss:0.2909488081932068\n",
      "Epoch 6[437/625] Time:0.695, Train Loss:0.3781317174434662\n",
      "Epoch 6[438/625] Time:0.692, Train Loss:0.3391432762145996\n",
      "Epoch 6[439/625] Time:0.693, Train Loss:0.28766971826553345\n",
      "Epoch 6[440/625] Time:0.705, Train Loss:0.24576014280319214\n",
      "Epoch 6[441/625] Time:0.705, Train Loss:0.27274826169013977\n",
      "Epoch 6[442/625] Time:0.703, Train Loss:0.29988616704940796\n",
      "Epoch 6[443/625] Time:0.705, Train Loss:0.3548920452594757\n",
      "Epoch 6[444/625] Time:0.703, Train Loss:0.2991766631603241\n",
      "Epoch 6[445/625] Time:0.7, Train Loss:0.28683531284332275\n",
      "Epoch 6[446/625] Time:0.702, Train Loss:0.22997818887233734\n",
      "Epoch 6[447/625] Time:0.702, Train Loss:0.295505553483963\n",
      "Epoch 6[448/625] Time:0.702, Train Loss:0.29786279797554016\n",
      "Epoch 6[449/625] Time:0.702, Train Loss:0.26210376620292664\n",
      "Epoch 6[450/625] Time:0.743, Train Loss:0.34648898243904114\n",
      "Epoch 6[451/625] Time:0.694, Train Loss:0.3017761707305908\n",
      "Epoch 6[452/625] Time:0.695, Train Loss:0.2690052092075348\n",
      "Epoch 6[453/625] Time:0.694, Train Loss:0.33786505460739136\n",
      "Epoch 6[454/625] Time:0.694, Train Loss:0.3344842791557312\n",
      "Epoch 6[455/625] Time:0.695, Train Loss:0.3112410306930542\n",
      "Epoch 6[456/625] Time:0.732, Train Loss:0.2655446529388428\n",
      "Epoch 6[457/625] Time:0.704, Train Loss:0.3386854827404022\n",
      "Epoch 6[458/625] Time:0.704, Train Loss:0.2969508171081543\n",
      "Epoch 6[459/625] Time:0.71, Train Loss:0.2570390999317169\n",
      "Epoch 6[460/625] Time:0.724, Train Loss:0.22098258137702942\n",
      "Epoch 6[461/625] Time:0.706, Train Loss:0.21892742812633514\n",
      "Epoch 6[462/625] Time:0.703, Train Loss:0.3279922604560852\n",
      "Epoch 6[463/625] Time:0.704, Train Loss:0.35966548323631287\n",
      "Epoch 6[464/625] Time:0.702, Train Loss:0.2775919437408447\n",
      "Epoch 6[465/625] Time:0.703, Train Loss:0.24606284499168396\n",
      "Epoch 6[466/625] Time:0.703, Train Loss:0.26173993945121765\n",
      "Epoch 6[467/625] Time:0.703, Train Loss:0.42281466722488403\n",
      "Epoch 6[468/625] Time:0.703, Train Loss:0.2159455418586731\n",
      "Epoch 6[469/625] Time:0.702, Train Loss:0.2961748242378235\n",
      "Epoch 6[470/625] Time:0.703, Train Loss:0.2428731769323349\n",
      "Epoch 6[471/625] Time:0.703, Train Loss:0.39359548687934875\n",
      "Epoch 6[472/625] Time:0.703, Train Loss:0.316835880279541\n",
      "Epoch 6[473/625] Time:0.704, Train Loss:0.3129381537437439\n",
      "Epoch 6[474/625] Time:0.703, Train Loss:0.30401214957237244\n",
      "Epoch 6[475/625] Time:0.703, Train Loss:0.21052660048007965\n",
      "Epoch 6[476/625] Time:0.71, Train Loss:0.34594619274139404\n",
      "Epoch 6[477/625] Time:0.704, Train Loss:0.29147958755493164\n",
      "Epoch 6[478/625] Time:0.702, Train Loss:0.2621957063674927\n",
      "Epoch 6[479/625] Time:0.703, Train Loss:0.3001798391342163\n",
      "Epoch 6[480/625] Time:0.704, Train Loss:0.3342300057411194\n",
      "Epoch 6[481/625] Time:0.703, Train Loss:0.2830880880355835\n",
      "Epoch 6[482/625] Time:0.703, Train Loss:0.34267550706863403\n",
      "Epoch 6[483/625] Time:0.704, Train Loss:0.24356122314929962\n",
      "Epoch 6[484/625] Time:0.705, Train Loss:0.3899286389350891\n",
      "Epoch 6[485/625] Time:0.704, Train Loss:0.31699272990226746\n",
      "Epoch 6[486/625] Time:0.703, Train Loss:0.2830182909965515\n",
      "Epoch 6[487/625] Time:0.703, Train Loss:0.35387542843818665\n",
      "Epoch 6[488/625] Time:0.705, Train Loss:0.3302687704563141\n",
      "Epoch 6[489/625] Time:0.703, Train Loss:0.260320782661438\n",
      "Epoch 6[490/625] Time:0.709, Train Loss:0.2975078523159027\n",
      "Epoch 6[491/625] Time:0.703, Train Loss:0.3276318609714508\n",
      "Epoch 6[492/625] Time:0.702, Train Loss:0.2619984745979309\n",
      "Epoch 6[493/625] Time:0.704, Train Loss:0.31443411111831665\n",
      "Epoch 6[494/625] Time:0.701, Train Loss:0.2954351305961609\n",
      "Epoch 6[495/625] Time:0.703, Train Loss:0.32540374994277954\n",
      "Epoch 6[496/625] Time:0.702, Train Loss:0.28705063462257385\n",
      "Epoch 6[497/625] Time:0.702, Train Loss:0.34620481729507446\n",
      "Epoch 6[498/625] Time:0.703, Train Loss:0.3151473104953766\n",
      "Epoch 6[499/625] Time:0.703, Train Loss:0.2665479779243469\n",
      "Epoch 6[500/625] Time:0.702, Train Loss:0.29839184880256653\n",
      "Epoch 6[501/625] Time:0.703, Train Loss:0.3234820067882538\n",
      "Epoch 6[502/625] Time:0.703, Train Loss:0.20667897164821625\n",
      "Epoch 6[503/625] Time:0.703, Train Loss:0.22737044095993042\n",
      "Epoch 6[504/625] Time:0.704, Train Loss:0.2715277671813965\n",
      "Epoch 6[505/625] Time:0.705, Train Loss:0.24159958958625793\n",
      "Epoch 6[506/625] Time:0.703, Train Loss:0.3143903315067291\n",
      "Epoch 6[507/625] Time:0.703, Train Loss:0.3532012104988098\n",
      "Epoch 6[508/625] Time:0.721, Train Loss:0.34070226550102234\n",
      "Epoch 6[509/625] Time:0.693, Train Loss:0.2671808898448944\n",
      "Epoch 6[510/625] Time:0.702, Train Loss:0.25460484623908997\n",
      "Epoch 6[511/625] Time:0.702, Train Loss:0.2925543189048767\n",
      "Epoch 6[512/625] Time:0.703, Train Loss:0.28170594573020935\n",
      "Epoch 6[513/625] Time:0.704, Train Loss:0.3663446605205536\n",
      "Epoch 6[514/625] Time:0.702, Train Loss:0.37229737639427185\n",
      "Epoch 6[515/625] Time:0.702, Train Loss:0.29092085361480713\n",
      "Epoch 6[516/625] Time:0.738, Train Loss:0.23793227970600128\n",
      "Epoch 6[517/625] Time:0.694, Train Loss:0.26660940051078796\n",
      "Epoch 6[518/625] Time:0.694, Train Loss:0.33228012919425964\n",
      "Epoch 6[519/625] Time:0.694, Train Loss:0.2594420909881592\n",
      "Epoch 6[520/625] Time:0.694, Train Loss:0.2875961363315582\n",
      "Epoch 6[521/625] Time:0.701, Train Loss:0.23572643101215363\n",
      "Epoch 6[522/625] Time:0.694, Train Loss:0.2686757743358612\n",
      "Epoch 6[523/625] Time:0.693, Train Loss:0.2244681864976883\n",
      "Epoch 6[524/625] Time:0.703, Train Loss:0.3382547199726105\n",
      "Epoch 6[525/625] Time:0.696, Train Loss:0.24040627479553223\n",
      "Epoch 6[526/625] Time:0.706, Train Loss:0.23686003684997559\n",
      "Epoch 6[527/625] Time:0.707, Train Loss:0.22884483635425568\n",
      "Epoch 6[528/625] Time:0.707, Train Loss:0.2743302881717682\n",
      "Epoch 6[529/625] Time:0.702, Train Loss:0.303415447473526\n",
      "Epoch 6[530/625] Time:0.703, Train Loss:0.26956605911254883\n",
      "Epoch 6[531/625] Time:0.706, Train Loss:0.30475687980651855\n",
      "Epoch 6[532/625] Time:0.696, Train Loss:0.30286070704460144\n",
      "Epoch 6[533/625] Time:0.695, Train Loss:0.27240580320358276\n",
      "Epoch 6[534/625] Time:0.695, Train Loss:0.26847776770591736\n",
      "Epoch 6[535/625] Time:0.694, Train Loss:0.29274237155914307\n",
      "Epoch 6[536/625] Time:0.695, Train Loss:0.24452222883701324\n",
      "Epoch 6[537/625] Time:0.705, Train Loss:0.26394128799438477\n",
      "Epoch 6[538/625] Time:0.703, Train Loss:0.3019932210445404\n",
      "Epoch 6[539/625] Time:0.702, Train Loss:0.2622913122177124\n",
      "Epoch 6[540/625] Time:0.707, Train Loss:0.332771897315979\n",
      "Epoch 6[541/625] Time:0.704, Train Loss:0.2845098376274109\n",
      "Epoch 6[542/625] Time:0.704, Train Loss:0.2642380893230438\n",
      "Epoch 6[543/625] Time:0.704, Train Loss:0.2926694452762604\n",
      "Epoch 6[544/625] Time:0.703, Train Loss:0.3388117849826813\n",
      "Epoch 6[545/625] Time:0.716, Train Loss:0.29251614212989807\n",
      "Epoch 6[546/625] Time:0.704, Train Loss:0.3972525894641876\n",
      "Epoch 6[547/625] Time:0.706, Train Loss:0.2776435315608978\n",
      "Epoch 6[548/625] Time:0.705, Train Loss:0.261857807636261\n",
      "Epoch 6[549/625] Time:0.704, Train Loss:0.22314724326133728\n",
      "Epoch 6[550/625] Time:0.705, Train Loss:0.37115514278411865\n",
      "Epoch 6[551/625] Time:0.709, Train Loss:0.2958354949951172\n",
      "Epoch 6[552/625] Time:0.704, Train Loss:0.2592630684375763\n",
      "Epoch 6[553/625] Time:0.708, Train Loss:0.24286605417728424\n",
      "Epoch 6[554/625] Time:0.712, Train Loss:0.3525617718696594\n",
      "Epoch 6[555/625] Time:0.703, Train Loss:0.33238768577575684\n",
      "Epoch 6[556/625] Time:0.704, Train Loss:0.2925399839878082\n",
      "Epoch 6[557/625] Time:0.702, Train Loss:0.27618831396102905\n",
      "Epoch 6[558/625] Time:0.703, Train Loss:0.2956356704235077\n",
      "Epoch 6[559/625] Time:0.707, Train Loss:0.39925020933151245\n",
      "Epoch 6[560/625] Time:0.703, Train Loss:0.3310270607471466\n",
      "Epoch 6[561/625] Time:0.703, Train Loss:0.2955140471458435\n",
      "Epoch 6[562/625] Time:0.704, Train Loss:0.24570968747138977\n",
      "Epoch 6[563/625] Time:0.703, Train Loss:0.2558347284793854\n",
      "Epoch 6[564/625] Time:0.734, Train Loss:0.33414286375045776\n",
      "Epoch 6[565/625] Time:0.694, Train Loss:0.29098325967788696\n",
      "Epoch 6[566/625] Time:0.695, Train Loss:0.26075345277786255\n",
      "Epoch 6[567/625] Time:0.694, Train Loss:0.28072527050971985\n",
      "Epoch 6[568/625] Time:0.694, Train Loss:0.22593159973621368\n",
      "Epoch 6[569/625] Time:0.694, Train Loss:0.2487117201089859\n",
      "Epoch 6[570/625] Time:0.694, Train Loss:0.22855743765830994\n",
      "Epoch 6[571/625] Time:0.694, Train Loss:0.21657776832580566\n",
      "Epoch 6[572/625] Time:0.698, Train Loss:0.40277400612831116\n",
      "Epoch 6[573/625] Time:0.697, Train Loss:0.3297426104545593\n",
      "Epoch 6[574/625] Time:0.702, Train Loss:0.2690253257751465\n",
      "Epoch 6[575/625] Time:0.703, Train Loss:0.2811742424964905\n",
      "Epoch 6[576/625] Time:0.703, Train Loss:0.2639293670654297\n",
      "Epoch 6[577/625] Time:0.703, Train Loss:0.2968675196170807\n",
      "Epoch 6[578/625] Time:0.703, Train Loss:0.23701894283294678\n",
      "Epoch 6[579/625] Time:0.703, Train Loss:0.27502402663230896\n",
      "Epoch 6[580/625] Time:0.737, Train Loss:0.2380252480506897\n",
      "Epoch 6[581/625] Time:0.693, Train Loss:0.39137616753578186\n",
      "Epoch 6[582/625] Time:0.703, Train Loss:0.2367520034313202\n",
      "Epoch 6[583/625] Time:0.703, Train Loss:0.28005099296569824\n",
      "Epoch 6[584/625] Time:0.703, Train Loss:0.2587541341781616\n",
      "Epoch 6[585/625] Time:0.703, Train Loss:0.30271852016448975\n",
      "Epoch 6[586/625] Time:0.702, Train Loss:0.1682673990726471\n",
      "Epoch 6[587/625] Time:0.702, Train Loss:0.2570081949234009\n",
      "Epoch 6[588/625] Time:0.703, Train Loss:0.27606260776519775\n",
      "Epoch 6[589/625] Time:0.704, Train Loss:0.3509085178375244\n",
      "Epoch 6[590/625] Time:0.708, Train Loss:0.3325446546077728\n",
      "Epoch 6[591/625] Time:0.702, Train Loss:0.3705364167690277\n",
      "Epoch 6[592/625] Time:0.733, Train Loss:0.36450648307800293\n",
      "Epoch 6[593/625] Time:0.693, Train Loss:0.34228089451789856\n",
      "Epoch 6[594/625] Time:0.701, Train Loss:0.26843690872192383\n",
      "Epoch 6[595/625] Time:0.703, Train Loss:0.28107208013534546\n",
      "Epoch 6[596/625] Time:0.702, Train Loss:0.32078883051872253\n",
      "Epoch 6[597/625] Time:0.704, Train Loss:0.27097874879837036\n",
      "Epoch 6[598/625] Time:0.702, Train Loss:0.23371195793151855\n",
      "Epoch 6[599/625] Time:0.704, Train Loss:0.30648645758628845\n",
      "Epoch 6[600/625] Time:0.702, Train Loss:0.2576952874660492\n",
      "Epoch 6[601/625] Time:0.704, Train Loss:0.33183473348617554\n",
      "Epoch 6[602/625] Time:0.705, Train Loss:0.36762988567352295\n",
      "Epoch 6[603/625] Time:0.704, Train Loss:0.24267613887786865\n",
      "Epoch 6[604/625] Time:0.704, Train Loss:0.2330118715763092\n",
      "Epoch 6[605/625] Time:0.704, Train Loss:0.2803543210029602\n",
      "Epoch 6[606/625] Time:0.703, Train Loss:0.3736525774002075\n",
      "Epoch 6[607/625] Time:0.704, Train Loss:0.2698765993118286\n",
      "Epoch 6[608/625] Time:0.703, Train Loss:0.40230274200439453\n",
      "Epoch 6[609/625] Time:0.704, Train Loss:0.44252124428749084\n",
      "Epoch 6[610/625] Time:0.704, Train Loss:0.3029356002807617\n",
      "Epoch 6[611/625] Time:0.704, Train Loss:0.27320602536201477\n",
      "Epoch 6[612/625] Time:0.704, Train Loss:0.2197079360485077\n",
      "Epoch 6[613/625] Time:0.705, Train Loss:0.31381964683532715\n",
      "Epoch 6[614/625] Time:0.703, Train Loss:0.2907879650592804\n",
      "Epoch 6[615/625] Time:0.703, Train Loss:0.24330618977546692\n",
      "Epoch 6[616/625] Time:0.703, Train Loss:0.2712710201740265\n",
      "Epoch 6[617/625] Time:0.702, Train Loss:0.2393304854631424\n",
      "Epoch 6[618/625] Time:0.708, Train Loss:0.2175954431295395\n",
      "Epoch 6[619/625] Time:0.695, Train Loss:0.3261655867099762\n",
      "Epoch 6[620/625] Time:0.694, Train Loss:0.2927919030189514\n",
      "Epoch 6[621/625] Time:0.744, Train Loss:0.23884855210781097\n",
      "Epoch 6[622/625] Time:0.692, Train Loss:0.2632686197757721\n",
      "Epoch 6[623/625] Time:0.702, Train Loss:0.27699291706085205\n",
      "Epoch 6[624/625] Time:0.706, Train Loss:0.20882776379585266\n",
      "Epoch 6[0/78] Val Loss:0.2813718318939209\n",
      "Epoch 6[1/78] Val Loss:0.2623317837715149\n",
      "Epoch 6[2/78] Val Loss:0.28094911575317383\n",
      "Epoch 6[3/78] Val Loss:0.3067624270915985\n",
      "Epoch 6[4/78] Val Loss:0.48052969574928284\n",
      "Epoch 6[5/78] Val Loss:0.49543213844299316\n",
      "Epoch 6[6/78] Val Loss:0.43278732895851135\n",
      "Epoch 6[7/78] Val Loss:0.5822106599807739\n",
      "Epoch 6[8/78] Val Loss:0.2889351546764374\n",
      "Epoch 6[9/78] Val Loss:0.15534546971321106\n",
      "Epoch 6[10/78] Val Loss:0.11416416615247726\n",
      "Epoch 6[11/78] Val Loss:0.1680547446012497\n",
      "Epoch 6[12/78] Val Loss:0.11815724521875381\n",
      "Epoch 6[13/78] Val Loss:0.1007249653339386\n",
      "Epoch 6[14/78] Val Loss:0.30390694737434387\n",
      "Epoch 6[15/78] Val Loss:0.2634059488773346\n",
      "Epoch 6[16/78] Val Loss:0.2869550287723541\n",
      "Epoch 6[17/78] Val Loss:0.2524840831756592\n",
      "Epoch 6[18/78] Val Loss:0.5461212992668152\n",
      "Epoch 6[19/78] Val Loss:0.661994993686676\n",
      "Epoch 6[20/78] Val Loss:0.47957485914230347\n",
      "Epoch 6[21/78] Val Loss:0.6408272385597229\n",
      "Epoch 6[22/78] Val Loss:0.8448715209960938\n",
      "Epoch 6[23/78] Val Loss:0.5674516558647156\n",
      "Epoch 6[24/78] Val Loss:0.41827958822250366\n",
      "Epoch 6[25/78] Val Loss:0.5537347793579102\n",
      "Epoch 6[26/78] Val Loss:0.5431306958198547\n",
      "Epoch 6[27/78] Val Loss:0.47139641642570496\n",
      "Epoch 6[28/78] Val Loss:0.44709497690200806\n",
      "Epoch 6[29/78] Val Loss:0.5359792113304138\n",
      "Epoch 6[30/78] Val Loss:1.4639040231704712\n",
      "Epoch 6[31/78] Val Loss:1.2699521780014038\n",
      "Epoch 6[32/78] Val Loss:1.2025197744369507\n",
      "Epoch 6[33/78] Val Loss:0.6891230344772339\n",
      "Epoch 6[34/78] Val Loss:0.5776746869087219\n",
      "Epoch 6[35/78] Val Loss:0.5145349502563477\n",
      "Epoch 6[36/78] Val Loss:0.5699134469032288\n",
      "Epoch 6[37/78] Val Loss:0.6299717426300049\n",
      "Epoch 6[38/78] Val Loss:0.2737399637699127\n",
      "Epoch 6[39/78] Val Loss:0.29526153206825256\n",
      "Epoch 6[40/78] Val Loss:0.2942778170108795\n",
      "Epoch 6[41/78] Val Loss:0.29629671573638916\n",
      "Epoch 6[42/78] Val Loss:0.3204194903373718\n",
      "Epoch 6[43/78] Val Loss:0.19676803052425385\n",
      "Epoch 6[44/78] Val Loss:0.11813975125551224\n",
      "Epoch 6[45/78] Val Loss:0.0924772098660469\n",
      "Epoch 6[46/78] Val Loss:0.09224627912044525\n",
      "Epoch 6[47/78] Val Loss:0.1257079690694809\n",
      "Epoch 6[48/78] Val Loss:0.13556258380413055\n",
      "Epoch 6[49/78] Val Loss:0.0807122066617012\n",
      "Epoch 6[50/78] Val Loss:0.10715041309595108\n",
      "Epoch 6[51/78] Val Loss:0.11149939149618149\n",
      "Epoch 6[52/78] Val Loss:0.14052274823188782\n",
      "Epoch 6[53/78] Val Loss:0.08178780227899551\n",
      "Epoch 6[54/78] Val Loss:0.10224846750497818\n",
      "Epoch 6[55/78] Val Loss:0.14047707617282867\n",
      "Epoch 6[56/78] Val Loss:0.5776082873344421\n",
      "Epoch 6[57/78] Val Loss:0.5291848182678223\n",
      "Epoch 6[58/78] Val Loss:0.49432191252708435\n",
      "Epoch 6[59/78] Val Loss:0.37475261092185974\n",
      "Epoch 6[60/78] Val Loss:0.34847623109817505\n",
      "Epoch 6[61/78] Val Loss:0.41640380024909973\n",
      "Epoch 6[62/78] Val Loss:0.46679049730300903\n",
      "Epoch 6[63/78] Val Loss:0.31012052297592163\n",
      "Epoch 6[64/78] Val Loss:0.2868832051753998\n",
      "Epoch 6[65/78] Val Loss:0.26178696751594543\n",
      "Epoch 6[66/78] Val Loss:0.35374903678894043\n",
      "Epoch 6[67/78] Val Loss:0.28619179129600525\n",
      "Epoch 6[68/78] Val Loss:0.7804310321807861\n",
      "Epoch 6[69/78] Val Loss:0.7750627994537354\n",
      "Epoch 6[70/78] Val Loss:0.7555783987045288\n",
      "Epoch 6[71/78] Val Loss:0.6075241565704346\n",
      "Epoch 6[72/78] Val Loss:0.3105463683605194\n",
      "Epoch 6[73/78] Val Loss:0.23932285606861115\n",
      "Epoch 6[74/78] Val Loss:0.7699600458145142\n",
      "Epoch 6[75/78] Val Loss:0.9025406837463379\n",
      "Epoch 6[76/78] Val Loss:0.8934319615364075\n",
      "Epoch 6[77/78] Val Loss:0.975156843662262\n",
      "Epoch 6[78/78] Val Loss:1.1038355827331543\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.81      0.88     15691\n",
      "           1       0.56      0.86      0.68      4309\n",
      "\n",
      "    accuracy                           0.82     20000\n",
      "   macro avg       0.76      0.84      0.78     20000\n",
      "weighted avg       0.87      0.82      0.83     20000\n",
      "\n",
      "Epoch 6: Train Loss 0.28937825334072115, Val Loss 0.44394250854085654, Train Time 803.4025421142578, Val Time 37.09890341758728\n",
      "Epoch 7[0/625] Time:0.688, Train Loss:0.2635264992713928\n",
      "Epoch 7[1/625] Time:0.703, Train Loss:0.3960348665714264\n",
      "Epoch 7[2/625] Time:0.702, Train Loss:0.2880098521709442\n",
      "Epoch 7[3/625] Time:0.702, Train Loss:0.2739311158657074\n",
      "Epoch 7[4/625] Time:0.702, Train Loss:0.3317873477935791\n",
      "Epoch 7[5/625] Time:0.703, Train Loss:0.2885664105415344\n",
      "Epoch 7[6/625] Time:0.701, Train Loss:0.24002623558044434\n",
      "Epoch 7[7/625] Time:0.703, Train Loss:0.3403027355670929\n",
      "Epoch 7[8/625] Time:0.703, Train Loss:0.35966476798057556\n",
      "Epoch 7[9/625] Time:0.703, Train Loss:0.27269893884658813\n",
      "Epoch 7[10/625] Time:0.702, Train Loss:0.30498361587524414\n",
      "Epoch 7[11/625] Time:0.702, Train Loss:0.22244277596473694\n",
      "Epoch 7[12/625] Time:0.703, Train Loss:0.2912540137767792\n",
      "Epoch 7[13/625] Time:0.699, Train Loss:0.31676387786865234\n",
      "Epoch 7[14/625] Time:0.702, Train Loss:0.31762516498565674\n",
      "Epoch 7[15/625] Time:0.704, Train Loss:0.2242184728384018\n",
      "Epoch 7[16/625] Time:0.702, Train Loss:0.33064979314804077\n",
      "Epoch 7[17/625] Time:0.703, Train Loss:0.27815690636634827\n",
      "Epoch 7[18/625] Time:0.702, Train Loss:0.26468831300735474\n",
      "Epoch 7[19/625] Time:0.703, Train Loss:0.25815409421920776\n",
      "Epoch 7[20/625] Time:0.703, Train Loss:0.29329949617385864\n",
      "Epoch 7[21/625] Time:0.704, Train Loss:0.36983636021614075\n",
      "Epoch 7[22/625] Time:0.704, Train Loss:0.27799972891807556\n",
      "Epoch 7[23/625] Time:0.703, Train Loss:0.2030566781759262\n",
      "Epoch 7[24/625] Time:0.702, Train Loss:0.3117899000644684\n",
      "Epoch 7[25/625] Time:0.702, Train Loss:0.2972852885723114\n",
      "Epoch 7[26/625] Time:0.702, Train Loss:0.21373715996742249\n",
      "Epoch 7[27/625] Time:0.703, Train Loss:0.25656241178512573\n",
      "Epoch 7[28/625] Time:0.703, Train Loss:0.2980058193206787\n",
      "Epoch 7[29/625] Time:0.705, Train Loss:0.2585892677307129\n",
      "Epoch 7[30/625] Time:0.704, Train Loss:0.19529989361763\n",
      "Epoch 7[31/625] Time:0.706, Train Loss:0.23939219117164612\n",
      "Epoch 7[32/625] Time:0.72, Train Loss:0.2843620479106903\n",
      "Epoch 7[33/625] Time:0.72, Train Loss:0.279615193605423\n",
      "Epoch 7[34/625] Time:0.705, Train Loss:0.2516429126262665\n",
      "Epoch 7[35/625] Time:0.747, Train Loss:0.2712482511997223\n",
      "Epoch 7[36/625] Time:0.692, Train Loss:0.34923040866851807\n",
      "Epoch 7[37/625] Time:0.694, Train Loss:0.2296023964881897\n",
      "Epoch 7[38/625] Time:0.695, Train Loss:0.28668344020843506\n",
      "Epoch 7[39/625] Time:0.731, Train Loss:0.35645031929016113\n",
      "Epoch 7[40/625] Time:0.702, Train Loss:0.29797419905662537\n",
      "Epoch 7[41/625] Time:0.703, Train Loss:0.2318955808877945\n",
      "Epoch 7[42/625] Time:0.704, Train Loss:0.37707749009132385\n",
      "Epoch 7[43/625] Time:0.704, Train Loss:0.2663303315639496\n",
      "Epoch 7[44/625] Time:0.703, Train Loss:0.23859915137290955\n",
      "Epoch 7[45/625] Time:0.702, Train Loss:0.3006029427051544\n",
      "Epoch 7[46/625] Time:0.703, Train Loss:0.25048691034317017\n",
      "Epoch 7[47/625] Time:0.702, Train Loss:0.3038063943386078\n",
      "Epoch 7[48/625] Time:0.703, Train Loss:0.3139859735965729\n",
      "Epoch 7[49/625] Time:0.702, Train Loss:0.29947590827941895\n",
      "Epoch 7[50/625] Time:0.701, Train Loss:0.24536339938640594\n",
      "Epoch 7[51/625] Time:0.702, Train Loss:0.30914488434791565\n",
      "Epoch 7[52/625] Time:0.703, Train Loss:0.27864205837249756\n",
      "Epoch 7[53/625] Time:0.703, Train Loss:0.28953614830970764\n",
      "Epoch 7[54/625] Time:0.704, Train Loss:0.2472880333662033\n",
      "Epoch 7[55/625] Time:0.703, Train Loss:0.37425708770751953\n",
      "Epoch 7[56/625] Time:0.702, Train Loss:0.26022687554359436\n",
      "Epoch 7[57/625] Time:0.703, Train Loss:0.26215723156929016\n",
      "Epoch 7[58/625] Time:0.706, Train Loss:0.26983776688575745\n",
      "Epoch 7[59/625] Time:0.705, Train Loss:0.26551562547683716\n",
      "Epoch 7[60/625] Time:0.703, Train Loss:0.23505845665931702\n",
      "Epoch 7[61/625] Time:0.705, Train Loss:0.2680128216743469\n",
      "Epoch 7[62/625] Time:0.703, Train Loss:0.27679118514060974\n",
      "Epoch 7[63/625] Time:0.703, Train Loss:0.24413570761680603\n",
      "Epoch 7[64/625] Time:0.703, Train Loss:0.25752004981040955\n",
      "Epoch 7[65/625] Time:0.703, Train Loss:0.32860273122787476\n",
      "Epoch 7[66/625] Time:0.703, Train Loss:0.2773456275463104\n",
      "Epoch 7[67/625] Time:0.703, Train Loss:0.22595369815826416\n",
      "Epoch 7[68/625] Time:0.702, Train Loss:0.2923310697078705\n",
      "Epoch 7[69/625] Time:0.702, Train Loss:0.25381413102149963\n",
      "Epoch 7[70/625] Time:0.703, Train Loss:0.2251942902803421\n",
      "Epoch 7[71/625] Time:0.704, Train Loss:0.2542377710342407\n",
      "Epoch 7[72/625] Time:0.703, Train Loss:0.27360546588897705\n",
      "Epoch 7[73/625] Time:0.703, Train Loss:0.24872055649757385\n",
      "Epoch 7[74/625] Time:0.702, Train Loss:0.16791129112243652\n",
      "Epoch 7[75/625] Time:0.702, Train Loss:0.28992047905921936\n",
      "Epoch 7[76/625] Time:0.703, Train Loss:0.4069005846977234\n",
      "Epoch 7[77/625] Time:0.703, Train Loss:0.30542492866516113\n",
      "Epoch 7[78/625] Time:0.703, Train Loss:0.27004700899124146\n",
      "Epoch 7[79/625] Time:0.702, Train Loss:0.2545846402645111\n",
      "Epoch 7[80/625] Time:0.702, Train Loss:0.23018337786197662\n",
      "Epoch 7[81/625] Time:0.704, Train Loss:0.3863324224948883\n",
      "Epoch 7[82/625] Time:0.703, Train Loss:0.34092286229133606\n",
      "Epoch 7[83/625] Time:0.704, Train Loss:0.28905272483825684\n",
      "Epoch 7[84/625] Time:0.703, Train Loss:0.24490976333618164\n",
      "Epoch 7[85/625] Time:0.702, Train Loss:0.2790980637073517\n",
      "Epoch 7[86/625] Time:0.704, Train Loss:0.31243273615837097\n",
      "Epoch 7[87/625] Time:0.707, Train Loss:0.3412083089351654\n",
      "Epoch 7[88/625] Time:0.702, Train Loss:0.2687188386917114\n",
      "Epoch 7[89/625] Time:0.704, Train Loss:0.20148544013500214\n",
      "Epoch 7[90/625] Time:0.703, Train Loss:0.30671271681785583\n",
      "Epoch 7[91/625] Time:0.703, Train Loss:0.26552367210388184\n",
      "Epoch 7[92/625] Time:0.703, Train Loss:0.3157690465450287\n",
      "Epoch 7[93/625] Time:0.703, Train Loss:0.2786445617675781\n",
      "Epoch 7[94/625] Time:0.703, Train Loss:0.2690536379814148\n",
      "Epoch 7[95/625] Time:0.703, Train Loss:0.28095361590385437\n",
      "Epoch 7[96/625] Time:0.703, Train Loss:0.33370441198349\n",
      "Epoch 7[97/625] Time:0.703, Train Loss:0.2623674273490906\n",
      "Epoch 7[98/625] Time:0.704, Train Loss:0.2730235159397125\n",
      "Epoch 7[99/625] Time:0.702, Train Loss:0.21884003281593323\n",
      "Epoch 7[100/625] Time:0.703, Train Loss:0.3170908987522125\n",
      "Epoch 7[101/625] Time:0.702, Train Loss:0.2937832772731781\n",
      "Epoch 7[102/625] Time:0.703, Train Loss:0.2961574196815491\n",
      "Epoch 7[103/625] Time:0.702, Train Loss:0.25049924850463867\n",
      "Epoch 7[104/625] Time:0.705, Train Loss:0.28215694427490234\n",
      "Epoch 7[105/625] Time:0.702, Train Loss:0.25793391466140747\n",
      "Epoch 7[106/625] Time:0.703, Train Loss:0.23493148386478424\n",
      "Epoch 7[107/625] Time:0.703, Train Loss:0.29140007495880127\n",
      "Epoch 7[108/625] Time:0.704, Train Loss:0.23635585606098175\n",
      "Epoch 7[109/625] Time:0.703, Train Loss:0.27229249477386475\n",
      "Epoch 7[110/625] Time:0.702, Train Loss:0.2887960970401764\n",
      "Epoch 7[111/625] Time:0.702, Train Loss:0.2806079089641571\n",
      "Epoch 7[112/625] Time:0.704, Train Loss:0.2530936002731323\n",
      "Epoch 7[113/625] Time:0.702, Train Loss:0.26757147908210754\n",
      "Epoch 7[114/625] Time:0.703, Train Loss:0.2998596131801605\n",
      "Epoch 7[115/625] Time:0.703, Train Loss:0.24024154245853424\n",
      "Epoch 7[116/625] Time:0.703, Train Loss:0.23589450120925903\n",
      "Epoch 7[117/625] Time:0.702, Train Loss:0.24800430238246918\n",
      "Epoch 7[118/625] Time:0.702, Train Loss:0.22354717552661896\n",
      "Epoch 7[119/625] Time:0.702, Train Loss:0.4138376712799072\n",
      "Epoch 7[120/625] Time:0.703, Train Loss:0.18079282343387604\n",
      "Epoch 7[121/625] Time:0.7, Train Loss:0.3301023244857788\n",
      "Epoch 7[122/625] Time:0.702, Train Loss:0.24741700291633606\n",
      "Epoch 7[123/625] Time:0.702, Train Loss:0.2670028805732727\n",
      "Epoch 7[124/625] Time:0.703, Train Loss:0.2572077512741089\n",
      "Epoch 7[125/625] Time:0.703, Train Loss:0.29655057191848755\n",
      "Epoch 7[126/625] Time:0.702, Train Loss:0.26872631907463074\n",
      "Epoch 7[127/625] Time:0.702, Train Loss:0.29545262455940247\n",
      "Epoch 7[128/625] Time:0.703, Train Loss:0.3371220529079437\n",
      "Epoch 7[129/625] Time:0.705, Train Loss:0.291477233171463\n",
      "Epoch 7[130/625] Time:0.702, Train Loss:0.21997089684009552\n",
      "Epoch 7[131/625] Time:0.703, Train Loss:0.25087398290634155\n",
      "Epoch 7[132/625] Time:0.704, Train Loss:0.3225260376930237\n",
      "Epoch 7[133/625] Time:0.722, Train Loss:0.34814101457595825\n",
      "Epoch 7[134/625] Time:0.704, Train Loss:0.3442695140838623\n",
      "Epoch 7[135/625] Time:0.703, Train Loss:0.24421535432338715\n",
      "Epoch 7[136/625] Time:0.703, Train Loss:0.29642829298973083\n",
      "Epoch 7[137/625] Time:0.704, Train Loss:0.2616277039051056\n",
      "Epoch 7[138/625] Time:0.702, Train Loss:0.2393403798341751\n",
      "Epoch 7[139/625] Time:0.702, Train Loss:0.24030150473117828\n",
      "Epoch 7[140/625] Time:0.702, Train Loss:0.3153069019317627\n",
      "Epoch 7[141/625] Time:0.705, Train Loss:0.31070369482040405\n",
      "Epoch 7[142/625] Time:0.694, Train Loss:0.33108973503112793\n",
      "Epoch 7[143/625] Time:0.704, Train Loss:0.25297093391418457\n",
      "Epoch 7[144/625] Time:0.728, Train Loss:0.2282165139913559\n",
      "Epoch 7[145/625] Time:0.693, Train Loss:0.2463519126176834\n",
      "Epoch 7[146/625] Time:0.694, Train Loss:0.21728816628456116\n",
      "Epoch 7[147/625] Time:0.695, Train Loss:0.2470942735671997\n",
      "Epoch 7[148/625] Time:0.695, Train Loss:0.24175618588924408\n",
      "Epoch 7[149/625] Time:0.695, Train Loss:0.259309321641922\n",
      "Epoch 7[150/625] Time:0.703, Train Loss:0.2573258578777313\n",
      "Epoch 7[151/625] Time:0.704, Train Loss:0.38788020610809326\n",
      "Epoch 7[152/625] Time:0.708, Train Loss:0.26711833477020264\n",
      "Epoch 7[153/625] Time:0.704, Train Loss:0.24278056621551514\n",
      "Epoch 7[154/625] Time:0.703, Train Loss:0.307082861661911\n",
      "Epoch 7[155/625] Time:0.703, Train Loss:0.21323072910308838\n",
      "Epoch 7[156/625] Time:0.707, Train Loss:0.3663586676120758\n",
      "Epoch 7[157/625] Time:0.703, Train Loss:0.2615536153316498\n",
      "Epoch 7[158/625] Time:0.703, Train Loss:0.2610614001750946\n",
      "Epoch 7[159/625] Time:0.704, Train Loss:0.3329375088214874\n",
      "Epoch 7[160/625] Time:0.704, Train Loss:0.30045467615127563\n",
      "Epoch 7[161/625] Time:0.704, Train Loss:0.3219296932220459\n",
      "Epoch 7[162/625] Time:0.704, Train Loss:0.19293206930160522\n",
      "Epoch 7[163/625] Time:0.703, Train Loss:0.29856690764427185\n",
      "Epoch 7[164/625] Time:0.704, Train Loss:0.27574074268341064\n",
      "Epoch 7[165/625] Time:0.703, Train Loss:0.2770337462425232\n",
      "Epoch 7[166/625] Time:0.703, Train Loss:0.3400896191596985\n",
      "Epoch 7[167/625] Time:0.703, Train Loss:0.2405925840139389\n",
      "Epoch 7[168/625] Time:0.703, Train Loss:0.2665503919124603\n",
      "Epoch 7[169/625] Time:0.707, Train Loss:0.201995387673378\n",
      "Epoch 7[170/625] Time:0.705, Train Loss:0.2610999345779419\n",
      "Epoch 7[171/625] Time:0.703, Train Loss:0.2993420958518982\n",
      "Epoch 7[172/625] Time:0.708, Train Loss:0.317739874124527\n",
      "Epoch 7[173/625] Time:0.702, Train Loss:0.30332517623901367\n",
      "Epoch 7[174/625] Time:0.702, Train Loss:0.31189730763435364\n",
      "Epoch 7[175/625] Time:0.747, Train Loss:0.2505641579627991\n",
      "Epoch 7[176/625] Time:0.692, Train Loss:0.28357651829719543\n",
      "Epoch 7[177/625] Time:0.702, Train Loss:0.24907231330871582\n",
      "Epoch 7[178/625] Time:0.744, Train Loss:0.2256459891796112\n",
      "Epoch 7[179/625] Time:0.72, Train Loss:0.2629926800727844\n",
      "Epoch 7[180/625] Time:0.702, Train Loss:0.3016895353794098\n",
      "Epoch 7[181/625] Time:0.702, Train Loss:0.21997790038585663\n",
      "Epoch 7[182/625] Time:0.703, Train Loss:0.27476441860198975\n",
      "Epoch 7[183/625] Time:0.703, Train Loss:0.2599698007106781\n",
      "Epoch 7[184/625] Time:0.703, Train Loss:0.3045896589756012\n",
      "Epoch 7[185/625] Time:0.702, Train Loss:0.26260706782341003\n",
      "Epoch 7[186/625] Time:0.704, Train Loss:0.3456902801990509\n",
      "Epoch 7[187/625] Time:0.703, Train Loss:0.3180089592933655\n",
      "Epoch 7[188/625] Time:0.705, Train Loss:0.2938721477985382\n",
      "Epoch 7[189/625] Time:0.703, Train Loss:0.2629193961620331\n",
      "Epoch 7[190/625] Time:0.703, Train Loss:0.2725861966609955\n",
      "Epoch 7[191/625] Time:0.704, Train Loss:0.3496078550815582\n",
      "Epoch 7[192/625] Time:0.703, Train Loss:0.24486519396305084\n",
      "Epoch 7[193/625] Time:0.707, Train Loss:0.21877853572368622\n",
      "Epoch 7[194/625] Time:0.705, Train Loss:0.22521887719631195\n",
      "Epoch 7[195/625] Time:0.703, Train Loss:0.28547149896621704\n",
      "Epoch 7[196/625] Time:0.703, Train Loss:0.26308518648147583\n",
      "Epoch 7[197/625] Time:0.702, Train Loss:0.275577574968338\n",
      "Epoch 7[198/625] Time:0.702, Train Loss:0.29839888215065\n",
      "Epoch 7[199/625] Time:0.703, Train Loss:0.23984470963478088\n",
      "Epoch 7[200/625] Time:0.702, Train Loss:0.2543286681175232\n",
      "Epoch 7[201/625] Time:0.703, Train Loss:0.3056168556213379\n",
      "Epoch 7[202/625] Time:0.703, Train Loss:0.3261701762676239\n",
      "Epoch 7[203/625] Time:0.7, Train Loss:0.3232986629009247\n",
      "Epoch 7[204/625] Time:0.703, Train Loss:0.3017823398113251\n",
      "Epoch 7[205/625] Time:0.705, Train Loss:0.26087918877601624\n",
      "Epoch 7[206/625] Time:0.702, Train Loss:0.32849299907684326\n",
      "Epoch 7[207/625] Time:0.703, Train Loss:0.2700863778591156\n",
      "Epoch 7[208/625] Time:0.702, Train Loss:0.2991761267185211\n",
      "Epoch 7[209/625] Time:0.703, Train Loss:0.21392463147640228\n",
      "Epoch 7[210/625] Time:0.703, Train Loss:0.3104757070541382\n",
      "Epoch 7[211/625] Time:0.704, Train Loss:0.28670188784599304\n",
      "Epoch 7[212/625] Time:0.703, Train Loss:0.2685147821903229\n",
      "Epoch 7[213/625] Time:0.703, Train Loss:0.2336512953042984\n",
      "Epoch 7[214/625] Time:0.703, Train Loss:0.24277479946613312\n",
      "Epoch 7[215/625] Time:0.703, Train Loss:0.39328205585479736\n",
      "Epoch 7[216/625] Time:0.702, Train Loss:0.24907472729682922\n",
      "Epoch 7[217/625] Time:0.702, Train Loss:0.3381049633026123\n",
      "Epoch 7[218/625] Time:0.703, Train Loss:0.2228747457265854\n",
      "Epoch 7[219/625] Time:0.702, Train Loss:0.21464426815509796\n",
      "Epoch 7[220/625] Time:0.703, Train Loss:0.27177855372428894\n",
      "Epoch 7[221/625] Time:0.702, Train Loss:0.32249459624290466\n",
      "Epoch 7[222/625] Time:0.703, Train Loss:0.2824985384941101\n",
      "Epoch 7[223/625] Time:0.703, Train Loss:0.29002615809440613\n",
      "Epoch 7[224/625] Time:0.746, Train Loss:0.21105550229549408\n",
      "Epoch 7[225/625] Time:0.693, Train Loss:0.24373899400234222\n",
      "Epoch 7[226/625] Time:0.694, Train Loss:0.2547956109046936\n",
      "Epoch 7[227/625] Time:0.716, Train Loss:0.257796049118042\n",
      "Epoch 7[228/625] Time:0.704, Train Loss:0.23028114438056946\n",
      "Epoch 7[229/625] Time:0.703, Train Loss:0.27520546317100525\n",
      "Epoch 7[230/625] Time:0.703, Train Loss:0.31873419880867004\n",
      "Epoch 7[231/625] Time:0.702, Train Loss:0.293838769197464\n",
      "Epoch 7[232/625] Time:0.703, Train Loss:0.35481587052345276\n",
      "Epoch 7[233/625] Time:0.703, Train Loss:0.26723262667655945\n",
      "Epoch 7[234/625] Time:0.704, Train Loss:0.30602291226387024\n",
      "Epoch 7[235/625] Time:0.709, Train Loss:0.2907818555831909\n",
      "Epoch 7[236/625] Time:0.705, Train Loss:0.2729876935482025\n",
      "Epoch 7[237/625] Time:0.702, Train Loss:0.3054834008216858\n",
      "Epoch 7[238/625] Time:0.703, Train Loss:0.28317978978157043\n",
      "Epoch 7[239/625] Time:0.703, Train Loss:0.27550941705703735\n",
      "Epoch 7[240/625] Time:0.703, Train Loss:0.2510715425014496\n",
      "Epoch 7[241/625] Time:0.702, Train Loss:0.25675034523010254\n",
      "Epoch 7[242/625] Time:0.703, Train Loss:0.29810631275177\n",
      "Epoch 7[243/625] Time:0.705, Train Loss:0.28355711698532104\n",
      "Epoch 7[244/625] Time:0.703, Train Loss:0.27880775928497314\n",
      "Epoch 7[245/625] Time:0.703, Train Loss:0.21338452398777008\n",
      "Epoch 7[246/625] Time:0.702, Train Loss:0.3376389741897583\n",
      "Epoch 7[247/625] Time:0.702, Train Loss:0.27419403195381165\n",
      "Epoch 7[248/625] Time:0.702, Train Loss:0.23603889346122742\n",
      "Epoch 7[249/625] Time:0.703, Train Loss:0.26105955243110657\n",
      "Epoch 7[250/625] Time:0.706, Train Loss:0.23339246213436127\n",
      "Epoch 7[251/625] Time:0.712, Train Loss:0.2468995898962021\n",
      "Epoch 7[252/625] Time:0.703, Train Loss:0.2911677360534668\n",
      "Epoch 7[253/625] Time:0.703, Train Loss:0.23307816684246063\n",
      "Epoch 7[254/625] Time:0.703, Train Loss:0.18963785469532013\n",
      "Epoch 7[255/625] Time:0.732, Train Loss:0.32301393151283264\n",
      "Epoch 7[256/625] Time:0.696, Train Loss:0.2941243052482605\n",
      "Epoch 7[257/625] Time:0.693, Train Loss:0.2896658182144165\n",
      "Epoch 7[258/625] Time:0.758, Train Loss:0.3202780783176422\n",
      "Epoch 7[259/625] Time:0.705, Train Loss:0.2172628939151764\n",
      "Epoch 7[260/625] Time:0.702, Train Loss:0.2965121567249298\n",
      "Epoch 7[261/625] Time:0.703, Train Loss:0.23184730112552643\n",
      "Epoch 7[262/625] Time:0.704, Train Loss:0.32066255807876587\n",
      "Epoch 7[263/625] Time:0.705, Train Loss:0.26542264223098755\n",
      "Epoch 7[264/625] Time:0.705, Train Loss:0.275020033121109\n",
      "Epoch 7[265/625] Time:0.704, Train Loss:0.29284441471099854\n",
      "Epoch 7[266/625] Time:0.703, Train Loss:0.22027823328971863\n",
      "Epoch 7[267/625] Time:0.705, Train Loss:0.3111995756626129\n",
      "Epoch 7[268/625] Time:0.704, Train Loss:0.4199097156524658\n",
      "Epoch 7[269/625] Time:0.703, Train Loss:0.3070203959941864\n",
      "Epoch 7[270/625] Time:0.703, Train Loss:0.2068905085325241\n",
      "Epoch 7[271/625] Time:0.704, Train Loss:0.2260511815547943\n",
      "Epoch 7[272/625] Time:0.706, Train Loss:0.17032665014266968\n",
      "Epoch 7[273/625] Time:0.704, Train Loss:0.2525506317615509\n",
      "Epoch 7[274/625] Time:0.692, Train Loss:0.25028282403945923\n",
      "Epoch 7[275/625] Time:0.696, Train Loss:0.25221431255340576\n",
      "Epoch 7[276/625] Time:0.695, Train Loss:0.27641916275024414\n",
      "Epoch 7[277/625] Time:0.693, Train Loss:0.2944779396057129\n",
      "Epoch 7[278/625] Time:0.693, Train Loss:0.34981733560562134\n",
      "Epoch 7[279/625] Time:0.718, Train Loss:0.24824263155460358\n",
      "Epoch 7[280/625] Time:0.704, Train Loss:0.34144625067710876\n",
      "Epoch 7[281/625] Time:0.701, Train Loss:0.25182172656059265\n",
      "Epoch 7[282/625] Time:0.702, Train Loss:0.2441774308681488\n",
      "Epoch 7[283/625] Time:0.703, Train Loss:0.23532767593860626\n",
      "Epoch 7[284/625] Time:0.703, Train Loss:0.22377221286296844\n",
      "Epoch 7[285/625] Time:0.702, Train Loss:0.26570960879325867\n",
      "Epoch 7[286/625] Time:0.702, Train Loss:0.3089323341846466\n",
      "Epoch 7[287/625] Time:0.709, Train Loss:0.1881982982158661\n",
      "Epoch 7[288/625] Time:0.708, Train Loss:0.227768674492836\n",
      "Epoch 7[289/625] Time:0.702, Train Loss:0.2590746581554413\n",
      "Epoch 7[290/625] Time:0.702, Train Loss:0.19723106920719147\n",
      "Epoch 7[291/625] Time:0.703, Train Loss:0.275650292634964\n",
      "Epoch 7[292/625] Time:0.704, Train Loss:0.29412853717803955\n",
      "Epoch 7[293/625] Time:0.702, Train Loss:0.2560742199420929\n",
      "Epoch 7[294/625] Time:0.703, Train Loss:0.3039153218269348\n",
      "Epoch 7[295/625] Time:0.703, Train Loss:0.28684720396995544\n",
      "Epoch 7[296/625] Time:0.702, Train Loss:0.26161372661590576\n",
      "Epoch 7[297/625] Time:0.703, Train Loss:0.2867295742034912\n",
      "Epoch 7[298/625] Time:0.703, Train Loss:0.27945229411125183\n",
      "Epoch 7[299/625] Time:0.702, Train Loss:0.2217732071876526\n",
      "Epoch 7[300/625] Time:0.703, Train Loss:0.24004827439785004\n",
      "Epoch 7[301/625] Time:0.702, Train Loss:0.21874232590198517\n",
      "Epoch 7[302/625] Time:0.703, Train Loss:0.24527816474437714\n",
      "Epoch 7[303/625] Time:0.702, Train Loss:0.28704655170440674\n",
      "Epoch 7[304/625] Time:0.703, Train Loss:0.20361071825027466\n",
      "Epoch 7[305/625] Time:0.702, Train Loss:0.3031606376171112\n",
      "Epoch 7[306/625] Time:0.703, Train Loss:0.3173028230667114\n",
      "Epoch 7[307/625] Time:0.703, Train Loss:0.29714521765708923\n",
      "Epoch 7[308/625] Time:0.703, Train Loss:0.28177642822265625\n",
      "Epoch 7[309/625] Time:0.703, Train Loss:0.2698271870613098\n",
      "Epoch 7[310/625] Time:0.703, Train Loss:0.42751339077949524\n",
      "Epoch 7[311/625] Time:0.703, Train Loss:0.28795576095581055\n",
      "Epoch 7[312/625] Time:0.704, Train Loss:0.22043997049331665\n",
      "Epoch 7[313/625] Time:0.703, Train Loss:0.2503751516342163\n",
      "Epoch 7[314/625] Time:0.703, Train Loss:0.27591896057128906\n",
      "Epoch 7[315/625] Time:0.704, Train Loss:0.19808152318000793\n",
      "Epoch 7[316/625] Time:0.704, Train Loss:0.24172642827033997\n",
      "Epoch 7[317/625] Time:0.703, Train Loss:0.3413464426994324\n",
      "Epoch 7[318/625] Time:0.703, Train Loss:0.28385287523269653\n",
      "Epoch 7[319/625] Time:0.702, Train Loss:0.19794006645679474\n",
      "Epoch 7[320/625] Time:0.704, Train Loss:0.2472335547208786\n",
      "Epoch 7[321/625] Time:0.703, Train Loss:0.27790412306785583\n",
      "Epoch 7[322/625] Time:0.702, Train Loss:0.24239550530910492\n",
      "Epoch 7[323/625] Time:0.735, Train Loss:0.23384900391101837\n",
      "Epoch 7[324/625] Time:0.694, Train Loss:0.3265339434146881\n",
      "Epoch 7[325/625] Time:0.695, Train Loss:0.43311354517936707\n",
      "Epoch 7[326/625] Time:0.694, Train Loss:0.37857601046562195\n",
      "Epoch 7[327/625] Time:0.694, Train Loss:0.234344020485878\n",
      "Epoch 7[328/625] Time:0.695, Train Loss:0.3705592453479767\n",
      "Epoch 7[329/625] Time:0.702, Train Loss:0.2513757646083832\n",
      "Epoch 7[330/625] Time:0.705, Train Loss:0.23124553263187408\n",
      "Epoch 7[331/625] Time:0.703, Train Loss:0.24990348517894745\n",
      "Epoch 7[332/625] Time:0.702, Train Loss:0.32536429166793823\n",
      "Epoch 7[333/625] Time:0.703, Train Loss:0.3291124701499939\n",
      "Epoch 7[334/625] Time:0.702, Train Loss:0.20755214989185333\n",
      "Epoch 7[335/625] Time:0.703, Train Loss:0.20071278512477875\n",
      "Epoch 7[336/625] Time:0.701, Train Loss:0.30557435750961304\n",
      "Epoch 7[337/625] Time:0.703, Train Loss:0.26971063017845154\n",
      "Epoch 7[338/625] Time:0.702, Train Loss:0.3342193365097046\n",
      "Epoch 7[339/625] Time:0.704, Train Loss:0.2234940081834793\n",
      "Epoch 7[340/625] Time:0.7, Train Loss:0.25131312012672424\n",
      "Epoch 7[341/625] Time:0.701, Train Loss:0.2536865770816803\n",
      "Epoch 7[342/625] Time:0.701, Train Loss:0.3407108783721924\n",
      "Epoch 7[343/625] Time:0.702, Train Loss:0.24175165593624115\n",
      "Epoch 7[344/625] Time:0.703, Train Loss:0.21498681604862213\n",
      "Epoch 7[345/625] Time:0.701, Train Loss:0.2817763686180115\n",
      "Epoch 7[346/625] Time:0.701, Train Loss:0.3189198970794678\n",
      "Epoch 7[347/625] Time:0.702, Train Loss:0.23183749616146088\n",
      "Epoch 7[348/625] Time:0.701, Train Loss:0.29625171422958374\n",
      "Epoch 7[349/625] Time:0.7, Train Loss:0.2800288200378418\n",
      "Epoch 7[350/625] Time:0.702, Train Loss:0.27266624569892883\n",
      "Epoch 7[351/625] Time:0.724, Train Loss:0.2512191832065582\n",
      "Epoch 7[352/625] Time:0.694, Train Loss:0.24967093765735626\n",
      "Epoch 7[353/625] Time:0.695, Train Loss:0.23329807817935944\n",
      "Epoch 7[354/625] Time:0.698, Train Loss:0.16999582946300507\n",
      "Epoch 7[355/625] Time:0.694, Train Loss:0.2038549929857254\n",
      "Epoch 7[356/625] Time:0.694, Train Loss:0.28340908885002136\n",
      "Epoch 7[357/625] Time:0.715, Train Loss:0.33372223377227783\n",
      "Epoch 7[358/625] Time:0.703, Train Loss:0.23378130793571472\n",
      "Epoch 7[359/625] Time:0.703, Train Loss:0.2950560450553894\n",
      "Epoch 7[360/625] Time:0.697, Train Loss:0.33547428250312805\n",
      "Epoch 7[361/625] Time:0.703, Train Loss:0.30897092819213867\n",
      "Epoch 7[362/625] Time:0.705, Train Loss:0.2627207636833191\n",
      "Epoch 7[363/625] Time:0.703, Train Loss:0.21516337990760803\n",
      "Epoch 7[364/625] Time:0.746, Train Loss:0.29969123005867004\n",
      "Epoch 7[365/625] Time:0.693, Train Loss:0.31296035647392273\n",
      "Epoch 7[366/625] Time:0.703, Train Loss:0.20920880138874054\n",
      "Epoch 7[367/625] Time:0.713, Train Loss:0.30112823843955994\n",
      "Epoch 7[368/625] Time:0.703, Train Loss:0.3371306359767914\n",
      "Epoch 7[369/625] Time:0.703, Train Loss:0.3015502989292145\n",
      "Epoch 7[370/625] Time:0.703, Train Loss:0.2247498780488968\n",
      "Epoch 7[371/625] Time:0.703, Train Loss:0.2464928925037384\n",
      "Epoch 7[372/625] Time:0.705, Train Loss:0.23711079359054565\n",
      "Epoch 7[373/625] Time:0.703, Train Loss:0.3657262623310089\n",
      "Epoch 7[374/625] Time:0.705, Train Loss:0.3055860102176666\n",
      "Epoch 7[375/625] Time:0.703, Train Loss:0.2234608381986618\n",
      "Epoch 7[376/625] Time:0.702, Train Loss:0.23988136649131775\n",
      "Epoch 7[377/625] Time:0.702, Train Loss:0.2408202588558197\n",
      "Epoch 7[378/625] Time:0.703, Train Loss:0.27129530906677246\n",
      "Epoch 7[379/625] Time:0.703, Train Loss:0.35910701751708984\n",
      "Epoch 7[380/625] Time:0.704, Train Loss:0.3066891133785248\n",
      "Epoch 7[381/625] Time:0.703, Train Loss:0.29563137888908386\n",
      "Epoch 7[382/625] Time:0.704, Train Loss:0.2886139452457428\n",
      "Epoch 7[383/625] Time:0.703, Train Loss:0.20359179377555847\n",
      "Epoch 7[384/625] Time:0.703, Train Loss:0.2530447840690613\n",
      "Epoch 7[385/625] Time:0.703, Train Loss:0.23777909576892853\n",
      "Epoch 7[386/625] Time:0.709, Train Loss:0.27581334114074707\n",
      "Epoch 7[387/625] Time:0.702, Train Loss:0.3587707579135895\n",
      "Epoch 7[388/625] Time:0.702, Train Loss:0.3102758526802063\n",
      "Epoch 7[389/625] Time:0.705, Train Loss:0.25466305017471313\n",
      "Epoch 7[390/625] Time:0.753, Train Loss:0.24385951459407806\n",
      "Epoch 7[391/625] Time:0.694, Train Loss:0.29630982875823975\n",
      "Epoch 7[392/625] Time:0.694, Train Loss:0.3301585018634796\n",
      "Epoch 7[393/625] Time:0.697, Train Loss:0.2650814950466156\n",
      "Epoch 7[394/625] Time:0.721, Train Loss:0.29370439052581787\n",
      "Epoch 7[395/625] Time:0.704, Train Loss:0.27552372217178345\n",
      "Epoch 7[396/625] Time:0.702, Train Loss:0.28229954838752747\n",
      "Epoch 7[397/625] Time:0.703, Train Loss:0.27651408314704895\n",
      "Epoch 7[398/625] Time:0.703, Train Loss:0.24081192910671234\n",
      "Epoch 7[399/625] Time:0.703, Train Loss:0.3022623658180237\n",
      "Epoch 7[400/625] Time:0.703, Train Loss:0.24879090487957\n",
      "Epoch 7[401/625] Time:0.71, Train Loss:0.22170618176460266\n",
      "Epoch 7[402/625] Time:0.702, Train Loss:0.24217040836811066\n",
      "Epoch 7[403/625] Time:0.702, Train Loss:0.2593306005001068\n",
      "Epoch 7[404/625] Time:0.702, Train Loss:0.2562651038169861\n",
      "Epoch 7[405/625] Time:0.703, Train Loss:0.325316458940506\n",
      "Epoch 7[406/625] Time:0.746, Train Loss:0.2899034023284912\n",
      "Epoch 7[407/625] Time:0.702, Train Loss:0.23396363854408264\n",
      "Epoch 7[408/625] Time:0.702, Train Loss:0.26807093620300293\n",
      "Epoch 7[409/625] Time:0.704, Train Loss:0.35418304800987244\n",
      "Epoch 7[410/625] Time:0.702, Train Loss:0.28112658858299255\n",
      "Epoch 7[411/625] Time:0.702, Train Loss:0.2836752235889435\n",
      "Epoch 7[412/625] Time:0.702, Train Loss:0.3236359655857086\n",
      "Epoch 7[413/625] Time:0.703, Train Loss:0.21300946176052094\n",
      "Epoch 7[414/625] Time:0.705, Train Loss:0.2720949649810791\n",
      "Epoch 7[415/625] Time:0.703, Train Loss:0.2367633730173111\n",
      "Epoch 7[416/625] Time:0.702, Train Loss:0.2838894724845886\n",
      "Epoch 7[417/625] Time:0.704, Train Loss:0.24310143291950226\n",
      "Epoch 7[418/625] Time:0.707, Train Loss:0.2235386073589325\n",
      "Epoch 7[419/625] Time:0.703, Train Loss:0.28882867097854614\n",
      "Epoch 7[420/625] Time:0.704, Train Loss:0.29562869668006897\n",
      "Epoch 7[421/625] Time:0.694, Train Loss:0.2883552312850952\n",
      "Epoch 7[422/625] Time:0.696, Train Loss:0.26754510402679443\n",
      "Epoch 7[423/625] Time:0.695, Train Loss:0.25223299860954285\n",
      "Epoch 7[424/625] Time:0.697, Train Loss:0.2578065097332001\n",
      "Epoch 7[425/625] Time:0.703, Train Loss:0.22048717737197876\n",
      "Epoch 7[426/625] Time:0.705, Train Loss:0.30624690651893616\n",
      "Epoch 7[427/625] Time:0.703, Train Loss:0.2358001470565796\n",
      "Epoch 7[428/625] Time:0.702, Train Loss:0.28045669198036194\n",
      "Epoch 7[429/625] Time:0.741, Train Loss:0.25829434394836426\n",
      "Epoch 7[430/625] Time:0.709, Train Loss:0.2986048758029938\n",
      "Epoch 7[431/625] Time:0.703, Train Loss:0.2839975953102112\n",
      "Epoch 7[432/625] Time:0.707, Train Loss:0.34956008195877075\n",
      "Epoch 7[433/625] Time:0.703, Train Loss:0.28712180256843567\n",
      "Epoch 7[434/625] Time:0.702, Train Loss:0.3188774883747101\n",
      "Epoch 7[435/625] Time:0.703, Train Loss:0.32214275002479553\n",
      "Epoch 7[436/625] Time:0.705, Train Loss:0.27564820647239685\n",
      "Epoch 7[437/625] Time:0.704, Train Loss:0.21916620433330536\n",
      "Epoch 7[438/625] Time:0.705, Train Loss:0.2422720193862915\n",
      "Epoch 7[439/625] Time:0.704, Train Loss:0.2703881561756134\n",
      "Epoch 7[440/625] Time:0.706, Train Loss:0.27125993371009827\n",
      "Epoch 7[441/625] Time:0.704, Train Loss:0.23639284074306488\n",
      "Epoch 7[442/625] Time:0.702, Train Loss:0.2431531846523285\n",
      "Epoch 7[443/625] Time:0.704, Train Loss:0.27233558893203735\n",
      "Epoch 7[444/625] Time:0.703, Train Loss:0.22338205575942993\n",
      "Epoch 7[445/625] Time:0.704, Train Loss:0.31148576736450195\n",
      "Epoch 7[446/625] Time:0.704, Train Loss:0.2606583833694458\n",
      "Epoch 7[447/625] Time:0.705, Train Loss:0.37505167722702026\n",
      "Epoch 7[448/625] Time:0.708, Train Loss:0.310011088848114\n",
      "Epoch 7[449/625] Time:0.703, Train Loss:0.2852848172187805\n",
      "Epoch 7[450/625] Time:0.703, Train Loss:0.23208433389663696\n",
      "Epoch 7[451/625] Time:0.703, Train Loss:0.2604384422302246\n",
      "Epoch 7[452/625] Time:0.704, Train Loss:0.24691571295261383\n",
      "Epoch 7[453/625] Time:0.704, Train Loss:0.2223825454711914\n",
      "Epoch 7[454/625] Time:0.704, Train Loss:0.2593671679496765\n",
      "Epoch 7[455/625] Time:0.703, Train Loss:0.24107934534549713\n",
      "Epoch 7[456/625] Time:0.703, Train Loss:0.33017420768737793\n",
      "Epoch 7[457/625] Time:0.703, Train Loss:0.25421807169914246\n",
      "Epoch 7[458/625] Time:0.703, Train Loss:0.2825028598308563\n",
      "Epoch 7[459/625] Time:0.704, Train Loss:0.22582678496837616\n",
      "Epoch 7[460/625] Time:0.738, Train Loss:0.28171953558921814\n",
      "Epoch 7[461/625] Time:0.723, Train Loss:0.2878430187702179\n",
      "Epoch 7[462/625] Time:0.706, Train Loss:0.23910577595233917\n",
      "Epoch 7[463/625] Time:0.705, Train Loss:0.25601428747177124\n",
      "Epoch 7[464/625] Time:0.707, Train Loss:0.2582732141017914\n",
      "Epoch 7[465/625] Time:0.703, Train Loss:0.29151588678359985\n",
      "Epoch 7[466/625] Time:0.703, Train Loss:0.35994404554367065\n",
      "Epoch 7[467/625] Time:0.703, Train Loss:0.29165399074554443\n",
      "Epoch 7[468/625] Time:0.706, Train Loss:0.265080988407135\n",
      "Epoch 7[469/625] Time:0.702, Train Loss:0.24507369101047516\n",
      "Epoch 7[470/625] Time:0.703, Train Loss:0.32843855023384094\n",
      "Epoch 7[471/625] Time:0.704, Train Loss:0.2716052830219269\n",
      "Epoch 7[472/625] Time:0.704, Train Loss:0.2947869300842285\n",
      "Epoch 7[473/625] Time:0.703, Train Loss:0.2604266107082367\n",
      "Epoch 7[474/625] Time:0.703, Train Loss:0.35904526710510254\n",
      "Epoch 7[475/625] Time:0.703, Train Loss:0.2815139591693878\n",
      "Epoch 7[476/625] Time:0.704, Train Loss:0.3115811049938202\n",
      "Epoch 7[477/625] Time:0.702, Train Loss:0.2708607316017151\n",
      "Epoch 7[478/625] Time:0.702, Train Loss:0.2940171957015991\n",
      "Epoch 7[479/625] Time:0.702, Train Loss:0.27992862462997437\n",
      "Epoch 7[480/625] Time:0.703, Train Loss:0.3498113453388214\n",
      "Epoch 7[481/625] Time:0.703, Train Loss:0.26745614409446716\n",
      "Epoch 7[482/625] Time:0.703, Train Loss:0.2051345705986023\n",
      "Epoch 7[483/625] Time:0.703, Train Loss:0.1664547324180603\n",
      "Epoch 7[484/625] Time:0.704, Train Loss:0.2340930998325348\n",
      "Epoch 7[485/625] Time:0.703, Train Loss:0.2685992419719696\n",
      "Epoch 7[486/625] Time:0.704, Train Loss:0.31539687514305115\n",
      "Epoch 7[487/625] Time:0.703, Train Loss:0.28914543986320496\n",
      "Epoch 7[488/625] Time:0.704, Train Loss:0.2678018808364868\n",
      "Epoch 7[489/625] Time:0.702, Train Loss:0.326980322599411\n",
      "Epoch 7[490/625] Time:0.704, Train Loss:0.2489897757768631\n",
      "Epoch 7[491/625] Time:0.702, Train Loss:0.31500059366226196\n",
      "Epoch 7[492/625] Time:0.703, Train Loss:0.2654736638069153\n",
      "Epoch 7[493/625] Time:0.703, Train Loss:0.31806686520576477\n",
      "Epoch 7[494/625] Time:0.703, Train Loss:0.318609356880188\n",
      "Epoch 7[495/625] Time:0.702, Train Loss:0.31723278760910034\n",
      "Epoch 7[496/625] Time:0.704, Train Loss:0.26910942792892456\n",
      "Epoch 7[497/625] Time:0.703, Train Loss:0.26432380080223083\n",
      "Epoch 7[498/625] Time:0.703, Train Loss:0.27373528480529785\n",
      "Epoch 7[499/625] Time:0.703, Train Loss:0.21774199604988098\n",
      "Epoch 7[500/625] Time:0.702, Train Loss:0.32316213846206665\n",
      "Epoch 7[501/625] Time:0.703, Train Loss:0.20681431889533997\n",
      "Epoch 7[502/625] Time:0.703, Train Loss:0.24342086911201477\n",
      "Epoch 7[503/625] Time:0.703, Train Loss:0.2879871726036072\n",
      "Epoch 7[504/625] Time:0.704, Train Loss:0.29297661781311035\n",
      "Epoch 7[505/625] Time:0.703, Train Loss:0.2334153652191162\n",
      "Epoch 7[506/625] Time:0.703, Train Loss:0.2508734464645386\n",
      "Epoch 7[507/625] Time:0.702, Train Loss:0.2659313380718231\n",
      "Epoch 7[508/625] Time:0.704, Train Loss:0.354605495929718\n",
      "Epoch 7[509/625] Time:0.737, Train Loss:0.20545341074466705\n",
      "Epoch 7[510/625] Time:0.693, Train Loss:0.27682092785835266\n",
      "Epoch 7[511/625] Time:0.694, Train Loss:0.1968993991613388\n",
      "Epoch 7[512/625] Time:0.697, Train Loss:0.23258930444717407\n",
      "Epoch 7[513/625] Time:0.703, Train Loss:0.22282645106315613\n",
      "Epoch 7[514/625] Time:0.739, Train Loss:0.22282278537750244\n",
      "Epoch 7[515/625] Time:0.712, Train Loss:0.24481742084026337\n",
      "Epoch 7[516/625] Time:0.702, Train Loss:0.2183363288640976\n",
      "Epoch 7[517/625] Time:0.745, Train Loss:0.2862505614757538\n",
      "Epoch 7[518/625] Time:0.702, Train Loss:0.21755829453468323\n",
      "Epoch 7[519/625] Time:0.711, Train Loss:0.2766365110874176\n",
      "Epoch 7[520/625] Time:0.703, Train Loss:0.3213834762573242\n",
      "Epoch 7[521/625] Time:0.732, Train Loss:0.22622592747211456\n",
      "Epoch 7[522/625] Time:0.703, Train Loss:0.3363402485847473\n",
      "Epoch 7[523/625] Time:0.703, Train Loss:0.25868478417396545\n",
      "Epoch 7[524/625] Time:0.702, Train Loss:0.2472565919160843\n",
      "Epoch 7[525/625] Time:0.704, Train Loss:0.23737403750419617\n",
      "Epoch 7[526/625] Time:0.704, Train Loss:0.29356637597084045\n",
      "Epoch 7[527/625] Time:0.704, Train Loss:0.3099697232246399\n",
      "Epoch 7[528/625] Time:0.703, Train Loss:0.22256867587566376\n",
      "Epoch 7[529/625] Time:0.704, Train Loss:0.3144109547138214\n",
      "Epoch 7[530/625] Time:0.702, Train Loss:0.2863445580005646\n",
      "Epoch 7[531/625] Time:0.702, Train Loss:0.2890492081642151\n",
      "Epoch 7[532/625] Time:0.702, Train Loss:0.36482614278793335\n",
      "Epoch 7[533/625] Time:0.703, Train Loss:0.2651421129703522\n",
      "Epoch 7[534/625] Time:0.703, Train Loss:0.21619769930839539\n",
      "Epoch 7[535/625] Time:0.702, Train Loss:0.25599929690361023\n",
      "Epoch 7[536/625] Time:0.703, Train Loss:0.2403939664363861\n",
      "Epoch 7[537/625] Time:0.702, Train Loss:0.3230217695236206\n",
      "Epoch 7[538/625] Time:0.7, Train Loss:0.30595752596855164\n",
      "Epoch 7[539/625] Time:0.7, Train Loss:0.256620854139328\n",
      "Epoch 7[540/625] Time:0.702, Train Loss:0.27673712372779846\n",
      "Epoch 7[541/625] Time:0.7, Train Loss:0.24202139675617218\n",
      "Epoch 7[542/625] Time:0.7, Train Loss:0.2552543878555298\n",
      "Epoch 7[543/625] Time:0.702, Train Loss:0.28991490602493286\n",
      "Epoch 7[544/625] Time:0.702, Train Loss:0.2787405550479889\n",
      "Epoch 7[545/625] Time:0.7, Train Loss:0.2859538495540619\n",
      "Epoch 7[546/625] Time:0.703, Train Loss:0.28445520997047424\n",
      "Epoch 7[547/625] Time:0.704, Train Loss:0.33095782995224\n",
      "Epoch 7[548/625] Time:0.704, Train Loss:0.2647339105606079\n",
      "Epoch 7[549/625] Time:0.702, Train Loss:0.18296657502651215\n",
      "Epoch 7[550/625] Time:0.702, Train Loss:0.2278025597333908\n",
      "Epoch 7[551/625] Time:0.703, Train Loss:0.26820307970046997\n",
      "Epoch 7[552/625] Time:0.703, Train Loss:0.3814830183982849\n",
      "Epoch 7[553/625] Time:0.703, Train Loss:0.26543962955474854\n",
      "Epoch 7[554/625] Time:0.703, Train Loss:0.276718407869339\n",
      "Epoch 7[555/625] Time:0.703, Train Loss:0.2233286201953888\n",
      "Epoch 7[556/625] Time:0.702, Train Loss:0.2422681450843811\n",
      "Epoch 7[557/625] Time:0.701, Train Loss:0.26683661341667175\n",
      "Epoch 7[558/625] Time:0.701, Train Loss:0.36529237031936646\n",
      "Epoch 7[559/625] Time:0.705, Train Loss:0.2766458988189697\n",
      "Epoch 7[560/625] Time:0.704, Train Loss:0.31818458437919617\n",
      "Epoch 7[561/625] Time:0.702, Train Loss:0.33721864223480225\n",
      "Epoch 7[562/625] Time:0.704, Train Loss:0.24943450093269348\n",
      "Epoch 7[563/625] Time:0.704, Train Loss:0.22736582159996033\n",
      "Epoch 7[564/625] Time:0.703, Train Loss:0.26669710874557495\n",
      "Epoch 7[565/625] Time:0.704, Train Loss:0.2835279405117035\n",
      "Epoch 7[566/625] Time:0.705, Train Loss:0.24997608363628387\n",
      "Epoch 7[567/625] Time:0.705, Train Loss:0.22632254660129547\n",
      "Epoch 7[568/625] Time:0.705, Train Loss:0.3055582344532013\n",
      "Epoch 7[569/625] Time:0.704, Train Loss:0.280925452709198\n",
      "Epoch 7[570/625] Time:0.704, Train Loss:0.28853145241737366\n",
      "Epoch 7[571/625] Time:0.704, Train Loss:0.2251066416501999\n",
      "Epoch 7[572/625] Time:0.704, Train Loss:0.2573603391647339\n",
      "Epoch 7[573/625] Time:0.712, Train Loss:0.29488348960876465\n",
      "Epoch 7[574/625] Time:0.691, Train Loss:0.3090112805366516\n",
      "Epoch 7[575/625] Time:0.693, Train Loss:0.2247547060251236\n",
      "Epoch 7[576/625] Time:0.693, Train Loss:0.24173763394355774\n",
      "Epoch 7[577/625] Time:0.703, Train Loss:0.22174471616744995\n",
      "Epoch 7[578/625] Time:0.704, Train Loss:0.23635190725326538\n",
      "Epoch 7[579/625] Time:0.704, Train Loss:0.32312285900115967\n",
      "Epoch 7[580/625] Time:0.705, Train Loss:0.25542232394218445\n",
      "Epoch 7[581/625] Time:0.705, Train Loss:0.2511407434940338\n",
      "Epoch 7[582/625] Time:0.704, Train Loss:0.2602153718471527\n",
      "Epoch 7[583/625] Time:0.703, Train Loss:0.3225066065788269\n",
      "Epoch 7[584/625] Time:0.705, Train Loss:0.24225760996341705\n",
      "Epoch 7[585/625] Time:0.705, Train Loss:0.23873408138751984\n",
      "Epoch 7[586/625] Time:0.705, Train Loss:0.37360474467277527\n",
      "Epoch 7[587/625] Time:0.707, Train Loss:0.33976393938064575\n",
      "Epoch 7[588/625] Time:0.703, Train Loss:0.304946631193161\n",
      "Epoch 7[589/625] Time:0.703, Train Loss:0.2984517216682434\n",
      "Epoch 7[590/625] Time:0.705, Train Loss:0.21914735436439514\n",
      "Epoch 7[591/625] Time:0.703, Train Loss:0.2730052173137665\n",
      "Epoch 7[592/625] Time:0.704, Train Loss:0.25928792357444763\n",
      "Epoch 7[593/625] Time:0.703, Train Loss:0.24595396220684052\n",
      "Epoch 7[594/625] Time:0.703, Train Loss:0.29884007573127747\n",
      "Epoch 7[595/625] Time:0.703, Train Loss:0.340015172958374\n",
      "Epoch 7[596/625] Time:0.702, Train Loss:0.23072344064712524\n",
      "Epoch 7[597/625] Time:0.701, Train Loss:0.27406203746795654\n",
      "Epoch 7[598/625] Time:0.701, Train Loss:0.2776597738265991\n",
      "Epoch 7[599/625] Time:0.701, Train Loss:0.2644668519496918\n",
      "Epoch 7[600/625] Time:0.702, Train Loss:0.2646285593509674\n",
      "Epoch 7[601/625] Time:0.703, Train Loss:0.30388665199279785\n",
      "Epoch 7[602/625] Time:0.703, Train Loss:0.29426687955856323\n",
      "Epoch 7[603/625] Time:0.703, Train Loss:0.23246146738529205\n",
      "Epoch 7[604/625] Time:0.704, Train Loss:0.3336341977119446\n",
      "Epoch 7[605/625] Time:0.706, Train Loss:0.2654401659965515\n",
      "Epoch 7[606/625] Time:0.703, Train Loss:0.3301096558570862\n",
      "Epoch 7[607/625] Time:0.703, Train Loss:0.30646753311157227\n",
      "Epoch 7[608/625] Time:0.703, Train Loss:0.23435337841510773\n",
      "Epoch 7[609/625] Time:0.704, Train Loss:0.2654649019241333\n",
      "Epoch 7[610/625] Time:0.693, Train Loss:0.28288882970809937\n",
      "Epoch 7[611/625] Time:0.695, Train Loss:0.21052920818328857\n",
      "Epoch 7[612/625] Time:0.702, Train Loss:0.32508501410484314\n",
      "Epoch 7[613/625] Time:0.703, Train Loss:0.2607243061065674\n",
      "Epoch 7[614/625] Time:0.704, Train Loss:0.3164248764514923\n",
      "Epoch 7[615/625] Time:0.703, Train Loss:0.2615790367126465\n",
      "Epoch 7[616/625] Time:0.703, Train Loss:0.2788567841053009\n",
      "Epoch 7[617/625] Time:0.703, Train Loss:0.2912147641181946\n",
      "Epoch 7[618/625] Time:0.703, Train Loss:0.2523348033428192\n",
      "Epoch 7[619/625] Time:0.703, Train Loss:0.2268662005662918\n",
      "Epoch 7[620/625] Time:0.704, Train Loss:0.2550133764743805\n",
      "Epoch 7[621/625] Time:0.703, Train Loss:0.2744689881801605\n",
      "Epoch 7[622/625] Time:0.705, Train Loss:0.23870785534381866\n",
      "Epoch 7[623/625] Time:0.702, Train Loss:0.3468712866306305\n",
      "Epoch 7[624/625] Time:0.704, Train Loss:0.2968387305736542\n",
      "Epoch 7[0/78] Val Loss:0.14385370910167694\n",
      "Epoch 7[1/78] Val Loss:0.13206806778907776\n",
      "Epoch 7[2/78] Val Loss:0.1182689368724823\n",
      "Epoch 7[3/78] Val Loss:0.13466723263263702\n",
      "Epoch 7[4/78] Val Loss:0.23951037228107452\n",
      "Epoch 7[5/78] Val Loss:0.18657059967517853\n",
      "Epoch 7[6/78] Val Loss:0.21433793008327484\n",
      "Epoch 7[7/78] Val Loss:0.30508536100387573\n",
      "Epoch 7[8/78] Val Loss:0.15043994784355164\n",
      "Epoch 7[9/78] Val Loss:0.1388019621372223\n",
      "Epoch 7[10/78] Val Loss:0.07236507534980774\n",
      "Epoch 7[11/78] Val Loss:0.1573629081249237\n",
      "Epoch 7[12/78] Val Loss:0.11133602261543274\n",
      "Epoch 7[13/78] Val Loss:0.1044207215309143\n",
      "Epoch 7[14/78] Val Loss:0.1278783082962036\n",
      "Epoch 7[15/78] Val Loss:0.1138002872467041\n",
      "Epoch 7[16/78] Val Loss:0.12383770197629929\n",
      "Epoch 7[17/78] Val Loss:0.11574580520391464\n",
      "Epoch 7[18/78] Val Loss:0.15938298404216766\n",
      "Epoch 7[19/78] Val Loss:0.2219032645225525\n",
      "Epoch 7[20/78] Val Loss:0.14598138630390167\n",
      "Epoch 7[21/78] Val Loss:0.5008125901222229\n",
      "Epoch 7[22/78] Val Loss:0.6827197074890137\n",
      "Epoch 7[23/78] Val Loss:0.5163570642471313\n",
      "Epoch 7[24/78] Val Loss:0.3858763575553894\n",
      "Epoch 7[25/78] Val Loss:0.48385608196258545\n",
      "Epoch 7[26/78] Val Loss:0.4119831323623657\n",
      "Epoch 7[27/78] Val Loss:0.4028257429599762\n",
      "Epoch 7[28/78] Val Loss:0.3819501996040344\n",
      "Epoch 7[29/78] Val Loss:0.4797331392765045\n",
      "Epoch 7[30/78] Val Loss:1.441924810409546\n",
      "Epoch 7[31/78] Val Loss:1.2886147499084473\n",
      "Epoch 7[32/78] Val Loss:0.9492725133895874\n",
      "Epoch 7[33/78] Val Loss:0.3138783276081085\n",
      "Epoch 7[34/78] Val Loss:0.24969977140426636\n",
      "Epoch 7[35/78] Val Loss:0.27014192938804626\n",
      "Epoch 7[36/78] Val Loss:0.2743903398513794\n",
      "Epoch 7[37/78] Val Loss:0.33149468898773193\n",
      "Epoch 7[38/78] Val Loss:0.16076286137104034\n",
      "Epoch 7[39/78] Val Loss:0.19002316892147064\n",
      "Epoch 7[40/78] Val Loss:0.19070638716220856\n",
      "Epoch 7[41/78] Val Loss:0.1849052459001541\n",
      "Epoch 7[42/78] Val Loss:0.18410935997962952\n",
      "Epoch 7[43/78] Val Loss:0.13838613033294678\n",
      "Epoch 7[44/78] Val Loss:0.12608565390110016\n",
      "Epoch 7[45/78] Val Loss:0.09276046603918076\n",
      "Epoch 7[46/78] Val Loss:0.13228163123130798\n",
      "Epoch 7[47/78] Val Loss:0.1253586858510971\n",
      "Epoch 7[48/78] Val Loss:0.14534176886081696\n",
      "Epoch 7[49/78] Val Loss:0.09575707465410233\n",
      "Epoch 7[50/78] Val Loss:0.12018389999866486\n",
      "Epoch 7[51/78] Val Loss:0.108707956969738\n",
      "Epoch 7[52/78] Val Loss:0.12527166306972504\n",
      "Epoch 7[53/78] Val Loss:0.10840143263339996\n",
      "Epoch 7[54/78] Val Loss:0.09451230615377426\n",
      "Epoch 7[55/78] Val Loss:0.10921633988618851\n",
      "Epoch 7[56/78] Val Loss:0.3669552206993103\n",
      "Epoch 7[57/78] Val Loss:0.3286779224872589\n",
      "Epoch 7[58/78] Val Loss:0.33148160576820374\n",
      "Epoch 7[59/78] Val Loss:0.36526569724082947\n",
      "Epoch 7[60/78] Val Loss:0.35153093934059143\n",
      "Epoch 7[61/78] Val Loss:0.4111536145210266\n",
      "Epoch 7[62/78] Val Loss:0.4312835931777954\n",
      "Epoch 7[63/78] Val Loss:0.23963060975074768\n",
      "Epoch 7[64/78] Val Loss:0.13531675934791565\n",
      "Epoch 7[65/78] Val Loss:0.11847657710313797\n",
      "Epoch 7[66/78] Val Loss:0.15639716386795044\n",
      "Epoch 7[67/78] Val Loss:0.1383855938911438\n",
      "Epoch 7[68/78] Val Loss:0.39453834295272827\n",
      "Epoch 7[69/78] Val Loss:0.4011998474597931\n",
      "Epoch 7[70/78] Val Loss:0.4044671654701233\n",
      "Epoch 7[71/78] Val Loss:0.30638939142227173\n",
      "Epoch 7[72/78] Val Loss:0.10772781074047089\n",
      "Epoch 7[73/78] Val Loss:0.09622635692358017\n",
      "Epoch 7[74/78] Val Loss:0.34725090861320496\n",
      "Epoch 7[75/78] Val Loss:0.41340145468711853\n",
      "Epoch 7[76/78] Val Loss:0.41667911410331726\n",
      "Epoch 7[77/78] Val Loss:0.5062177181243896\n",
      "Epoch 7[78/78] Val Loss:0.4039146602153778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.93     15691\n",
      "           1       0.71      0.78      0.75      4309\n",
      "\n",
      "    accuracy                           0.88     20000\n",
      "   macro avg       0.83      0.85      0.84     20000\n",
      "weighted avg       0.89      0.88      0.89     20000\n",
      "\n",
      "Epoch 7: Train Loss 0.27503749027252195, Val Loss 0.2844678183587698, Train Time 790.8198094367981, Val Time 36.427711486816406\n",
      "New best model at epoch 7\n",
      "Epoch 8[0/625] Time:0.688, Train Loss:0.2357105165719986\n",
      "Epoch 8[1/625] Time:0.738, Train Loss:0.32698214054107666\n",
      "Epoch 8[2/625] Time:0.702, Train Loss:0.2821851074695587\n",
      "Epoch 8[3/625] Time:0.702, Train Loss:0.24471381306648254\n",
      "Epoch 8[4/625] Time:0.702, Train Loss:0.25879448652267456\n",
      "Epoch 8[5/625] Time:0.703, Train Loss:0.17066970467567444\n",
      "Epoch 8[6/625] Time:0.702, Train Loss:0.22495736181735992\n",
      "Epoch 8[7/625] Time:0.704, Train Loss:0.3064175248146057\n",
      "Epoch 8[8/625] Time:0.701, Train Loss:0.25795266032218933\n",
      "Epoch 8[9/625] Time:0.744, Train Loss:0.32443350553512573\n",
      "Epoch 8[10/625] Time:0.693, Train Loss:0.2700464725494385\n",
      "Epoch 8[11/625] Time:0.703, Train Loss:0.250935435295105\n",
      "Epoch 8[12/625] Time:0.704, Train Loss:0.28079044818878174\n",
      "Epoch 8[13/625] Time:0.702, Train Loss:0.2142317295074463\n",
      "Epoch 8[14/625] Time:0.703, Train Loss:0.3477412164211273\n",
      "Epoch 8[15/625] Time:0.703, Train Loss:0.16793511807918549\n",
      "Epoch 8[16/625] Time:0.702, Train Loss:0.22109483182430267\n",
      "Epoch 8[17/625] Time:0.702, Train Loss:0.235258549451828\n",
      "Epoch 8[18/625] Time:0.702, Train Loss:0.1814374327659607\n",
      "Epoch 8[19/625] Time:0.745, Train Loss:0.31872987747192383\n",
      "Epoch 8[20/625] Time:0.702, Train Loss:0.2635151445865631\n",
      "Epoch 8[21/625] Time:0.702, Train Loss:0.2852565348148346\n",
      "Epoch 8[22/625] Time:0.708, Train Loss:0.21260645985603333\n",
      "Epoch 8[23/625] Time:0.703, Train Loss:0.23958827555179596\n",
      "Epoch 8[24/625] Time:0.704, Train Loss:0.2491893470287323\n",
      "Epoch 8[25/625] Time:0.704, Train Loss:0.2913025915622711\n",
      "Epoch 8[26/625] Time:0.703, Train Loss:0.20963984727859497\n",
      "Epoch 8[27/625] Time:0.703, Train Loss:0.35523322224617004\n",
      "Epoch 8[28/625] Time:0.703, Train Loss:0.31661808490753174\n",
      "Epoch 8[29/625] Time:0.705, Train Loss:0.3136090040206909\n",
      "Epoch 8[30/625] Time:0.711, Train Loss:0.28155046701431274\n",
      "Epoch 8[31/625] Time:0.704, Train Loss:0.23710483312606812\n",
      "Epoch 8[32/625] Time:0.703, Train Loss:0.19338300824165344\n",
      "Epoch 8[33/625] Time:0.707, Train Loss:0.2019995152950287\n",
      "Epoch 8[34/625] Time:0.704, Train Loss:0.3036341965198517\n",
      "Epoch 8[35/625] Time:0.703, Train Loss:0.2870146632194519\n",
      "Epoch 8[36/625] Time:0.704, Train Loss:0.22700370848178864\n",
      "Epoch 8[37/625] Time:0.703, Train Loss:0.28711605072021484\n",
      "Epoch 8[38/625] Time:0.705, Train Loss:0.2412000149488449\n",
      "Epoch 8[39/625] Time:0.704, Train Loss:0.19666312634944916\n",
      "Epoch 8[40/625] Time:0.703, Train Loss:0.32193660736083984\n",
      "Epoch 8[41/625] Time:0.704, Train Loss:0.30364733934402466\n",
      "Epoch 8[42/625] Time:0.704, Train Loss:0.27722984552383423\n",
      "Epoch 8[43/625] Time:0.706, Train Loss:0.17696800827980042\n",
      "Epoch 8[44/625] Time:0.704, Train Loss:0.2069668173789978\n",
      "Epoch 8[45/625] Time:0.704, Train Loss:0.2911309003829956\n",
      "Epoch 8[46/625] Time:0.707, Train Loss:0.28375527262687683\n",
      "Epoch 8[47/625] Time:0.704, Train Loss:0.2801612317562103\n",
      "Epoch 8[48/625] Time:0.703, Train Loss:0.2595473527908325\n",
      "Epoch 8[49/625] Time:0.704, Train Loss:0.24298687279224396\n",
      "Epoch 8[50/625] Time:0.704, Train Loss:0.318860799074173\n",
      "Epoch 8[51/625] Time:0.704, Train Loss:0.26282328367233276\n",
      "Epoch 8[52/625] Time:0.703, Train Loss:0.29396480321884155\n",
      "Epoch 8[53/625] Time:0.703, Train Loss:0.23125651478767395\n",
      "Epoch 8[54/625] Time:0.705, Train Loss:0.2725568413734436\n",
      "Epoch 8[55/625] Time:0.704, Train Loss:0.24463307857513428\n",
      "Epoch 8[56/625] Time:0.703, Train Loss:0.2745717763900757\n",
      "Epoch 8[57/625] Time:0.703, Train Loss:0.2576966881752014\n",
      "Epoch 8[58/625] Time:0.703, Train Loss:0.31602099537849426\n",
      "Epoch 8[59/625] Time:0.702, Train Loss:0.2475048452615738\n",
      "Epoch 8[60/625] Time:0.704, Train Loss:0.28091225028038025\n",
      "Epoch 8[61/625] Time:0.704, Train Loss:0.26398730278015137\n",
      "Epoch 8[62/625] Time:0.703, Train Loss:0.21707144379615784\n",
      "Epoch 8[63/625] Time:0.703, Train Loss:0.220742329955101\n",
      "Epoch 8[64/625] Time:0.703, Train Loss:0.2947295904159546\n",
      "Epoch 8[65/625] Time:0.703, Train Loss:0.3027873635292053\n",
      "Epoch 8[66/625] Time:0.703, Train Loss:0.3197973668575287\n",
      "Epoch 8[67/625] Time:0.703, Train Loss:0.26418665051460266\n",
      "Epoch 8[68/625] Time:0.702, Train Loss:0.20240969955921173\n",
      "Epoch 8[69/625] Time:0.704, Train Loss:0.18753306567668915\n",
      "Epoch 8[70/625] Time:0.704, Train Loss:0.29246628284454346\n",
      "Epoch 8[71/625] Time:0.703, Train Loss:0.4044066369533539\n",
      "Epoch 8[72/625] Time:0.703, Train Loss:0.2610214352607727\n",
      "Epoch 8[73/625] Time:0.705, Train Loss:0.2708810865879059\n",
      "Epoch 8[74/625] Time:0.703, Train Loss:0.20242898166179657\n",
      "Epoch 8[75/625] Time:0.705, Train Loss:0.37108948826789856\n",
      "Epoch 8[76/625] Time:0.707, Train Loss:0.26522672176361084\n",
      "Epoch 8[77/625] Time:0.705, Train Loss:0.18594713509082794\n",
      "Epoch 8[78/625] Time:0.704, Train Loss:0.26472020149230957\n",
      "Epoch 8[79/625] Time:0.704, Train Loss:0.2694561779499054\n",
      "Epoch 8[80/625] Time:0.703, Train Loss:0.21251614391803741\n",
      "Epoch 8[81/625] Time:0.703, Train Loss:0.2508675754070282\n",
      "Epoch 8[82/625] Time:0.702, Train Loss:0.27642154693603516\n",
      "Epoch 8[83/625] Time:0.735, Train Loss:0.2496843785047531\n",
      "Epoch 8[84/625] Time:0.702, Train Loss:0.38928157091140747\n",
      "Epoch 8[85/625] Time:0.707, Train Loss:0.2754971385002136\n",
      "Epoch 8[86/625] Time:0.708, Train Loss:0.3145224153995514\n",
      "Epoch 8[87/625] Time:0.702, Train Loss:0.29301953315734863\n",
      "Epoch 8[88/625] Time:0.702, Train Loss:0.28024807572364807\n",
      "Epoch 8[89/625] Time:0.703, Train Loss:0.32571908831596375\n",
      "Epoch 8[90/625] Time:0.702, Train Loss:0.3757472336292267\n",
      "Epoch 8[91/625] Time:0.704, Train Loss:0.2789393961429596\n",
      "Epoch 8[92/625] Time:0.703, Train Loss:0.25254201889038086\n",
      "Epoch 8[93/625] Time:0.703, Train Loss:0.2823021709918976\n",
      "Epoch 8[94/625] Time:0.704, Train Loss:0.2584911286830902\n",
      "Epoch 8[95/625] Time:0.702, Train Loss:0.30074840784072876\n",
      "Epoch 8[96/625] Time:0.703, Train Loss:0.2879711985588074\n",
      "Epoch 8[97/625] Time:0.702, Train Loss:0.28570103645324707\n",
      "Epoch 8[98/625] Time:0.703, Train Loss:0.3261134922504425\n",
      "Epoch 8[99/625] Time:0.703, Train Loss:0.3474956750869751\n",
      "Epoch 8[100/625] Time:0.703, Train Loss:0.2994546890258789\n",
      "Epoch 8[101/625] Time:0.704, Train Loss:0.21664398908615112\n",
      "Epoch 8[102/625] Time:0.709, Train Loss:0.26483771204948425\n",
      "Epoch 8[103/625] Time:0.703, Train Loss:0.27145957946777344\n",
      "Epoch 8[104/625] Time:0.704, Train Loss:0.3978508710861206\n",
      "Epoch 8[105/625] Time:0.707, Train Loss:0.2295973300933838\n",
      "Epoch 8[106/625] Time:0.703, Train Loss:0.24695651233196259\n",
      "Epoch 8[107/625] Time:0.703, Train Loss:0.2627609372138977\n",
      "Epoch 8[108/625] Time:0.703, Train Loss:0.2384849339723587\n",
      "Epoch 8[109/625] Time:0.703, Train Loss:0.26261788606643677\n",
      "Epoch 8[110/625] Time:0.703, Train Loss:0.31064558029174805\n",
      "Epoch 8[111/625] Time:0.707, Train Loss:0.277555376291275\n",
      "Epoch 8[112/625] Time:0.706, Train Loss:0.26792946457862854\n",
      "Epoch 8[113/625] Time:0.704, Train Loss:0.31375852227211\n",
      "Epoch 8[114/625] Time:0.704, Train Loss:0.24943293631076813\n",
      "Epoch 8[115/625] Time:0.704, Train Loss:0.24151179194450378\n",
      "Epoch 8[116/625] Time:0.704, Train Loss:0.45287832617759705\n",
      "Epoch 8[117/625] Time:0.704, Train Loss:0.24260862171649933\n",
      "Epoch 8[118/625] Time:0.705, Train Loss:0.31849682331085205\n",
      "Epoch 8[119/625] Time:0.702, Train Loss:0.1761331856250763\n",
      "Epoch 8[120/625] Time:0.703, Train Loss:0.21718338131904602\n",
      "Epoch 8[121/625] Time:0.703, Train Loss:0.30559468269348145\n",
      "Epoch 8[122/625] Time:0.704, Train Loss:0.23069773614406586\n",
      "Epoch 8[123/625] Time:0.703, Train Loss:0.25515016913414\n",
      "Epoch 8[124/625] Time:0.703, Train Loss:0.28803834319114685\n",
      "Epoch 8[125/625] Time:0.703, Train Loss:0.28709718585014343\n",
      "Epoch 8[126/625] Time:0.714, Train Loss:0.2568923830986023\n",
      "Epoch 8[127/625] Time:0.695, Train Loss:0.21828314661979675\n",
      "Epoch 8[128/625] Time:0.696, Train Loss:0.25112539529800415\n",
      "Epoch 8[129/625] Time:0.694, Train Loss:0.3094428777694702\n",
      "Epoch 8[130/625] Time:0.694, Train Loss:0.29945749044418335\n",
      "Epoch 8[131/625] Time:0.694, Train Loss:0.26750659942626953\n",
      "Epoch 8[132/625] Time:0.695, Train Loss:0.31602635979652405\n",
      "Epoch 8[133/625] Time:0.694, Train Loss:0.26034802198410034\n",
      "Epoch 8[134/625] Time:0.699, Train Loss:0.26311683654785156\n",
      "Epoch 8[135/625] Time:0.741, Train Loss:0.26009222865104675\n",
      "Epoch 8[136/625] Time:0.702, Train Loss:0.2512747347354889\n",
      "Epoch 8[137/625] Time:0.703, Train Loss:0.2274188995361328\n",
      "Epoch 8[138/625] Time:0.704, Train Loss:0.27146095037460327\n",
      "Epoch 8[139/625] Time:0.705, Train Loss:0.2545425593852997\n",
      "Epoch 8[140/625] Time:0.702, Train Loss:0.2583528757095337\n",
      "Epoch 8[141/625] Time:0.703, Train Loss:0.2556380033493042\n",
      "Epoch 8[142/625] Time:0.702, Train Loss:0.25814345479011536\n",
      "Epoch 8[143/625] Time:0.702, Train Loss:0.2432939112186432\n",
      "Epoch 8[144/625] Time:0.701, Train Loss:0.2846207916736603\n",
      "Epoch 8[145/625] Time:0.703, Train Loss:0.2466629147529602\n",
      "Epoch 8[146/625] Time:0.704, Train Loss:0.2904690206050873\n",
      "Epoch 8[147/625] Time:0.703, Train Loss:0.23280273377895355\n",
      "Epoch 8[148/625] Time:0.702, Train Loss:0.24984845519065857\n",
      "Epoch 8[149/625] Time:0.707, Train Loss:0.2648327350616455\n",
      "Epoch 8[150/625] Time:0.705, Train Loss:0.2808492183685303\n",
      "Epoch 8[151/625] Time:0.704, Train Loss:0.25118789076805115\n",
      "Epoch 8[152/625] Time:0.703, Train Loss:0.2888578772544861\n",
      "Epoch 8[153/625] Time:0.703, Train Loss:0.2631675899028778\n",
      "Epoch 8[154/625] Time:0.704, Train Loss:0.29751870036125183\n",
      "Epoch 8[155/625] Time:0.703, Train Loss:0.3145945370197296\n",
      "Epoch 8[156/625] Time:0.747, Train Loss:0.23368315398693085\n",
      "Epoch 8[157/625] Time:0.699, Train Loss:0.229797825217247\n",
      "Epoch 8[158/625] Time:0.703, Train Loss:0.1960568130016327\n",
      "Epoch 8[159/625] Time:0.708, Train Loss:0.25937169790267944\n",
      "Epoch 8[160/625] Time:0.702, Train Loss:0.2271229475736618\n",
      "Epoch 8[161/625] Time:0.703, Train Loss:0.19678965210914612\n",
      "Epoch 8[162/625] Time:0.703, Train Loss:0.2747420072555542\n",
      "Epoch 8[163/625] Time:0.703, Train Loss:0.3051963746547699\n",
      "Epoch 8[164/625] Time:0.704, Train Loss:0.2261991649866104\n",
      "Epoch 8[165/625] Time:0.71, Train Loss:0.28984445333480835\n",
      "Epoch 8[166/625] Time:0.703, Train Loss:0.28157684206962585\n",
      "Epoch 8[167/625] Time:0.702, Train Loss:0.19761453568935394\n",
      "Epoch 8[168/625] Time:0.703, Train Loss:0.23276516795158386\n",
      "Epoch 8[169/625] Time:0.703, Train Loss:0.29276591539382935\n",
      "Epoch 8[170/625] Time:0.704, Train Loss:0.25935789942741394\n",
      "Epoch 8[171/625] Time:0.702, Train Loss:0.25699305534362793\n",
      "Epoch 8[172/625] Time:0.705, Train Loss:0.18571187555789948\n",
      "Epoch 8[173/625] Time:0.709, Train Loss:0.351791650056839\n",
      "Epoch 8[174/625] Time:0.702, Train Loss:0.2837831974029541\n",
      "Epoch 8[175/625] Time:0.703, Train Loss:0.359435498714447\n",
      "Epoch 8[176/625] Time:0.702, Train Loss:0.23323273658752441\n",
      "Epoch 8[177/625] Time:0.703, Train Loss:0.2553861439228058\n",
      "Epoch 8[178/625] Time:0.703, Train Loss:0.3393537700176239\n",
      "Epoch 8[179/625] Time:0.702, Train Loss:0.31754380464553833\n",
      "Epoch 8[180/625] Time:0.703, Train Loss:0.2967434525489807\n",
      "Epoch 8[181/625] Time:0.711, Train Loss:0.2838616967201233\n",
      "Epoch 8[182/625] Time:0.703, Train Loss:0.2447323203086853\n",
      "Epoch 8[183/625] Time:0.702, Train Loss:0.22003182768821716\n",
      "Epoch 8[184/625] Time:0.707, Train Loss:0.21177317202091217\n",
      "Epoch 8[185/625] Time:0.697, Train Loss:0.1989055722951889\n",
      "Epoch 8[186/625] Time:0.693, Train Loss:0.3236314058303833\n",
      "Epoch 8[187/625] Time:0.693, Train Loss:0.27433091402053833\n",
      "Epoch 8[188/625] Time:0.694, Train Loss:0.17052572965621948\n",
      "Epoch 8[189/625] Time:0.696, Train Loss:0.22127407789230347\n",
      "Epoch 8[190/625] Time:0.702, Train Loss:0.21816875040531158\n",
      "Epoch 8[191/625] Time:0.703, Train Loss:0.302800714969635\n",
      "Epoch 8[192/625] Time:0.705, Train Loss:0.20972169935703278\n",
      "Epoch 8[193/625] Time:0.705, Train Loss:0.2760780155658722\n",
      "Epoch 8[194/625] Time:0.704, Train Loss:0.24781434237957\n",
      "Epoch 8[195/625] Time:0.703, Train Loss:0.30872759222984314\n",
      "Epoch 8[196/625] Time:0.704, Train Loss:0.22841499745845795\n",
      "Epoch 8[197/625] Time:0.703, Train Loss:0.21234965324401855\n",
      "Epoch 8[198/625] Time:0.703, Train Loss:0.239005908370018\n",
      "Epoch 8[199/625] Time:0.703, Train Loss:0.2966746389865875\n",
      "Epoch 8[200/625] Time:0.703, Train Loss:0.2479168325662613\n",
      "Epoch 8[201/625] Time:0.702, Train Loss:0.22256766259670258\n",
      "Epoch 8[202/625] Time:0.703, Train Loss:0.23428088426589966\n",
      "Epoch 8[203/625] Time:0.743, Train Loss:0.244226336479187\n",
      "Epoch 8[204/625] Time:0.697, Train Loss:0.28270024061203003\n",
      "Epoch 8[205/625] Time:0.705, Train Loss:0.23393186926841736\n",
      "Epoch 8[206/625] Time:0.703, Train Loss:0.20521600544452667\n",
      "Epoch 8[207/625] Time:0.705, Train Loss:0.3311873972415924\n",
      "Epoch 8[208/625] Time:0.745, Train Loss:0.2208796888589859\n",
      "Epoch 8[209/625] Time:0.691, Train Loss:0.23893702030181885\n",
      "Epoch 8[210/625] Time:0.693, Train Loss:0.27111440896987915\n",
      "Epoch 8[211/625] Time:0.693, Train Loss:0.24617689847946167\n",
      "Epoch 8[212/625] Time:0.69, Train Loss:0.27829521894454956\n",
      "Epoch 8[213/625] Time:0.69, Train Loss:0.2646128833293915\n",
      "Epoch 8[214/625] Time:0.691, Train Loss:0.348673552274704\n",
      "Epoch 8[215/625] Time:0.689, Train Loss:0.17631588876247406\n",
      "Epoch 8[216/625] Time:0.704, Train Loss:0.2495097666978836\n",
      "Epoch 8[217/625] Time:0.705, Train Loss:0.29745393991470337\n",
      "Epoch 8[218/625] Time:0.704, Train Loss:0.2633526921272278\n",
      "Epoch 8[219/625] Time:0.704, Train Loss:0.2536579966545105\n",
      "Epoch 8[220/625] Time:0.707, Train Loss:0.20807883143424988\n",
      "Epoch 8[221/625] Time:0.702, Train Loss:0.2527211606502533\n",
      "Epoch 8[222/625] Time:0.703, Train Loss:0.16383762657642365\n",
      "Epoch 8[223/625] Time:0.706, Train Loss:0.2961002588272095\n",
      "Epoch 8[224/625] Time:0.703, Train Loss:0.29666662216186523\n",
      "Epoch 8[225/625] Time:0.706, Train Loss:0.24234677851200104\n",
      "Epoch 8[226/625] Time:0.703, Train Loss:0.2802116572856903\n",
      "Epoch 8[227/625] Time:0.705, Train Loss:0.24195200204849243\n",
      "Epoch 8[228/625] Time:0.709, Train Loss:0.2898049056529999\n",
      "Epoch 8[229/625] Time:0.705, Train Loss:0.2039027363061905\n",
      "Epoch 8[230/625] Time:0.703, Train Loss:0.18276576697826385\n",
      "Epoch 8[231/625] Time:0.702, Train Loss:0.22004809975624084\n",
      "Epoch 8[232/625] Time:0.703, Train Loss:0.1862727254629135\n",
      "Epoch 8[233/625] Time:0.703, Train Loss:0.30467355251312256\n",
      "Epoch 8[234/625] Time:0.703, Train Loss:0.1806580275297165\n",
      "Epoch 8[235/625] Time:0.703, Train Loss:0.21837599575519562\n",
      "Epoch 8[236/625] Time:0.704, Train Loss:0.23762348294258118\n",
      "Epoch 8[237/625] Time:0.703, Train Loss:0.26842981576919556\n",
      "Epoch 8[238/625] Time:0.703, Train Loss:0.245627760887146\n",
      "Epoch 8[239/625] Time:0.703, Train Loss:0.21558527648448944\n",
      "Epoch 8[240/625] Time:0.706, Train Loss:0.23575609922409058\n",
      "Epoch 8[241/625] Time:0.732, Train Loss:0.2642040252685547\n",
      "Epoch 8[242/625] Time:0.702, Train Loss:0.39199408888816833\n",
      "Epoch 8[243/625] Time:0.704, Train Loss:0.321965754032135\n",
      "Epoch 8[244/625] Time:0.705, Train Loss:0.22616612911224365\n",
      "Epoch 8[245/625] Time:0.703, Train Loss:0.2839975357055664\n",
      "Epoch 8[246/625] Time:0.703, Train Loss:0.30593234300613403\n",
      "Epoch 8[247/625] Time:0.702, Train Loss:0.3237656354904175\n",
      "Epoch 8[248/625] Time:0.703, Train Loss:0.2826835811138153\n",
      "Epoch 8[249/625] Time:0.703, Train Loss:0.1867445707321167\n",
      "Epoch 8[250/625] Time:0.703, Train Loss:0.2137829065322876\n",
      "Epoch 8[251/625] Time:0.729, Train Loss:0.21087956428527832\n",
      "Epoch 8[252/625] Time:0.714, Train Loss:0.2795911133289337\n",
      "Epoch 8[253/625] Time:0.703, Train Loss:0.21012765169143677\n",
      "Epoch 8[254/625] Time:0.704, Train Loss:0.16401124000549316\n",
      "Epoch 8[255/625] Time:0.704, Train Loss:0.21773825585842133\n",
      "Epoch 8[256/625] Time:0.703, Train Loss:0.2814006805419922\n",
      "Epoch 8[257/625] Time:0.703, Train Loss:0.2613724172115326\n",
      "Epoch 8[258/625] Time:0.734, Train Loss:0.2504784166812897\n",
      "Epoch 8[259/625] Time:0.704, Train Loss:0.28508105874061584\n",
      "Epoch 8[260/625] Time:0.703, Train Loss:0.31662771105766296\n",
      "Epoch 8[261/625] Time:0.703, Train Loss:0.23904955387115479\n",
      "Epoch 8[262/625] Time:0.704, Train Loss:0.38132578134536743\n",
      "Epoch 8[263/625] Time:0.704, Train Loss:0.2646609842777252\n",
      "Epoch 8[264/625] Time:0.706, Train Loss:0.21372090280056\n",
      "Epoch 8[265/625] Time:0.704, Train Loss:0.3351805806159973\n",
      "Epoch 8[266/625] Time:0.704, Train Loss:0.19499093294143677\n",
      "Epoch 8[267/625] Time:0.703, Train Loss:0.2077970802783966\n",
      "Epoch 8[268/625] Time:0.702, Train Loss:0.21527479588985443\n",
      "Epoch 8[269/625] Time:0.704, Train Loss:0.2828458249568939\n",
      "Epoch 8[270/625] Time:0.702, Train Loss:0.2939002811908722\n",
      "Epoch 8[271/625] Time:0.702, Train Loss:0.3127422034740448\n",
      "Epoch 8[272/625] Time:0.703, Train Loss:0.2535032331943512\n",
      "Epoch 8[273/625] Time:0.703, Train Loss:0.34443798661231995\n",
      "Epoch 8[274/625] Time:0.702, Train Loss:0.3071821630001068\n",
      "Epoch 8[275/625] Time:0.7, Train Loss:0.28802961111068726\n",
      "Epoch 8[276/625] Time:0.701, Train Loss:0.292915940284729\n",
      "Epoch 8[277/625] Time:0.703, Train Loss:0.2494959682226181\n",
      "Epoch 8[278/625] Time:0.702, Train Loss:0.31775254011154175\n",
      "Epoch 8[279/625] Time:0.703, Train Loss:0.30400416254997253\n",
      "Epoch 8[280/625] Time:0.701, Train Loss:0.26303038001060486\n",
      "Epoch 8[281/625] Time:0.702, Train Loss:0.22056052088737488\n",
      "Epoch 8[282/625] Time:0.703, Train Loss:0.2828046977519989\n",
      "Epoch 8[283/625] Time:0.704, Train Loss:0.23374804854393005\n",
      "Epoch 8[284/625] Time:0.704, Train Loss:0.2428610622882843\n",
      "Epoch 8[285/625] Time:0.704, Train Loss:0.24506285786628723\n",
      "Epoch 8[286/625] Time:0.702, Train Loss:0.37692341208457947\n",
      "Epoch 8[287/625] Time:0.703, Train Loss:0.27869653701782227\n",
      "Epoch 8[288/625] Time:0.701, Train Loss:0.23424260318279266\n",
      "Epoch 8[289/625] Time:0.704, Train Loss:0.24705415964126587\n",
      "Epoch 8[290/625] Time:0.703, Train Loss:0.2290172576904297\n",
      "Epoch 8[291/625] Time:0.694, Train Loss:0.30699408054351807\n",
      "Epoch 8[292/625] Time:0.694, Train Loss:0.26337864995002747\n",
      "Epoch 8[293/625] Time:0.694, Train Loss:0.2220427542924881\n",
      "Epoch 8[294/625] Time:0.692, Train Loss:0.22686640918254852\n",
      "Epoch 8[295/625] Time:0.695, Train Loss:0.2557544410228729\n",
      "Epoch 8[296/625] Time:0.695, Train Loss:0.22836242616176605\n",
      "Epoch 8[297/625] Time:0.694, Train Loss:0.22171124815940857\n",
      "Epoch 8[298/625] Time:0.694, Train Loss:0.2032664269208908\n",
      "Epoch 8[299/625] Time:0.694, Train Loss:0.27479663491249084\n",
      "Epoch 8[300/625] Time:0.694, Train Loss:0.26450419425964355\n",
      "Epoch 8[301/625] Time:0.692, Train Loss:0.19488081336021423\n",
      "Epoch 8[302/625] Time:0.694, Train Loss:0.2258339673280716\n",
      "Epoch 8[303/625] Time:0.694, Train Loss:0.31011292338371277\n",
      "Epoch 8[304/625] Time:0.694, Train Loss:0.30387434363365173\n",
      "Epoch 8[305/625] Time:0.694, Train Loss:0.20754405856132507\n",
      "Epoch 8[306/625] Time:0.695, Train Loss:0.24418064951896667\n",
      "Epoch 8[307/625] Time:0.708, Train Loss:0.328035831451416\n",
      "Epoch 8[308/625] Time:0.702, Train Loss:0.2386409044265747\n",
      "Epoch 8[309/625] Time:0.703, Train Loss:0.264024943113327\n",
      "Epoch 8[310/625] Time:0.734, Train Loss:0.273865669965744\n",
      "Epoch 8[311/625] Time:0.695, Train Loss:0.31408268213272095\n",
      "Epoch 8[312/625] Time:0.704, Train Loss:0.23484459519386292\n",
      "Epoch 8[313/625] Time:0.703, Train Loss:0.20073428750038147\n",
      "Epoch 8[314/625] Time:0.703, Train Loss:0.18795034289360046\n",
      "Epoch 8[315/625] Time:0.71, Train Loss:0.24760040640830994\n",
      "Epoch 8[316/625] Time:0.704, Train Loss:0.19362027943134308\n",
      "Epoch 8[317/625] Time:0.702, Train Loss:0.22146785259246826\n",
      "Epoch 8[318/625] Time:0.704, Train Loss:0.20828065276145935\n",
      "Epoch 8[319/625] Time:0.704, Train Loss:0.2762497663497925\n",
      "Epoch 8[320/625] Time:0.705, Train Loss:0.25007084012031555\n",
      "Epoch 8[321/625] Time:0.703, Train Loss:0.2871568500995636\n",
      "Epoch 8[322/625] Time:0.703, Train Loss:0.24418379366397858\n",
      "Epoch 8[323/625] Time:0.705, Train Loss:0.3088219165802002\n",
      "Epoch 8[324/625] Time:0.704, Train Loss:0.2670896351337433\n",
      "Epoch 8[325/625] Time:0.704, Train Loss:0.2031150907278061\n",
      "Epoch 8[326/625] Time:0.703, Train Loss:0.2560931146144867\n",
      "Epoch 8[327/625] Time:0.703, Train Loss:0.2681347131729126\n",
      "Epoch 8[328/625] Time:0.703, Train Loss:0.28766068816185\n",
      "Epoch 8[329/625] Time:0.703, Train Loss:0.24658779799938202\n",
      "Epoch 8[330/625] Time:0.702, Train Loss:0.2543627619743347\n",
      "Epoch 8[331/625] Time:0.703, Train Loss:0.25114431977272034\n",
      "Epoch 8[332/625] Time:0.702, Train Loss:0.21133877336978912\n",
      "Epoch 8[333/625] Time:0.704, Train Loss:0.22499309480190277\n",
      "Epoch 8[334/625] Time:0.702, Train Loss:0.2638848125934601\n",
      "Epoch 8[335/625] Time:0.702, Train Loss:0.3124038875102997\n",
      "Epoch 8[336/625] Time:0.703, Train Loss:0.2589147686958313\n",
      "Epoch 8[337/625] Time:0.703, Train Loss:0.28831416368484497\n",
      "Epoch 8[338/625] Time:0.704, Train Loss:0.27083730697631836\n",
      "Epoch 8[339/625] Time:0.706, Train Loss:0.21838685870170593\n",
      "Epoch 8[340/625] Time:0.703, Train Loss:0.29914015531539917\n",
      "Epoch 8[341/625] Time:0.702, Train Loss:0.29301366209983826\n",
      "Epoch 8[342/625] Time:0.705, Train Loss:0.24628008902072906\n",
      "Epoch 8[343/625] Time:0.703, Train Loss:0.28704604506492615\n",
      "Epoch 8[344/625] Time:0.705, Train Loss:0.3243412971496582\n",
      "Epoch 8[345/625] Time:0.703, Train Loss:0.2433089017868042\n",
      "Epoch 8[346/625] Time:0.703, Train Loss:0.26000726222991943\n",
      "Epoch 8[347/625] Time:0.705, Train Loss:0.2496103197336197\n",
      "Epoch 8[348/625] Time:0.702, Train Loss:0.3340928554534912\n",
      "Epoch 8[349/625] Time:0.703, Train Loss:0.2832525074481964\n",
      "Epoch 8[350/625] Time:0.702, Train Loss:0.2221325933933258\n",
      "Epoch 8[351/625] Time:0.705, Train Loss:0.25130578875541687\n",
      "Epoch 8[352/625] Time:0.704, Train Loss:0.29739606380462646\n",
      "Epoch 8[353/625] Time:0.703, Train Loss:0.2426447868347168\n",
      "Epoch 8[354/625] Time:0.736, Train Loss:0.2385183423757553\n",
      "Epoch 8[355/625] Time:0.704, Train Loss:0.3011573851108551\n",
      "Epoch 8[356/625] Time:0.702, Train Loss:0.21183213591575623\n",
      "Epoch 8[357/625] Time:0.703, Train Loss:0.2843717336654663\n",
      "Epoch 8[358/625] Time:0.704, Train Loss:0.258863240480423\n",
      "Epoch 8[359/625] Time:0.703, Train Loss:0.1907956302165985\n",
      "Epoch 8[360/625] Time:0.703, Train Loss:0.3196764588356018\n",
      "Epoch 8[361/625] Time:0.703, Train Loss:0.24765545129776\n",
      "Epoch 8[362/625] Time:0.705, Train Loss:0.25139501690864563\n",
      "Epoch 8[363/625] Time:0.706, Train Loss:0.28650832176208496\n",
      "Epoch 8[364/625] Time:0.706, Train Loss:0.31165075302124023\n",
      "Epoch 8[365/625] Time:0.705, Train Loss:0.2470172941684723\n",
      "Epoch 8[366/625] Time:0.739, Train Loss:0.26471567153930664\n",
      "Epoch 8[367/625] Time:0.727, Train Loss:0.2882659435272217\n",
      "Epoch 8[368/625] Time:0.701, Train Loss:0.29286399483680725\n",
      "Epoch 8[369/625] Time:0.722, Train Loss:0.22730755805969238\n",
      "Epoch 8[370/625] Time:0.694, Train Loss:0.23661714792251587\n",
      "Epoch 8[371/625] Time:0.711, Train Loss:0.2527739107608795\n",
      "Epoch 8[372/625] Time:0.744, Train Loss:0.1931430548429489\n",
      "Epoch 8[373/625] Time:0.693, Train Loss:0.2274947315454483\n",
      "Epoch 8[374/625] Time:0.694, Train Loss:0.285232812166214\n",
      "Epoch 8[375/625] Time:0.693, Train Loss:0.22822223603725433\n",
      "Epoch 8[376/625] Time:0.693, Train Loss:0.2032795399427414\n",
      "Epoch 8[377/625] Time:0.693, Train Loss:0.3271508812904358\n",
      "Epoch 8[378/625] Time:0.694, Train Loss:0.3242300748825073\n",
      "Epoch 8[379/625] Time:0.695, Train Loss:0.2591588497161865\n",
      "Epoch 8[380/625] Time:0.693, Train Loss:0.23253171145915985\n",
      "Epoch 8[381/625] Time:0.695, Train Loss:0.1798393875360489\n",
      "Epoch 8[382/625] Time:0.694, Train Loss:0.27081844210624695\n",
      "Epoch 8[383/625] Time:0.693, Train Loss:0.26694461703300476\n",
      "Epoch 8[384/625] Time:0.711, Train Loss:0.33561959862709045\n",
      "Epoch 8[385/625] Time:0.703, Train Loss:0.23537024855613708\n",
      "Epoch 8[386/625] Time:0.707, Train Loss:0.23533836007118225\n",
      "Epoch 8[387/625] Time:0.703, Train Loss:0.24171596765518188\n",
      "Epoch 8[388/625] Time:0.704, Train Loss:0.27331671118736267\n",
      "Epoch 8[389/625] Time:0.704, Train Loss:0.31491342186927795\n",
      "Epoch 8[390/625] Time:0.703, Train Loss:0.2302187979221344\n",
      "Epoch 8[391/625] Time:0.703, Train Loss:0.2534525692462921\n",
      "Epoch 8[392/625] Time:0.704, Train Loss:0.2619369924068451\n",
      "Epoch 8[393/625] Time:0.704, Train Loss:0.22006206214427948\n",
      "Epoch 8[394/625] Time:0.707, Train Loss:0.2856022119522095\n",
      "Epoch 8[395/625] Time:0.704, Train Loss:0.22839224338531494\n",
      "Epoch 8[396/625] Time:0.703, Train Loss:0.19403749704360962\n",
      "Epoch 8[397/625] Time:0.704, Train Loss:0.29608532786369324\n",
      "Epoch 8[398/625] Time:0.704, Train Loss:0.322372168302536\n",
      "Epoch 8[399/625] Time:0.703, Train Loss:0.32308346033096313\n",
      "Epoch 8[400/625] Time:0.704, Train Loss:0.23030324280261993\n",
      "Epoch 8[401/625] Time:0.704, Train Loss:0.24664469063282013\n",
      "Epoch 8[402/625] Time:0.707, Train Loss:0.29521939158439636\n",
      "Epoch 8[403/625] Time:0.703, Train Loss:0.29028716683387756\n",
      "Epoch 8[404/625] Time:0.703, Train Loss:0.293041467666626\n",
      "Epoch 8[405/625] Time:0.703, Train Loss:0.23235608637332916\n",
      "Epoch 8[406/625] Time:0.703, Train Loss:0.31016865372657776\n",
      "Epoch 8[407/625] Time:0.703, Train Loss:0.23515330255031586\n",
      "Epoch 8[408/625] Time:0.704, Train Loss:0.20112422108650208\n",
      "Epoch 8[409/625] Time:0.703, Train Loss:0.26557451486587524\n",
      "Epoch 8[410/625] Time:0.703, Train Loss:0.22295810282230377\n",
      "Epoch 8[411/625] Time:0.703, Train Loss:0.3063892126083374\n",
      "Epoch 8[412/625] Time:0.703, Train Loss:0.2537611126899719\n",
      "Epoch 8[413/625] Time:0.704, Train Loss:0.2951744794845581\n",
      "Epoch 8[414/625] Time:0.703, Train Loss:0.18476316332817078\n",
      "Epoch 8[415/625] Time:0.704, Train Loss:0.19396431744098663\n",
      "Epoch 8[416/625] Time:0.704, Train Loss:0.1767483502626419\n",
      "Epoch 8[417/625] Time:0.703, Train Loss:0.3088386356830597\n",
      "Epoch 8[418/625] Time:0.707, Train Loss:0.22067423164844513\n",
      "Epoch 8[419/625] Time:0.703, Train Loss:0.2798350751399994\n",
      "Epoch 8[420/625] Time:0.702, Train Loss:0.35663890838623047\n",
      "Epoch 8[421/625] Time:0.703, Train Loss:0.2405303716659546\n",
      "Epoch 8[422/625] Time:0.702, Train Loss:0.32624170184135437\n",
      "Epoch 8[423/625] Time:0.702, Train Loss:0.2276500165462494\n",
      "Epoch 8[424/625] Time:0.703, Train Loss:0.337579607963562\n",
      "Epoch 8[425/625] Time:0.704, Train Loss:0.31995102763175964\n",
      "Epoch 8[426/625] Time:0.706, Train Loss:0.24751479923725128\n",
      "Epoch 8[427/625] Time:0.703, Train Loss:0.239845871925354\n",
      "Epoch 8[428/625] Time:0.703, Train Loss:0.2768672704696655\n",
      "Epoch 8[429/625] Time:0.703, Train Loss:0.28014838695526123\n",
      "Epoch 8[430/625] Time:0.693, Train Loss:0.2523418068885803\n",
      "Epoch 8[431/625] Time:0.692, Train Loss:0.23796266317367554\n",
      "Epoch 8[432/625] Time:0.695, Train Loss:0.3547464907169342\n",
      "Epoch 8[433/625] Time:0.694, Train Loss:0.15915925800800323\n",
      "Epoch 8[434/625] Time:0.741, Train Loss:0.1947532296180725\n",
      "Epoch 8[435/625] Time:0.702, Train Loss:0.24012023210525513\n",
      "Epoch 8[436/625] Time:0.705, Train Loss:0.2867501676082611\n",
      "Epoch 8[437/625] Time:0.707, Train Loss:0.3038572669029236\n",
      "Epoch 8[438/625] Time:0.693, Train Loss:0.22708189487457275\n",
      "Epoch 8[439/625] Time:0.693, Train Loss:0.28698378801345825\n",
      "Epoch 8[440/625] Time:0.733, Train Loss:0.2264609932899475\n",
      "Epoch 8[441/625] Time:0.703, Train Loss:0.2008688896894455\n",
      "Epoch 8[442/625] Time:0.709, Train Loss:0.21954277157783508\n",
      "Epoch 8[443/625] Time:0.703, Train Loss:0.3034265637397766\n",
      "Epoch 8[444/625] Time:0.703, Train Loss:0.3140558898448944\n",
      "Epoch 8[445/625] Time:0.703, Train Loss:0.15839289128780365\n",
      "Epoch 8[446/625] Time:0.703, Train Loss:0.28788191080093384\n",
      "Epoch 8[447/625] Time:0.703, Train Loss:0.31452035903930664\n",
      "Epoch 8[448/625] Time:0.724, Train Loss:0.22060097754001617\n",
      "Epoch 8[449/625] Time:0.702, Train Loss:0.2595936954021454\n",
      "Epoch 8[450/625] Time:0.704, Train Loss:0.25085562467575073\n",
      "Epoch 8[451/625] Time:0.702, Train Loss:0.2786814868450165\n",
      "Epoch 8[452/625] Time:0.702, Train Loss:0.2445005476474762\n",
      "Epoch 8[453/625] Time:0.703, Train Loss:0.2044144570827484\n",
      "Epoch 8[454/625] Time:0.702, Train Loss:0.20741941034793854\n",
      "Epoch 8[455/625] Time:0.703, Train Loss:0.1967666894197464\n",
      "Epoch 8[456/625] Time:0.725, Train Loss:0.2258547991514206\n",
      "Epoch 8[457/625] Time:0.703, Train Loss:0.28913095593452454\n",
      "Epoch 8[458/625] Time:0.706, Train Loss:0.3245198428630829\n",
      "Epoch 8[459/625] Time:0.703, Train Loss:0.21112559735774994\n",
      "Epoch 8[460/625] Time:0.704, Train Loss:0.24909819662570953\n",
      "Epoch 8[461/625] Time:0.704, Train Loss:0.2530671954154968\n",
      "Epoch 8[462/625] Time:0.704, Train Loss:0.23930959403514862\n",
      "Epoch 8[463/625] Time:0.704, Train Loss:0.21844972670078278\n",
      "Epoch 8[464/625] Time:0.703, Train Loss:0.26016080379486084\n",
      "Epoch 8[465/625] Time:0.703, Train Loss:0.23331651091575623\n",
      "Epoch 8[466/625] Time:0.704, Train Loss:0.20927654206752777\n",
      "Epoch 8[467/625] Time:0.703, Train Loss:0.29605191946029663\n",
      "Epoch 8[468/625] Time:0.703, Train Loss:0.34032732248306274\n",
      "Epoch 8[469/625] Time:0.703, Train Loss:0.393029123544693\n",
      "Epoch 8[470/625] Time:0.702, Train Loss:0.33116793632507324\n",
      "Epoch 8[471/625] Time:0.708, Train Loss:0.2403961420059204\n",
      "Epoch 8[472/625] Time:0.703, Train Loss:0.25501593947410583\n",
      "Epoch 8[473/625] Time:0.711, Train Loss:0.2886824905872345\n",
      "Epoch 8[474/625] Time:0.703, Train Loss:0.32633277773857117\n",
      "Epoch 8[475/625] Time:0.703, Train Loss:0.18637074530124664\n",
      "Epoch 8[476/625] Time:0.702, Train Loss:0.24278844892978668\n",
      "Epoch 8[477/625] Time:0.704, Train Loss:0.21366365253925323\n",
      "Epoch 8[478/625] Time:0.704, Train Loss:0.29994526505470276\n",
      "Epoch 8[479/625] Time:0.748, Train Loss:0.24125275015830994\n",
      "Epoch 8[480/625] Time:0.702, Train Loss:0.22805656492710114\n",
      "Epoch 8[481/625] Time:0.706, Train Loss:0.2735169529914856\n",
      "Epoch 8[482/625] Time:0.704, Train Loss:0.24116021394729614\n",
      "Epoch 8[483/625] Time:0.703, Train Loss:0.3293113708496094\n",
      "Epoch 8[484/625] Time:0.702, Train Loss:0.2724779546260834\n",
      "Epoch 8[485/625] Time:0.702, Train Loss:0.25984814763069153\n",
      "Epoch 8[486/625] Time:0.703, Train Loss:0.20284651219844818\n",
      "Epoch 8[487/625] Time:0.706, Train Loss:0.199741393327713\n",
      "Epoch 8[488/625] Time:0.703, Train Loss:0.26467686891555786\n",
      "Epoch 8[489/625] Time:0.706, Train Loss:0.27324244379997253\n",
      "Epoch 8[490/625] Time:0.706, Train Loss:0.20729519426822662\n",
      "Epoch 8[491/625] Time:0.705, Train Loss:0.24984779953956604\n",
      "Epoch 8[492/625] Time:0.703, Train Loss:0.26994290947914124\n",
      "Epoch 8[493/625] Time:0.706, Train Loss:0.2921867370605469\n",
      "Epoch 8[494/625] Time:0.703, Train Loss:0.3527914881706238\n",
      "Epoch 8[495/625] Time:0.703, Train Loss:0.2681724727153778\n",
      "Epoch 8[496/625] Time:0.703, Train Loss:0.2609356939792633\n",
      "Epoch 8[497/625] Time:0.704, Train Loss:0.3152831196784973\n",
      "Epoch 8[498/625] Time:0.702, Train Loss:0.26996028423309326\n",
      "Epoch 8[499/625] Time:0.702, Train Loss:0.2559173107147217\n",
      "Epoch 8[500/625] Time:0.703, Train Loss:0.3026163578033447\n",
      "Epoch 8[501/625] Time:0.702, Train Loss:0.20709072053432465\n",
      "Epoch 8[502/625] Time:0.703, Train Loss:0.2542960047721863\n",
      "Epoch 8[503/625] Time:0.704, Train Loss:0.2458949089050293\n",
      "Epoch 8[504/625] Time:0.703, Train Loss:0.216827392578125\n",
      "Epoch 8[505/625] Time:0.704, Train Loss:0.3333885371685028\n",
      "Epoch 8[506/625] Time:0.703, Train Loss:0.2170817106962204\n",
      "Epoch 8[507/625] Time:0.703, Train Loss:0.29635676741600037\n",
      "Epoch 8[508/625] Time:0.702, Train Loss:0.21010999381542206\n",
      "Epoch 8[509/625] Time:0.703, Train Loss:0.2691125273704529\n",
      "Epoch 8[510/625] Time:0.703, Train Loss:0.2008250653743744\n",
      "Epoch 8[511/625] Time:0.704, Train Loss:0.3016737997531891\n",
      "Epoch 8[512/625] Time:0.702, Train Loss:0.2772665321826935\n",
      "Epoch 8[513/625] Time:0.705, Train Loss:0.25522348284721375\n",
      "Epoch 8[514/625] Time:0.704, Train Loss:0.24553853273391724\n",
      "Epoch 8[515/625] Time:0.703, Train Loss:0.23364563286304474\n",
      "Epoch 8[516/625] Time:0.702, Train Loss:0.2537422478199005\n",
      "Epoch 8[517/625] Time:0.703, Train Loss:0.2583394944667816\n",
      "Epoch 8[518/625] Time:0.702, Train Loss:0.23229928314685822\n",
      "Epoch 8[519/625] Time:0.703, Train Loss:0.23863889276981354\n",
      "Epoch 8[520/625] Time:0.703, Train Loss:0.2895544469356537\n",
      "Epoch 8[521/625] Time:0.703, Train Loss:0.27338823676109314\n",
      "Epoch 8[522/625] Time:0.703, Train Loss:0.21477073431015015\n",
      "Epoch 8[523/625] Time:0.702, Train Loss:0.26301810145378113\n",
      "Epoch 8[524/625] Time:0.703, Train Loss:0.23074671626091003\n",
      "Epoch 8[525/625] Time:0.704, Train Loss:0.2776578664779663\n",
      "Epoch 8[526/625] Time:0.702, Train Loss:0.25135016441345215\n",
      "Epoch 8[527/625] Time:0.702, Train Loss:0.32434332370758057\n",
      "Epoch 8[528/625] Time:0.702, Train Loss:0.29094967246055603\n",
      "Epoch 8[529/625] Time:0.703, Train Loss:0.1817188709974289\n",
      "Epoch 8[530/625] Time:0.703, Train Loss:0.22436422109603882\n",
      "Epoch 8[531/625] Time:0.703, Train Loss:0.19173365831375122\n",
      "Epoch 8[532/625] Time:0.703, Train Loss:0.24853220582008362\n",
      "Epoch 8[533/625] Time:0.703, Train Loss:0.2503122389316559\n",
      "Epoch 8[534/625] Time:0.706, Train Loss:0.2616203725337982\n",
      "Epoch 8[535/625] Time:0.703, Train Loss:0.2074519693851471\n",
      "Epoch 8[536/625] Time:0.707, Train Loss:0.307217001914978\n",
      "Epoch 8[537/625] Time:0.701, Train Loss:0.24546073377132416\n",
      "Epoch 8[538/625] Time:0.74, Train Loss:0.25379112362861633\n",
      "Epoch 8[539/625] Time:0.692, Train Loss:0.2728670835494995\n",
      "Epoch 8[540/625] Time:0.694, Train Loss:0.22620783746242523\n",
      "Epoch 8[541/625] Time:0.693, Train Loss:0.22797761857509613\n",
      "Epoch 8[542/625] Time:0.695, Train Loss:0.31996485590934753\n",
      "Epoch 8[543/625] Time:0.702, Train Loss:0.25249454379081726\n",
      "Epoch 8[544/625] Time:0.704, Train Loss:0.29742661118507385\n",
      "Epoch 8[545/625] Time:0.704, Train Loss:0.2520217001438141\n",
      "Epoch 8[546/625] Time:0.703, Train Loss:0.19999535381793976\n",
      "Epoch 8[547/625] Time:0.703, Train Loss:0.2552186846733093\n",
      "Epoch 8[548/625] Time:0.703, Train Loss:0.2593499720096588\n",
      "Epoch 8[549/625] Time:0.702, Train Loss:0.20016227662563324\n",
      "Epoch 8[550/625] Time:0.703, Train Loss:0.25656938552856445\n",
      "Epoch 8[551/625] Time:0.703, Train Loss:0.23475514352321625\n",
      "Epoch 8[552/625] Time:0.703, Train Loss:0.2211337834596634\n",
      "Epoch 8[553/625] Time:0.703, Train Loss:0.2705150842666626\n",
      "Epoch 8[554/625] Time:0.703, Train Loss:0.18561792373657227\n",
      "Epoch 8[555/625] Time:0.702, Train Loss:0.2273845672607422\n",
      "Epoch 8[556/625] Time:0.703, Train Loss:0.23625096678733826\n",
      "Epoch 8[557/625] Time:0.705, Train Loss:0.2785688042640686\n",
      "Epoch 8[558/625] Time:0.702, Train Loss:0.24200750887393951\n",
      "Epoch 8[559/625] Time:0.703, Train Loss:0.29005250334739685\n",
      "Epoch 8[560/625] Time:0.705, Train Loss:0.30756816267967224\n",
      "Epoch 8[561/625] Time:0.703, Train Loss:0.1999906599521637\n",
      "Epoch 8[562/625] Time:0.702, Train Loss:0.25198879837989807\n",
      "Epoch 8[563/625] Time:0.703, Train Loss:0.216811403632164\n",
      "Epoch 8[564/625] Time:0.703, Train Loss:0.29197290539741516\n",
      "Epoch 8[565/625] Time:0.702, Train Loss:0.2351464033126831\n",
      "Epoch 8[566/625] Time:0.704, Train Loss:0.3135109841823578\n",
      "Epoch 8[567/625] Time:0.702, Train Loss:0.27177610993385315\n",
      "Epoch 8[568/625] Time:0.708, Train Loss:0.2904280126094818\n",
      "Epoch 8[569/625] Time:0.703, Train Loss:0.2894400954246521\n",
      "Epoch 8[570/625] Time:0.726, Train Loss:0.28720623254776\n",
      "Epoch 8[571/625] Time:0.702, Train Loss:0.21888725459575653\n",
      "Epoch 8[572/625] Time:0.705, Train Loss:0.3107921779155731\n",
      "Epoch 8[573/625] Time:0.703, Train Loss:0.24327051639556885\n",
      "Epoch 8[574/625] Time:0.704, Train Loss:0.2881070077419281\n",
      "Epoch 8[575/625] Time:0.703, Train Loss:0.2195330262184143\n",
      "Epoch 8[576/625] Time:0.703, Train Loss:0.24927806854248047\n",
      "Epoch 8[577/625] Time:0.702, Train Loss:0.2567842900753021\n",
      "Epoch 8[578/625] Time:0.702, Train Loss:0.276641845703125\n",
      "Epoch 8[579/625] Time:0.702, Train Loss:0.2950168550014496\n",
      "Epoch 8[580/625] Time:0.703, Train Loss:0.23194508254528046\n",
      "Epoch 8[581/625] Time:0.703, Train Loss:0.20523817837238312\n",
      "Epoch 8[582/625] Time:0.703, Train Loss:0.22877462208271027\n",
      "Epoch 8[583/625] Time:0.705, Train Loss:0.2383211851119995\n",
      "Epoch 8[584/625] Time:0.7, Train Loss:0.23154321312904358\n",
      "Epoch 8[585/625] Time:0.693, Train Loss:0.22454029321670532\n",
      "Epoch 8[586/625] Time:0.694, Train Loss:0.2744527757167816\n",
      "Epoch 8[587/625] Time:0.694, Train Loss:0.23098425567150116\n",
      "Epoch 8[588/625] Time:0.694, Train Loss:0.27869507670402527\n",
      "Epoch 8[589/625] Time:0.693, Train Loss:0.3176405727863312\n",
      "Epoch 8[590/625] Time:0.694, Train Loss:0.23931445181369781\n",
      "Epoch 8[591/625] Time:0.694, Train Loss:0.25144752860069275\n",
      "Epoch 8[592/625] Time:0.702, Train Loss:0.2846015989780426\n",
      "Epoch 8[593/625] Time:0.702, Train Loss:0.2836017310619354\n",
      "Epoch 8[594/625] Time:0.702, Train Loss:0.27857446670532227\n",
      "Epoch 8[595/625] Time:0.703, Train Loss:0.27772629261016846\n",
      "Epoch 8[596/625] Time:0.703, Train Loss:0.3352912664413452\n",
      "Epoch 8[597/625] Time:0.703, Train Loss:0.25307872891426086\n",
      "Epoch 8[598/625] Time:0.703, Train Loss:0.2663591504096985\n",
      "Epoch 8[599/625] Time:0.703, Train Loss:0.2581149935722351\n",
      "Epoch 8[600/625] Time:0.71, Train Loss:0.24550200998783112\n",
      "Epoch 8[601/625] Time:0.704, Train Loss:0.30469369888305664\n",
      "Epoch 8[602/625] Time:0.704, Train Loss:0.25293371081352234\n",
      "Epoch 8[603/625] Time:0.715, Train Loss:0.3833814561367035\n",
      "Epoch 8[604/625] Time:0.705, Train Loss:0.29267197847366333\n",
      "Epoch 8[605/625] Time:0.704, Train Loss:0.2946462035179138\n",
      "Epoch 8[606/625] Time:0.704, Train Loss:0.25127390027046204\n",
      "Epoch 8[607/625] Time:0.704, Train Loss:0.29388171434402466\n",
      "Epoch 8[608/625] Time:0.704, Train Loss:0.21426162123680115\n",
      "Epoch 8[609/625] Time:0.704, Train Loss:0.25509804487228394\n",
      "Epoch 8[610/625] Time:0.704, Train Loss:0.25452831387519836\n",
      "Epoch 8[611/625] Time:0.705, Train Loss:0.2498319447040558\n",
      "Epoch 8[612/625] Time:0.703, Train Loss:0.23385585844516754\n",
      "Epoch 8[613/625] Time:0.705, Train Loss:0.22149603068828583\n",
      "Epoch 8[614/625] Time:0.692, Train Loss:0.24325039982795715\n",
      "Epoch 8[615/625] Time:0.692, Train Loss:0.26882806420326233\n",
      "Epoch 8[616/625] Time:0.693, Train Loss:0.28367742896080017\n",
      "Epoch 8[617/625] Time:0.704, Train Loss:0.2630699574947357\n",
      "Epoch 8[618/625] Time:0.734, Train Loss:0.2866005003452301\n",
      "Epoch 8[619/625] Time:0.692, Train Loss:0.3912320137023926\n",
      "Epoch 8[620/625] Time:0.693, Train Loss:0.3680427670478821\n",
      "Epoch 8[621/625] Time:0.694, Train Loss:0.24984179437160492\n",
      "Epoch 8[622/625] Time:0.693, Train Loss:0.26683762669563293\n",
      "Epoch 8[623/625] Time:0.693, Train Loss:0.21917109191417694\n",
      "Epoch 8[624/625] Time:0.693, Train Loss:0.2803262770175934\n",
      "Epoch 8[0/78] Val Loss:0.18796594440937042\n",
      "Epoch 8[1/78] Val Loss:0.16977955400943756\n",
      "Epoch 8[2/78] Val Loss:0.17091724276542664\n",
      "Epoch 8[3/78] Val Loss:0.2048165649175644\n",
      "Epoch 8[4/78] Val Loss:0.2893792688846588\n",
      "Epoch 8[5/78] Val Loss:0.21313121914863586\n",
      "Epoch 8[6/78] Val Loss:0.2337205857038498\n",
      "Epoch 8[7/78] Val Loss:0.32113441824913025\n",
      "Epoch 8[8/78] Val Loss:0.1655457466840744\n",
      "Epoch 8[9/78] Val Loss:0.13766193389892578\n",
      "Epoch 8[10/78] Val Loss:0.07880496978759766\n",
      "Epoch 8[11/78] Val Loss:0.1418313831090927\n",
      "Epoch 8[12/78] Val Loss:0.10693591088056564\n",
      "Epoch 8[13/78] Val Loss:0.0828067734837532\n",
      "Epoch 8[14/78] Val Loss:0.15808752179145813\n",
      "Epoch 8[15/78] Val Loss:0.12581604719161987\n",
      "Epoch 8[16/78] Val Loss:0.15851257741451263\n",
      "Epoch 8[17/78] Val Loss:0.13113495707511902\n",
      "Epoch 8[18/78] Val Loss:0.21581190824508667\n",
      "Epoch 8[19/78] Val Loss:0.2905663549900055\n",
      "Epoch 8[20/78] Val Loss:0.19037172198295593\n",
      "Epoch 8[21/78] Val Loss:0.47254064679145813\n",
      "Epoch 8[22/78] Val Loss:0.6755169034004211\n",
      "Epoch 8[23/78] Val Loss:0.48440951108932495\n",
      "Epoch 8[24/78] Val Loss:0.34137317538261414\n",
      "Epoch 8[25/78] Val Loss:0.4490906596183777\n",
      "Epoch 8[26/78] Val Loss:0.3793444335460663\n",
      "Epoch 8[27/78] Val Loss:0.3680837154388428\n",
      "Epoch 8[28/78] Val Loss:0.32637327909469604\n",
      "Epoch 8[29/78] Val Loss:0.4201400876045227\n",
      "Epoch 8[30/78] Val Loss:1.5211683511734009\n",
      "Epoch 8[31/78] Val Loss:1.2629408836364746\n",
      "Epoch 8[32/78] Val Loss:1.035636305809021\n",
      "Epoch 8[33/78] Val Loss:0.37085065245628357\n",
      "Epoch 8[34/78] Val Loss:0.28872135281562805\n",
      "Epoch 8[35/78] Val Loss:0.2859202027320862\n",
      "Epoch 8[36/78] Val Loss:0.27532055974006653\n",
      "Epoch 8[37/78] Val Loss:0.3445224463939667\n",
      "Epoch 8[38/78] Val Loss:0.1885232776403427\n",
      "Epoch 8[39/78] Val Loss:0.22214780747890472\n",
      "Epoch 8[40/78] Val Loss:0.21159720420837402\n",
      "Epoch 8[41/78] Val Loss:0.22533509135246277\n",
      "Epoch 8[42/78] Val Loss:0.23398421704769135\n",
      "Epoch 8[43/78] Val Loss:0.13887105882167816\n",
      "Epoch 8[44/78] Val Loss:0.10293322056531906\n",
      "Epoch 8[45/78] Val Loss:0.06964676082134247\n",
      "Epoch 8[46/78] Val Loss:0.08748509734869003\n",
      "Epoch 8[47/78] Val Loss:0.10870567709207535\n",
      "Epoch 8[48/78] Val Loss:0.12515582144260406\n",
      "Epoch 8[49/78] Val Loss:0.08339311182498932\n",
      "Epoch 8[50/78] Val Loss:0.08376704156398773\n",
      "Epoch 8[51/78] Val Loss:0.08115462213754654\n",
      "Epoch 8[52/78] Val Loss:0.09821728616952896\n",
      "Epoch 8[53/78] Val Loss:0.07973530143499374\n",
      "Epoch 8[54/78] Val Loss:0.07183033227920532\n",
      "Epoch 8[55/78] Val Loss:0.10397446155548096\n",
      "Epoch 8[56/78] Val Loss:0.3627690076828003\n",
      "Epoch 8[57/78] Val Loss:0.32194945216178894\n",
      "Epoch 8[58/78] Val Loss:0.30579981207847595\n",
      "Epoch 8[59/78] Val Loss:0.32904601097106934\n",
      "Epoch 8[60/78] Val Loss:0.32062748074531555\n",
      "Epoch 8[61/78] Val Loss:0.3973396420478821\n",
      "Epoch 8[62/78] Val Loss:0.4247185289859772\n",
      "Epoch 8[63/78] Val Loss:0.23356519639492035\n",
      "Epoch 8[64/78] Val Loss:0.1329529583454132\n",
      "Epoch 8[65/78] Val Loss:0.1327843815088272\n",
      "Epoch 8[66/78] Val Loss:0.16434630751609802\n",
      "Epoch 8[67/78] Val Loss:0.15075016021728516\n",
      "Epoch 8[68/78] Val Loss:0.4225981533527374\n",
      "Epoch 8[69/78] Val Loss:0.4261544346809387\n",
      "Epoch 8[70/78] Val Loss:0.41940221190452576\n",
      "Epoch 8[71/78] Val Loss:0.31778278946876526\n",
      "Epoch 8[72/78] Val Loss:0.1693110466003418\n",
      "Epoch 8[73/78] Val Loss:0.142604798078537\n",
      "Epoch 8[74/78] Val Loss:0.38893020153045654\n",
      "Epoch 8[75/78] Val Loss:0.4981788992881775\n",
      "Epoch 8[76/78] Val Loss:0.4821861982345581\n",
      "Epoch 8[77/78] Val Loss:0.5308292508125305\n",
      "Epoch 8[78/78] Val Loss:0.593964159488678\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.92     15691\n",
      "           1       0.69      0.84      0.76      4309\n",
      "\n",
      "    accuracy                           0.88     20000\n",
      "   macro avg       0.82      0.87      0.84     20000\n",
      "weighted avg       0.90      0.88      0.89     20000\n",
      "\n",
      "Epoch 8: Train Loss 0.2612735409259796, Val Loss 0.29566325954137707, Train Time 790.3659949302673, Val Time 36.6709520816803\n",
      "Epoch 9[0/625] Time:0.689, Train Loss:0.26851773262023926\n",
      "Epoch 9[1/625] Time:0.718, Train Loss:0.2286994606256485\n",
      "Epoch 9[2/625] Time:0.693, Train Loss:0.29307863116264343\n",
      "Epoch 9[3/625] Time:0.693, Train Loss:0.26539647579193115\n",
      "Epoch 9[4/625] Time:0.693, Train Loss:0.25786730647087097\n",
      "Epoch 9[5/625] Time:0.695, Train Loss:0.3230118155479431\n",
      "Epoch 9[6/625] Time:0.693, Train Loss:0.2696057856082916\n",
      "Epoch 9[7/625] Time:0.693, Train Loss:0.28987985849380493\n",
      "Epoch 9[8/625] Time:0.693, Train Loss:0.293342649936676\n",
      "Epoch 9[9/625] Time:0.699, Train Loss:0.2381536066532135\n",
      "Epoch 9[10/625] Time:0.731, Train Loss:0.20728546380996704\n",
      "Epoch 9[11/625] Time:0.692, Train Loss:0.2611376643180847\n",
      "Epoch 9[12/625] Time:0.693, Train Loss:0.20864328742027283\n",
      "Epoch 9[13/625] Time:0.693, Train Loss:0.2180594801902771\n",
      "Epoch 9[14/625] Time:0.694, Train Loss:0.29523828625679016\n",
      "Epoch 9[15/625] Time:0.693, Train Loss:0.21915429830551147\n",
      "Epoch 9[16/625] Time:0.693, Train Loss:0.2380206435918808\n",
      "Epoch 9[17/625] Time:0.693, Train Loss:0.28998351097106934\n",
      "Epoch 9[18/625] Time:0.693, Train Loss:0.318205326795578\n",
      "Epoch 9[19/625] Time:0.692, Train Loss:0.2990227937698364\n",
      "Epoch 9[20/625] Time:0.693, Train Loss:0.25968682765960693\n",
      "Epoch 9[21/625] Time:0.694, Train Loss:0.20404468476772308\n",
      "Epoch 9[22/625] Time:0.695, Train Loss:0.18589964509010315\n",
      "Epoch 9[23/625] Time:0.707, Train Loss:0.23370200395584106\n",
      "Epoch 9[24/625] Time:0.703, Train Loss:0.35416775941848755\n",
      "Epoch 9[25/625] Time:0.703, Train Loss:0.2501733601093292\n",
      "Epoch 9[26/625] Time:0.704, Train Loss:0.25680360198020935\n",
      "Epoch 9[27/625] Time:0.704, Train Loss:0.3330470323562622\n",
      "Epoch 9[28/625] Time:0.702, Train Loss:0.21774965524673462\n",
      "Epoch 9[29/625] Time:0.705, Train Loss:0.3373842239379883\n",
      "Epoch 9[30/625] Time:0.703, Train Loss:0.24007642269134521\n",
      "Epoch 9[31/625] Time:0.703, Train Loss:0.22833319008350372\n",
      "Epoch 9[32/625] Time:0.702, Train Loss:0.242380291223526\n",
      "Epoch 9[33/625] Time:0.702, Train Loss:0.2065800577402115\n",
      "Epoch 9[34/625] Time:0.702, Train Loss:0.3165375292301178\n",
      "Epoch 9[35/625] Time:0.702, Train Loss:0.3204602301120758\n",
      "Epoch 9[36/625] Time:0.702, Train Loss:0.2565615177154541\n",
      "Epoch 9[37/625] Time:0.703, Train Loss:0.3447825610637665\n",
      "Epoch 9[38/625] Time:0.703, Train Loss:0.21178023517131805\n",
      "Epoch 9[39/625] Time:0.704, Train Loss:0.324772447347641\n",
      "Epoch 9[40/625] Time:0.705, Train Loss:0.2333662211894989\n",
      "Epoch 9[41/625] Time:0.703, Train Loss:0.2540804445743561\n",
      "Epoch 9[42/625] Time:0.703, Train Loss:0.20439180731773376\n",
      "Epoch 9[43/625] Time:0.705, Train Loss:0.33218270540237427\n",
      "Epoch 9[44/625] Time:0.704, Train Loss:0.30988526344299316\n",
      "Epoch 9[45/625] Time:0.703, Train Loss:0.31749778985977173\n",
      "Epoch 9[46/625] Time:0.702, Train Loss:0.22042766213417053\n",
      "Epoch 9[47/625] Time:0.704, Train Loss:0.25425300002098083\n",
      "Epoch 9[48/625] Time:0.703, Train Loss:0.24289093911647797\n",
      "Epoch 9[49/625] Time:0.704, Train Loss:0.23979227244853973\n",
      "Epoch 9[50/625] Time:0.703, Train Loss:0.31071266531944275\n",
      "Epoch 9[51/625] Time:0.702, Train Loss:0.18995721638202667\n",
      "Epoch 9[52/625] Time:0.704, Train Loss:0.21708182990550995\n",
      "Epoch 9[53/625] Time:0.704, Train Loss:0.23328256607055664\n",
      "Epoch 9[54/625] Time:0.705, Train Loss:0.20282484591007233\n",
      "Epoch 9[55/625] Time:0.703, Train Loss:0.27941685914993286\n",
      "Epoch 9[56/625] Time:0.703, Train Loss:0.21346034109592438\n",
      "Epoch 9[57/625] Time:0.703, Train Loss:0.2747873067855835\n",
      "Epoch 9[58/625] Time:0.703, Train Loss:0.336288183927536\n",
      "Epoch 9[59/625] Time:0.737, Train Loss:0.3068932592868805\n",
      "Epoch 9[60/625] Time:0.723, Train Loss:0.22303852438926697\n",
      "Epoch 9[61/625] Time:0.704, Train Loss:0.20539996027946472\n",
      "Epoch 9[62/625] Time:0.732, Train Loss:0.2604709267616272\n",
      "Epoch 9[63/625] Time:0.72, Train Loss:0.23506955802440643\n",
      "Epoch 9[64/625] Time:0.707, Train Loss:0.22882992029190063\n",
      "Epoch 9[65/625] Time:0.704, Train Loss:0.2690601944923401\n",
      "Epoch 9[66/625] Time:0.704, Train Loss:0.2780565023422241\n",
      "Epoch 9[67/625] Time:0.703, Train Loss:0.2731820046901703\n",
      "Epoch 9[68/625] Time:0.703, Train Loss:0.23413372039794922\n",
      "Epoch 9[69/625] Time:0.704, Train Loss:0.23218855261802673\n",
      "Epoch 9[70/625] Time:0.704, Train Loss:0.25547561049461365\n",
      "Epoch 9[71/625] Time:0.704, Train Loss:0.2291395366191864\n",
      "Epoch 9[72/625] Time:0.703, Train Loss:0.291424036026001\n",
      "Epoch 9[73/625] Time:0.703, Train Loss:0.2980455756187439\n",
      "Epoch 9[74/625] Time:0.704, Train Loss:0.3108813464641571\n",
      "Epoch 9[75/625] Time:0.703, Train Loss:0.2361116111278534\n",
      "Epoch 9[76/625] Time:0.703, Train Loss:0.29732197523117065\n",
      "Epoch 9[77/625] Time:0.703, Train Loss:0.22997517883777618\n",
      "Epoch 9[78/625] Time:0.704, Train Loss:0.34704601764678955\n",
      "Epoch 9[79/625] Time:0.703, Train Loss:0.286980539560318\n",
      "Epoch 9[80/625] Time:0.704, Train Loss:0.2825734615325928\n",
      "Epoch 9[81/625] Time:0.704, Train Loss:0.22294782102108002\n",
      "Epoch 9[82/625] Time:0.704, Train Loss:0.24723142385482788\n",
      "Epoch 9[83/625] Time:0.704, Train Loss:0.23488642275333405\n",
      "Epoch 9[84/625] Time:0.703, Train Loss:0.26676884293556213\n",
      "Epoch 9[85/625] Time:0.703, Train Loss:0.2654588222503662\n",
      "Epoch 9[86/625] Time:0.705, Train Loss:0.26994410157203674\n",
      "Epoch 9[87/625] Time:0.703, Train Loss:0.18471118807792664\n",
      "Epoch 9[88/625] Time:0.704, Train Loss:0.18919460475444794\n",
      "Epoch 9[89/625] Time:0.703, Train Loss:0.18623356521129608\n",
      "Epoch 9[90/625] Time:0.703, Train Loss:0.25516894459724426\n",
      "Epoch 9[91/625] Time:0.703, Train Loss:0.23488600552082062\n",
      "Epoch 9[92/625] Time:0.703, Train Loss:0.2404284030199051\n",
      "Epoch 9[93/625] Time:0.702, Train Loss:0.21378827095031738\n",
      "Epoch 9[94/625] Time:0.705, Train Loss:0.3282192349433899\n",
      "Epoch 9[95/625] Time:0.702, Train Loss:0.2539593577384949\n",
      "Epoch 9[96/625] Time:0.705, Train Loss:0.18342295289039612\n",
      "Epoch 9[97/625] Time:0.703, Train Loss:0.23999783396720886\n",
      "Epoch 9[98/625] Time:0.704, Train Loss:0.2573283612728119\n",
      "Epoch 9[99/625] Time:0.703, Train Loss:0.2561662793159485\n",
      "Epoch 9[100/625] Time:0.703, Train Loss:0.25428852438926697\n",
      "Epoch 9[101/625] Time:0.703, Train Loss:0.26836323738098145\n",
      "Epoch 9[102/625] Time:0.703, Train Loss:0.19726134836673737\n",
      "Epoch 9[103/625] Time:0.703, Train Loss:0.2367318719625473\n",
      "Epoch 9[104/625] Time:0.704, Train Loss:0.3104795515537262\n",
      "Epoch 9[105/625] Time:0.703, Train Loss:0.2288011610507965\n",
      "Epoch 9[106/625] Time:0.703, Train Loss:0.23420152068138123\n",
      "Epoch 9[107/625] Time:0.703, Train Loss:0.22977423667907715\n",
      "Epoch 9[108/625] Time:0.702, Train Loss:0.2688203752040863\n",
      "Epoch 9[109/625] Time:0.704, Train Loss:0.2847307622432709\n",
      "Epoch 9[110/625] Time:0.705, Train Loss:0.24582642316818237\n",
      "Epoch 9[111/625] Time:0.703, Train Loss:0.3044397234916687\n",
      "Epoch 9[112/625] Time:0.704, Train Loss:0.26709452271461487\n",
      "Epoch 9[113/625] Time:0.704, Train Loss:0.2813354432582855\n",
      "Epoch 9[114/625] Time:0.702, Train Loss:0.3058534264564514\n",
      "Epoch 9[115/625] Time:0.702, Train Loss:0.2255728840827942\n",
      "Epoch 9[116/625] Time:0.704, Train Loss:0.2565379738807678\n",
      "Epoch 9[117/625] Time:0.704, Train Loss:0.22838319838047028\n",
      "Epoch 9[118/625] Time:0.704, Train Loss:0.25448480248451233\n",
      "Epoch 9[119/625] Time:0.703, Train Loss:0.2614719569683075\n",
      "Epoch 9[120/625] Time:0.704, Train Loss:0.2380649894475937\n",
      "Epoch 9[121/625] Time:0.703, Train Loss:0.3137354552745819\n",
      "Epoch 9[122/625] Time:0.705, Train Loss:0.23327895998954773\n",
      "Epoch 9[123/625] Time:0.703, Train Loss:0.22415941953659058\n",
      "Epoch 9[124/625] Time:0.704, Train Loss:0.28688809275627136\n",
      "Epoch 9[125/625] Time:0.703, Train Loss:0.23375724256038666\n",
      "Epoch 9[126/625] Time:0.703, Train Loss:0.2377808392047882\n",
      "Epoch 9[127/625] Time:0.702, Train Loss:0.36117246747016907\n",
      "Epoch 9[128/625] Time:0.706, Train Loss:0.2454882264137268\n",
      "Epoch 9[129/625] Time:0.704, Train Loss:0.2781548500061035\n",
      "Epoch 9[130/625] Time:0.703, Train Loss:0.24998407065868378\n",
      "Epoch 9[131/625] Time:0.704, Train Loss:0.3051586449146271\n",
      "Epoch 9[132/625] Time:0.704, Train Loss:0.30406901240348816\n",
      "Epoch 9[133/625] Time:0.704, Train Loss:0.18817338347434998\n",
      "Epoch 9[134/625] Time:0.704, Train Loss:0.25550657510757446\n",
      "Epoch 9[135/625] Time:0.704, Train Loss:0.20272675156593323\n",
      "Epoch 9[136/625] Time:0.703, Train Loss:0.24288849532604218\n",
      "Epoch 9[137/625] Time:0.703, Train Loss:0.2916024625301361\n",
      "Epoch 9[138/625] Time:0.702, Train Loss:0.2159639298915863\n",
      "Epoch 9[139/625] Time:0.703, Train Loss:0.24035680294036865\n",
      "Epoch 9[140/625] Time:0.703, Train Loss:0.21356341242790222\n",
      "Epoch 9[141/625] Time:0.703, Train Loss:0.23904334008693695\n",
      "Epoch 9[142/625] Time:0.703, Train Loss:0.20277567207813263\n",
      "Epoch 9[143/625] Time:0.702, Train Loss:0.24048180878162384\n",
      "Epoch 9[144/625] Time:0.705, Train Loss:0.33744335174560547\n",
      "Epoch 9[145/625] Time:0.702, Train Loss:0.2902248203754425\n",
      "Epoch 9[146/625] Time:0.704, Train Loss:0.26813673973083496\n",
      "Epoch 9[147/625] Time:0.704, Train Loss:0.2685169279575348\n",
      "Epoch 9[148/625] Time:0.704, Train Loss:0.24640122056007385\n",
      "Epoch 9[149/625] Time:0.704, Train Loss:0.22283995151519775\n",
      "Epoch 9[150/625] Time:0.704, Train Loss:0.23287183046340942\n",
      "Epoch 9[151/625] Time:0.703, Train Loss:0.3587955832481384\n",
      "Epoch 9[152/625] Time:0.704, Train Loss:0.25369560718536377\n",
      "Epoch 9[153/625] Time:0.701, Train Loss:0.24914108216762543\n",
      "Epoch 9[154/625] Time:0.703, Train Loss:0.36542534828186035\n",
      "Epoch 9[155/625] Time:0.704, Train Loss:0.2578602433204651\n",
      "Epoch 9[156/625] Time:0.704, Train Loss:0.23106347024440765\n",
      "Epoch 9[157/625] Time:0.704, Train Loss:0.3156791925430298\n",
      "Epoch 9[158/625] Time:0.703, Train Loss:0.26279962062835693\n",
      "Epoch 9[159/625] Time:0.703, Train Loss:0.2704364061355591\n",
      "Epoch 9[160/625] Time:0.704, Train Loss:0.23730380833148956\n",
      "Epoch 9[161/625] Time:0.704, Train Loss:0.25029146671295166\n",
      "Epoch 9[162/625] Time:0.705, Train Loss:0.24368657171726227\n",
      "Epoch 9[163/625] Time:0.704, Train Loss:0.29423317313194275\n",
      "Epoch 9[164/625] Time:0.704, Train Loss:0.18148332834243774\n",
      "Epoch 9[165/625] Time:0.704, Train Loss:0.2688838839530945\n",
      "Epoch 9[166/625] Time:0.705, Train Loss:0.28801819682121277\n",
      "Epoch 9[167/625] Time:0.705, Train Loss:0.28026822209358215\n",
      "Epoch 9[168/625] Time:0.708, Train Loss:0.22063419222831726\n",
      "Epoch 9[169/625] Time:0.704, Train Loss:0.2814823389053345\n",
      "Epoch 9[170/625] Time:0.703, Train Loss:0.21745480597019196\n",
      "Epoch 9[171/625] Time:0.704, Train Loss:0.23889774084091187\n",
      "Epoch 9[172/625] Time:0.706, Train Loss:0.29204392433166504\n",
      "Epoch 9[173/625] Time:0.704, Train Loss:0.2930600941181183\n",
      "Epoch 9[174/625] Time:0.706, Train Loss:0.3285866975784302\n",
      "Epoch 9[175/625] Time:0.704, Train Loss:0.2800169289112091\n",
      "Epoch 9[176/625] Time:0.709, Train Loss:0.2710724174976349\n",
      "Epoch 9[177/625] Time:0.703, Train Loss:0.29565829038619995\n",
      "Epoch 9[178/625] Time:0.703, Train Loss:0.21186582744121552\n",
      "Epoch 9[179/625] Time:0.703, Train Loss:0.2976427972316742\n",
      "Epoch 9[180/625] Time:0.702, Train Loss:0.20052573084831238\n",
      "Epoch 9[181/625] Time:0.702, Train Loss:0.30184218287467957\n",
      "Epoch 9[182/625] Time:0.703, Train Loss:0.23330055177211761\n",
      "Epoch 9[183/625] Time:0.702, Train Loss:0.35183364152908325\n",
      "Epoch 9[184/625] Time:0.706, Train Loss:0.29773610830307007\n",
      "Epoch 9[185/625] Time:0.702, Train Loss:0.29187336564064026\n",
      "Epoch 9[186/625] Time:0.703, Train Loss:0.23839907348155975\n",
      "Epoch 9[187/625] Time:0.703, Train Loss:0.2766815721988678\n",
      "Epoch 9[188/625] Time:0.704, Train Loss:0.20421314239501953\n",
      "Epoch 9[189/625] Time:0.703, Train Loss:0.2949453890323639\n",
      "Epoch 9[190/625] Time:0.705, Train Loss:0.22833772003650665\n",
      "Epoch 9[191/625] Time:0.703, Train Loss:0.28952381014823914\n",
      "Epoch 9[192/625] Time:0.705, Train Loss:0.23523074388504028\n",
      "Epoch 9[193/625] Time:0.703, Train Loss:0.20225834846496582\n",
      "Epoch 9[194/625] Time:0.705, Train Loss:0.20764613151550293\n",
      "Epoch 9[195/625] Time:0.702, Train Loss:0.2797248363494873\n",
      "Epoch 9[196/625] Time:0.703, Train Loss:0.2675834000110626\n",
      "Epoch 9[197/625] Time:0.703, Train Loss:0.26983463764190674\n",
      "Epoch 9[198/625] Time:0.702, Train Loss:0.1825287640094757\n",
      "Epoch 9[199/625] Time:0.704, Train Loss:0.2340620905160904\n",
      "Epoch 9[200/625] Time:0.703, Train Loss:0.22479698061943054\n",
      "Epoch 9[201/625] Time:0.742, Train Loss:0.24796056747436523\n",
      "Epoch 9[202/625] Time:0.732, Train Loss:0.20788173377513885\n",
      "Epoch 9[203/625] Time:0.702, Train Loss:0.2374371737241745\n",
      "Epoch 9[204/625] Time:0.703, Train Loss:0.261388897895813\n",
      "Epoch 9[205/625] Time:0.703, Train Loss:0.2585761845111847\n",
      "Epoch 9[206/625] Time:0.703, Train Loss:0.17780666053295135\n",
      "Epoch 9[207/625] Time:0.702, Train Loss:0.217977836728096\n",
      "Epoch 9[208/625] Time:0.706, Train Loss:0.22269193828105927\n",
      "Epoch 9[209/625] Time:0.703, Train Loss:0.2325979620218277\n",
      "Epoch 9[210/625] Time:0.703, Train Loss:0.22578084468841553\n",
      "Epoch 9[211/625] Time:0.703, Train Loss:0.22390350699424744\n",
      "Epoch 9[212/625] Time:0.704, Train Loss:0.33442506194114685\n",
      "Epoch 9[213/625] Time:0.705, Train Loss:0.27323776483535767\n",
      "Epoch 9[214/625] Time:0.705, Train Loss:0.2728670835494995\n",
      "Epoch 9[215/625] Time:0.703, Train Loss:0.29631784558296204\n",
      "Epoch 9[216/625] Time:0.705, Train Loss:0.26458966732025146\n",
      "Epoch 9[217/625] Time:0.703, Train Loss:0.22012554109096527\n",
      "Epoch 9[218/625] Time:0.703, Train Loss:0.2951809763908386\n",
      "Epoch 9[219/625] Time:0.703, Train Loss:0.2180139720439911\n",
      "Epoch 9[220/625] Time:0.704, Train Loss:0.24803437292575836\n",
      "Epoch 9[221/625] Time:0.704, Train Loss:0.25552234053611755\n",
      "Epoch 9[222/625] Time:0.703, Train Loss:0.23316390812397003\n",
      "Epoch 9[223/625] Time:0.703, Train Loss:0.28419220447540283\n",
      "Epoch 9[224/625] Time:0.703, Train Loss:0.23531939089298248\n",
      "Epoch 9[225/625] Time:0.739, Train Loss:0.31661906838417053\n",
      "Epoch 9[226/625] Time:0.692, Train Loss:0.20893853902816772\n",
      "Epoch 9[227/625] Time:0.694, Train Loss:0.3076770603656769\n",
      "Epoch 9[228/625] Time:0.703, Train Loss:0.25414007902145386\n",
      "Epoch 9[229/625] Time:0.694, Train Loss:0.2641981244087219\n",
      "Epoch 9[230/625] Time:0.692, Train Loss:0.23959724605083466\n",
      "Epoch 9[231/625] Time:0.692, Train Loss:0.21744728088378906\n",
      "Epoch 9[232/625] Time:0.696, Train Loss:0.2618047893047333\n",
      "Epoch 9[233/625] Time:0.695, Train Loss:0.27726006507873535\n",
      "Epoch 9[234/625] Time:0.693, Train Loss:0.19462594389915466\n",
      "Epoch 9[235/625] Time:0.692, Train Loss:0.16150204837322235\n",
      "Epoch 9[236/625] Time:0.694, Train Loss:0.26575803756713867\n",
      "Epoch 9[237/625] Time:0.692, Train Loss:0.2715066373348236\n",
      "Epoch 9[238/625] Time:0.738, Train Loss:0.2251824289560318\n",
      "Epoch 9[239/625] Time:0.725, Train Loss:0.2876134514808655\n",
      "Epoch 9[240/625] Time:0.697, Train Loss:0.2240564376115799\n",
      "Epoch 9[241/625] Time:0.702, Train Loss:0.23368242383003235\n",
      "Epoch 9[242/625] Time:0.703, Train Loss:0.2550911605358124\n",
      "Epoch 9[243/625] Time:0.703, Train Loss:0.25912201404571533\n",
      "Epoch 9[244/625] Time:0.702, Train Loss:0.21575172245502472\n",
      "Epoch 9[245/625] Time:0.703, Train Loss:0.22211138904094696\n",
      "Epoch 9[246/625] Time:0.703, Train Loss:0.2385447919368744\n",
      "Epoch 9[247/625] Time:0.702, Train Loss:0.25598734617233276\n",
      "Epoch 9[248/625] Time:0.705, Train Loss:0.23821137845516205\n",
      "Epoch 9[249/625] Time:0.702, Train Loss:0.21971361339092255\n",
      "Epoch 9[250/625] Time:0.707, Train Loss:0.2503761351108551\n",
      "Epoch 9[251/625] Time:0.706, Train Loss:0.2311447262763977\n",
      "Epoch 9[252/625] Time:0.704, Train Loss:0.2395455688238144\n",
      "Epoch 9[253/625] Time:0.708, Train Loss:0.23910516500473022\n",
      "Epoch 9[254/625] Time:0.704, Train Loss:0.25173941254615784\n",
      "Epoch 9[255/625] Time:0.705, Train Loss:0.2987174391746521\n",
      "Epoch 9[256/625] Time:0.699, Train Loss:0.26107892394065857\n",
      "Epoch 9[257/625] Time:0.704, Train Loss:0.17752216756343842\n",
      "Epoch 9[258/625] Time:0.704, Train Loss:0.2289968729019165\n",
      "Epoch 9[259/625] Time:0.703, Train Loss:0.2430693358182907\n",
      "Epoch 9[260/625] Time:0.704, Train Loss:0.2462073713541031\n",
      "Epoch 9[261/625] Time:0.704, Train Loss:0.24163885414600372\n",
      "Epoch 9[262/625] Time:0.722, Train Loss:0.26821017265319824\n",
      "Epoch 9[263/625] Time:0.693, Train Loss:0.22998645901679993\n",
      "Epoch 9[264/625] Time:0.694, Train Loss:0.24882151186466217\n",
      "Epoch 9[265/625] Time:0.693, Train Loss:0.21315015852451324\n",
      "Epoch 9[266/625] Time:0.694, Train Loss:0.2591058611869812\n",
      "Epoch 9[267/625] Time:0.694, Train Loss:0.2688358426094055\n",
      "Epoch 9[268/625] Time:0.694, Train Loss:0.27212953567504883\n",
      "Epoch 9[269/625] Time:0.695, Train Loss:0.24042990803718567\n",
      "Epoch 9[270/625] Time:0.708, Train Loss:0.22874508798122406\n",
      "Epoch 9[271/625] Time:0.706, Train Loss:0.2551906108856201\n",
      "Epoch 9[272/625] Time:0.704, Train Loss:0.24488846957683563\n",
      "Epoch 9[273/625] Time:0.704, Train Loss:0.22803783416748047\n",
      "Epoch 9[274/625] Time:0.716, Train Loss:0.22664479911327362\n",
      "Epoch 9[275/625] Time:0.703, Train Loss:0.1999787539243698\n",
      "Epoch 9[276/625] Time:0.706, Train Loss:0.21337692439556122\n",
      "Epoch 9[277/625] Time:0.703, Train Loss:0.24917779862880707\n",
      "Epoch 9[278/625] Time:0.703, Train Loss:0.29349809885025024\n",
      "Epoch 9[279/625] Time:0.704, Train Loss:0.32669705152511597\n",
      "Epoch 9[280/625] Time:0.703, Train Loss:0.21916966140270233\n",
      "Epoch 9[281/625] Time:0.702, Train Loss:0.2925425171852112\n",
      "Epoch 9[282/625] Time:0.702, Train Loss:0.23832814395427704\n",
      "Epoch 9[283/625] Time:0.702, Train Loss:0.27720990777015686\n",
      "Epoch 9[284/625] Time:0.703, Train Loss:0.3015107214450836\n",
      "Epoch 9[285/625] Time:0.703, Train Loss:0.23464235663414001\n",
      "Epoch 9[286/625] Time:0.703, Train Loss:0.33170223236083984\n",
      "Epoch 9[287/625] Time:0.701, Train Loss:0.18051983416080475\n",
      "Epoch 9[288/625] Time:0.704, Train Loss:0.24968178570270538\n",
      "Epoch 9[289/625] Time:0.703, Train Loss:0.24127429723739624\n",
      "Epoch 9[290/625] Time:0.704, Train Loss:0.2800821363925934\n",
      "Epoch 9[291/625] Time:0.724, Train Loss:0.23063915967941284\n",
      "Epoch 9[292/625] Time:0.703, Train Loss:0.22652582824230194\n",
      "Epoch 9[293/625] Time:0.703, Train Loss:0.2783736288547516\n",
      "Epoch 9[294/625] Time:0.704, Train Loss:0.25232017040252686\n",
      "Epoch 9[295/625] Time:0.703, Train Loss:0.31689274311065674\n",
      "Epoch 9[296/625] Time:0.704, Train Loss:0.23493815958499908\n",
      "Epoch 9[297/625] Time:0.703, Train Loss:0.32692551612854004\n",
      "Epoch 9[298/625] Time:0.703, Train Loss:0.23094983398914337\n",
      "Epoch 9[299/625] Time:0.705, Train Loss:0.3468577265739441\n",
      "Epoch 9[300/625] Time:0.745, Train Loss:0.24812084436416626\n",
      "Epoch 9[301/625] Time:0.69, Train Loss:0.2750546932220459\n",
      "Epoch 9[302/625] Time:0.693, Train Loss:0.20560409128665924\n",
      "Epoch 9[303/625] Time:0.695, Train Loss:0.22563301026821136\n",
      "Epoch 9[304/625] Time:0.694, Train Loss:0.2594722807407379\n",
      "Epoch 9[305/625] Time:0.697, Train Loss:0.27172333002090454\n",
      "Epoch 9[306/625] Time:0.694, Train Loss:0.28751853108406067\n",
      "Epoch 9[307/625] Time:0.703, Train Loss:0.1832149624824524\n",
      "Epoch 9[308/625] Time:0.704, Train Loss:0.24598626792430878\n",
      "Epoch 9[309/625] Time:0.703, Train Loss:0.23569703102111816\n",
      "Epoch 9[310/625] Time:0.703, Train Loss:0.2844988703727722\n",
      "Epoch 9[311/625] Time:0.703, Train Loss:0.1983782947063446\n",
      "Epoch 9[312/625] Time:0.704, Train Loss:0.3185388147830963\n",
      "Epoch 9[313/625] Time:0.703, Train Loss:0.22533711791038513\n",
      "Epoch 9[314/625] Time:0.703, Train Loss:0.23841747641563416\n",
      "Epoch 9[315/625] Time:0.703, Train Loss:0.23748411238193512\n",
      "Epoch 9[316/625] Time:0.704, Train Loss:0.3300028443336487\n",
      "Epoch 9[317/625] Time:0.73, Train Loss:0.2082442045211792\n",
      "Epoch 9[318/625] Time:0.692, Train Loss:0.22385212779045105\n",
      "Epoch 9[319/625] Time:0.695, Train Loss:0.26385825872421265\n",
      "Epoch 9[320/625] Time:0.694, Train Loss:0.2260298877954483\n",
      "Epoch 9[321/625] Time:0.696, Train Loss:0.30035242438316345\n",
      "Epoch 9[322/625] Time:0.696, Train Loss:0.2707493007183075\n",
      "Epoch 9[323/625] Time:0.716, Train Loss:0.3116284906864166\n",
      "Epoch 9[324/625] Time:0.703, Train Loss:0.22700420022010803\n",
      "Epoch 9[325/625] Time:0.703, Train Loss:0.21048282086849213\n",
      "Epoch 9[326/625] Time:0.704, Train Loss:0.32429391145706177\n",
      "Epoch 9[327/625] Time:0.704, Train Loss:0.22623607516288757\n",
      "Epoch 9[328/625] Time:0.703, Train Loss:0.2365424931049347\n",
      "Epoch 9[329/625] Time:0.693, Train Loss:0.27749770879745483\n",
      "Epoch 9[330/625] Time:0.693, Train Loss:0.24895402789115906\n",
      "Epoch 9[331/625] Time:0.694, Train Loss:0.260987788438797\n",
      "Epoch 9[332/625] Time:0.694, Train Loss:0.19504551589488983\n",
      "Epoch 9[333/625] Time:0.693, Train Loss:0.2870006859302521\n",
      "Epoch 9[334/625] Time:0.697, Train Loss:0.2524239718914032\n",
      "Epoch 9[335/625] Time:0.694, Train Loss:0.16456885635852814\n",
      "Epoch 9[336/625] Time:0.694, Train Loss:0.3107869327068329\n",
      "Epoch 9[337/625] Time:0.694, Train Loss:0.274202436208725\n",
      "Epoch 9[338/625] Time:0.694, Train Loss:0.2663317620754242\n",
      "Epoch 9[339/625] Time:0.694, Train Loss:0.20462438464164734\n",
      "Epoch 9[340/625] Time:0.746, Train Loss:0.29855865240097046\n",
      "Epoch 9[341/625] Time:0.703, Train Loss:0.27037766575813293\n",
      "Epoch 9[342/625] Time:0.704, Train Loss:0.20040538907051086\n",
      "Epoch 9[343/625] Time:0.707, Train Loss:0.32510221004486084\n",
      "Epoch 9[344/625] Time:0.703, Train Loss:0.2873617112636566\n",
      "Epoch 9[345/625] Time:0.702, Train Loss:0.18760451674461365\n",
      "Epoch 9[346/625] Time:0.704, Train Loss:0.29601967334747314\n",
      "Epoch 9[347/625] Time:0.705, Train Loss:0.26360803842544556\n",
      "Epoch 9[348/625] Time:0.706, Train Loss:0.23606719076633453\n",
      "Epoch 9[349/625] Time:0.704, Train Loss:0.19990698993206024\n",
      "Epoch 9[350/625] Time:0.7, Train Loss:0.19649477303028107\n",
      "Epoch 9[351/625] Time:0.704, Train Loss:0.2607834041118622\n",
      "Epoch 9[352/625] Time:0.719, Train Loss:0.25203844904899597\n",
      "Epoch 9[353/625] Time:0.702, Train Loss:0.34567320346832275\n",
      "Epoch 9[354/625] Time:0.695, Train Loss:0.20591430366039276\n",
      "Epoch 9[355/625] Time:0.704, Train Loss:0.2144407033920288\n",
      "Epoch 9[356/625] Time:0.702, Train Loss:0.1735403835773468\n",
      "Epoch 9[357/625] Time:0.703, Train Loss:0.2394428849220276\n",
      "Epoch 9[358/625] Time:0.703, Train Loss:0.26305243372917175\n",
      "Epoch 9[359/625] Time:0.703, Train Loss:0.2588149309158325\n",
      "Epoch 9[360/625] Time:0.703, Train Loss:0.19472910463809967\n",
      "Epoch 9[361/625] Time:0.704, Train Loss:0.26336830854415894\n",
      "Epoch 9[362/625] Time:0.704, Train Loss:0.2712774872779846\n",
      "Epoch 9[363/625] Time:0.704, Train Loss:0.17356134951114655\n",
      "Epoch 9[364/625] Time:0.705, Train Loss:0.30799153447151184\n",
      "Epoch 9[365/625] Time:0.703, Train Loss:0.30168965458869934\n",
      "Epoch 9[366/625] Time:0.704, Train Loss:0.235752671957016\n",
      "Epoch 9[367/625] Time:0.703, Train Loss:0.2946609556674957\n",
      "Epoch 9[368/625] Time:0.705, Train Loss:0.26037195324897766\n",
      "Epoch 9[369/625] Time:0.704, Train Loss:0.23472285270690918\n",
      "Epoch 9[370/625] Time:0.703, Train Loss:0.23293159902095795\n",
      "Epoch 9[371/625] Time:0.704, Train Loss:0.2428789734840393\n",
      "Epoch 9[372/625] Time:0.702, Train Loss:0.2896929681301117\n",
      "Epoch 9[373/625] Time:0.704, Train Loss:0.2631465792655945\n",
      "Epoch 9[374/625] Time:0.703, Train Loss:0.201646089553833\n",
      "Epoch 9[375/625] Time:0.703, Train Loss:0.25641611218452454\n",
      "Epoch 9[376/625] Time:0.703, Train Loss:0.24404597282409668\n",
      "Epoch 9[377/625] Time:0.703, Train Loss:0.26060181856155396\n",
      "Epoch 9[378/625] Time:0.703, Train Loss:0.3243275284767151\n",
      "Epoch 9[379/625] Time:0.739, Train Loss:0.30343809723854065\n",
      "Epoch 9[380/625] Time:0.696, Train Loss:0.222075417637825\n",
      "Epoch 9[381/625] Time:0.695, Train Loss:0.25315067172050476\n",
      "Epoch 9[382/625] Time:0.694, Train Loss:0.24258315563201904\n",
      "Epoch 9[383/625] Time:0.693, Train Loss:0.26521584391593933\n",
      "Epoch 9[384/625] Time:0.693, Train Loss:0.308460533618927\n",
      "Epoch 9[385/625] Time:0.695, Train Loss:0.22341392934322357\n",
      "Epoch 9[386/625] Time:0.693, Train Loss:0.2430656999349594\n",
      "Epoch 9[387/625] Time:0.694, Train Loss:0.25024592876434326\n",
      "Epoch 9[388/625] Time:0.704, Train Loss:0.2076168954372406\n",
      "Epoch 9[389/625] Time:0.708, Train Loss:0.22636276483535767\n",
      "Epoch 9[390/625] Time:0.703, Train Loss:0.2614442706108093\n",
      "Epoch 9[391/625] Time:0.703, Train Loss:0.16946569085121155\n",
      "Epoch 9[392/625] Time:0.704, Train Loss:0.2096879631280899\n",
      "Epoch 9[393/625] Time:0.703, Train Loss:0.19500626623630524\n",
      "Epoch 9[394/625] Time:0.703, Train Loss:0.3400960862636566\n",
      "Epoch 9[395/625] Time:0.703, Train Loss:0.2449875771999359\n",
      "Epoch 9[396/625] Time:0.703, Train Loss:0.2744145393371582\n",
      "Epoch 9[397/625] Time:0.704, Train Loss:0.2640867829322815\n",
      "Epoch 9[398/625] Time:0.703, Train Loss:0.30020636320114136\n",
      "Epoch 9[399/625] Time:0.703, Train Loss:0.3094789683818817\n",
      "Epoch 9[400/625] Time:0.704, Train Loss:0.1954091489315033\n",
      "Epoch 9[401/625] Time:0.702, Train Loss:0.22345948219299316\n",
      "Epoch 9[402/625] Time:0.706, Train Loss:0.18613307178020477\n",
      "Epoch 9[403/625] Time:0.703, Train Loss:0.376781165599823\n",
      "Epoch 9[404/625] Time:0.705, Train Loss:0.29486024379730225\n",
      "Epoch 9[405/625] Time:0.709, Train Loss:0.30343425273895264\n",
      "Epoch 9[406/625] Time:0.703, Train Loss:0.24146632850170135\n",
      "Epoch 9[407/625] Time:0.703, Train Loss:0.24331077933311462\n",
      "Epoch 9[408/625] Time:0.705, Train Loss:0.28587448596954346\n",
      "Epoch 9[409/625] Time:0.704, Train Loss:0.19477812945842743\n",
      "Epoch 9[410/625] Time:0.702, Train Loss:0.1538340151309967\n",
      "Epoch 9[411/625] Time:0.703, Train Loss:0.1932523250579834\n",
      "Epoch 9[412/625] Time:0.704, Train Loss:0.2566591501235962\n",
      "Epoch 9[413/625] Time:0.709, Train Loss:0.22049519419670105\n",
      "Epoch 9[414/625] Time:0.703, Train Loss:0.23382212221622467\n",
      "Epoch 9[415/625] Time:0.703, Train Loss:0.28527840971946716\n",
      "Epoch 9[416/625] Time:0.702, Train Loss:0.29389163851737976\n",
      "Epoch 9[417/625] Time:0.703, Train Loss:0.28844955563545227\n",
      "Epoch 9[418/625] Time:0.705, Train Loss:0.20733897387981415\n",
      "Epoch 9[419/625] Time:0.704, Train Loss:0.2535285949707031\n",
      "Epoch 9[420/625] Time:0.703, Train Loss:0.377106636762619\n",
      "Epoch 9[421/625] Time:0.71, Train Loss:0.25805267691612244\n",
      "Epoch 9[422/625] Time:0.704, Train Loss:0.2613619267940521\n",
      "Epoch 9[423/625] Time:0.703, Train Loss:0.26576632261276245\n",
      "Epoch 9[424/625] Time:0.704, Train Loss:0.14257609844207764\n",
      "Epoch 9[425/625] Time:0.704, Train Loss:0.27096039056777954\n",
      "Epoch 9[426/625] Time:0.709, Train Loss:0.290000319480896\n",
      "Epoch 9[427/625] Time:0.704, Train Loss:0.23408007621765137\n",
      "Epoch 9[428/625] Time:0.703, Train Loss:0.21433673799037933\n",
      "Epoch 9[429/625] Time:0.711, Train Loss:0.31334537267684937\n",
      "Epoch 9[430/625] Time:0.703, Train Loss:0.24022157490253448\n",
      "Epoch 9[431/625] Time:0.704, Train Loss:0.1737973392009735\n",
      "Epoch 9[432/625] Time:0.703, Train Loss:0.23577243089675903\n",
      "Epoch 9[433/625] Time:0.704, Train Loss:0.2390342652797699\n",
      "Epoch 9[434/625] Time:0.704, Train Loss:0.16293083131313324\n",
      "Epoch 9[435/625] Time:0.703, Train Loss:0.23770007491111755\n",
      "Epoch 9[436/625] Time:0.703, Train Loss:0.26155146956443787\n",
      "Epoch 9[437/625] Time:0.706, Train Loss:0.28322774171829224\n",
      "Epoch 9[438/625] Time:0.703, Train Loss:0.21644346415996552\n",
      "Epoch 9[439/625] Time:0.704, Train Loss:0.3105718195438385\n",
      "Epoch 9[440/625] Time:0.705, Train Loss:0.2000812590122223\n",
      "Epoch 9[441/625] Time:0.706, Train Loss:0.2596043348312378\n",
      "Epoch 9[442/625] Time:0.705, Train Loss:0.17872360348701477\n",
      "Epoch 9[443/625] Time:0.706, Train Loss:0.20416571199893951\n",
      "Epoch 9[444/625] Time:0.704, Train Loss:0.25458940863609314\n",
      "Epoch 9[445/625] Time:0.709, Train Loss:0.22645337879657745\n",
      "Epoch 9[446/625] Time:0.702, Train Loss:0.2761378586292267\n",
      "Epoch 9[447/625] Time:0.702, Train Loss:0.26327699422836304\n",
      "Epoch 9[448/625] Time:0.733, Train Loss:0.22749008238315582\n",
      "Epoch 9[449/625] Time:0.705, Train Loss:0.2892063558101654\n",
      "Epoch 9[450/625] Time:0.703, Train Loss:0.20878866314888\n",
      "Epoch 9[451/625] Time:0.711, Train Loss:0.27979081869125366\n",
      "Epoch 9[452/625] Time:0.703, Train Loss:0.17807798087596893\n",
      "Epoch 9[453/625] Time:0.709, Train Loss:0.2978821098804474\n",
      "Epoch 9[454/625] Time:0.703, Train Loss:0.23139558732509613\n",
      "Epoch 9[455/625] Time:0.703, Train Loss:0.18634873628616333\n",
      "Epoch 9[456/625] Time:0.703, Train Loss:0.2772146463394165\n",
      "Epoch 9[457/625] Time:0.705, Train Loss:0.263787180185318\n",
      "Epoch 9[458/625] Time:0.703, Train Loss:0.19355100393295288\n",
      "Epoch 9[459/625] Time:0.705, Train Loss:0.33560848236083984\n",
      "Epoch 9[460/625] Time:0.703, Train Loss:0.3080388605594635\n",
      "Epoch 9[461/625] Time:0.707, Train Loss:0.27744194865226746\n",
      "Epoch 9[462/625] Time:0.703, Train Loss:0.21166107058525085\n",
      "Epoch 9[463/625] Time:0.703, Train Loss:0.21540743112564087\n",
      "Epoch 9[464/625] Time:0.703, Train Loss:0.22102223336696625\n",
      "Epoch 9[465/625] Time:0.703, Train Loss:0.3127942681312561\n",
      "Epoch 9[466/625] Time:0.702, Train Loss:0.23885288834571838\n",
      "Epoch 9[467/625] Time:0.703, Train Loss:0.2999771535396576\n",
      "Epoch 9[468/625] Time:0.703, Train Loss:0.349262535572052\n",
      "Epoch 9[469/625] Time:0.708, Train Loss:0.327106773853302\n",
      "Epoch 9[470/625] Time:0.702, Train Loss:0.23011960089206696\n",
      "Epoch 9[471/625] Time:0.703, Train Loss:0.21483907103538513\n",
      "Epoch 9[472/625] Time:0.704, Train Loss:0.2217801809310913\n",
      "Epoch 9[473/625] Time:0.704, Train Loss:0.2995307445526123\n",
      "Epoch 9[474/625] Time:0.704, Train Loss:0.2052750289440155\n",
      "Epoch 9[475/625] Time:0.703, Train Loss:0.20891067385673523\n",
      "Epoch 9[476/625] Time:0.703, Train Loss:0.2762696444988251\n",
      "Epoch 9[477/625] Time:0.705, Train Loss:0.18948808312416077\n",
      "Epoch 9[478/625] Time:0.702, Train Loss:0.25727176666259766\n",
      "Epoch 9[479/625] Time:0.703, Train Loss:0.2692979574203491\n",
      "Epoch 9[480/625] Time:0.703, Train Loss:0.27713412046432495\n",
      "Epoch 9[481/625] Time:0.704, Train Loss:0.23142556846141815\n",
      "Epoch 9[482/625] Time:0.704, Train Loss:0.23298542201519012\n",
      "Epoch 9[483/625] Time:0.703, Train Loss:0.2764783203601837\n",
      "Epoch 9[484/625] Time:0.704, Train Loss:0.24836549162864685\n",
      "Epoch 9[485/625] Time:0.703, Train Loss:0.25730323791503906\n",
      "Epoch 9[486/625] Time:0.704, Train Loss:0.24187059700489044\n",
      "Epoch 9[487/625] Time:0.705, Train Loss:0.23942230641841888\n",
      "Epoch 9[488/625] Time:0.704, Train Loss:0.2971447706222534\n",
      "Epoch 9[489/625] Time:0.704, Train Loss:0.2884834408760071\n",
      "Epoch 9[490/625] Time:0.702, Train Loss:0.22226370871067047\n",
      "Epoch 9[491/625] Time:0.703, Train Loss:0.2699200212955475\n",
      "Epoch 9[492/625] Time:0.704, Train Loss:0.20644286274909973\n",
      "Epoch 9[493/625] Time:0.704, Train Loss:0.1635761708021164\n",
      "Epoch 9[494/625] Time:0.702, Train Loss:0.2494516521692276\n",
      "Epoch 9[495/625] Time:0.703, Train Loss:0.35027632117271423\n",
      "Epoch 9[496/625] Time:0.704, Train Loss:0.2478979229927063\n",
      "Epoch 9[497/625] Time:0.704, Train Loss:0.26518601179122925\n",
      "Epoch 9[498/625] Time:0.704, Train Loss:0.20533475279808044\n",
      "Epoch 9[499/625] Time:0.704, Train Loss:0.29397353529930115\n",
      "Epoch 9[500/625] Time:0.704, Train Loss:0.29300519824028015\n",
      "Epoch 9[501/625] Time:0.704, Train Loss:0.24668347835540771\n",
      "Epoch 9[502/625] Time:0.703, Train Loss:0.1978757083415985\n",
      "Epoch 9[503/625] Time:0.704, Train Loss:0.26519426703453064\n",
      "Epoch 9[504/625] Time:0.705, Train Loss:0.20450885593891144\n",
      "Epoch 9[505/625] Time:0.704, Train Loss:0.2677220106124878\n",
      "Epoch 9[506/625] Time:0.704, Train Loss:0.25549840927124023\n",
      "Epoch 9[507/625] Time:0.706, Train Loss:0.25763946771621704\n",
      "Epoch 9[508/625] Time:0.704, Train Loss:0.19795414805412292\n",
      "Epoch 9[509/625] Time:0.703, Train Loss:0.2965400218963623\n",
      "Epoch 9[510/625] Time:0.706, Train Loss:0.26032501459121704\n",
      "Epoch 9[511/625] Time:0.704, Train Loss:0.29685351252555847\n",
      "Epoch 9[512/625] Time:0.704, Train Loss:0.2836347222328186\n",
      "Epoch 9[513/625] Time:0.705, Train Loss:0.2921273708343506\n",
      "Epoch 9[514/625] Time:0.703, Train Loss:0.22783201932907104\n",
      "Epoch 9[515/625] Time:0.703, Train Loss:0.22515659034252167\n",
      "Epoch 9[516/625] Time:0.704, Train Loss:0.2927666902542114\n",
      "Epoch 9[517/625] Time:0.704, Train Loss:0.250296026468277\n",
      "Epoch 9[518/625] Time:0.703, Train Loss:0.24852637946605682\n",
      "Epoch 9[519/625] Time:0.704, Train Loss:0.2600839138031006\n",
      "Epoch 9[520/625] Time:0.705, Train Loss:0.26917311549186707\n",
      "Epoch 9[521/625] Time:0.705, Train Loss:0.22158201038837433\n",
      "Epoch 9[522/625] Time:0.703, Train Loss:0.23335294425487518\n",
      "Epoch 9[523/625] Time:0.704, Train Loss:0.20399032533168793\n",
      "Epoch 9[524/625] Time:0.704, Train Loss:0.25447991490364075\n",
      "Epoch 9[525/625] Time:0.705, Train Loss:0.19584229588508606\n",
      "Epoch 9[526/625] Time:0.702, Train Loss:0.2249784767627716\n",
      "Epoch 9[527/625] Time:0.704, Train Loss:0.2850685119628906\n",
      "Epoch 9[528/625] Time:0.703, Train Loss:0.17186576128005981\n",
      "Epoch 9[529/625] Time:0.703, Train Loss:0.243730828166008\n",
      "Epoch 9[530/625] Time:0.704, Train Loss:0.22441326081752777\n",
      "Epoch 9[531/625] Time:0.703, Train Loss:0.19915729761123657\n",
      "Epoch 9[532/625] Time:0.704, Train Loss:0.1992032527923584\n",
      "Epoch 9[533/625] Time:0.703, Train Loss:0.33109578490257263\n",
      "Epoch 9[534/625] Time:0.703, Train Loss:0.21576261520385742\n",
      "Epoch 9[535/625] Time:0.704, Train Loss:0.280659019947052\n",
      "Epoch 9[536/625] Time:0.703, Train Loss:0.1894996017217636\n",
      "Epoch 9[537/625] Time:0.703, Train Loss:0.22727817296981812\n",
      "Epoch 9[538/625] Time:0.703, Train Loss:0.20852434635162354\n",
      "Epoch 9[539/625] Time:0.703, Train Loss:0.19285137951374054\n",
      "Epoch 9[540/625] Time:0.702, Train Loss:0.1971054971218109\n",
      "Epoch 9[541/625] Time:0.702, Train Loss:0.20884525775909424\n",
      "Epoch 9[542/625] Time:0.703, Train Loss:0.20768286287784576\n",
      "Epoch 9[543/625] Time:0.703, Train Loss:0.2066170871257782\n",
      "Epoch 9[544/625] Time:0.704, Train Loss:0.2530224323272705\n",
      "Epoch 9[545/625] Time:0.703, Train Loss:0.2556617856025696\n",
      "Epoch 9[546/625] Time:0.702, Train Loss:0.34104296565055847\n",
      "Epoch 9[547/625] Time:0.704, Train Loss:0.2184925675392151\n",
      "Epoch 9[548/625] Time:0.702, Train Loss:0.20958924293518066\n",
      "Epoch 9[549/625] Time:0.704, Train Loss:0.20955534279346466\n",
      "Epoch 9[550/625] Time:0.703, Train Loss:0.21717652678489685\n",
      "Epoch 9[551/625] Time:0.703, Train Loss:0.26563385128974915\n",
      "Epoch 9[552/625] Time:0.703, Train Loss:0.21483324468135834\n",
      "Epoch 9[553/625] Time:0.703, Train Loss:0.1939040720462799\n",
      "Epoch 9[554/625] Time:0.694, Train Loss:0.3816891312599182\n",
      "Epoch 9[555/625] Time:0.693, Train Loss:0.2439899742603302\n",
      "Epoch 9[556/625] Time:0.693, Train Loss:0.23170137405395508\n",
      "Epoch 9[557/625] Time:0.693, Train Loss:0.2132200449705124\n",
      "Epoch 9[558/625] Time:0.696, Train Loss:0.24588164687156677\n",
      "Epoch 9[559/625] Time:0.693, Train Loss:0.14632438123226166\n",
      "Epoch 9[560/625] Time:0.693, Train Loss:0.28515446186065674\n",
      "Epoch 9[561/625] Time:0.703, Train Loss:0.2557235062122345\n",
      "Epoch 9[562/625] Time:0.704, Train Loss:0.24340523779392242\n",
      "Epoch 9[563/625] Time:0.704, Train Loss:0.296242356300354\n",
      "Epoch 9[564/625] Time:0.703, Train Loss:0.30197805166244507\n",
      "Epoch 9[565/625] Time:0.704, Train Loss:0.282856285572052\n",
      "Epoch 9[566/625] Time:0.732, Train Loss:0.26367852091789246\n",
      "Epoch 9[567/625] Time:0.705, Train Loss:0.16179189085960388\n",
      "Epoch 9[568/625] Time:0.733, Train Loss:0.21940076351165771\n",
      "Epoch 9[569/625] Time:0.702, Train Loss:0.20667409896850586\n",
      "Epoch 9[570/625] Time:0.707, Train Loss:0.25559890270233154\n",
      "Epoch 9[571/625] Time:0.707, Train Loss:0.26563236117362976\n",
      "Epoch 9[572/625] Time:0.699, Train Loss:0.23735100030899048\n",
      "Epoch 9[573/625] Time:0.703, Train Loss:0.38128533959388733\n",
      "Epoch 9[574/625] Time:0.703, Train Loss:0.2583732604980469\n",
      "Epoch 9[575/625] Time:0.703, Train Loss:0.33645564317703247\n",
      "Epoch 9[576/625] Time:0.71, Train Loss:0.29315513372421265\n",
      "Epoch 9[577/625] Time:0.713, Train Loss:0.2746884822845459\n",
      "Epoch 9[578/625] Time:0.703, Train Loss:0.24763606488704681\n",
      "Epoch 9[579/625] Time:0.702, Train Loss:0.2421129196882248\n",
      "Epoch 9[580/625] Time:0.706, Train Loss:0.2762041389942169\n",
      "Epoch 9[581/625] Time:0.704, Train Loss:0.21782442927360535\n",
      "Epoch 9[582/625] Time:0.703, Train Loss:0.21449100971221924\n",
      "Epoch 9[583/625] Time:0.693, Train Loss:0.2103181928396225\n",
      "Epoch 9[584/625] Time:0.695, Train Loss:0.22020459175109863\n",
      "Epoch 9[585/625] Time:0.704, Train Loss:0.359707772731781\n",
      "Epoch 9[586/625] Time:0.703, Train Loss:0.21898327767848969\n",
      "Epoch 9[587/625] Time:0.704, Train Loss:0.30487748980522156\n",
      "Epoch 9[588/625] Time:0.705, Train Loss:0.25401172041893005\n",
      "Epoch 9[589/625] Time:0.703, Train Loss:0.24293546378612518\n",
      "Epoch 9[590/625] Time:0.702, Train Loss:0.18445302546024323\n",
      "Epoch 9[591/625] Time:0.704, Train Loss:0.197836235165596\n",
      "Epoch 9[592/625] Time:0.704, Train Loss:0.20951081812381744\n",
      "Epoch 9[593/625] Time:0.705, Train Loss:0.25746574997901917\n",
      "Epoch 9[594/625] Time:0.703, Train Loss:0.23839618265628815\n",
      "Epoch 9[595/625] Time:0.704, Train Loss:0.27567410469055176\n",
      "Epoch 9[596/625] Time:0.704, Train Loss:0.2550281882286072\n",
      "Epoch 9[597/625] Time:0.704, Train Loss:0.29962679743766785\n",
      "Epoch 9[598/625] Time:0.704, Train Loss:0.308012992143631\n",
      "Epoch 9[599/625] Time:0.704, Train Loss:0.22528040409088135\n",
      "Epoch 9[600/625] Time:0.704, Train Loss:0.2610892653465271\n",
      "Epoch 9[601/625] Time:0.704, Train Loss:0.27259397506713867\n",
      "Epoch 9[602/625] Time:0.703, Train Loss:0.20582769811153412\n",
      "Epoch 9[603/625] Time:0.704, Train Loss:0.2775478661060333\n",
      "Epoch 9[604/625] Time:0.708, Train Loss:0.1969527006149292\n",
      "Epoch 9[605/625] Time:0.705, Train Loss:0.3423367142677307\n",
      "Epoch 9[606/625] Time:0.703, Train Loss:0.26801490783691406\n",
      "Epoch 9[607/625] Time:0.703, Train Loss:0.2594045102596283\n",
      "Epoch 9[608/625] Time:0.704, Train Loss:0.3098512887954712\n",
      "Epoch 9[609/625] Time:0.703, Train Loss:0.1971946507692337\n",
      "Epoch 9[610/625] Time:0.703, Train Loss:0.26329305768013\n",
      "Epoch 9[611/625] Time:0.703, Train Loss:0.20507045090198517\n",
      "Epoch 9[612/625] Time:0.707, Train Loss:0.18261705338954926\n",
      "Epoch 9[613/625] Time:0.703, Train Loss:0.3871796429157257\n",
      "Epoch 9[614/625] Time:0.703, Train Loss:0.2274988740682602\n",
      "Epoch 9[615/625] Time:0.705, Train Loss:0.2336503118276596\n",
      "Epoch 9[616/625] Time:0.704, Train Loss:0.2618606686592102\n",
      "Epoch 9[617/625] Time:0.705, Train Loss:0.2433454841375351\n",
      "Epoch 9[618/625] Time:0.706, Train Loss:0.23023079335689545\n",
      "Epoch 9[619/625] Time:0.706, Train Loss:0.22970397770404816\n",
      "Epoch 9[620/625] Time:0.704, Train Loss:0.225443497300148\n",
      "Epoch 9[621/625] Time:0.703, Train Loss:0.22320005297660828\n",
      "Epoch 9[622/625] Time:0.703, Train Loss:0.2600860297679901\n",
      "Epoch 9[623/625] Time:0.703, Train Loss:0.16094417870044708\n",
      "Epoch 9[624/625] Time:0.703, Train Loss:0.29141902923583984\n",
      "Epoch 9[0/78] Val Loss:0.20307143032550812\n",
      "Epoch 9[1/78] Val Loss:0.17350530624389648\n",
      "Epoch 9[2/78] Val Loss:0.16770552098751068\n",
      "Epoch 9[3/78] Val Loss:0.21471062302589417\n",
      "Epoch 9[4/78] Val Loss:0.30036038160324097\n",
      "Epoch 9[5/78] Val Loss:0.20618002116680145\n",
      "Epoch 9[6/78] Val Loss:0.2356518656015396\n",
      "Epoch 9[7/78] Val Loss:0.31952497363090515\n",
      "Epoch 9[8/78] Val Loss:0.1551627218723297\n",
      "Epoch 9[9/78] Val Loss:0.132953941822052\n",
      "Epoch 9[10/78] Val Loss:0.08586444705724716\n",
      "Epoch 9[11/78] Val Loss:0.13801273703575134\n",
      "Epoch 9[12/78] Val Loss:0.11209171265363693\n",
      "Epoch 9[13/78] Val Loss:0.09054689109325409\n",
      "Epoch 9[14/78] Val Loss:0.18076294660568237\n",
      "Epoch 9[15/78] Val Loss:0.1497642695903778\n",
      "Epoch 9[16/78] Val Loss:0.1743437647819519\n",
      "Epoch 9[17/78] Val Loss:0.14115755259990692\n",
      "Epoch 9[18/78] Val Loss:0.18766383826732635\n",
      "Epoch 9[19/78] Val Loss:0.28091156482696533\n",
      "Epoch 9[20/78] Val Loss:0.16371352970600128\n",
      "Epoch 9[21/78] Val Loss:0.44558557868003845\n",
      "Epoch 9[22/78] Val Loss:0.6534699201583862\n",
      "Epoch 9[23/78] Val Loss:0.48447299003601074\n",
      "Epoch 9[24/78] Val Loss:0.3546287715435028\n",
      "Epoch 9[25/78] Val Loss:0.43304598331451416\n",
      "Epoch 9[26/78] Val Loss:0.37173280119895935\n",
      "Epoch 9[27/78] Val Loss:0.3647230863571167\n",
      "Epoch 9[28/78] Val Loss:0.3225090503692627\n",
      "Epoch 9[29/78] Val Loss:0.48421838879585266\n",
      "Epoch 9[30/78] Val Loss:1.830066442489624\n",
      "Epoch 9[31/78] Val Loss:1.5625231266021729\n",
      "Epoch 9[32/78] Val Loss:1.235344409942627\n",
      "Epoch 9[33/78] Val Loss:0.3507053852081299\n",
      "Epoch 9[34/78] Val Loss:0.2707135081291199\n",
      "Epoch 9[35/78] Val Loss:0.2704988718032837\n",
      "Epoch 9[36/78] Val Loss:0.2611476182937622\n",
      "Epoch 9[37/78] Val Loss:0.3282698392868042\n",
      "Epoch 9[38/78] Val Loss:0.1710960865020752\n",
      "Epoch 9[39/78] Val Loss:0.1906437873840332\n",
      "Epoch 9[40/78] Val Loss:0.18373437225818634\n",
      "Epoch 9[41/78] Val Loss:0.20026473701000214\n",
      "Epoch 9[42/78] Val Loss:0.1947816014289856\n",
      "Epoch 9[43/78] Val Loss:0.12723684310913086\n",
      "Epoch 9[44/78] Val Loss:0.09214369207620621\n",
      "Epoch 9[45/78] Val Loss:0.07553227245807648\n",
      "Epoch 9[46/78] Val Loss:0.08273442834615707\n",
      "Epoch 9[47/78] Val Loss:0.097474105656147\n",
      "Epoch 9[48/78] Val Loss:0.1285373419523239\n",
      "Epoch 9[49/78] Val Loss:0.08046527951955795\n",
      "Epoch 9[50/78] Val Loss:0.0856892466545105\n",
      "Epoch 9[51/78] Val Loss:0.07875899225473404\n",
      "Epoch 9[52/78] Val Loss:0.09912276268005371\n",
      "Epoch 9[53/78] Val Loss:0.07967177778482437\n",
      "Epoch 9[54/78] Val Loss:0.07046716660261154\n",
      "Epoch 9[55/78] Val Loss:0.10484036803245544\n",
      "Epoch 9[56/78] Val Loss:0.4298959970474243\n",
      "Epoch 9[57/78] Val Loss:0.37472572922706604\n",
      "Epoch 9[58/78] Val Loss:0.36503419280052185\n",
      "Epoch 9[59/78] Val Loss:0.3474562168121338\n",
      "Epoch 9[60/78] Val Loss:0.3191972076892853\n",
      "Epoch 9[61/78] Val Loss:0.40351033210754395\n",
      "Epoch 9[62/78] Val Loss:0.41061967611312866\n",
      "Epoch 9[63/78] Val Loss:0.23702526092529297\n",
      "Epoch 9[64/78] Val Loss:0.14783896505832672\n",
      "Epoch 9[65/78] Val Loss:0.15136761963367462\n",
      "Epoch 9[66/78] Val Loss:0.16916027665138245\n",
      "Epoch 9[67/78] Val Loss:0.16242174804210663\n",
      "Epoch 9[68/78] Val Loss:0.46676233410835266\n",
      "Epoch 9[69/78] Val Loss:0.4876767694950104\n",
      "Epoch 9[70/78] Val Loss:0.4964384436607361\n",
      "Epoch 9[71/78] Val Loss:0.37513718008995056\n",
      "Epoch 9[72/78] Val Loss:0.18680338561534882\n",
      "Epoch 9[73/78] Val Loss:0.1671002209186554\n",
      "Epoch 9[74/78] Val Loss:0.37472590804100037\n",
      "Epoch 9[75/78] Val Loss:0.5143705010414124\n",
      "Epoch 9[76/78] Val Loss:0.46210116147994995\n",
      "Epoch 9[77/78] Val Loss:0.5224053859710693\n",
      "Epoch 9[78/78] Val Loss:0.5531992316246033\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.89      0.92     15691\n",
      "           1       0.68      0.84      0.75      4309\n",
      "\n",
      "    accuracy                           0.88     20000\n",
      "   macro avg       0.82      0.87      0.84     20000\n",
      "weighted avg       0.89      0.88      0.88     20000\n",
      "\n",
      "Epoch 9: Train Loss 0.25238318955898287, Val Loss 0.3090446720329615, Train Time 788.0139162540436, Val Time 36.31367015838623\n",
      "Epoch 10[0/625] Time:0.689, Train Loss:0.33598220348358154\n",
      "Epoch 10[1/625] Time:0.704, Train Loss:0.271721214056015\n",
      "Epoch 10[2/625] Time:0.704, Train Loss:0.2741902470588684\n",
      "Epoch 10[3/625] Time:0.703, Train Loss:0.25698521733283997\n",
      "Epoch 10[4/625] Time:0.725, Train Loss:0.22726279497146606\n",
      "Epoch 10[5/625] Time:0.704, Train Loss:0.3101752996444702\n",
      "Epoch 10[6/625] Time:0.704, Train Loss:0.3000025153160095\n",
      "Epoch 10[7/625] Time:0.705, Train Loss:0.2681325078010559\n",
      "Epoch 10[8/625] Time:0.704, Train Loss:0.3443570137023926\n",
      "Epoch 10[9/625] Time:0.704, Train Loss:0.2957194447517395\n",
      "Epoch 10[10/625] Time:0.703, Train Loss:0.4655141234397888\n",
      "Epoch 10[11/625] Time:0.703, Train Loss:0.3222351670265198\n",
      "Epoch 10[12/625] Time:0.704, Train Loss:0.28753650188446045\n",
      "Epoch 10[13/625] Time:0.704, Train Loss:0.3230653405189514\n",
      "Epoch 10[14/625] Time:0.704, Train Loss:0.3359116315841675\n",
      "Epoch 10[15/625] Time:0.704, Train Loss:0.24943722784519196\n",
      "Epoch 10[16/625] Time:0.704, Train Loss:0.4547666013240814\n",
      "Epoch 10[17/625] Time:0.705, Train Loss:0.1918977051973343\n",
      "Epoch 10[18/625] Time:0.703, Train Loss:0.33603814244270325\n",
      "Epoch 10[19/625] Time:0.703, Train Loss:0.4373696744441986\n",
      "Epoch 10[20/625] Time:0.704, Train Loss:0.33607152104377747\n",
      "Epoch 10[21/625] Time:0.704, Train Loss:0.41908538341522217\n",
      "Epoch 10[22/625] Time:0.704, Train Loss:0.3150254487991333\n",
      "Epoch 10[23/625] Time:0.704, Train Loss:0.2980911433696747\n",
      "Epoch 10[24/625] Time:0.704, Train Loss:0.3395744264125824\n",
      "Epoch 10[25/625] Time:0.703, Train Loss:0.28252583742141724\n",
      "Epoch 10[26/625] Time:0.704, Train Loss:0.37017589807510376\n",
      "Epoch 10[27/625] Time:0.703, Train Loss:0.3337194323539734\n",
      "Epoch 10[28/625] Time:0.703, Train Loss:0.28141745924949646\n",
      "Epoch 10[29/625] Time:0.703, Train Loss:0.29968246817588806\n",
      "Epoch 10[30/625] Time:0.703, Train Loss:0.3402218520641327\n",
      "Epoch 10[31/625] Time:0.703, Train Loss:0.41556528210639954\n",
      "Epoch 10[32/625] Time:0.694, Train Loss:0.4018900394439697\n",
      "Epoch 10[33/625] Time:0.706, Train Loss:0.4187396168708801\n",
      "Epoch 10[34/625] Time:0.703, Train Loss:0.3129168748855591\n",
      "Epoch 10[35/625] Time:0.704, Train Loss:0.3103456497192383\n",
      "Epoch 10[36/625] Time:0.706, Train Loss:0.3863309919834137\n",
      "Epoch 10[37/625] Time:0.694, Train Loss:0.3213794529438019\n",
      "Epoch 10[38/625] Time:0.705, Train Loss:0.3934628665447235\n",
      "Epoch 10[39/625] Time:0.704, Train Loss:0.39730218052864075\n",
      "Epoch 10[40/625] Time:0.705, Train Loss:0.3012506067752838\n",
      "Epoch 10[41/625] Time:0.704, Train Loss:0.3122047185897827\n",
      "Epoch 10[42/625] Time:0.704, Train Loss:0.34125661849975586\n",
      "Epoch 10[43/625] Time:0.706, Train Loss:0.3274065852165222\n",
      "Epoch 10[44/625] Time:0.705, Train Loss:0.3687010705471039\n",
      "Epoch 10[45/625] Time:0.703, Train Loss:0.33369702100753784\n",
      "Epoch 10[46/625] Time:0.704, Train Loss:0.33569490909576416\n",
      "Epoch 10[47/625] Time:0.704, Train Loss:0.3452146053314209\n",
      "Epoch 10[48/625] Time:0.694, Train Loss:0.300027459859848\n",
      "Epoch 10[49/625] Time:0.694, Train Loss:0.29169267416000366\n",
      "Epoch 10[50/625] Time:0.693, Train Loss:0.31323686242103577\n",
      "Epoch 10[51/625] Time:0.695, Train Loss:0.26458486914634705\n",
      "Epoch 10[52/625] Time:0.703, Train Loss:0.38002246618270874\n",
      "Epoch 10[53/625] Time:0.704, Train Loss:0.3696601390838623\n",
      "Epoch 10[54/625] Time:0.703, Train Loss:0.33707916736602783\n",
      "Epoch 10[55/625] Time:0.703, Train Loss:0.26785457134246826\n",
      "Epoch 10[56/625] Time:0.703, Train Loss:0.2683163285255432\n",
      "Epoch 10[57/625] Time:0.704, Train Loss:0.36063215136528015\n",
      "Epoch 10[58/625] Time:0.705, Train Loss:0.3073115646839142\n",
      "Epoch 10[59/625] Time:0.703, Train Loss:0.25755149126052856\n",
      "Epoch 10[60/625] Time:0.704, Train Loss:0.3232307434082031\n",
      "Epoch 10[61/625] Time:0.704, Train Loss:0.27540987730026245\n",
      "Epoch 10[62/625] Time:0.705, Train Loss:0.32187798619270325\n",
      "Epoch 10[63/625] Time:0.703, Train Loss:0.2823382616043091\n",
      "Epoch 10[64/625] Time:0.703, Train Loss:0.3323049545288086\n",
      "Epoch 10[65/625] Time:0.703, Train Loss:0.3417206108570099\n",
      "Epoch 10[66/625] Time:0.704, Train Loss:0.26538264751434326\n",
      "Epoch 10[67/625] Time:0.703, Train Loss:0.3220599293708801\n",
      "Epoch 10[68/625] Time:0.704, Train Loss:0.2660354673862457\n",
      "Epoch 10[69/625] Time:0.706, Train Loss:0.3134819567203522\n",
      "Epoch 10[70/625] Time:0.703, Train Loss:0.30000680685043335\n",
      "Epoch 10[71/625] Time:0.703, Train Loss:0.35537904500961304\n",
      "Epoch 10[72/625] Time:0.703, Train Loss:0.32154667377471924\n",
      "Epoch 10[73/625] Time:0.703, Train Loss:0.32134515047073364\n",
      "Epoch 10[74/625] Time:0.703, Train Loss:0.34094783663749695\n",
      "Epoch 10[75/625] Time:0.703, Train Loss:0.41565006971359253\n",
      "Epoch 10[76/625] Time:0.75, Train Loss:0.3900980055332184\n",
      "Epoch 10[77/625] Time:0.692, Train Loss:0.33918556571006775\n",
      "Epoch 10[78/625] Time:0.705, Train Loss:0.3548206388950348\n",
      "Epoch 10[79/625] Time:0.703, Train Loss:0.29867303371429443\n",
      "Epoch 10[80/625] Time:0.702, Train Loss:0.34998705983161926\n",
      "Epoch 10[81/625] Time:0.703, Train Loss:0.288367360830307\n",
      "Epoch 10[82/625] Time:0.703, Train Loss:0.3234786093235016\n",
      "Epoch 10[83/625] Time:0.712, Train Loss:0.31250888109207153\n",
      "Epoch 10[84/625] Time:0.722, Train Loss:0.33103147149086\n",
      "Epoch 10[85/625] Time:0.702, Train Loss:0.3079781234264374\n",
      "Epoch 10[86/625] Time:0.703, Train Loss:0.2805778980255127\n",
      "Epoch 10[87/625] Time:0.703, Train Loss:0.3846971094608307\n",
      "Epoch 10[88/625] Time:0.703, Train Loss:0.30359888076782227\n",
      "Epoch 10[89/625] Time:0.704, Train Loss:0.336845725774765\n",
      "Epoch 10[90/625] Time:0.703, Train Loss:0.3795441687107086\n",
      "Epoch 10[91/625] Time:0.698, Train Loss:0.28326281905174255\n",
      "Epoch 10[92/625] Time:0.693, Train Loss:0.42759114503860474\n",
      "Epoch 10[93/625] Time:0.693, Train Loss:0.325306236743927\n",
      "Epoch 10[94/625] Time:0.694, Train Loss:0.28365907073020935\n",
      "Epoch 10[95/625] Time:0.693, Train Loss:0.3141479194164276\n",
      "Epoch 10[96/625] Time:0.693, Train Loss:0.3126443326473236\n",
      "Epoch 10[97/625] Time:0.694, Train Loss:0.3105623126029968\n",
      "Epoch 10[98/625] Time:0.693, Train Loss:0.31509673595428467\n",
      "Epoch 10[99/625] Time:0.693, Train Loss:0.2900109887123108\n",
      "Epoch 10[100/625] Time:0.698, Train Loss:0.346998393535614\n",
      "Epoch 10[101/625] Time:0.702, Train Loss:0.30533555150032043\n",
      "Epoch 10[102/625] Time:0.705, Train Loss:0.3197917938232422\n",
      "Epoch 10[103/625] Time:0.703, Train Loss:0.3563316762447357\n",
      "Epoch 10[104/625] Time:0.702, Train Loss:0.31775835156440735\n",
      "Epoch 10[105/625] Time:0.702, Train Loss:0.30521464347839355\n",
      "Epoch 10[106/625] Time:0.704, Train Loss:0.30082178115844727\n",
      "Epoch 10[107/625] Time:0.703, Train Loss:0.3425518572330475\n",
      "Epoch 10[108/625] Time:0.705, Train Loss:0.3208833336830139\n",
      "Epoch 10[109/625] Time:0.707, Train Loss:0.23950324952602386\n",
      "Epoch 10[110/625] Time:0.703, Train Loss:0.3712739646434784\n",
      "Epoch 10[111/625] Time:0.703, Train Loss:0.2808380126953125\n",
      "Epoch 10[112/625] Time:0.703, Train Loss:0.26799923181533813\n",
      "Epoch 10[113/625] Time:0.742, Train Loss:0.24640798568725586\n",
      "Epoch 10[114/625] Time:0.73, Train Loss:0.28496089577674866\n",
      "Epoch 10[115/625] Time:0.69, Train Loss:0.3159360885620117\n",
      "Epoch 10[116/625] Time:0.695, Train Loss:0.41774362325668335\n",
      "Epoch 10[117/625] Time:0.694, Train Loss:0.3324377238750458\n",
      "Epoch 10[118/625] Time:0.696, Train Loss:0.332124799489975\n",
      "Epoch 10[119/625] Time:0.694, Train Loss:0.2469136118888855\n",
      "Epoch 10[120/625] Time:0.73, Train Loss:0.3526683747768402\n",
      "Epoch 10[121/625] Time:0.703, Train Loss:0.2861292362213135\n",
      "Epoch 10[122/625] Time:0.705, Train Loss:0.32494068145751953\n",
      "Epoch 10[123/625] Time:0.703, Train Loss:0.2589322030544281\n",
      "Epoch 10[124/625] Time:0.703, Train Loss:0.30295953154563904\n",
      "Epoch 10[125/625] Time:0.704, Train Loss:0.29650717973709106\n",
      "Epoch 10[126/625] Time:0.705, Train Loss:0.35299503803253174\n",
      "Epoch 10[127/625] Time:0.703, Train Loss:0.3773515224456787\n",
      "Epoch 10[128/625] Time:0.704, Train Loss:0.4003842771053314\n",
      "Epoch 10[129/625] Time:0.703, Train Loss:0.32335853576660156\n",
      "Epoch 10[130/625] Time:0.702, Train Loss:0.3191709518432617\n",
      "Epoch 10[131/625] Time:0.703, Train Loss:0.29457953572273254\n",
      "Epoch 10[132/625] Time:0.705, Train Loss:0.30663827061653137\n",
      "Epoch 10[133/625] Time:0.703, Train Loss:0.34162476658821106\n",
      "Epoch 10[134/625] Time:0.703, Train Loss:0.2500954866409302\n",
      "Epoch 10[135/625] Time:0.703, Train Loss:0.3363846242427826\n",
      "Epoch 10[136/625] Time:0.703, Train Loss:0.40087172389030457\n",
      "Epoch 10[137/625] Time:0.704, Train Loss:0.27125000953674316\n",
      "Epoch 10[138/625] Time:0.703, Train Loss:0.31069424748420715\n",
      "Epoch 10[139/625] Time:0.703, Train Loss:0.2987605333328247\n",
      "Epoch 10[140/625] Time:0.704, Train Loss:0.2381943315267563\n",
      "Epoch 10[141/625] Time:0.704, Train Loss:0.29361292719841003\n",
      "Epoch 10[142/625] Time:0.693, Train Loss:0.2725840210914612\n",
      "Epoch 10[143/625] Time:0.702, Train Loss:0.3063265383243561\n",
      "Epoch 10[144/625] Time:0.748, Train Loss:0.2605953514575958\n",
      "Epoch 10[145/625] Time:0.691, Train Loss:0.37699994444847107\n",
      "Epoch 10[146/625] Time:0.692, Train Loss:0.3028997778892517\n",
      "Epoch 10[147/625] Time:0.691, Train Loss:0.3360547423362732\n",
      "Epoch 10[148/625] Time:0.691, Train Loss:0.3299381732940674\n",
      "Epoch 10[149/625] Time:0.718, Train Loss:0.3868439793586731\n",
      "Epoch 10[150/625] Time:0.736, Train Loss:0.29885411262512207\n",
      "Epoch 10[151/625] Time:0.705, Train Loss:0.31428349018096924\n",
      "Epoch 10[152/625] Time:0.703, Train Loss:0.24653935432434082\n",
      "Epoch 10[153/625] Time:0.708, Train Loss:0.28330549597740173\n",
      "Epoch 10[154/625] Time:0.702, Train Loss:0.3519093692302704\n",
      "Epoch 10[155/625] Time:0.713, Train Loss:0.30066168308258057\n",
      "Epoch 10[156/625] Time:0.734, Train Loss:0.3496582806110382\n",
      "Epoch 10[157/625] Time:0.692, Train Loss:0.2875045835971832\n",
      "Epoch 10[158/625] Time:0.693, Train Loss:0.2489975094795227\n",
      "Epoch 10[159/625] Time:0.693, Train Loss:0.2603713572025299\n",
      "Epoch 10[160/625] Time:0.703, Train Loss:0.2672649919986725\n",
      "Epoch 10[161/625] Time:0.702, Train Loss:0.35272786021232605\n",
      "Epoch 10[162/625] Time:0.703, Train Loss:0.3865680396556854\n",
      "Epoch 10[163/625] Time:0.709, Train Loss:0.35691455006599426\n",
      "Epoch 10[164/625] Time:0.707, Train Loss:0.3050103485584259\n",
      "Epoch 10[165/625] Time:0.703, Train Loss:0.37783971428871155\n",
      "Epoch 10[166/625] Time:0.705, Train Loss:0.32741767168045044\n",
      "Epoch 10[167/625] Time:0.704, Train Loss:0.260454386472702\n",
      "Epoch 10[168/625] Time:0.703, Train Loss:0.26674917340278625\n",
      "Epoch 10[169/625] Time:0.703, Train Loss:0.3540973365306854\n",
      "Epoch 10[170/625] Time:0.702, Train Loss:0.3993530869483948\n",
      "Epoch 10[171/625] Time:0.705, Train Loss:0.3452460765838623\n",
      "Epoch 10[172/625] Time:0.705, Train Loss:0.2784229516983032\n",
      "Epoch 10[173/625] Time:0.704, Train Loss:0.3044588565826416\n",
      "Epoch 10[174/625] Time:0.703, Train Loss:0.2391408383846283\n",
      "Epoch 10[175/625] Time:0.738, Train Loss:0.3173932433128357\n",
      "Epoch 10[176/625] Time:0.692, Train Loss:0.2544589042663574\n",
      "Epoch 10[177/625] Time:0.694, Train Loss:0.2765917181968689\n",
      "Epoch 10[178/625] Time:0.694, Train Loss:0.29684969782829285\n",
      "Epoch 10[179/625] Time:0.696, Train Loss:0.3309836983680725\n",
      "Epoch 10[180/625] Time:0.704, Train Loss:0.2598598599433899\n",
      "Epoch 10[181/625] Time:0.703, Train Loss:0.29171615839004517\n",
      "Epoch 10[182/625] Time:0.702, Train Loss:0.2526554465293884\n",
      "Epoch 10[183/625] Time:0.703, Train Loss:0.24694131314754486\n",
      "Epoch 10[184/625] Time:0.703, Train Loss:0.3352106511592865\n",
      "Epoch 10[185/625] Time:0.703, Train Loss:0.24520239233970642\n",
      "Epoch 10[186/625] Time:0.704, Train Loss:0.31859633326530457\n",
      "Epoch 10[187/625] Time:0.708, Train Loss:0.3917161226272583\n",
      "Epoch 10[188/625] Time:0.702, Train Loss:0.334562212228775\n",
      "Epoch 10[189/625] Time:0.702, Train Loss:0.33205845952033997\n",
      "Epoch 10[190/625] Time:0.704, Train Loss:0.31784316897392273\n",
      "Epoch 10[191/625] Time:0.703, Train Loss:0.4035804569721222\n",
      "Epoch 10[192/625] Time:0.703, Train Loss:0.3013862073421478\n",
      "Epoch 10[193/625] Time:0.704, Train Loss:0.29695066809654236\n",
      "Epoch 10[194/625] Time:0.702, Train Loss:0.31524041295051575\n",
      "Epoch 10[195/625] Time:0.71, Train Loss:0.3255901634693146\n",
      "Epoch 10[196/625] Time:0.703, Train Loss:0.29798731207847595\n",
      "Epoch 10[197/625] Time:0.704, Train Loss:0.3781073987483978\n",
      "Epoch 10[198/625] Time:0.704, Train Loss:0.3747730255126953\n",
      "Epoch 10[199/625] Time:0.703, Train Loss:0.2327585220336914\n",
      "Epoch 10[200/625] Time:0.703, Train Loss:0.28292742371559143\n",
      "Epoch 10[201/625] Time:0.703, Train Loss:0.31424492597579956\n",
      "Epoch 10[202/625] Time:0.703, Train Loss:0.3644380569458008\n",
      "Epoch 10[203/625] Time:0.704, Train Loss:0.34507033228874207\n",
      "Epoch 10[204/625] Time:0.703, Train Loss:0.2934057116508484\n",
      "Epoch 10[205/625] Time:0.703, Train Loss:0.29741084575653076\n",
      "Epoch 10[206/625] Time:0.703, Train Loss:0.2872416079044342\n",
      "Epoch 10[207/625] Time:0.703, Train Loss:0.38131263852119446\n",
      "Epoch 10[208/625] Time:0.703, Train Loss:0.3196762204170227\n",
      "Epoch 10[209/625] Time:0.704, Train Loss:0.2009752094745636\n",
      "Epoch 10[210/625] Time:0.703, Train Loss:0.3498973846435547\n",
      "Epoch 10[211/625] Time:0.704, Train Loss:0.32198694348335266\n",
      "Epoch 10[212/625] Time:0.703, Train Loss:0.27486708760261536\n",
      "Epoch 10[213/625] Time:0.703, Train Loss:0.2882579267024994\n",
      "Epoch 10[214/625] Time:0.703, Train Loss:0.2258562445640564\n",
      "Epoch 10[215/625] Time:0.691, Train Loss:0.29642975330352783\n",
      "Epoch 10[216/625] Time:0.691, Train Loss:0.271979957818985\n",
      "Epoch 10[217/625] Time:0.691, Train Loss:0.3106701970100403\n",
      "Epoch 10[218/625] Time:0.692, Train Loss:0.26930662989616394\n",
      "Epoch 10[219/625] Time:0.691, Train Loss:0.26536446809768677\n",
      "Epoch 10[220/625] Time:0.715, Train Loss:0.34640976786613464\n",
      "Epoch 10[221/625] Time:0.703, Train Loss:0.28905782103538513\n",
      "Epoch 10[222/625] Time:0.705, Train Loss:0.28849396109580994\n",
      "Epoch 10[223/625] Time:0.705, Train Loss:0.2702653706073761\n",
      "Epoch 10[224/625] Time:0.704, Train Loss:0.31276488304138184\n",
      "Epoch 10[225/625] Time:0.704, Train Loss:0.3534875214099884\n",
      "Epoch 10[226/625] Time:0.705, Train Loss:0.30330950021743774\n",
      "Epoch 10[227/625] Time:0.705, Train Loss:0.2316046953201294\n",
      "Epoch 10[228/625] Time:0.704, Train Loss:0.32302311062812805\n",
      "Epoch 10[229/625] Time:0.707, Train Loss:0.3145243227481842\n",
      "Epoch 10[230/625] Time:0.706, Train Loss:0.2652008533477783\n",
      "Epoch 10[231/625] Time:0.705, Train Loss:0.27699515223503113\n",
      "Epoch 10[232/625] Time:0.706, Train Loss:0.3636295795440674\n",
      "Epoch 10[233/625] Time:0.706, Train Loss:0.4109639823436737\n",
      "Epoch 10[234/625] Time:0.704, Train Loss:0.35128793120384216\n",
      "Epoch 10[235/625] Time:0.703, Train Loss:0.3121227025985718\n",
      "Epoch 10[236/625] Time:0.703, Train Loss:0.24007047712802887\n",
      "Epoch 10[237/625] Time:0.703, Train Loss:0.3266204297542572\n",
      "Epoch 10[238/625] Time:0.703, Train Loss:0.2919033169746399\n",
      "Epoch 10[239/625] Time:0.704, Train Loss:0.34714630246162415\n",
      "Epoch 10[240/625] Time:0.702, Train Loss:0.26515719294548035\n",
      "Epoch 10[241/625] Time:0.702, Train Loss:0.2727734446525574\n",
      "Epoch 10[242/625] Time:0.703, Train Loss:0.3000827133655548\n",
      "Epoch 10[243/625] Time:0.702, Train Loss:0.35311397910118103\n",
      "Epoch 10[244/625] Time:0.702, Train Loss:0.3746323883533478\n",
      "Epoch 10[245/625] Time:0.704, Train Loss:0.3036196827888489\n",
      "Epoch 10[246/625] Time:0.703, Train Loss:0.25892946124076843\n",
      "Epoch 10[247/625] Time:0.703, Train Loss:0.2884848117828369\n",
      "Epoch 10[248/625] Time:0.702, Train Loss:0.31011152267456055\n",
      "Epoch 10[249/625] Time:0.703, Train Loss:0.21534672379493713\n",
      "Epoch 10[250/625] Time:0.705, Train Loss:0.2461814433336258\n",
      "Epoch 10[251/625] Time:0.704, Train Loss:0.38483038544654846\n",
      "Epoch 10[252/625] Time:0.703, Train Loss:0.29069802165031433\n",
      "Epoch 10[253/625] Time:0.704, Train Loss:0.2795795798301697\n",
      "Epoch 10[254/625] Time:0.703, Train Loss:0.3228456974029541\n",
      "Epoch 10[255/625] Time:0.703, Train Loss:0.24295178055763245\n",
      "Epoch 10[256/625] Time:0.703, Train Loss:0.21855807304382324\n",
      "Epoch 10[257/625] Time:0.703, Train Loss:0.2772645950317383\n",
      "Epoch 10[258/625] Time:0.704, Train Loss:0.32042405009269714\n",
      "Epoch 10[259/625] Time:0.703, Train Loss:0.2695937156677246\n",
      "Epoch 10[260/625] Time:0.704, Train Loss:0.2826076149940491\n",
      "Epoch 10[261/625] Time:0.703, Train Loss:0.34423136711120605\n",
      "Epoch 10[262/625] Time:0.704, Train Loss:0.22960618138313293\n",
      "Epoch 10[263/625] Time:0.703, Train Loss:0.31893637776374817\n",
      "Epoch 10[264/625] Time:0.703, Train Loss:0.32828304171562195\n",
      "Epoch 10[265/625] Time:0.703, Train Loss:0.33630409836769104\n",
      "Epoch 10[266/625] Time:0.704, Train Loss:0.2846074402332306\n",
      "Epoch 10[267/625] Time:0.703, Train Loss:0.2698187530040741\n",
      "Epoch 10[268/625] Time:0.71, Train Loss:0.31534266471862793\n",
      "Epoch 10[269/625] Time:0.729, Train Loss:0.3107543885707855\n",
      "Epoch 10[270/625] Time:0.703, Train Loss:0.3047920763492584\n",
      "Epoch 10[271/625] Time:0.703, Train Loss:0.314613938331604\n",
      "Epoch 10[272/625] Time:0.705, Train Loss:0.2628190219402313\n",
      "Epoch 10[273/625] Time:0.702, Train Loss:0.30009591579437256\n",
      "Epoch 10[274/625] Time:0.703, Train Loss:0.2638442814350128\n",
      "Epoch 10[275/625] Time:0.703, Train Loss:0.3843315541744232\n",
      "Epoch 10[276/625] Time:0.704, Train Loss:0.2832546532154083\n",
      "Epoch 10[277/625] Time:0.703, Train Loss:0.33710238337516785\n",
      "Epoch 10[278/625] Time:0.703, Train Loss:0.29234981536865234\n",
      "Epoch 10[279/625] Time:0.703, Train Loss:0.31520402431488037\n",
      "Epoch 10[280/625] Time:0.704, Train Loss:0.3141995966434479\n",
      "Epoch 10[281/625] Time:0.704, Train Loss:0.4094219505786896\n",
      "Epoch 10[282/625] Time:0.709, Train Loss:0.24778804183006287\n",
      "Epoch 10[283/625] Time:0.702, Train Loss:0.35762569308280945\n",
      "Epoch 10[284/625] Time:0.702, Train Loss:0.30575525760650635\n",
      "Epoch 10[285/625] Time:0.702, Train Loss:0.3742813467979431\n",
      "Epoch 10[286/625] Time:0.703, Train Loss:0.2576279938220978\n",
      "Epoch 10[287/625] Time:0.703, Train Loss:0.4014875292778015\n",
      "Epoch 10[288/625] Time:0.703, Train Loss:0.333848774433136\n",
      "Epoch 10[289/625] Time:0.704, Train Loss:0.31766584515571594\n",
      "Epoch 10[290/625] Time:0.707, Train Loss:0.33078762888908386\n",
      "Epoch 10[291/625] Time:0.707, Train Loss:0.24062038958072662\n",
      "Epoch 10[292/625] Time:0.711, Train Loss:0.2638738751411438\n",
      "Epoch 10[293/625] Time:0.704, Train Loss:0.25886452198028564\n",
      "Epoch 10[294/625] Time:0.704, Train Loss:0.2841995656490326\n",
      "Epoch 10[295/625] Time:0.704, Train Loss:0.2663254141807556\n",
      "Epoch 10[296/625] Time:0.704, Train Loss:0.2669017016887665\n",
      "Epoch 10[297/625] Time:0.704, Train Loss:0.2831806242465973\n",
      "Epoch 10[298/625] Time:0.703, Train Loss:0.261179655790329\n",
      "Epoch 10[299/625] Time:0.703, Train Loss:0.3137223422527313\n",
      "Epoch 10[300/625] Time:0.702, Train Loss:0.30144941806793213\n",
      "Epoch 10[301/625] Time:0.706, Train Loss:0.30503714084625244\n",
      "Epoch 10[302/625] Time:0.705, Train Loss:0.35267195105552673\n",
      "Epoch 10[303/625] Time:0.703, Train Loss:0.27935492992401123\n",
      "Epoch 10[304/625] Time:0.704, Train Loss:0.3156167268753052\n",
      "Epoch 10[305/625] Time:0.703, Train Loss:0.324146032333374\n",
      "Epoch 10[306/625] Time:0.703, Train Loss:0.24783259630203247\n",
      "Epoch 10[307/625] Time:0.704, Train Loss:0.3720684051513672\n",
      "Epoch 10[308/625] Time:0.704, Train Loss:0.2819178104400635\n",
      "Epoch 10[309/625] Time:0.703, Train Loss:0.33224040269851685\n",
      "Epoch 10[310/625] Time:0.704, Train Loss:0.3223472237586975\n",
      "Epoch 10[311/625] Time:0.702, Train Loss:0.34886348247528076\n",
      "Epoch 10[312/625] Time:0.704, Train Loss:0.33545976877212524\n",
      "Epoch 10[313/625] Time:0.702, Train Loss:0.29195141792297363\n",
      "Epoch 10[314/625] Time:0.708, Train Loss:0.334581196308136\n",
      "Epoch 10[315/625] Time:0.703, Train Loss:0.30999982357025146\n",
      "Epoch 10[316/625] Time:0.703, Train Loss:0.34076160192489624\n",
      "Epoch 10[317/625] Time:0.703, Train Loss:0.31029772758483887\n",
      "Epoch 10[318/625] Time:0.703, Train Loss:0.2908518612384796\n",
      "Epoch 10[319/625] Time:0.703, Train Loss:0.348404198884964\n",
      "Epoch 10[320/625] Time:0.705, Train Loss:0.27806803584098816\n",
      "Epoch 10[321/625] Time:0.703, Train Loss:0.32775452733039856\n",
      "Epoch 10[322/625] Time:0.705, Train Loss:0.3267703652381897\n",
      "Epoch 10[323/625] Time:0.703, Train Loss:0.28240421414375305\n",
      "Epoch 10[324/625] Time:0.704, Train Loss:0.3137372136116028\n",
      "Epoch 10[325/625] Time:0.705, Train Loss:0.27629831433296204\n",
      "Epoch 10[326/625] Time:0.704, Train Loss:0.2704150080680847\n",
      "Epoch 10[327/625] Time:0.704, Train Loss:0.42386335134506226\n",
      "Epoch 10[328/625] Time:0.703, Train Loss:0.2946354150772095\n",
      "Epoch 10[329/625] Time:0.702, Train Loss:0.3029801845550537\n",
      "Epoch 10[330/625] Time:0.706, Train Loss:0.27433449029922485\n",
      "Epoch 10[331/625] Time:0.703, Train Loss:0.3050330877304077\n",
      "Epoch 10[332/625] Time:0.723, Train Loss:0.29339146614074707\n",
      "Epoch 10[333/625] Time:0.704, Train Loss:0.3938274681568146\n",
      "Epoch 10[334/625] Time:0.704, Train Loss:0.3070186376571655\n",
      "Epoch 10[335/625] Time:0.702, Train Loss:0.29984498023986816\n",
      "Epoch 10[336/625] Time:0.703, Train Loss:0.3339313268661499\n",
      "Epoch 10[337/625] Time:0.703, Train Loss:0.31274133920669556\n",
      "Epoch 10[338/625] Time:0.704, Train Loss:0.32987648248672485\n",
      "Epoch 10[339/625] Time:0.737, Train Loss:0.25207987427711487\n",
      "Epoch 10[340/625] Time:0.693, Train Loss:0.2696729004383087\n",
      "Epoch 10[341/625] Time:0.694, Train Loss:0.25306153297424316\n",
      "Epoch 10[342/625] Time:0.696, Train Loss:0.2905595600605011\n",
      "Epoch 10[343/625] Time:0.695, Train Loss:0.28640568256378174\n",
      "Epoch 10[344/625] Time:0.695, Train Loss:0.2751785218715668\n",
      "Epoch 10[345/625] Time:0.694, Train Loss:0.23688967525959015\n",
      "Epoch 10[346/625] Time:0.698, Train Loss:0.2688268721103668\n",
      "Epoch 10[347/625] Time:0.696, Train Loss:0.2537496089935303\n",
      "Epoch 10[348/625] Time:0.704, Train Loss:0.23498083651065826\n",
      "Epoch 10[349/625] Time:0.702, Train Loss:0.38291478157043457\n",
      "Epoch 10[350/625] Time:0.702, Train Loss:0.4698205292224884\n",
      "Epoch 10[351/625] Time:0.7, Train Loss:0.31804561614990234\n",
      "Epoch 10[352/625] Time:0.72, Train Loss:0.22115245461463928\n",
      "Epoch 10[353/625] Time:0.702, Train Loss:0.35476669669151306\n",
      "Epoch 10[354/625] Time:0.704, Train Loss:0.2807815968990326\n",
      "Epoch 10[355/625] Time:0.703, Train Loss:0.23612651228904724\n",
      "Epoch 10[356/625] Time:0.703, Train Loss:0.36061733961105347\n",
      "Epoch 10[357/625] Time:0.703, Train Loss:0.3694506287574768\n",
      "Epoch 10[358/625] Time:0.703, Train Loss:0.385073721408844\n",
      "Epoch 10[359/625] Time:0.703, Train Loss:0.3204646706581116\n",
      "Epoch 10[360/625] Time:0.704, Train Loss:0.2743159532546997\n",
      "Epoch 10[361/625] Time:0.703, Train Loss:0.3335159420967102\n",
      "Epoch 10[362/625] Time:0.706, Train Loss:0.3249807357788086\n",
      "Epoch 10[363/625] Time:0.703, Train Loss:0.3464599847793579\n",
      "Epoch 10[364/625] Time:0.705, Train Loss:0.262373685836792\n",
      "Epoch 10[365/625] Time:0.703, Train Loss:0.32177218794822693\n",
      "Epoch 10[366/625] Time:0.703, Train Loss:0.27012839913368225\n",
      "Epoch 10[367/625] Time:0.702, Train Loss:0.3376632630825043\n",
      "Epoch 10[368/625] Time:0.702, Train Loss:0.2780539393424988\n",
      "Epoch 10[369/625] Time:0.704, Train Loss:0.38665127754211426\n",
      "Epoch 10[370/625] Time:0.709, Train Loss:0.30227553844451904\n",
      "Epoch 10[371/625] Time:0.703, Train Loss:0.32543012499809265\n",
      "Epoch 10[372/625] Time:0.704, Train Loss:0.3365647494792938\n",
      "Epoch 10[373/625] Time:0.703, Train Loss:0.2529992163181305\n",
      "Epoch 10[374/625] Time:0.703, Train Loss:0.3325347602367401\n",
      "Epoch 10[375/625] Time:0.702, Train Loss:0.32137230038642883\n",
      "Epoch 10[376/625] Time:0.702, Train Loss:0.2412797212600708\n",
      "Epoch 10[377/625] Time:0.702, Train Loss:0.2457677721977234\n",
      "Epoch 10[378/625] Time:0.704, Train Loss:0.3225255310535431\n",
      "Epoch 10[379/625] Time:0.702, Train Loss:0.30824071168899536\n",
      "Epoch 10[380/625] Time:0.703, Train Loss:0.23811613023281097\n",
      "Epoch 10[381/625] Time:0.704, Train Loss:0.2962656617164612\n",
      "Epoch 10[382/625] Time:0.704, Train Loss:0.3202991187572479\n",
      "Epoch 10[383/625] Time:0.704, Train Loss:0.23305334150791168\n",
      "Epoch 10[384/625] Time:0.704, Train Loss:0.3035949766635895\n",
      "Epoch 10[385/625] Time:0.706, Train Loss:0.25222620368003845\n",
      "Epoch 10[386/625] Time:0.709, Train Loss:0.2811225354671478\n",
      "Epoch 10[387/625] Time:0.706, Train Loss:0.24633681774139404\n",
      "Epoch 10[388/625] Time:0.73, Train Loss:0.2631520926952362\n",
      "Epoch 10[389/625] Time:0.693, Train Loss:0.3494216203689575\n",
      "Epoch 10[390/625] Time:0.705, Train Loss:0.28260037302970886\n",
      "Epoch 10[391/625] Time:0.703, Train Loss:0.29774391651153564\n",
      "Epoch 10[392/625] Time:0.703, Train Loss:0.2679048776626587\n",
      "Epoch 10[393/625] Time:0.703, Train Loss:0.2306661754846573\n",
      "Epoch 10[394/625] Time:0.703, Train Loss:0.2971836030483246\n",
      "Epoch 10[395/625] Time:0.703, Train Loss:0.2182331085205078\n",
      "Epoch 10[396/625] Time:0.702, Train Loss:0.3209669291973114\n",
      "Epoch 10[397/625] Time:0.704, Train Loss:0.3297629952430725\n",
      "Epoch 10[398/625] Time:0.703, Train Loss:0.41779494285583496\n",
      "Epoch 10[399/625] Time:0.703, Train Loss:0.31487295031547546\n",
      "Epoch 10[400/625] Time:0.702, Train Loss:0.33411017060279846\n",
      "Epoch 10[401/625] Time:0.703, Train Loss:0.328059583902359\n",
      "Epoch 10[402/625] Time:0.703, Train Loss:0.4071638286113739\n",
      "Epoch 10[403/625] Time:0.704, Train Loss:0.327290803194046\n",
      "Epoch 10[404/625] Time:0.703, Train Loss:0.3164508640766144\n",
      "Epoch 10[405/625] Time:0.719, Train Loss:0.26787060499191284\n",
      "Epoch 10[406/625] Time:0.704, Train Loss:0.29444894194602966\n",
      "Epoch 10[407/625] Time:0.704, Train Loss:0.3232855498790741\n",
      "Epoch 10[408/625] Time:0.702, Train Loss:0.25994643568992615\n",
      "Epoch 10[409/625] Time:0.703, Train Loss:0.3141099214553833\n",
      "Epoch 10[410/625] Time:0.703, Train Loss:0.3051842153072357\n",
      "Epoch 10[411/625] Time:0.703, Train Loss:0.3299936056137085\n",
      "Epoch 10[412/625] Time:0.703, Train Loss:0.3437597453594208\n",
      "Epoch 10[413/625] Time:0.703, Train Loss:0.31333959102630615\n",
      "Epoch 10[414/625] Time:0.702, Train Loss:0.2639661133289337\n",
      "Epoch 10[415/625] Time:0.706, Train Loss:0.33016303181648254\n",
      "Epoch 10[416/625] Time:0.693, Train Loss:0.32953014969825745\n",
      "Epoch 10[417/625] Time:0.693, Train Loss:0.3924641013145447\n",
      "Epoch 10[418/625] Time:0.695, Train Loss:0.34433096647262573\n",
      "Epoch 10[419/625] Time:0.705, Train Loss:0.25209641456604004\n",
      "Epoch 10[420/625] Time:0.704, Train Loss:0.29690101742744446\n",
      "Epoch 10[421/625] Time:0.703, Train Loss:0.3034648001194\n",
      "Epoch 10[422/625] Time:0.703, Train Loss:0.2985312044620514\n",
      "Epoch 10[423/625] Time:0.705, Train Loss:0.2790849804878235\n",
      "Epoch 10[424/625] Time:0.704, Train Loss:0.3711591958999634\n",
      "Epoch 10[425/625] Time:0.711, Train Loss:0.34431540966033936\n",
      "Epoch 10[426/625] Time:0.704, Train Loss:0.38039591908454895\n",
      "Epoch 10[427/625] Time:0.704, Train Loss:0.2698385715484619\n",
      "Epoch 10[428/625] Time:0.706, Train Loss:0.2728583812713623\n",
      "Epoch 10[429/625] Time:0.693, Train Loss:0.3179459869861603\n",
      "Epoch 10[430/625] Time:0.704, Train Loss:0.2873609662055969\n",
      "Epoch 10[431/625] Time:0.704, Train Loss:0.3026253879070282\n",
      "Epoch 10[432/625] Time:0.703, Train Loss:0.34652629494667053\n",
      "Epoch 10[433/625] Time:0.708, Train Loss:0.34315064549446106\n",
      "Epoch 10[434/625] Time:0.703, Train Loss:0.2927030622959137\n",
      "Epoch 10[435/625] Time:0.704, Train Loss:0.32455119490623474\n",
      "Epoch 10[436/625] Time:0.706, Train Loss:0.32198387384414673\n",
      "Epoch 10[437/625] Time:0.704, Train Loss:0.28946900367736816\n",
      "Epoch 10[438/625] Time:0.703, Train Loss:0.22728657722473145\n",
      "Epoch 10[439/625] Time:0.704, Train Loss:0.4673111140727997\n",
      "Epoch 10[440/625] Time:0.703, Train Loss:0.29191794991493225\n",
      "Epoch 10[441/625] Time:0.703, Train Loss:0.3088623583316803\n",
      "Epoch 10[442/625] Time:0.702, Train Loss:0.31299954652786255\n",
      "Epoch 10[443/625] Time:0.702, Train Loss:0.2920794188976288\n",
      "Epoch 10[444/625] Time:0.703, Train Loss:0.33901017904281616\n",
      "Epoch 10[445/625] Time:0.703, Train Loss:0.3270184397697449\n",
      "Epoch 10[446/625] Time:0.703, Train Loss:0.2664445638656616\n",
      "Epoch 10[447/625] Time:0.704, Train Loss:0.31760960817337036\n",
      "Epoch 10[448/625] Time:0.703, Train Loss:0.3847155272960663\n",
      "Epoch 10[449/625] Time:0.703, Train Loss:0.30205249786376953\n",
      "Epoch 10[450/625] Time:0.702, Train Loss:0.2765584886074066\n",
      "Epoch 10[451/625] Time:0.703, Train Loss:0.37234804034233093\n",
      "Epoch 10[452/625] Time:0.702, Train Loss:0.3335167467594147\n",
      "Epoch 10[453/625] Time:0.703, Train Loss:0.29120227694511414\n",
      "Epoch 10[454/625] Time:0.703, Train Loss:0.22787095606327057\n",
      "Epoch 10[455/625] Time:0.705, Train Loss:0.2816612720489502\n",
      "Epoch 10[456/625] Time:0.704, Train Loss:0.333094984292984\n",
      "Epoch 10[457/625] Time:0.704, Train Loss:0.24363331496715546\n",
      "Epoch 10[458/625] Time:0.703, Train Loss:0.24805836379528046\n",
      "Epoch 10[459/625] Time:0.702, Train Loss:0.36077335476875305\n",
      "Epoch 10[460/625] Time:0.704, Train Loss:0.3076309859752655\n",
      "Epoch 10[461/625] Time:0.703, Train Loss:0.29327577352523804\n",
      "Epoch 10[462/625] Time:0.704, Train Loss:0.3507765233516693\n",
      "Epoch 10[463/625] Time:0.703, Train Loss:0.37310871481895447\n",
      "Epoch 10[464/625] Time:0.704, Train Loss:0.26949378848075867\n",
      "Epoch 10[465/625] Time:0.711, Train Loss:0.2945438623428345\n",
      "Epoch 10[466/625] Time:0.702, Train Loss:0.3247012197971344\n",
      "Epoch 10[467/625] Time:0.702, Train Loss:0.30584460496902466\n",
      "Epoch 10[468/625] Time:0.713, Train Loss:0.28862565755844116\n",
      "Epoch 10[469/625] Time:0.716, Train Loss:0.4548536241054535\n",
      "Epoch 10[470/625] Time:0.704, Train Loss:0.2435900866985321\n",
      "Epoch 10[471/625] Time:0.704, Train Loss:0.37828171253204346\n",
      "Epoch 10[472/625] Time:0.702, Train Loss:0.32440701127052307\n",
      "Epoch 10[473/625] Time:0.704, Train Loss:0.3481913208961487\n",
      "Epoch 10[474/625] Time:0.704, Train Loss:0.3472093641757965\n",
      "Epoch 10[475/625] Time:0.694, Train Loss:0.40195077657699585\n",
      "Epoch 10[476/625] Time:0.704, Train Loss:0.2957775592803955\n",
      "Epoch 10[477/625] Time:0.703, Train Loss:0.307656854391098\n",
      "Epoch 10[478/625] Time:0.703, Train Loss:0.2784349322319031\n",
      "Epoch 10[479/625] Time:0.706, Train Loss:0.36338555812835693\n",
      "Epoch 10[480/625] Time:0.705, Train Loss:0.24092639982700348\n",
      "Epoch 10[481/625] Time:0.703, Train Loss:0.28092044591903687\n",
      "Epoch 10[482/625] Time:0.702, Train Loss:0.30709829926490784\n",
      "Epoch 10[483/625] Time:0.703, Train Loss:0.3420545160770416\n",
      "Epoch 10[484/625] Time:0.703, Train Loss:0.2996236979961395\n",
      "Epoch 10[485/625] Time:0.703, Train Loss:0.2886379063129425\n",
      "Epoch 10[486/625] Time:0.703, Train Loss:0.23462025821208954\n",
      "Epoch 10[487/625] Time:0.702, Train Loss:0.3012944757938385\n",
      "Epoch 10[488/625] Time:0.703, Train Loss:0.29632097482681274\n",
      "Epoch 10[489/625] Time:0.703, Train Loss:0.28605493903160095\n",
      "Epoch 10[490/625] Time:0.702, Train Loss:0.36332136392593384\n",
      "Epoch 10[491/625] Time:0.704, Train Loss:0.26470646262168884\n",
      "Epoch 10[492/625] Time:0.704, Train Loss:0.26228997111320496\n",
      "Epoch 10[493/625] Time:0.705, Train Loss:0.29319506883621216\n",
      "Epoch 10[494/625] Time:0.703, Train Loss:0.2684003412723541\n",
      "Epoch 10[495/625] Time:0.704, Train Loss:0.34861329197883606\n",
      "Epoch 10[496/625] Time:0.703, Train Loss:0.37175253033638\n",
      "Epoch 10[497/625] Time:0.704, Train Loss:0.3027304410934448\n",
      "Epoch 10[498/625] Time:0.703, Train Loss:0.3395901024341583\n",
      "Epoch 10[499/625] Time:0.702, Train Loss:0.18472567200660706\n",
      "Epoch 10[500/625] Time:0.705, Train Loss:0.35516029596328735\n",
      "Epoch 10[501/625] Time:0.702, Train Loss:0.2821756899356842\n",
      "Epoch 10[502/625] Time:0.703, Train Loss:0.31582823395729065\n",
      "Epoch 10[503/625] Time:0.703, Train Loss:0.26805350184440613\n",
      "Epoch 10[504/625] Time:0.703, Train Loss:0.42570218443870544\n",
      "Epoch 10[505/625] Time:0.702, Train Loss:0.33389008045196533\n",
      "Epoch 10[506/625] Time:0.703, Train Loss:0.3566356599330902\n",
      "Epoch 10[507/625] Time:0.702, Train Loss:0.3462006151676178\n",
      "Epoch 10[508/625] Time:0.703, Train Loss:0.40625154972076416\n",
      "Epoch 10[509/625] Time:0.702, Train Loss:0.3065991699695587\n",
      "Epoch 10[510/625] Time:0.702, Train Loss:0.2996477782726288\n",
      "Epoch 10[511/625] Time:0.711, Train Loss:0.2926070988178253\n",
      "Epoch 10[512/625] Time:0.727, Train Loss:0.3743560016155243\n",
      "Epoch 10[513/625] Time:0.702, Train Loss:0.2447354793548584\n",
      "Epoch 10[514/625] Time:0.702, Train Loss:0.2997364401817322\n",
      "Epoch 10[515/625] Time:0.704, Train Loss:0.3430761992931366\n",
      "Epoch 10[516/625] Time:0.704, Train Loss:0.33272379636764526\n",
      "Epoch 10[517/625] Time:0.703, Train Loss:0.28627488017082214\n",
      "Epoch 10[518/625] Time:0.704, Train Loss:0.3387000858783722\n",
      "Epoch 10[519/625] Time:0.704, Train Loss:0.25603821873664856\n",
      "Epoch 10[520/625] Time:0.704, Train Loss:0.3344840407371521\n",
      "Epoch 10[521/625] Time:0.703, Train Loss:0.2411310374736786\n",
      "Epoch 10[522/625] Time:0.704, Train Loss:0.3149285614490509\n",
      "Epoch 10[523/625] Time:0.702, Train Loss:0.25713977217674255\n",
      "Epoch 10[524/625] Time:0.703, Train Loss:0.2553153336048126\n",
      "Epoch 10[525/625] Time:0.703, Train Loss:0.2704870402812958\n",
      "Epoch 10[526/625] Time:0.703, Train Loss:0.35570237040519714\n",
      "Epoch 10[527/625] Time:0.703, Train Loss:0.2757374942302704\n",
      "Epoch 10[528/625] Time:0.703, Train Loss:0.31531035900115967\n",
      "Epoch 10[529/625] Time:0.705, Train Loss:0.3031849265098572\n",
      "Epoch 10[530/625] Time:0.704, Train Loss:0.2621152698993683\n",
      "Epoch 10[531/625] Time:0.703, Train Loss:0.3277997374534607\n",
      "Epoch 10[532/625] Time:0.705, Train Loss:0.36702021956443787\n",
      "Epoch 10[533/625] Time:0.702, Train Loss:0.300297349691391\n",
      "Epoch 10[534/625] Time:0.702, Train Loss:0.23076434433460236\n",
      "Epoch 10[535/625] Time:0.718, Train Loss:0.3294522166252136\n",
      "Epoch 10[536/625] Time:0.734, Train Loss:0.3060881495475769\n",
      "Epoch 10[537/625] Time:0.703, Train Loss:0.3377467691898346\n",
      "Epoch 10[538/625] Time:0.703, Train Loss:0.2326432317495346\n",
      "Epoch 10[539/625] Time:0.702, Train Loss:0.28475138545036316\n",
      "Epoch 10[540/625] Time:0.703, Train Loss:0.23771589994430542\n",
      "Epoch 10[541/625] Time:0.703, Train Loss:0.22696533799171448\n",
      "Epoch 10[542/625] Time:0.704, Train Loss:0.2717752158641815\n",
      "Epoch 10[543/625] Time:0.702, Train Loss:0.3466920852661133\n",
      "Epoch 10[544/625] Time:0.705, Train Loss:0.18043309450149536\n",
      "Epoch 10[545/625] Time:0.704, Train Loss:0.3246400058269501\n",
      "Epoch 10[546/625] Time:0.703, Train Loss:0.3186609745025635\n",
      "Epoch 10[547/625] Time:0.703, Train Loss:0.27483806014060974\n",
      "Epoch 10[548/625] Time:0.704, Train Loss:0.27865129709243774\n",
      "Epoch 10[549/625] Time:0.704, Train Loss:0.2893088757991791\n",
      "Epoch 10[550/625] Time:0.703, Train Loss:0.3539396822452545\n",
      "Epoch 10[551/625] Time:0.702, Train Loss:0.3095495104789734\n",
      "Epoch 10[552/625] Time:0.702, Train Loss:0.22183528542518616\n",
      "Epoch 10[553/625] Time:0.703, Train Loss:0.3039076328277588\n",
      "Epoch 10[554/625] Time:0.702, Train Loss:0.2847049832344055\n",
      "Epoch 10[555/625] Time:0.703, Train Loss:0.3163886070251465\n",
      "Epoch 10[556/625] Time:0.703, Train Loss:0.25628697872161865\n",
      "Epoch 10[557/625] Time:0.703, Train Loss:0.28419795632362366\n",
      "Epoch 10[558/625] Time:0.703, Train Loss:0.30300626158714294\n",
      "Epoch 10[559/625] Time:0.703, Train Loss:0.2626529037952423\n",
      "Epoch 10[560/625] Time:0.704, Train Loss:0.2958064377307892\n",
      "Epoch 10[561/625] Time:0.704, Train Loss:0.29244592785835266\n",
      "Epoch 10[562/625] Time:0.705, Train Loss:0.3051070272922516\n",
      "Epoch 10[563/625] Time:0.725, Train Loss:0.28899097442626953\n",
      "Epoch 10[564/625] Time:0.708, Train Loss:0.3210096061229706\n",
      "Epoch 10[565/625] Time:0.703, Train Loss:0.3266732096672058\n",
      "Epoch 10[566/625] Time:0.703, Train Loss:0.33396676182746887\n",
      "Epoch 10[567/625] Time:0.703, Train Loss:0.34899771213531494\n",
      "Epoch 10[568/625] Time:0.702, Train Loss:0.22055292129516602\n",
      "Epoch 10[569/625] Time:0.702, Train Loss:0.2935236096382141\n",
      "Epoch 10[570/625] Time:0.703, Train Loss:0.27756235003471375\n",
      "Epoch 10[571/625] Time:0.704, Train Loss:0.3598320186138153\n",
      "Epoch 10[572/625] Time:0.702, Train Loss:0.21864335238933563\n",
      "Epoch 10[573/625] Time:0.702, Train Loss:0.2928687334060669\n",
      "Epoch 10[574/625] Time:0.703, Train Loss:0.27759823203086853\n",
      "Epoch 10[575/625] Time:0.702, Train Loss:0.28361764550209045\n",
      "Epoch 10[576/625] Time:0.702, Train Loss:0.35677507519721985\n",
      "Epoch 10[577/625] Time:0.704, Train Loss:0.3106330931186676\n",
      "Epoch 10[578/625] Time:0.703, Train Loss:0.306729257106781\n",
      "Epoch 10[579/625] Time:0.702, Train Loss:0.26675641536712646\n",
      "Epoch 10[580/625] Time:0.703, Train Loss:0.32828786969184875\n",
      "Epoch 10[581/625] Time:0.704, Train Loss:0.340893030166626\n",
      "Epoch 10[582/625] Time:0.703, Train Loss:0.27069011330604553\n",
      "Epoch 10[583/625] Time:0.703, Train Loss:0.3395084738731384\n",
      "Epoch 10[584/625] Time:0.703, Train Loss:0.3177885115146637\n",
      "Epoch 10[585/625] Time:0.703, Train Loss:0.2152758538722992\n",
      "Epoch 10[586/625] Time:0.704, Train Loss:0.316699743270874\n",
      "Epoch 10[587/625] Time:0.75, Train Loss:0.285947322845459\n",
      "Epoch 10[588/625] Time:0.693, Train Loss:0.26856479048728943\n",
      "Epoch 10[589/625] Time:0.704, Train Loss:0.26418980956077576\n",
      "Epoch 10[590/625] Time:0.727, Train Loss:0.33096933364868164\n",
      "Epoch 10[591/625] Time:0.713, Train Loss:0.26597869396209717\n",
      "Epoch 10[592/625] Time:0.707, Train Loss:0.3812655806541443\n",
      "Epoch 10[593/625] Time:0.72, Train Loss:0.23463331162929535\n",
      "Epoch 10[594/625] Time:0.703, Train Loss:0.30716556310653687\n",
      "Epoch 10[595/625] Time:0.703, Train Loss:0.2478567659854889\n",
      "Epoch 10[596/625] Time:0.702, Train Loss:0.26143765449523926\n",
      "Epoch 10[597/625] Time:0.702, Train Loss:0.25694167613983154\n",
      "Epoch 10[598/625] Time:0.703, Train Loss:0.23367494344711304\n",
      "Epoch 10[599/625] Time:0.702, Train Loss:0.3264085650444031\n",
      "Epoch 10[600/625] Time:0.708, Train Loss:0.32736191153526306\n",
      "Epoch 10[601/625] Time:0.747, Train Loss:0.2395358830690384\n",
      "Epoch 10[602/625] Time:0.695, Train Loss:0.3417494297027588\n",
      "Epoch 10[603/625] Time:0.706, Train Loss:0.22971981763839722\n",
      "Epoch 10[604/625] Time:0.702, Train Loss:0.39614391326904297\n",
      "Epoch 10[605/625] Time:0.702, Train Loss:0.2901214063167572\n",
      "Epoch 10[606/625] Time:0.703, Train Loss:0.26058006286621094\n",
      "Epoch 10[607/625] Time:0.702, Train Loss:0.2645301818847656\n",
      "Epoch 10[608/625] Time:0.709, Train Loss:0.2550462782382965\n",
      "Epoch 10[609/625] Time:0.703, Train Loss:0.3185584247112274\n",
      "Epoch 10[610/625] Time:0.702, Train Loss:0.31112921237945557\n",
      "Epoch 10[611/625] Time:0.703, Train Loss:0.3058934509754181\n",
      "Epoch 10[612/625] Time:0.703, Train Loss:0.21923749148845673\n",
      "Epoch 10[613/625] Time:0.703, Train Loss:0.2762865126132965\n",
      "Epoch 10[614/625] Time:0.703, Train Loss:0.2365126609802246\n",
      "Epoch 10[615/625] Time:0.703, Train Loss:0.3128817081451416\n",
      "Epoch 10[616/625] Time:0.701, Train Loss:0.3051995038986206\n",
      "Epoch 10[617/625] Time:0.701, Train Loss:0.33939605951309204\n",
      "Epoch 10[618/625] Time:0.703, Train Loss:0.3182104229927063\n",
      "Epoch 10[619/625] Time:0.7, Train Loss:0.3074271082878113\n",
      "Epoch 10[620/625] Time:0.706, Train Loss:0.2717391550540924\n",
      "Epoch 10[621/625] Time:0.702, Train Loss:0.3906509280204773\n",
      "Epoch 10[622/625] Time:0.703, Train Loss:0.3741125464439392\n",
      "Epoch 10[623/625] Time:0.703, Train Loss:0.386184960603714\n",
      "Epoch 10[624/625] Time:0.709, Train Loss:0.1702623963356018\n",
      "Epoch 10[0/78] Val Loss:0.23945613205432892\n",
      "Epoch 10[1/78] Val Loss:0.22570449113845825\n",
      "Epoch 10[2/78] Val Loss:0.26218271255493164\n",
      "Epoch 10[3/78] Val Loss:0.2591916620731354\n",
      "Epoch 10[4/78] Val Loss:0.34860649704933167\n",
      "Epoch 10[5/78] Val Loss:0.2547557055950165\n",
      "Epoch 10[6/78] Val Loss:0.2522246837615967\n",
      "Epoch 10[7/78] Val Loss:0.3931991457939148\n",
      "Epoch 10[8/78] Val Loss:0.22551210224628448\n",
      "Epoch 10[9/78] Val Loss:0.14365921914577484\n",
      "Epoch 10[10/78] Val Loss:0.09934654086828232\n",
      "Epoch 10[11/78] Val Loss:0.1518823504447937\n",
      "Epoch 10[12/78] Val Loss:0.10510868579149246\n",
      "Epoch 10[13/78] Val Loss:0.09337279945611954\n",
      "Epoch 10[14/78] Val Loss:0.2541816234588623\n",
      "Epoch 10[15/78] Val Loss:0.19426670670509338\n",
      "Epoch 10[16/78] Val Loss:0.2495245486497879\n",
      "Epoch 10[17/78] Val Loss:0.1686299443244934\n",
      "Epoch 10[18/78] Val Loss:0.3147491216659546\n",
      "Epoch 10[19/78] Val Loss:0.44519564509391785\n",
      "Epoch 10[20/78] Val Loss:0.29491886496543884\n",
      "Epoch 10[21/78] Val Loss:0.5066407918930054\n",
      "Epoch 10[22/78] Val Loss:0.7256261110305786\n",
      "Epoch 10[23/78] Val Loss:0.5356854796409607\n",
      "Epoch 10[24/78] Val Loss:0.38387221097946167\n",
      "Epoch 10[25/78] Val Loss:0.46447068452835083\n",
      "Epoch 10[26/78] Val Loss:0.44197043776512146\n",
      "Epoch 10[27/78] Val Loss:0.40165814757347107\n",
      "Epoch 10[28/78] Val Loss:0.36952319741249084\n",
      "Epoch 10[29/78] Val Loss:0.45397230982780457\n",
      "Epoch 10[30/78] Val Loss:1.3320499658584595\n",
      "Epoch 10[31/78] Val Loss:1.1705487966537476\n",
      "Epoch 10[32/78] Val Loss:0.9700940251350403\n",
      "Epoch 10[33/78] Val Loss:0.4418184757232666\n",
      "Epoch 10[34/78] Val Loss:0.37905555963516235\n",
      "Epoch 10[35/78] Val Loss:0.335840106010437\n",
      "Epoch 10[36/78] Val Loss:0.3457392156124115\n",
      "Epoch 10[37/78] Val Loss:0.42749229073524475\n",
      "Epoch 10[38/78] Val Loss:0.22859343886375427\n",
      "Epoch 10[39/78] Val Loss:0.26548540592193604\n",
      "Epoch 10[40/78] Val Loss:0.24954918026924133\n",
      "Epoch 10[41/78] Val Loss:0.2548436224460602\n",
      "Epoch 10[42/78] Val Loss:0.2691265046596527\n",
      "Epoch 10[43/78] Val Loss:0.12496259063482285\n",
      "Epoch 10[44/78] Val Loss:0.09602151066064835\n",
      "Epoch 10[45/78] Val Loss:0.09088431298732758\n",
      "Epoch 10[46/78] Val Loss:0.0920599028468132\n",
      "Epoch 10[47/78] Val Loss:0.11010798066854477\n",
      "Epoch 10[48/78] Val Loss:0.1362425535917282\n",
      "Epoch 10[49/78] Val Loss:0.11701427400112152\n",
      "Epoch 10[50/78] Val Loss:0.08837541192770004\n",
      "Epoch 10[51/78] Val Loss:0.10103548318147659\n",
      "Epoch 10[52/78] Val Loss:0.12555022537708282\n",
      "Epoch 10[53/78] Val Loss:0.10844732820987701\n",
      "Epoch 10[54/78] Val Loss:0.07309504598379135\n",
      "Epoch 10[55/78] Val Loss:0.12743020057678223\n",
      "Epoch 10[56/78] Val Loss:0.41313061118125916\n",
      "Epoch 10[57/78] Val Loss:0.36876168847084045\n",
      "Epoch 10[58/78] Val Loss:0.3593408763408661\n",
      "Epoch 10[59/78] Val Loss:0.4046403169631958\n",
      "Epoch 10[60/78] Val Loss:0.3798381984233856\n",
      "Epoch 10[61/78] Val Loss:0.4443085491657257\n",
      "Epoch 10[62/78] Val Loss:0.46499690413475037\n",
      "Epoch 10[63/78] Val Loss:0.26740145683288574\n",
      "Epoch 10[64/78] Val Loss:0.1655733436346054\n",
      "Epoch 10[65/78] Val Loss:0.15140394866466522\n",
      "Epoch 10[66/78] Val Loss:0.19010497629642487\n",
      "Epoch 10[67/78] Val Loss:0.1899838000535965\n",
      "Epoch 10[68/78] Val Loss:0.4842657744884491\n",
      "Epoch 10[69/78] Val Loss:0.4190939962863922\n",
      "Epoch 10[70/78] Val Loss:0.45998501777648926\n",
      "Epoch 10[71/78] Val Loss:0.3852170407772064\n",
      "Epoch 10[72/78] Val Loss:0.301135390996933\n",
      "Epoch 10[73/78] Val Loss:0.2635374665260315\n",
      "Epoch 10[74/78] Val Loss:0.5514464378356934\n",
      "Epoch 10[75/78] Val Loss:0.6683441400527954\n",
      "Epoch 10[76/78] Val Loss:0.6017569899559021\n",
      "Epoch 10[77/78] Val Loss:0.6310399174690247\n",
      "Epoch 10[78/78] Val Loss:0.78473961353302\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.87      0.91     15691\n",
      "           1       0.64      0.82      0.72      4309\n",
      "\n",
      "    accuracy                           0.86     20000\n",
      "   macro avg       0.79      0.85      0.82     20000\n",
      "weighted avg       0.88      0.86      0.87     20000\n",
      "\n",
      "Epoch 10: Train Loss 0.3087588275432587, Val Loss 0.3419307489425708, Train Time 788.2106595039368, Val Time 36.54996371269226\n",
      "Epoch 11[0/625] Time:0.692, Train Loss:0.3332690894603729\n",
      "Epoch 11[1/625] Time:0.703, Train Loss:0.3172018826007843\n",
      "Epoch 11[2/625] Time:0.708, Train Loss:0.2634084224700928\n",
      "Epoch 11[3/625] Time:0.702, Train Loss:0.31552407145500183\n",
      "Epoch 11[4/625] Time:0.703, Train Loss:0.3052946627140045\n",
      "Epoch 11[5/625] Time:0.703, Train Loss:0.2851744294166565\n",
      "Epoch 11[6/625] Time:0.703, Train Loss:0.3329581916332245\n",
      "Epoch 11[7/625] Time:0.703, Train Loss:0.29462888836860657\n",
      "Epoch 11[8/625] Time:0.703, Train Loss:0.19979198276996613\n",
      "Epoch 11[9/625] Time:0.705, Train Loss:0.2696531116962433\n",
      "Epoch 11[10/625] Time:0.704, Train Loss:0.2381313294172287\n",
      "Epoch 11[11/625] Time:0.703, Train Loss:0.3069746792316437\n",
      "Epoch 11[12/625] Time:0.703, Train Loss:0.25977689027786255\n",
      "Epoch 11[13/625] Time:0.703, Train Loss:0.2560778856277466\n",
      "Epoch 11[14/625] Time:0.703, Train Loss:0.32526764273643494\n",
      "Epoch 11[15/625] Time:0.703, Train Loss:0.2553054094314575\n",
      "Epoch 11[16/625] Time:0.703, Train Loss:0.30195313692092896\n",
      "Epoch 11[17/625] Time:0.705, Train Loss:0.30391138792037964\n",
      "Epoch 11[18/625] Time:0.705, Train Loss:0.36774590611457825\n",
      "Epoch 11[19/625] Time:0.702, Train Loss:0.3009031414985657\n",
      "Epoch 11[20/625] Time:0.703, Train Loss:0.28460758924484253\n",
      "Epoch 11[21/625] Time:0.703, Train Loss:0.30248287320137024\n",
      "Epoch 11[22/625] Time:0.703, Train Loss:0.2959009110927582\n",
      "Epoch 11[23/625] Time:0.702, Train Loss:0.24408920109272003\n",
      "Epoch 11[24/625] Time:0.702, Train Loss:0.3062410354614258\n",
      "Epoch 11[25/625] Time:0.702, Train Loss:0.3279789388179779\n",
      "Epoch 11[26/625] Time:0.704, Train Loss:0.29236939549446106\n",
      "Epoch 11[27/625] Time:0.702, Train Loss:0.34230485558509827\n",
      "Epoch 11[28/625] Time:0.702, Train Loss:0.24216492474079132\n",
      "Epoch 11[29/625] Time:0.704, Train Loss:0.32966673374176025\n",
      "Epoch 11[30/625] Time:0.704, Train Loss:0.31532418727874756\n",
      "Epoch 11[31/625] Time:0.702, Train Loss:0.23502348363399506\n",
      "Epoch 11[32/625] Time:0.702, Train Loss:0.4421002268791199\n",
      "Epoch 11[33/625] Time:0.703, Train Loss:0.35854220390319824\n",
      "Epoch 11[34/625] Time:0.706, Train Loss:0.26341813802719116\n",
      "Epoch 11[35/625] Time:0.702, Train Loss:0.27246683835983276\n",
      "Epoch 11[36/625] Time:0.704, Train Loss:0.23313447833061218\n",
      "Epoch 11[37/625] Time:0.702, Train Loss:0.3551709055900574\n",
      "Epoch 11[38/625] Time:0.703, Train Loss:0.27467966079711914\n",
      "Epoch 11[39/625] Time:0.704, Train Loss:0.21685463190078735\n",
      "Epoch 11[40/625] Time:0.702, Train Loss:0.375469446182251\n",
      "Epoch 11[41/625] Time:0.703, Train Loss:0.22311963140964508\n",
      "Epoch 11[42/625] Time:0.703, Train Loss:0.2073313146829605\n",
      "Epoch 11[43/625] Time:0.704, Train Loss:0.3262844383716583\n",
      "Epoch 11[44/625] Time:0.703, Train Loss:0.2785971462726593\n",
      "Epoch 11[45/625] Time:0.702, Train Loss:0.33582207560539246\n",
      "Epoch 11[46/625] Time:0.703, Train Loss:0.27973830699920654\n",
      "Epoch 11[47/625] Time:0.703, Train Loss:0.29914480447769165\n",
      "Epoch 11[48/625] Time:0.702, Train Loss:0.32804372906684875\n",
      "Epoch 11[49/625] Time:0.702, Train Loss:0.2794942259788513\n",
      "Epoch 11[50/625] Time:0.704, Train Loss:0.2871864438056946\n",
      "Epoch 11[51/625] Time:0.723, Train Loss:0.3439019024372101\n",
      "Epoch 11[52/625] Time:0.732, Train Loss:0.2920681834220886\n",
      "Epoch 11[53/625] Time:0.7, Train Loss:0.3186569809913635\n",
      "Epoch 11[54/625] Time:0.714, Train Loss:0.3179512917995453\n",
      "Epoch 11[55/625] Time:0.694, Train Loss:0.38690653443336487\n",
      "Epoch 11[56/625] Time:0.694, Train Loss:0.26350414752960205\n",
      "Epoch 11[57/625] Time:0.726, Train Loss:0.31980258226394653\n",
      "Epoch 11[58/625] Time:0.704, Train Loss:0.34196987748146057\n",
      "Epoch 11[59/625] Time:0.704, Train Loss:0.2548660337924957\n",
      "Epoch 11[60/625] Time:0.704, Train Loss:0.4064818322658539\n",
      "Epoch 11[61/625] Time:0.702, Train Loss:0.287035197019577\n",
      "Epoch 11[62/625] Time:0.703, Train Loss:0.3014654517173767\n",
      "Epoch 11[63/625] Time:0.733, Train Loss:0.2934281826019287\n",
      "Epoch 11[64/625] Time:0.731, Train Loss:0.2555159032344818\n",
      "Epoch 11[65/625] Time:0.702, Train Loss:0.27086788415908813\n",
      "Epoch 11[66/625] Time:0.703, Train Loss:0.42158591747283936\n",
      "Epoch 11[67/625] Time:0.702, Train Loss:0.324065238237381\n",
      "Epoch 11[68/625] Time:0.703, Train Loss:0.3148646652698517\n",
      "Epoch 11[69/625] Time:0.703, Train Loss:0.30110517144203186\n",
      "Epoch 11[70/625] Time:0.693, Train Loss:0.3252709209918976\n",
      "Epoch 11[71/625] Time:0.706, Train Loss:0.2670521140098572\n",
      "Epoch 11[72/625] Time:0.704, Train Loss:0.24482057988643646\n",
      "Epoch 11[73/625] Time:0.712, Train Loss:0.3011513352394104\n",
      "Epoch 11[74/625] Time:0.704, Train Loss:0.3016709089279175\n",
      "Epoch 11[75/625] Time:0.703, Train Loss:0.3152048885822296\n",
      "Epoch 11[76/625] Time:0.704, Train Loss:0.36953985691070557\n",
      "Epoch 11[77/625] Time:0.706, Train Loss:0.24984754621982574\n",
      "Epoch 11[78/625] Time:0.705, Train Loss:0.388317734003067\n",
      "Epoch 11[79/625] Time:0.708, Train Loss:0.2931314706802368\n",
      "Epoch 11[80/625] Time:0.703, Train Loss:0.386696457862854\n",
      "Epoch 11[81/625] Time:0.709, Train Loss:0.2664412260055542\n",
      "Epoch 11[82/625] Time:0.704, Train Loss:0.27762413024902344\n",
      "Epoch 11[83/625] Time:0.706, Train Loss:0.2714463174343109\n",
      "Epoch 11[84/625] Time:0.703, Train Loss:0.29756560921669006\n",
      "Epoch 11[85/625] Time:0.703, Train Loss:0.27041491866111755\n",
      "Epoch 11[86/625] Time:0.703, Train Loss:0.3868672847747803\n",
      "Epoch 11[87/625] Time:0.703, Train Loss:0.26352235674858093\n",
      "Epoch 11[88/625] Time:0.703, Train Loss:0.28386399149894714\n",
      "Epoch 11[89/625] Time:0.706, Train Loss:0.26696866750717163\n",
      "Epoch 11[90/625] Time:0.703, Train Loss:0.306768536567688\n",
      "Epoch 11[91/625] Time:0.703, Train Loss:0.3592168688774109\n",
      "Epoch 11[92/625] Time:0.703, Train Loss:0.24837860465049744\n",
      "Epoch 11[93/625] Time:0.703, Train Loss:0.3230559527873993\n",
      "Epoch 11[94/625] Time:0.719, Train Loss:0.3450668752193451\n",
      "Epoch 11[95/625] Time:0.736, Train Loss:0.3519696593284607\n",
      "Epoch 11[96/625] Time:0.702, Train Loss:0.282142698764801\n",
      "Epoch 11[97/625] Time:0.711, Train Loss:0.27969369292259216\n",
      "Epoch 11[98/625] Time:0.701, Train Loss:0.2822403907775879\n",
      "Epoch 11[99/625] Time:0.703, Train Loss:0.3319876194000244\n",
      "Epoch 11[100/625] Time:0.703, Train Loss:0.2929714620113373\n",
      "Epoch 11[101/625] Time:0.703, Train Loss:0.32016244530677795\n",
      "Epoch 11[102/625] Time:0.704, Train Loss:0.2986196279525757\n",
      "Epoch 11[103/625] Time:0.703, Train Loss:0.3166067600250244\n",
      "Epoch 11[104/625] Time:0.703, Train Loss:0.3432886302471161\n",
      "Epoch 11[105/625] Time:0.709, Train Loss:0.31264761090278625\n",
      "Epoch 11[106/625] Time:0.702, Train Loss:0.277605265378952\n",
      "Epoch 11[107/625] Time:0.709, Train Loss:0.3359641432762146\n",
      "Epoch 11[108/625] Time:0.702, Train Loss:0.3423506021499634\n",
      "Epoch 11[109/625] Time:0.698, Train Loss:0.32223671674728394\n",
      "Epoch 11[110/625] Time:0.693, Train Loss:0.29507455229759216\n",
      "Epoch 11[111/625] Time:0.69, Train Loss:0.29659968614578247\n",
      "Epoch 11[112/625] Time:0.691, Train Loss:0.2545393705368042\n",
      "Epoch 11[113/625] Time:0.698, Train Loss:0.29142555594444275\n",
      "Epoch 11[114/625] Time:0.703, Train Loss:0.3242117762565613\n",
      "Epoch 11[115/625] Time:0.707, Train Loss:0.2935970425605774\n",
      "Epoch 11[116/625] Time:0.706, Train Loss:0.2745220363140106\n",
      "Epoch 11[117/625] Time:0.705, Train Loss:0.28371182084083557\n",
      "Epoch 11[118/625] Time:0.705, Train Loss:0.3175272047519684\n",
      "Epoch 11[119/625] Time:0.705, Train Loss:0.34166938066482544\n",
      "Epoch 11[120/625] Time:0.708, Train Loss:0.32588034868240356\n",
      "Epoch 11[121/625] Time:0.726, Train Loss:0.31939294934272766\n",
      "Epoch 11[122/625] Time:0.688, Train Loss:0.285042405128479\n",
      "Epoch 11[123/625] Time:0.69, Train Loss:0.30738458037376404\n",
      "Epoch 11[124/625] Time:0.691, Train Loss:0.2822103798389435\n",
      "Epoch 11[125/625] Time:0.704, Train Loss:0.3366546034812927\n",
      "Epoch 11[126/625] Time:0.74, Train Loss:0.23509927093982697\n",
      "Epoch 11[127/625] Time:0.692, Train Loss:0.34432438015937805\n",
      "Epoch 11[128/625] Time:0.696, Train Loss:0.27755001187324524\n",
      "Epoch 11[129/625] Time:0.704, Train Loss:0.3271460235118866\n",
      "Epoch 11[130/625] Time:0.702, Train Loss:0.3893507421016693\n",
      "Epoch 11[131/625] Time:0.702, Train Loss:0.23227708041667938\n",
      "Epoch 11[132/625] Time:0.704, Train Loss:0.2995394468307495\n",
      "Epoch 11[133/625] Time:0.701, Train Loss:0.27599281072616577\n",
      "Epoch 11[134/625] Time:0.702, Train Loss:0.3857722878456116\n",
      "Epoch 11[135/625] Time:0.702, Train Loss:0.28350186347961426\n",
      "Epoch 11[136/625] Time:0.705, Train Loss:0.322468101978302\n",
      "Epoch 11[137/625] Time:0.703, Train Loss:0.22733885049819946\n",
      "Epoch 11[138/625] Time:0.706, Train Loss:0.3017479181289673\n",
      "Epoch 11[139/625] Time:0.703, Train Loss:0.3430934548377991\n",
      "Epoch 11[140/625] Time:0.702, Train Loss:0.24025198817253113\n",
      "Epoch 11[141/625] Time:0.703, Train Loss:0.26719215512275696\n",
      "Epoch 11[142/625] Time:0.703, Train Loss:0.2961618900299072\n",
      "Epoch 11[143/625] Time:0.702, Train Loss:0.26085591316223145\n",
      "Epoch 11[144/625] Time:0.703, Train Loss:0.3496072590351105\n",
      "Epoch 11[145/625] Time:0.703, Train Loss:0.25649744272232056\n",
      "Epoch 11[146/625] Time:0.703, Train Loss:0.24997183680534363\n",
      "Epoch 11[147/625] Time:0.703, Train Loss:0.3869333267211914\n",
      "Epoch 11[148/625] Time:0.705, Train Loss:0.27167466282844543\n",
      "Epoch 11[149/625] Time:0.705, Train Loss:0.31854528188705444\n",
      "Epoch 11[150/625] Time:0.702, Train Loss:0.30547913908958435\n",
      "Epoch 11[151/625] Time:0.704, Train Loss:0.2742341458797455\n",
      "Epoch 11[152/625] Time:0.704, Train Loss:0.21215912699699402\n",
      "Epoch 11[153/625] Time:0.702, Train Loss:0.2604895532131195\n",
      "Epoch 11[154/625] Time:0.703, Train Loss:0.32520702481269836\n",
      "Epoch 11[155/625] Time:0.703, Train Loss:0.2937893569469452\n",
      "Epoch 11[156/625] Time:0.705, Train Loss:0.31985849142074585\n",
      "Epoch 11[157/625] Time:0.702, Train Loss:0.3636445105075836\n",
      "Epoch 11[158/625] Time:0.706, Train Loss:0.25862130522727966\n",
      "Epoch 11[159/625] Time:0.702, Train Loss:0.3023473620414734\n",
      "Epoch 11[160/625] Time:0.703, Train Loss:0.2995433509349823\n",
      "Epoch 11[161/625] Time:0.706, Train Loss:0.3273945450782776\n",
      "Epoch 11[162/625] Time:0.692, Train Loss:0.35049304366111755\n",
      "Epoch 11[163/625] Time:0.691, Train Loss:0.3297039866447449\n",
      "Epoch 11[164/625] Time:0.692, Train Loss:0.26651543378829956\n",
      "Epoch 11[165/625] Time:0.691, Train Loss:0.3628726601600647\n",
      "Epoch 11[166/625] Time:0.693, Train Loss:0.2820580303668976\n",
      "Epoch 11[167/625] Time:0.693, Train Loss:0.2650800347328186\n",
      "Epoch 11[168/625] Time:0.692, Train Loss:0.22893868386745453\n",
      "Epoch 11[169/625] Time:0.692, Train Loss:0.2603532373905182\n",
      "Epoch 11[170/625] Time:0.691, Train Loss:0.31117820739746094\n",
      "Epoch 11[171/625] Time:0.692, Train Loss:0.2993002235889435\n",
      "Epoch 11[172/625] Time:0.694, Train Loss:0.3028632402420044\n",
      "Epoch 11[173/625] Time:0.693, Train Loss:0.279265820980072\n",
      "Epoch 11[174/625] Time:0.693, Train Loss:0.30901142954826355\n",
      "Epoch 11[175/625] Time:0.693, Train Loss:0.2546304166316986\n",
      "Epoch 11[176/625] Time:0.722, Train Loss:0.27780240774154663\n",
      "Epoch 11[177/625] Time:0.702, Train Loss:0.30988895893096924\n",
      "Epoch 11[178/625] Time:0.703, Train Loss:0.2935805916786194\n",
      "Epoch 11[179/625] Time:0.703, Train Loss:0.30182236433029175\n",
      "Epoch 11[180/625] Time:0.704, Train Loss:0.2907300591468811\n",
      "Epoch 11[181/625] Time:0.703, Train Loss:0.2774999141693115\n",
      "Epoch 11[182/625] Time:0.703, Train Loss:0.3299204707145691\n",
      "Epoch 11[183/625] Time:0.704, Train Loss:0.34652405977249146\n",
      "Epoch 11[184/625] Time:0.72, Train Loss:0.3211594820022583\n",
      "Epoch 11[185/625] Time:0.703, Train Loss:0.3141033947467804\n",
      "Epoch 11[186/625] Time:0.703, Train Loss:0.32813557982444763\n",
      "Epoch 11[187/625] Time:0.703, Train Loss:0.3228384554386139\n",
      "Epoch 11[188/625] Time:0.707, Train Loss:0.3110863268375397\n",
      "Epoch 11[189/625] Time:0.702, Train Loss:0.3074604570865631\n",
      "Epoch 11[190/625] Time:0.702, Train Loss:0.3146459758281708\n",
      "Epoch 11[191/625] Time:0.709, Train Loss:0.30287158489227295\n",
      "Epoch 11[192/625] Time:0.703, Train Loss:0.28053539991378784\n",
      "Epoch 11[193/625] Time:0.703, Train Loss:0.23022285103797913\n",
      "Epoch 11[194/625] Time:0.703, Train Loss:0.36008477210998535\n",
      "Epoch 11[195/625] Time:0.702, Train Loss:0.3288130760192871\n",
      "Epoch 11[196/625] Time:0.702, Train Loss:0.27296581864356995\n",
      "Epoch 11[197/625] Time:0.702, Train Loss:0.23625178635120392\n",
      "Epoch 11[198/625] Time:0.702, Train Loss:0.2807236611843109\n",
      "Epoch 11[199/625] Time:0.703, Train Loss:0.35506686568260193\n",
      "Epoch 11[200/625] Time:0.704, Train Loss:0.2713829576969147\n",
      "Epoch 11[201/625] Time:0.702, Train Loss:0.23369358479976654\n",
      "Epoch 11[202/625] Time:0.703, Train Loss:0.2956894040107727\n",
      "Epoch 11[203/625] Time:0.703, Train Loss:0.28805574774742126\n",
      "Epoch 11[204/625] Time:0.702, Train Loss:0.2816041111946106\n",
      "Epoch 11[205/625] Time:0.703, Train Loss:0.260689377784729\n",
      "Epoch 11[206/625] Time:0.702, Train Loss:0.26048439741134644\n",
      "Epoch 11[207/625] Time:0.704, Train Loss:0.21194924414157867\n",
      "Epoch 11[208/625] Time:0.704, Train Loss:0.2669980823993683\n",
      "Epoch 11[209/625] Time:0.703, Train Loss:0.3188156485557556\n",
      "Epoch 11[210/625] Time:0.703, Train Loss:0.2982887029647827\n",
      "Epoch 11[211/625] Time:0.703, Train Loss:0.29390642046928406\n",
      "Epoch 11[212/625] Time:0.702, Train Loss:0.2837051451206207\n",
      "Epoch 11[213/625] Time:0.702, Train Loss:0.3752659559249878\n",
      "Epoch 11[214/625] Time:0.702, Train Loss:0.33520007133483887\n",
      "Epoch 11[215/625] Time:0.702, Train Loss:0.25716158747673035\n",
      "Epoch 11[216/625] Time:0.703, Train Loss:0.2986181676387787\n",
      "Epoch 11[217/625] Time:0.702, Train Loss:0.2400038242340088\n",
      "Epoch 11[218/625] Time:0.721, Train Loss:0.37042346596717834\n",
      "Epoch 11[219/625] Time:0.693, Train Loss:0.3503001630306244\n",
      "Epoch 11[220/625] Time:0.692, Train Loss:0.261584609746933\n",
      "Epoch 11[221/625] Time:0.722, Train Loss:0.34508228302001953\n",
      "Epoch 11[222/625] Time:0.707, Train Loss:0.28662562370300293\n",
      "Epoch 11[223/625] Time:0.693, Train Loss:0.31116071343421936\n",
      "Epoch 11[224/625] Time:0.695, Train Loss:0.313558429479599\n",
      "Epoch 11[225/625] Time:0.702, Train Loss:0.31954678893089294\n",
      "Epoch 11[226/625] Time:0.702, Train Loss:0.37334346771240234\n",
      "Epoch 11[227/625] Time:0.702, Train Loss:0.3016144037246704\n",
      "Epoch 11[228/625] Time:0.703, Train Loss:0.3671557903289795\n",
      "Epoch 11[229/625] Time:0.703, Train Loss:0.304911732673645\n",
      "Epoch 11[230/625] Time:0.693, Train Loss:0.320787638425827\n",
      "Epoch 11[231/625] Time:0.694, Train Loss:0.37051889300346375\n",
      "Epoch 11[232/625] Time:0.693, Train Loss:0.3193179666996002\n",
      "Epoch 11[233/625] Time:0.703, Train Loss:0.32974663376808167\n",
      "Epoch 11[234/625] Time:0.703, Train Loss:0.28964728116989136\n",
      "Epoch 11[235/625] Time:0.704, Train Loss:0.3140208423137665\n",
      "Epoch 11[236/625] Time:0.704, Train Loss:0.3967567980289459\n",
      "Epoch 11[237/625] Time:0.703, Train Loss:0.38048598170280457\n",
      "Epoch 11[238/625] Time:0.71, Train Loss:0.3365999758243561\n",
      "Epoch 11[239/625] Time:0.693, Train Loss:0.3350743055343628\n",
      "Epoch 11[240/625] Time:0.693, Train Loss:0.39776718616485596\n",
      "Epoch 11[241/625] Time:0.704, Train Loss:0.35604724287986755\n",
      "Epoch 11[242/625] Time:0.704, Train Loss:0.23612305521965027\n",
      "Epoch 11[243/625] Time:0.704, Train Loss:0.254675954580307\n",
      "Epoch 11[244/625] Time:0.705, Train Loss:0.2787410020828247\n",
      "Epoch 11[245/625] Time:0.706, Train Loss:0.32483670115470886\n",
      "Epoch 11[246/625] Time:0.704, Train Loss:0.352245032787323\n",
      "Epoch 11[247/625] Time:0.703, Train Loss:0.291815847158432\n",
      "Epoch 11[248/625] Time:0.705, Train Loss:0.31491583585739136\n",
      "Epoch 11[249/625] Time:0.705, Train Loss:0.28110793232917786\n",
      "Epoch 11[250/625] Time:0.703, Train Loss:0.2915448844432831\n",
      "Epoch 11[251/625] Time:0.705, Train Loss:0.3089698851108551\n",
      "Epoch 11[252/625] Time:0.703, Train Loss:0.3102378845214844\n",
      "Epoch 11[253/625] Time:0.707, Train Loss:0.25046631693840027\n",
      "Epoch 11[254/625] Time:0.702, Train Loss:0.40474823117256165\n",
      "Epoch 11[255/625] Time:0.703, Train Loss:0.3017635643482208\n",
      "Epoch 11[256/625] Time:0.704, Train Loss:0.4315060079097748\n",
      "Epoch 11[257/625] Time:0.702, Train Loss:0.27243971824645996\n",
      "Epoch 11[258/625] Time:0.702, Train Loss:0.3300725817680359\n",
      "Epoch 11[259/625] Time:0.703, Train Loss:0.35231825709342957\n",
      "Epoch 11[260/625] Time:0.702, Train Loss:0.3087109923362732\n",
      "Epoch 11[261/625] Time:0.703, Train Loss:0.22159229218959808\n",
      "Epoch 11[262/625] Time:0.704, Train Loss:0.22596704959869385\n",
      "Epoch 11[263/625] Time:0.703, Train Loss:0.2879589796066284\n",
      "Epoch 11[264/625] Time:0.703, Train Loss:0.2603810727596283\n",
      "Epoch 11[265/625] Time:0.702, Train Loss:0.2555142939090729\n",
      "Epoch 11[266/625] Time:0.703, Train Loss:0.3556762635707855\n",
      "Epoch 11[267/625] Time:0.703, Train Loss:0.2506680488586426\n",
      "Epoch 11[268/625] Time:0.703, Train Loss:0.3310927450656891\n",
      "Epoch 11[269/625] Time:0.702, Train Loss:0.21339264512062073\n",
      "Epoch 11[270/625] Time:0.703, Train Loss:0.2253163456916809\n",
      "Epoch 11[271/625] Time:0.703, Train Loss:0.269344300031662\n",
      "Epoch 11[272/625] Time:0.703, Train Loss:0.2783547043800354\n",
      "Epoch 11[273/625] Time:0.704, Train Loss:0.26549267768859863\n",
      "Epoch 11[274/625] Time:0.703, Train Loss:0.26837074756622314\n",
      "Epoch 11[275/625] Time:0.703, Train Loss:0.26870718598365784\n",
      "Epoch 11[276/625] Time:0.704, Train Loss:0.3471900522708893\n",
      "Epoch 11[277/625] Time:0.703, Train Loss:0.29862722754478455\n",
      "Epoch 11[278/625] Time:0.703, Train Loss:0.31715622544288635\n",
      "Epoch 11[279/625] Time:0.704, Train Loss:0.2782866358757019\n",
      "Epoch 11[280/625] Time:0.719, Train Loss:0.2761618196964264\n",
      "Epoch 11[281/625] Time:0.703, Train Loss:0.37140798568725586\n",
      "Epoch 11[282/625] Time:0.703, Train Loss:0.2668413817882538\n",
      "Epoch 11[283/625] Time:0.703, Train Loss:0.25509917736053467\n",
      "Epoch 11[284/625] Time:0.703, Train Loss:0.24075792729854584\n",
      "Epoch 11[285/625] Time:0.702, Train Loss:0.2761877179145813\n",
      "Epoch 11[286/625] Time:0.702, Train Loss:0.3344157338142395\n",
      "Epoch 11[287/625] Time:0.703, Train Loss:0.2694893181324005\n",
      "Epoch 11[288/625] Time:0.703, Train Loss:0.27363520860671997\n",
      "Epoch 11[289/625] Time:0.7, Train Loss:0.27204185724258423\n",
      "Epoch 11[290/625] Time:0.724, Train Loss:0.34830185770988464\n",
      "Epoch 11[291/625] Time:0.718, Train Loss:0.25284701585769653\n",
      "Epoch 11[292/625] Time:0.705, Train Loss:0.34661370515823364\n",
      "Epoch 11[293/625] Time:0.704, Train Loss:0.3625677227973938\n",
      "Epoch 11[294/625] Time:0.703, Train Loss:0.2635698914527893\n",
      "Epoch 11[295/625] Time:0.703, Train Loss:0.27886345982551575\n",
      "Epoch 11[296/625] Time:0.703, Train Loss:0.2855677306652069\n",
      "Epoch 11[297/625] Time:0.704, Train Loss:0.33535701036453247\n",
      "Epoch 11[298/625] Time:0.703, Train Loss:0.33043205738067627\n",
      "Epoch 11[299/625] Time:0.705, Train Loss:0.27658766508102417\n",
      "Epoch 11[300/625] Time:0.703, Train Loss:0.2988234758377075\n",
      "Epoch 11[301/625] Time:0.704, Train Loss:0.3119535744190216\n",
      "Epoch 11[302/625] Time:0.704, Train Loss:0.3672362267971039\n",
      "Epoch 11[303/625] Time:0.703, Train Loss:0.2746402323246002\n",
      "Epoch 11[304/625] Time:0.704, Train Loss:0.233042910695076\n",
      "Epoch 11[305/625] Time:0.704, Train Loss:0.3227541446685791\n",
      "Epoch 11[306/625] Time:0.704, Train Loss:0.3551247715950012\n",
      "Epoch 11[307/625] Time:0.704, Train Loss:0.2732546627521515\n",
      "Epoch 11[308/625] Time:0.703, Train Loss:0.3861435353755951\n",
      "Epoch 11[309/625] Time:0.709, Train Loss:0.3504428565502167\n",
      "Epoch 11[310/625] Time:0.706, Train Loss:0.3036295771598816\n",
      "Epoch 11[311/625] Time:0.711, Train Loss:0.25013962388038635\n",
      "Epoch 11[312/625] Time:0.703, Train Loss:0.3169122338294983\n",
      "Epoch 11[313/625] Time:0.704, Train Loss:0.30930060148239136\n",
      "Epoch 11[314/625] Time:0.722, Train Loss:0.4136662185192108\n",
      "Epoch 11[315/625] Time:0.692, Train Loss:0.21035534143447876\n",
      "Epoch 11[316/625] Time:0.694, Train Loss:0.2573961317539215\n",
      "Epoch 11[317/625] Time:0.692, Train Loss:0.27631518244743347\n",
      "Epoch 11[318/625] Time:0.693, Train Loss:0.3128664195537567\n",
      "Epoch 11[319/625] Time:0.704, Train Loss:0.35245129466056824\n",
      "Epoch 11[320/625] Time:0.703, Train Loss:0.3320501148700714\n",
      "Epoch 11[321/625] Time:0.703, Train Loss:0.3439065217971802\n",
      "Epoch 11[322/625] Time:0.703, Train Loss:0.30403801798820496\n",
      "Epoch 11[323/625] Time:0.703, Train Loss:0.2835882604122162\n",
      "Epoch 11[324/625] Time:0.703, Train Loss:0.30732569098472595\n",
      "Epoch 11[325/625] Time:0.706, Train Loss:0.3483775556087494\n",
      "Epoch 11[326/625] Time:0.704, Train Loss:0.30958759784698486\n",
      "Epoch 11[327/625] Time:0.707, Train Loss:0.23977282643318176\n",
      "Epoch 11[328/625] Time:0.705, Train Loss:0.30594974756240845\n",
      "Epoch 11[329/625] Time:0.702, Train Loss:0.31550332903862\n",
      "Epoch 11[330/625] Time:0.702, Train Loss:0.2583842873573303\n",
      "Epoch 11[331/625] Time:0.696, Train Loss:0.24732114374637604\n",
      "Epoch 11[332/625] Time:0.701, Train Loss:0.31707313656806946\n",
      "Epoch 11[333/625] Time:0.702, Train Loss:0.3916736841201782\n",
      "Epoch 11[334/625] Time:0.704, Train Loss:0.3533593416213989\n",
      "Epoch 11[335/625] Time:0.706, Train Loss:0.305713951587677\n",
      "Epoch 11[336/625] Time:0.703, Train Loss:0.3340302109718323\n",
      "Epoch 11[337/625] Time:0.705, Train Loss:0.33334407210350037\n",
      "Epoch 11[338/625] Time:0.705, Train Loss:0.22821687161922455\n",
      "Epoch 11[339/625] Time:0.705, Train Loss:0.2739836573600769\n",
      "Epoch 11[340/625] Time:0.705, Train Loss:0.303127259016037\n",
      "Epoch 11[341/625] Time:0.705, Train Loss:0.37318646907806396\n",
      "Epoch 11[342/625] Time:0.706, Train Loss:0.39785686135292053\n",
      "Epoch 11[343/625] Time:0.708, Train Loss:0.3916243314743042\n",
      "Epoch 11[344/625] Time:0.705, Train Loss:0.3469192385673523\n",
      "Epoch 11[345/625] Time:0.705, Train Loss:0.3255598247051239\n",
      "Epoch 11[346/625] Time:0.705, Train Loss:0.22963111102581024\n",
      "Epoch 11[347/625] Time:0.706, Train Loss:0.2389346808195114\n",
      "Epoch 11[348/625] Time:0.705, Train Loss:0.2169041633605957\n",
      "Epoch 11[349/625] Time:0.705, Train Loss:0.294180691242218\n",
      "Epoch 11[350/625] Time:0.704, Train Loss:0.21578827500343323\n",
      "Epoch 11[351/625] Time:0.71, Train Loss:0.3689068853855133\n",
      "Epoch 11[352/625] Time:0.705, Train Loss:0.2876323461532593\n",
      "Epoch 11[353/625] Time:0.705, Train Loss:0.3842606544494629\n",
      "Epoch 11[354/625] Time:0.704, Train Loss:0.4040539860725403\n",
      "Epoch 11[355/625] Time:0.706, Train Loss:0.29944175481796265\n",
      "Epoch 11[356/625] Time:0.704, Train Loss:0.31928521394729614\n",
      "Epoch 11[357/625] Time:0.705, Train Loss:0.2898128032684326\n",
      "Epoch 11[358/625] Time:0.705, Train Loss:0.3072730302810669\n",
      "Epoch 11[359/625] Time:0.705, Train Loss:0.32616186141967773\n",
      "Epoch 11[360/625] Time:0.705, Train Loss:0.29331356287002563\n",
      "Epoch 11[361/625] Time:0.705, Train Loss:0.2992079555988312\n",
      "Epoch 11[362/625] Time:0.706, Train Loss:0.33591794967651367\n",
      "Epoch 11[363/625] Time:0.706, Train Loss:0.3083341121673584\n",
      "Epoch 11[364/625] Time:0.705, Train Loss:0.3520168662071228\n",
      "Epoch 11[365/625] Time:0.705, Train Loss:0.3082999885082245\n",
      "Epoch 11[366/625] Time:0.707, Train Loss:0.2674749791622162\n",
      "Epoch 11[367/625] Time:0.709, Train Loss:0.30796733498573303\n",
      "Epoch 11[368/625] Time:0.708, Train Loss:0.37523016333580017\n",
      "Epoch 11[369/625] Time:0.704, Train Loss:0.2569008767604828\n",
      "Epoch 11[370/625] Time:0.703, Train Loss:0.2971560060977936\n",
      "Epoch 11[371/625] Time:0.704, Train Loss:0.23715969920158386\n",
      "Epoch 11[372/625] Time:0.704, Train Loss:0.2962474226951599\n",
      "Epoch 11[373/625] Time:0.703, Train Loss:0.29714125394821167\n",
      "Epoch 11[374/625] Time:0.71, Train Loss:0.31701889634132385\n",
      "Epoch 11[375/625] Time:0.729, Train Loss:0.27384302020072937\n",
      "Epoch 11[376/625] Time:0.719, Train Loss:0.22016677260398865\n",
      "Epoch 11[377/625] Time:0.706, Train Loss:0.17011228203773499\n",
      "Epoch 11[378/625] Time:0.747, Train Loss:0.22300492227077484\n",
      "Epoch 11[379/625] Time:0.693, Train Loss:0.26200681924819946\n",
      "Epoch 11[380/625] Time:0.692, Train Loss:0.23280055820941925\n",
      "Epoch 11[381/625] Time:0.692, Train Loss:0.28414538502693176\n",
      "Epoch 11[382/625] Time:0.695, Train Loss:0.2878335416316986\n",
      "Epoch 11[383/625] Time:0.693, Train Loss:0.3372499942779541\n",
      "Epoch 11[384/625] Time:0.693, Train Loss:0.2817074954509735\n",
      "Epoch 11[385/625] Time:0.692, Train Loss:0.21175584197044373\n",
      "Epoch 11[386/625] Time:0.705, Train Loss:0.34546706080436707\n",
      "Epoch 11[387/625] Time:0.705, Train Loss:0.2369188517332077\n",
      "Epoch 11[388/625] Time:0.704, Train Loss:0.24735072255134583\n",
      "Epoch 11[389/625] Time:0.704, Train Loss:0.2574285864830017\n",
      "Epoch 11[390/625] Time:0.708, Train Loss:0.35876229405403137\n",
      "Epoch 11[391/625] Time:0.704, Train Loss:0.28296151757240295\n",
      "Epoch 11[392/625] Time:0.705, Train Loss:0.3653331696987152\n",
      "Epoch 11[393/625] Time:0.704, Train Loss:0.3318227529525757\n",
      "Epoch 11[394/625] Time:0.705, Train Loss:0.28503894805908203\n",
      "Epoch 11[395/625] Time:0.705, Train Loss:0.34964704513549805\n",
      "Epoch 11[396/625] Time:0.704, Train Loss:0.26312780380249023\n",
      "Epoch 11[397/625] Time:0.705, Train Loss:0.2510608732700348\n",
      "Epoch 11[398/625] Time:0.705, Train Loss:0.2694346010684967\n",
      "Epoch 11[399/625] Time:0.705, Train Loss:0.28024229407310486\n",
      "Epoch 11[400/625] Time:0.707, Train Loss:0.21779124438762665\n",
      "Epoch 11[401/625] Time:0.703, Train Loss:0.3046998977661133\n",
      "Epoch 11[402/625] Time:0.703, Train Loss:0.27532634139060974\n",
      "Epoch 11[403/625] Time:0.703, Train Loss:0.2669798731803894\n",
      "Epoch 11[404/625] Time:0.703, Train Loss:0.21763120591640472\n",
      "Epoch 11[405/625] Time:0.703, Train Loss:0.2972329258918762\n",
      "Epoch 11[406/625] Time:0.703, Train Loss:0.2829234004020691\n",
      "Epoch 11[407/625] Time:0.703, Train Loss:0.302316814661026\n",
      "Epoch 11[408/625] Time:0.724, Train Loss:0.3309188187122345\n",
      "Epoch 11[409/625] Time:0.726, Train Loss:0.3666653037071228\n",
      "Epoch 11[410/625] Time:0.7, Train Loss:0.3054909408092499\n",
      "Epoch 11[411/625] Time:0.729, Train Loss:0.2939164340496063\n",
      "Epoch 11[412/625] Time:0.693, Train Loss:0.2869795858860016\n",
      "Epoch 11[413/625] Time:0.698, Train Loss:0.24615821242332458\n",
      "Epoch 11[414/625] Time:0.701, Train Loss:0.2174166738986969\n",
      "Epoch 11[415/625] Time:0.703, Train Loss:0.2878706753253937\n",
      "Epoch 11[416/625] Time:0.703, Train Loss:0.2764008641242981\n",
      "Epoch 11[417/625] Time:0.705, Train Loss:0.25366199016571045\n",
      "Epoch 11[418/625] Time:0.704, Train Loss:0.36780405044555664\n",
      "Epoch 11[419/625] Time:0.702, Train Loss:0.3032679855823517\n",
      "Epoch 11[420/625] Time:0.703, Train Loss:0.278654545545578\n",
      "Epoch 11[421/625] Time:0.704, Train Loss:0.3086809813976288\n",
      "Epoch 11[422/625] Time:0.702, Train Loss:0.30862873792648315\n",
      "Epoch 11[423/625] Time:0.705, Train Loss:0.2358837127685547\n",
      "Epoch 11[424/625] Time:0.704, Train Loss:0.2575741708278656\n",
      "Epoch 11[425/625] Time:0.705, Train Loss:0.3130534887313843\n",
      "Epoch 11[426/625] Time:0.704, Train Loss:0.2274274379014969\n",
      "Epoch 11[427/625] Time:0.704, Train Loss:0.29377996921539307\n",
      "Epoch 11[428/625] Time:0.704, Train Loss:0.23940278589725494\n",
      "Epoch 11[429/625] Time:0.704, Train Loss:0.2712710201740265\n",
      "Epoch 11[430/625] Time:0.704, Train Loss:0.3231521248817444\n",
      "Epoch 11[431/625] Time:0.704, Train Loss:0.24401751160621643\n",
      "Epoch 11[432/625] Time:0.705, Train Loss:0.30975404381752014\n",
      "Epoch 11[433/625] Time:0.704, Train Loss:0.35674893856048584\n",
      "Epoch 11[434/625] Time:0.724, Train Loss:0.30580610036849976\n",
      "Epoch 11[435/625] Time:0.69, Train Loss:0.23652856051921844\n",
      "Epoch 11[436/625] Time:0.695, Train Loss:0.24616298079490662\n",
      "Epoch 11[437/625] Time:0.695, Train Loss:0.3256895840167999\n",
      "Epoch 11[438/625] Time:0.703, Train Loss:0.29118984937667847\n",
      "Epoch 11[439/625] Time:0.703, Train Loss:0.35209450125694275\n",
      "Epoch 11[440/625] Time:0.703, Train Loss:0.3095768690109253\n",
      "Epoch 11[441/625] Time:0.703, Train Loss:0.30133455991744995\n",
      "Epoch 11[442/625] Time:0.703, Train Loss:0.2980123460292816\n",
      "Epoch 11[443/625] Time:0.703, Train Loss:0.2887037694454193\n",
      "Epoch 11[444/625] Time:0.704, Train Loss:0.3437851667404175\n",
      "Epoch 11[445/625] Time:0.706, Train Loss:0.39455872774124146\n",
      "Epoch 11[446/625] Time:0.703, Train Loss:0.41902053356170654\n",
      "Epoch 11[447/625] Time:0.705, Train Loss:0.2021956592798233\n",
      "Epoch 11[448/625] Time:0.702, Train Loss:0.3592456877231598\n",
      "Epoch 11[449/625] Time:0.703, Train Loss:0.34213241934776306\n",
      "Epoch 11[450/625] Time:0.706, Train Loss:0.24626381695270538\n",
      "Epoch 11[451/625] Time:0.704, Train Loss:0.318787157535553\n",
      "Epoch 11[452/625] Time:0.705, Train Loss:0.2792005240917206\n",
      "Epoch 11[453/625] Time:0.709, Train Loss:0.28736522793769836\n",
      "Epoch 11[454/625] Time:0.702, Train Loss:0.3179047107696533\n",
      "Epoch 11[455/625] Time:0.704, Train Loss:0.30691537261009216\n",
      "Epoch 11[456/625] Time:0.704, Train Loss:0.35418111085891724\n",
      "Epoch 11[457/625] Time:0.703, Train Loss:0.22019393742084503\n",
      "Epoch 11[458/625] Time:0.702, Train Loss:0.332211971282959\n",
      "Epoch 11[459/625] Time:0.704, Train Loss:0.256244421005249\n",
      "Epoch 11[460/625] Time:0.724, Train Loss:0.34634292125701904\n",
      "Epoch 11[461/625] Time:0.7, Train Loss:0.4353007674217224\n",
      "Epoch 11[462/625] Time:0.702, Train Loss:0.2359735667705536\n",
      "Epoch 11[463/625] Time:0.704, Train Loss:0.27430757880210876\n",
      "Epoch 11[464/625] Time:0.704, Train Loss:0.263129860162735\n",
      "Epoch 11[465/625] Time:0.706, Train Loss:0.3231790363788605\n",
      "Epoch 11[466/625] Time:0.706, Train Loss:0.30254316329956055\n",
      "Epoch 11[467/625] Time:0.703, Train Loss:0.31870967149734497\n",
      "Epoch 11[468/625] Time:0.703, Train Loss:0.3045581877231598\n",
      "Epoch 11[469/625] Time:0.708, Train Loss:0.3069750964641571\n",
      "Epoch 11[470/625] Time:0.704, Train Loss:0.25024694204330444\n",
      "Epoch 11[471/625] Time:0.703, Train Loss:0.3436259329319\n",
      "Epoch 11[472/625] Time:0.703, Train Loss:0.33238446712493896\n",
      "Epoch 11[473/625] Time:0.703, Train Loss:0.3390960693359375\n",
      "Epoch 11[474/625] Time:0.703, Train Loss:0.33433797955513\n",
      "Epoch 11[475/625] Time:0.703, Train Loss:0.3930852711200714\n",
      "Epoch 11[476/625] Time:0.703, Train Loss:0.2514421343803406\n",
      "Epoch 11[477/625] Time:0.703, Train Loss:0.2922689914703369\n",
      "Epoch 11[478/625] Time:0.704, Train Loss:0.31692713499069214\n",
      "Epoch 11[479/625] Time:0.703, Train Loss:0.31768378615379333\n",
      "Epoch 11[480/625] Time:0.704, Train Loss:0.3137030303478241\n",
      "Epoch 11[481/625] Time:0.703, Train Loss:0.3163174092769623\n",
      "Epoch 11[482/625] Time:0.704, Train Loss:0.37269923090934753\n",
      "Epoch 11[483/625] Time:0.704, Train Loss:0.32456931471824646\n",
      "Epoch 11[484/625] Time:0.704, Train Loss:0.3077618479728699\n",
      "Epoch 11[485/625] Time:0.704, Train Loss:0.3070811331272125\n",
      "Epoch 11[486/625] Time:0.702, Train Loss:0.3874187767505646\n",
      "Epoch 11[487/625] Time:0.704, Train Loss:0.321540892124176\n",
      "Epoch 11[488/625] Time:0.702, Train Loss:0.2704782783985138\n",
      "Epoch 11[489/625] Time:0.701, Train Loss:0.27785274386405945\n",
      "Epoch 11[490/625] Time:0.703, Train Loss:0.2872067987918854\n",
      "Epoch 11[491/625] Time:0.704, Train Loss:0.2717689871788025\n",
      "Epoch 11[492/625] Time:0.702, Train Loss:0.20970995724201202\n",
      "Epoch 11[493/625] Time:0.729, Train Loss:0.3092295527458191\n",
      "Epoch 11[494/625] Time:0.693, Train Loss:0.33846697211265564\n",
      "Epoch 11[495/625] Time:0.693, Train Loss:0.3127099573612213\n",
      "Epoch 11[496/625] Time:0.702, Train Loss:0.3108562231063843\n",
      "Epoch 11[497/625] Time:0.703, Train Loss:0.24640770256519318\n",
      "Epoch 11[498/625] Time:0.704, Train Loss:0.3990072011947632\n",
      "Epoch 11[499/625] Time:0.703, Train Loss:0.263452410697937\n",
      "Epoch 11[500/625] Time:0.704, Train Loss:0.2564798593521118\n",
      "Epoch 11[501/625] Time:0.703, Train Loss:0.24756550788879395\n",
      "Epoch 11[502/625] Time:0.705, Train Loss:0.2675139605998993\n",
      "Epoch 11[503/625] Time:0.704, Train Loss:0.23027658462524414\n",
      "Epoch 11[504/625] Time:0.695, Train Loss:0.27872276306152344\n",
      "Epoch 11[505/625] Time:0.705, Train Loss:0.3122708797454834\n",
      "Epoch 11[506/625] Time:0.705, Train Loss:0.2489495426416397\n",
      "Epoch 11[507/625] Time:0.703, Train Loss:0.27014413475990295\n",
      "Epoch 11[508/625] Time:0.703, Train Loss:0.2899194657802582\n",
      "Epoch 11[509/625] Time:0.703, Train Loss:0.3704133927822113\n",
      "Epoch 11[510/625] Time:0.703, Train Loss:0.2894535958766937\n",
      "Epoch 11[511/625] Time:0.702, Train Loss:0.26492345333099365\n",
      "Epoch 11[512/625] Time:0.703, Train Loss:0.30261972546577454\n",
      "Epoch 11[513/625] Time:0.703, Train Loss:0.41348594427108765\n",
      "Epoch 11[514/625] Time:0.693, Train Loss:0.30272117257118225\n",
      "Epoch 11[515/625] Time:0.712, Train Loss:0.4031648635864258\n",
      "Epoch 11[516/625] Time:0.712, Train Loss:0.3202531039714813\n",
      "Epoch 11[517/625] Time:0.694, Train Loss:0.3086099624633789\n",
      "Epoch 11[518/625] Time:0.694, Train Loss:0.26829952001571655\n",
      "Epoch 11[519/625] Time:0.692, Train Loss:0.3257308304309845\n",
      "Epoch 11[520/625] Time:0.693, Train Loss:0.26363375782966614\n",
      "Epoch 11[521/625] Time:0.692, Train Loss:0.36565592885017395\n",
      "Epoch 11[522/625] Time:0.694, Train Loss:0.324998676776886\n",
      "Epoch 11[523/625] Time:0.693, Train Loss:0.2743241786956787\n",
      "Epoch 11[524/625] Time:0.697, Train Loss:0.24350060522556305\n",
      "Epoch 11[525/625] Time:0.719, Train Loss:0.27716895937919617\n",
      "Epoch 11[526/625] Time:0.704, Train Loss:0.27874284982681274\n",
      "Epoch 11[527/625] Time:0.714, Train Loss:0.25754019618034363\n",
      "Epoch 11[528/625] Time:0.744, Train Loss:0.29566240310668945\n",
      "Epoch 11[529/625] Time:0.702, Train Loss:0.26915687322616577\n",
      "Epoch 11[530/625] Time:0.702, Train Loss:0.19987818598747253\n",
      "Epoch 11[531/625] Time:0.703, Train Loss:0.3132700026035309\n",
      "Epoch 11[532/625] Time:0.721, Train Loss:0.2986811399459839\n",
      "Epoch 11[533/625] Time:0.703, Train Loss:0.20938539505004883\n",
      "Epoch 11[534/625] Time:0.703, Train Loss:0.27336588501930237\n",
      "Epoch 11[535/625] Time:0.704, Train Loss:0.28865012526512146\n",
      "Epoch 11[536/625] Time:0.706, Train Loss:0.26381051540374756\n",
      "Epoch 11[537/625] Time:0.704, Train Loss:0.37853699922561646\n",
      "Epoch 11[538/625] Time:0.704, Train Loss:0.24824047088623047\n",
      "Epoch 11[539/625] Time:0.705, Train Loss:0.317465603351593\n",
      "Epoch 11[540/625] Time:0.704, Train Loss:0.2816791236400604\n",
      "Epoch 11[541/625] Time:0.704, Train Loss:0.306702584028244\n",
      "Epoch 11[542/625] Time:0.704, Train Loss:0.3152942955493927\n",
      "Epoch 11[543/625] Time:0.703, Train Loss:0.2843654751777649\n",
      "Epoch 11[544/625] Time:0.703, Train Loss:0.3428119719028473\n",
      "Epoch 11[545/625] Time:0.705, Train Loss:0.23254701495170593\n",
      "Epoch 11[546/625] Time:0.703, Train Loss:0.2807619869709015\n",
      "Epoch 11[547/625] Time:0.703, Train Loss:0.23553481698036194\n",
      "Epoch 11[548/625] Time:0.707, Train Loss:0.31121623516082764\n",
      "Epoch 11[549/625] Time:0.703, Train Loss:0.2644166350364685\n",
      "Epoch 11[550/625] Time:0.704, Train Loss:0.27627572417259216\n",
      "Epoch 11[551/625] Time:0.703, Train Loss:0.3527768552303314\n",
      "Epoch 11[552/625] Time:0.703, Train Loss:0.31808048486709595\n",
      "Epoch 11[553/625] Time:0.703, Train Loss:0.30929040908813477\n",
      "Epoch 11[554/625] Time:0.703, Train Loss:0.3335111737251282\n",
      "Epoch 11[555/625] Time:0.731, Train Loss:0.27530592679977417\n",
      "Epoch 11[556/625] Time:0.693, Train Loss:0.3395368754863739\n",
      "Epoch 11[557/625] Time:0.703, Train Loss:0.25231319665908813\n",
      "Epoch 11[558/625] Time:0.721, Train Loss:0.26648759841918945\n",
      "Epoch 11[559/625] Time:0.693, Train Loss:0.30136650800704956\n",
      "Epoch 11[560/625] Time:0.695, Train Loss:0.33359935879707336\n",
      "Epoch 11[561/625] Time:0.693, Train Loss:0.315318763256073\n",
      "Epoch 11[562/625] Time:0.696, Train Loss:0.2769123911857605\n",
      "Epoch 11[563/625] Time:0.694, Train Loss:0.28901466727256775\n",
      "Epoch 11[564/625] Time:0.693, Train Loss:0.3267963230609894\n",
      "Epoch 11[565/625] Time:0.702, Train Loss:0.3901701867580414\n",
      "Epoch 11[566/625] Time:0.702, Train Loss:0.22266650199890137\n",
      "Epoch 11[567/625] Time:0.702, Train Loss:0.2907404899597168\n",
      "Epoch 11[568/625] Time:0.703, Train Loss:0.3133237063884735\n",
      "Epoch 11[569/625] Time:0.704, Train Loss:0.3080148696899414\n",
      "Epoch 11[570/625] Time:0.703, Train Loss:0.33600708842277527\n",
      "Epoch 11[571/625] Time:0.715, Train Loss:0.2999124526977539\n",
      "Epoch 11[572/625] Time:0.693, Train Loss:0.26314109563827515\n",
      "Epoch 11[573/625] Time:0.694, Train Loss:0.28714653849601746\n",
      "Epoch 11[574/625] Time:0.704, Train Loss:0.3382623791694641\n",
      "Epoch 11[575/625] Time:0.702, Train Loss:0.27674010396003723\n",
      "Epoch 11[576/625] Time:0.703, Train Loss:0.26076239347457886\n",
      "Epoch 11[577/625] Time:0.704, Train Loss:0.28961965441703796\n",
      "Epoch 11[578/625] Time:0.704, Train Loss:0.24952177703380585\n",
      "Epoch 11[579/625] Time:0.703, Train Loss:0.3261234164237976\n",
      "Epoch 11[580/625] Time:0.703, Train Loss:0.37938106060028076\n",
      "Epoch 11[581/625] Time:0.703, Train Loss:0.23245811462402344\n",
      "Epoch 11[582/625] Time:0.702, Train Loss:0.2674369215965271\n",
      "Epoch 11[583/625] Time:0.704, Train Loss:0.2768944501876831\n",
      "Epoch 11[584/625] Time:0.703, Train Loss:0.37334948778152466\n",
      "Epoch 11[585/625] Time:0.705, Train Loss:0.36378154158592224\n",
      "Epoch 11[586/625] Time:0.704, Train Loss:0.2492675930261612\n",
      "Epoch 11[587/625] Time:0.704, Train Loss:0.3373209536075592\n",
      "Epoch 11[588/625] Time:0.742, Train Loss:0.29136431217193604\n",
      "Epoch 11[589/625] Time:0.695, Train Loss:0.262342244386673\n",
      "Epoch 11[590/625] Time:0.694, Train Loss:0.29511991143226624\n",
      "Epoch 11[591/625] Time:0.696, Train Loss:0.2661939263343811\n",
      "Epoch 11[592/625] Time:0.693, Train Loss:0.25678324699401855\n",
      "Epoch 11[593/625] Time:0.692, Train Loss:0.3681279420852661\n",
      "Epoch 11[594/625] Time:0.695, Train Loss:0.3013015687465668\n",
      "Epoch 11[595/625] Time:0.703, Train Loss:0.3146057724952698\n",
      "Epoch 11[596/625] Time:0.703, Train Loss:0.2830353379249573\n",
      "Epoch 11[597/625] Time:0.704, Train Loss:0.2832314968109131\n",
      "Epoch 11[598/625] Time:0.719, Train Loss:0.2722940444946289\n",
      "Epoch 11[599/625] Time:0.693, Train Loss:0.24651852250099182\n",
      "Epoch 11[600/625] Time:0.692, Train Loss:0.2380169779062271\n",
      "Epoch 11[601/625] Time:0.693, Train Loss:0.2872362434864044\n",
      "Epoch 11[602/625] Time:0.703, Train Loss:0.3602554500102997\n",
      "Epoch 11[603/625] Time:0.704, Train Loss:0.35100582242012024\n",
      "Epoch 11[604/625] Time:0.704, Train Loss:0.33009129762649536\n",
      "Epoch 11[605/625] Time:0.732, Train Loss:0.3676518201828003\n",
      "Epoch 11[606/625] Time:0.703, Train Loss:0.31494584679603577\n",
      "Epoch 11[607/625] Time:0.713, Train Loss:0.3670312464237213\n",
      "Epoch 11[608/625] Time:0.736, Train Loss:0.3552706241607666\n",
      "Epoch 11[609/625] Time:0.704, Train Loss:0.3761547803878784\n",
      "Epoch 11[610/625] Time:0.707, Train Loss:0.26348552107810974\n",
      "Epoch 11[611/625] Time:0.704, Train Loss:0.2879420816898346\n",
      "Epoch 11[612/625] Time:0.704, Train Loss:0.34777361154556274\n",
      "Epoch 11[613/625] Time:0.704, Train Loss:0.30280181765556335\n",
      "Epoch 11[614/625] Time:0.705, Train Loss:0.3406379222869873\n",
      "Epoch 11[615/625] Time:0.749, Train Loss:0.31364956498146057\n",
      "Epoch 11[616/625] Time:0.702, Train Loss:0.30623650550842285\n",
      "Epoch 11[617/625] Time:0.703, Train Loss:0.27393171191215515\n",
      "Epoch 11[618/625] Time:0.71, Train Loss:0.3263377547264099\n",
      "Epoch 11[619/625] Time:0.703, Train Loss:0.34562167525291443\n",
      "Epoch 11[620/625] Time:0.702, Train Loss:0.36664921045303345\n",
      "Epoch 11[621/625] Time:0.704, Train Loss:0.2647164762020111\n",
      "Epoch 11[622/625] Time:0.703, Train Loss:0.32543572783470154\n",
      "Epoch 11[623/625] Time:0.703, Train Loss:0.25297731161117554\n",
      "Epoch 11[624/625] Time:0.704, Train Loss:0.2871461808681488\n",
      "Epoch 11[0/78] Val Loss:0.2796047627925873\n",
      "Epoch 11[1/78] Val Loss:0.271796852350235\n",
      "Epoch 11[2/78] Val Loss:0.3274092972278595\n",
      "Epoch 11[3/78] Val Loss:0.30709582567214966\n",
      "Epoch 11[4/78] Val Loss:0.3961503505706787\n",
      "Epoch 11[5/78] Val Loss:0.3171258270740509\n",
      "Epoch 11[6/78] Val Loss:0.33493882417678833\n",
      "Epoch 11[7/78] Val Loss:0.41903167963027954\n",
      "Epoch 11[8/78] Val Loss:0.2530508041381836\n",
      "Epoch 11[9/78] Val Loss:0.16075602173805237\n",
      "Epoch 11[10/78] Val Loss:0.11849553138017654\n",
      "Epoch 11[11/78] Val Loss:0.1503864824771881\n",
      "Epoch 11[12/78] Val Loss:0.1240328848361969\n",
      "Epoch 11[13/78] Val Loss:0.1147303432226181\n",
      "Epoch 11[14/78] Val Loss:0.28548291325569153\n",
      "Epoch 11[15/78] Val Loss:0.2312631458044052\n",
      "Epoch 11[16/78] Val Loss:0.2562744617462158\n",
      "Epoch 11[17/78] Val Loss:0.22075307369232178\n",
      "Epoch 11[18/78] Val Loss:0.41744035482406616\n",
      "Epoch 11[19/78] Val Loss:0.5242976546287537\n",
      "Epoch 11[20/78] Val Loss:0.3771428167819977\n",
      "Epoch 11[21/78] Val Loss:0.5821812748908997\n",
      "Epoch 11[22/78] Val Loss:0.7783423662185669\n",
      "Epoch 11[23/78] Val Loss:0.5528565049171448\n",
      "Epoch 11[24/78] Val Loss:0.3581169545650482\n",
      "Epoch 11[25/78] Val Loss:0.42460110783576965\n",
      "Epoch 11[26/78] Val Loss:0.46498677134513855\n",
      "Epoch 11[27/78] Val Loss:0.40179678797721863\n",
      "Epoch 11[28/78] Val Loss:0.37693411111831665\n",
      "Epoch 11[29/78] Val Loss:0.4329717457294464\n",
      "Epoch 11[30/78] Val Loss:1.0230491161346436\n",
      "Epoch 11[31/78] Val Loss:0.8995125889778137\n",
      "Epoch 11[32/78] Val Loss:0.8441194295883179\n",
      "Epoch 11[33/78] Val Loss:0.6218754053115845\n",
      "Epoch 11[34/78] Val Loss:0.5446168184280396\n",
      "Epoch 11[35/78] Val Loss:0.4683045446872711\n",
      "Epoch 11[36/78] Val Loss:0.5446343421936035\n",
      "Epoch 11[37/78] Val Loss:0.5774101614952087\n",
      "Epoch 11[38/78] Val Loss:0.35567137598991394\n",
      "Epoch 11[39/78] Val Loss:0.3703642785549164\n",
      "Epoch 11[40/78] Val Loss:0.3764350116252899\n",
      "Epoch 11[41/78] Val Loss:0.3705146610736847\n",
      "Epoch 11[42/78] Val Loss:0.38719403743743896\n",
      "Epoch 11[43/78] Val Loss:0.20300111174583435\n",
      "Epoch 11[44/78] Val Loss:0.11687150597572327\n",
      "Epoch 11[45/78] Val Loss:0.13150948286056519\n",
      "Epoch 11[46/78] Val Loss:0.0941498875617981\n",
      "Epoch 11[47/78] Val Loss:0.11647666990756989\n",
      "Epoch 11[48/78] Val Loss:0.14401774108409882\n",
      "Epoch 11[49/78] Val Loss:0.08865639567375183\n",
      "Epoch 11[50/78] Val Loss:0.09187744557857513\n",
      "Epoch 11[51/78] Val Loss:0.10196184366941452\n",
      "Epoch 11[52/78] Val Loss:0.1469011902809143\n",
      "Epoch 11[53/78] Val Loss:0.08560290932655334\n",
      "Epoch 11[54/78] Val Loss:0.1002153605222702\n",
      "Epoch 11[55/78] Val Loss:0.1541144847869873\n",
      "Epoch 11[56/78] Val Loss:0.767611563205719\n",
      "Epoch 11[57/78] Val Loss:0.7020177245140076\n",
      "Epoch 11[58/78] Val Loss:0.6560417413711548\n",
      "Epoch 11[59/78] Val Loss:0.37451088428497314\n",
      "Epoch 11[60/78] Val Loss:0.340543657541275\n",
      "Epoch 11[61/78] Val Loss:0.3775416314601898\n",
      "Epoch 11[62/78] Val Loss:0.4302588999271393\n",
      "Epoch 11[63/78] Val Loss:0.2737389802932739\n",
      "Epoch 11[64/78] Val Loss:0.2614087462425232\n",
      "Epoch 11[65/78] Val Loss:0.27805790305137634\n",
      "Epoch 11[66/78] Val Loss:0.30822721123695374\n",
      "Epoch 11[67/78] Val Loss:0.28770679235458374\n",
      "Epoch 11[68/78] Val Loss:0.6004822254180908\n",
      "Epoch 11[69/78] Val Loss:0.6577896475791931\n",
      "Epoch 11[70/78] Val Loss:0.6068459153175354\n",
      "Epoch 11[71/78] Val Loss:0.5709093809127808\n",
      "Epoch 11[72/78] Val Loss:0.3362113833427429\n",
      "Epoch 11[73/78] Val Loss:0.2968355715274811\n",
      "Epoch 11[74/78] Val Loss:0.6571457386016846\n",
      "Epoch 11[75/78] Val Loss:0.7583751082420349\n",
      "Epoch 11[76/78] Val Loss:0.7542115449905396\n",
      "Epoch 11[77/78] Val Loss:0.8100984692573547\n",
      "Epoch 11[78/78] Val Loss:0.9663378596305847\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.82      0.88     15691\n",
      "           1       0.57      0.89      0.70      4309\n",
      "\n",
      "    accuracy                           0.83     20000\n",
      "   macro avg       0.77      0.85      0.79     20000\n",
      "weighted avg       0.88      0.83      0.84     20000\n",
      "\n",
      "Epoch 11: Train Loss 0.3005726791381836, Val Loss 0.4011796655563208, Train Time 791.1220285892487, Val Time 36.60655736923218\n",
      "Epoch 12[0/625] Time:0.689, Train Loss:0.363506555557251\n",
      "Epoch 12[1/625] Time:0.703, Train Loss:0.2578165531158447\n",
      "Epoch 12[2/625] Time:0.704, Train Loss:0.27919211983680725\n",
      "Epoch 12[3/625] Time:0.724, Train Loss:0.34251561760902405\n",
      "Epoch 12[4/625] Time:0.693, Train Loss:0.26196080446243286\n",
      "Epoch 12[5/625] Time:0.699, Train Loss:0.298129677772522\n",
      "Epoch 12[6/625] Time:0.704, Train Loss:0.2970285713672638\n",
      "Epoch 12[7/625] Time:0.704, Train Loss:0.28339850902557373\n",
      "Epoch 12[8/625] Time:0.704, Train Loss:0.29883190989494324\n",
      "Epoch 12[9/625] Time:0.703, Train Loss:0.27760693430900574\n",
      "Epoch 12[10/625] Time:0.721, Train Loss:0.2653173506259918\n",
      "Epoch 12[11/625] Time:0.704, Train Loss:0.2851286232471466\n",
      "Epoch 12[12/625] Time:0.704, Train Loss:0.29072755575180054\n",
      "Epoch 12[13/625] Time:0.692, Train Loss:0.39434048533439636\n",
      "Epoch 12[14/625] Time:0.701, Train Loss:0.38821282982826233\n",
      "Epoch 12[15/625] Time:0.705, Train Loss:0.3094397187232971\n",
      "Epoch 12[16/625] Time:0.705, Train Loss:0.3066198527812958\n",
      "Epoch 12[17/625] Time:0.704, Train Loss:0.2839929759502411\n",
      "Epoch 12[18/625] Time:0.703, Train Loss:0.3120832145214081\n",
      "Epoch 12[19/625] Time:0.703, Train Loss:0.2721707820892334\n",
      "Epoch 12[20/625] Time:0.703, Train Loss:0.35917747020721436\n",
      "Epoch 12[21/625] Time:0.703, Train Loss:0.3160875141620636\n",
      "Epoch 12[22/625] Time:0.703, Train Loss:0.22909672558307648\n",
      "Epoch 12[23/625] Time:0.703, Train Loss:0.22845995426177979\n",
      "Epoch 12[24/625] Time:0.705, Train Loss:0.1987859308719635\n",
      "Epoch 12[25/625] Time:0.704, Train Loss:0.3455180525779724\n",
      "Epoch 12[26/625] Time:0.704, Train Loss:0.3109774887561798\n",
      "Epoch 12[27/625] Time:0.705, Train Loss:0.3138486444950104\n",
      "Epoch 12[28/625] Time:0.703, Train Loss:0.2670137286186218\n",
      "Epoch 12[29/625] Time:0.706, Train Loss:0.2992350459098816\n",
      "Epoch 12[30/625] Time:0.703, Train Loss:0.28694143891334534\n",
      "Epoch 12[31/625] Time:0.703, Train Loss:0.27869194746017456\n",
      "Epoch 12[32/625] Time:0.703, Train Loss:0.23773784935474396\n",
      "Epoch 12[33/625] Time:0.704, Train Loss:0.2934841513633728\n",
      "Epoch 12[34/625] Time:0.706, Train Loss:0.29534730315208435\n",
      "Epoch 12[35/625] Time:0.702, Train Loss:0.24961940944194794\n",
      "Epoch 12[36/625] Time:0.703, Train Loss:0.3217730224132538\n",
      "Epoch 12[37/625] Time:0.705, Train Loss:0.41364800930023193\n",
      "Epoch 12[38/625] Time:0.692, Train Loss:0.2681853771209717\n",
      "Epoch 12[39/625] Time:0.694, Train Loss:0.277499258518219\n",
      "Epoch 12[40/625] Time:0.693, Train Loss:0.3350149095058441\n",
      "Epoch 12[41/625] Time:0.693, Train Loss:0.26897183060646057\n",
      "Epoch 12[42/625] Time:0.693, Train Loss:0.26286476850509644\n",
      "Epoch 12[43/625] Time:0.695, Train Loss:0.2752542197704315\n",
      "Epoch 12[44/625] Time:0.693, Train Loss:0.27656716108322144\n",
      "Epoch 12[45/625] Time:0.708, Train Loss:0.3256492018699646\n",
      "Epoch 12[46/625] Time:0.719, Train Loss:0.34006497263908386\n",
      "Epoch 12[47/625] Time:0.691, Train Loss:0.23496973514556885\n",
      "Epoch 12[48/625] Time:0.691, Train Loss:0.2825661301612854\n",
      "Epoch 12[49/625] Time:0.692, Train Loss:0.23722167313098907\n",
      "Epoch 12[50/625] Time:0.728, Train Loss:0.3086428642272949\n",
      "Epoch 12[51/625] Time:0.705, Train Loss:0.3099488914012909\n",
      "Epoch 12[52/625] Time:0.703, Train Loss:0.3176921308040619\n",
      "Epoch 12[53/625] Time:0.704, Train Loss:0.3039815425872803\n",
      "Epoch 12[54/625] Time:0.704, Train Loss:0.32097166776657104\n",
      "Epoch 12[55/625] Time:0.704, Train Loss:0.2519473433494568\n",
      "Epoch 12[56/625] Time:0.704, Train Loss:0.2646808326244354\n",
      "Epoch 12[57/625] Time:0.703, Train Loss:0.36993786692619324\n",
      "Epoch 12[58/625] Time:0.702, Train Loss:0.32434600591659546\n",
      "Epoch 12[59/625] Time:0.709, Train Loss:0.23827894032001495\n",
      "Epoch 12[60/625] Time:0.701, Train Loss:0.2969472408294678\n",
      "Epoch 12[61/625] Time:0.704, Train Loss:0.23852017521858215\n",
      "Epoch 12[62/625] Time:0.704, Train Loss:0.26253727078437805\n",
      "Epoch 12[63/625] Time:0.704, Train Loss:0.3122487962245941\n",
      "Epoch 12[64/625] Time:0.707, Train Loss:0.24340364336967468\n",
      "Epoch 12[65/625] Time:0.704, Train Loss:0.270947128534317\n",
      "Epoch 12[66/625] Time:0.703, Train Loss:0.3030441999435425\n",
      "Epoch 12[67/625] Time:0.71, Train Loss:0.2745661437511444\n",
      "Epoch 12[68/625] Time:0.703, Train Loss:0.3354029059410095\n",
      "Epoch 12[69/625] Time:0.706, Train Loss:0.321613073348999\n",
      "Epoch 12[70/625] Time:0.693, Train Loss:0.3098102807998657\n",
      "Epoch 12[71/625] Time:0.703, Train Loss:0.27229028940200806\n",
      "Epoch 12[72/625] Time:0.703, Train Loss:0.2875041663646698\n",
      "Epoch 12[73/625] Time:0.703, Train Loss:0.3108094036579132\n",
      "Epoch 12[74/625] Time:0.702, Train Loss:0.3578135669231415\n",
      "Epoch 12[75/625] Time:0.704, Train Loss:0.22933006286621094\n",
      "Epoch 12[76/625] Time:0.731, Train Loss:0.2793666124343872\n",
      "Epoch 12[77/625] Time:0.702, Train Loss:0.33227235078811646\n",
      "Epoch 12[78/625] Time:0.703, Train Loss:0.30946245789527893\n",
      "Epoch 12[79/625] Time:0.704, Train Loss:0.3495294749736786\n",
      "Epoch 12[80/625] Time:0.703, Train Loss:0.3060602843761444\n",
      "Epoch 12[81/625] Time:0.703, Train Loss:0.24352192878723145\n",
      "Epoch 12[82/625] Time:0.704, Train Loss:0.3472696840763092\n",
      "Epoch 12[83/625] Time:0.706, Train Loss:0.3706381916999817\n",
      "Epoch 12[84/625] Time:0.705, Train Loss:0.326916366815567\n",
      "Epoch 12[85/625] Time:0.706, Train Loss:0.29633885622024536\n",
      "Epoch 12[86/625] Time:0.703, Train Loss:0.2585468888282776\n",
      "Epoch 12[87/625] Time:0.703, Train Loss:0.25648435950279236\n",
      "Epoch 12[88/625] Time:0.703, Train Loss:0.27312394976615906\n",
      "Epoch 12[89/625] Time:0.706, Train Loss:0.25714364647865295\n",
      "Epoch 12[90/625] Time:0.701, Train Loss:0.31354430317878723\n",
      "Epoch 12[91/625] Time:0.704, Train Loss:0.2892455756664276\n",
      "Epoch 12[92/625] Time:0.704, Train Loss:0.31257718801498413\n",
      "Epoch 12[93/625] Time:0.704, Train Loss:0.3657515347003937\n",
      "Epoch 12[94/625] Time:0.704, Train Loss:0.30302348732948303\n",
      "Epoch 12[95/625] Time:0.704, Train Loss:0.2403792440891266\n",
      "Epoch 12[96/625] Time:0.704, Train Loss:0.2764910161495209\n",
      "Epoch 12[97/625] Time:0.704, Train Loss:0.2885509431362152\n",
      "Epoch 12[98/625] Time:0.704, Train Loss:0.26686951518058777\n",
      "Epoch 12[99/625] Time:0.712, Train Loss:0.3063131272792816\n",
      "Epoch 12[100/625] Time:0.692, Train Loss:0.31711235642433167\n",
      "Epoch 12[101/625] Time:0.693, Train Loss:0.28966933488845825\n",
      "Epoch 12[102/625] Time:0.693, Train Loss:0.28790104389190674\n",
      "Epoch 12[103/625] Time:0.692, Train Loss:0.24588188529014587\n",
      "Epoch 12[104/625] Time:0.692, Train Loss:0.34569859504699707\n",
      "Epoch 12[105/625] Time:0.692, Train Loss:0.21034570038318634\n",
      "Epoch 12[106/625] Time:0.692, Train Loss:0.3414674699306488\n",
      "Epoch 12[107/625] Time:0.692, Train Loss:0.33032000064849854\n",
      "Epoch 12[108/625] Time:0.693, Train Loss:0.27992379665374756\n",
      "Epoch 12[109/625] Time:0.696, Train Loss:0.293985515832901\n",
      "Epoch 12[110/625] Time:0.694, Train Loss:0.24790529906749725\n",
      "Epoch 12[111/625] Time:0.694, Train Loss:0.2876001298427582\n",
      "Epoch 12[112/625] Time:0.702, Train Loss:0.3880018889904022\n",
      "Epoch 12[113/625] Time:0.703, Train Loss:0.23937781155109406\n",
      "Epoch 12[114/625] Time:0.705, Train Loss:0.2727099657058716\n",
      "Epoch 12[115/625] Time:0.703, Train Loss:0.21479880809783936\n",
      "Epoch 12[116/625] Time:0.702, Train Loss:0.34473538398742676\n",
      "Epoch 12[117/625] Time:0.703, Train Loss:0.2604850232601166\n",
      "Epoch 12[118/625] Time:0.703, Train Loss:0.2648104429244995\n",
      "Epoch 12[119/625] Time:0.703, Train Loss:0.41267722845077515\n",
      "Epoch 12[120/625] Time:0.703, Train Loss:0.2828276753425598\n",
      "Epoch 12[121/625] Time:0.702, Train Loss:0.2968161702156067\n",
      "Epoch 12[122/625] Time:0.71, Train Loss:0.25528088212013245\n",
      "Epoch 12[123/625] Time:0.703, Train Loss:0.3100668787956238\n",
      "Epoch 12[124/625] Time:0.705, Train Loss:0.27584710717201233\n",
      "Epoch 12[125/625] Time:0.703, Train Loss:0.36294764280319214\n",
      "Epoch 12[126/625] Time:0.704, Train Loss:0.29578888416290283\n",
      "Epoch 12[127/625] Time:0.703, Train Loss:0.31102272868156433\n",
      "Epoch 12[128/625] Time:0.704, Train Loss:0.38296669721603394\n",
      "Epoch 12[129/625] Time:0.703, Train Loss:0.27239111065864563\n",
      "Epoch 12[130/625] Time:0.703, Train Loss:0.2459079921245575\n",
      "Epoch 12[131/625] Time:0.704, Train Loss:0.2493719458580017\n",
      "Epoch 12[132/625] Time:0.702, Train Loss:0.2896876931190491\n",
      "Epoch 12[133/625] Time:0.703, Train Loss:0.31961771845817566\n",
      "Epoch 12[134/625] Time:0.703, Train Loss:0.37041500210762024\n",
      "Epoch 12[135/625] Time:0.703, Train Loss:0.2859859764575958\n",
      "Epoch 12[136/625] Time:0.746, Train Loss:0.3358086049556732\n",
      "Epoch 12[137/625] Time:0.701, Train Loss:0.3169860243797302\n",
      "Epoch 12[138/625] Time:0.702, Train Loss:0.3093973994255066\n",
      "Epoch 12[139/625] Time:0.703, Train Loss:0.3822329640388489\n",
      "Epoch 12[140/625] Time:0.703, Train Loss:0.29564595222473145\n",
      "Epoch 12[141/625] Time:0.704, Train Loss:0.3148360252380371\n",
      "Epoch 12[142/625] Time:0.703, Train Loss:0.2622588276863098\n",
      "Epoch 12[143/625] Time:0.703, Train Loss:0.2776801586151123\n",
      "Epoch 12[144/625] Time:0.706, Train Loss:0.2913866341114044\n",
      "Epoch 12[145/625] Time:0.704, Train Loss:0.27744489908218384\n",
      "Epoch 12[146/625] Time:0.703, Train Loss:0.31749650835990906\n",
      "Epoch 12[147/625] Time:0.703, Train Loss:0.28080785274505615\n",
      "Epoch 12[148/625] Time:0.745, Train Loss:0.24442611634731293\n",
      "Epoch 12[149/625] Time:0.702, Train Loss:0.2794194221496582\n",
      "Epoch 12[150/625] Time:0.703, Train Loss:0.2698918282985687\n",
      "Epoch 12[151/625] Time:0.704, Train Loss:0.27586427330970764\n",
      "Epoch 12[152/625] Time:0.702, Train Loss:0.29335126280784607\n",
      "Epoch 12[153/625] Time:0.703, Train Loss:0.26163485646247864\n",
      "Epoch 12[154/625] Time:0.702, Train Loss:0.19330470263957977\n",
      "Epoch 12[155/625] Time:0.702, Train Loss:0.2849995791912079\n",
      "Epoch 12[156/625] Time:0.702, Train Loss:0.34351029992103577\n",
      "Epoch 12[157/625] Time:0.702, Train Loss:0.3056100904941559\n",
      "Epoch 12[158/625] Time:0.704, Train Loss:0.33121782541275024\n",
      "Epoch 12[159/625] Time:0.704, Train Loss:0.29762792587280273\n",
      "Epoch 12[160/625] Time:0.703, Train Loss:0.3377015292644501\n",
      "Epoch 12[161/625] Time:0.703, Train Loss:0.3899078369140625\n",
      "Epoch 12[162/625] Time:0.702, Train Loss:0.30371174216270447\n",
      "Epoch 12[163/625] Time:0.703, Train Loss:0.2731785178184509\n",
      "Epoch 12[164/625] Time:0.704, Train Loss:0.24597413837909698\n",
      "Epoch 12[165/625] Time:0.702, Train Loss:0.2865724563598633\n",
      "Epoch 12[166/625] Time:0.702, Train Loss:0.2677008807659149\n",
      "Epoch 12[167/625] Time:0.702, Train Loss:0.42918309569358826\n",
      "Epoch 12[168/625] Time:0.702, Train Loss:0.34488773345947266\n",
      "Epoch 12[169/625] Time:0.702, Train Loss:0.318655788898468\n",
      "Epoch 12[170/625] Time:0.702, Train Loss:0.29049232602119446\n",
      "Epoch 12[171/625] Time:0.702, Train Loss:0.36419910192489624\n",
      "Epoch 12[172/625] Time:0.702, Train Loss:0.23070836067199707\n",
      "Epoch 12[173/625] Time:0.703, Train Loss:0.33658015727996826\n",
      "Epoch 12[174/625] Time:0.703, Train Loss:0.33503448963165283\n",
      "Epoch 12[175/625] Time:0.703, Train Loss:0.23430661857128143\n",
      "Epoch 12[176/625] Time:0.702, Train Loss:0.37284138798713684\n",
      "Epoch 12[177/625] Time:0.702, Train Loss:0.31052395701408386\n",
      "Epoch 12[178/625] Time:0.702, Train Loss:0.3685274124145508\n",
      "Epoch 12[179/625] Time:0.703, Train Loss:0.39651012420654297\n",
      "Epoch 12[180/625] Time:0.701, Train Loss:0.39402732253074646\n",
      "Epoch 12[181/625] Time:0.704, Train Loss:0.26650482416152954\n",
      "Epoch 12[182/625] Time:0.703, Train Loss:0.28286048769950867\n",
      "Epoch 12[183/625] Time:0.702, Train Loss:0.2803531587123871\n",
      "Epoch 12[184/625] Time:0.704, Train Loss:0.307917058467865\n",
      "Epoch 12[185/625] Time:0.704, Train Loss:0.37091922760009766\n",
      "Epoch 12[186/625] Time:0.702, Train Loss:0.294465035200119\n",
      "Epoch 12[187/625] Time:0.702, Train Loss:0.3403356969356537\n",
      "Epoch 12[188/625] Time:0.702, Train Loss:0.2795051634311676\n",
      "Epoch 12[189/625] Time:0.702, Train Loss:0.2886032164096832\n",
      "Epoch 12[190/625] Time:0.702, Train Loss:0.31098130345344543\n",
      "Epoch 12[191/625] Time:0.701, Train Loss:0.2846473455429077\n",
      "Epoch 12[192/625] Time:0.702, Train Loss:0.3000026047229767\n",
      "Epoch 12[193/625] Time:0.736, Train Loss:0.25470900535583496\n",
      "Epoch 12[194/625] Time:0.697, Train Loss:0.2498449832201004\n",
      "Epoch 12[195/625] Time:0.697, Train Loss:0.26752015948295593\n",
      "Epoch 12[196/625] Time:0.704, Train Loss:0.24478384852409363\n",
      "Epoch 12[197/625] Time:0.694, Train Loss:0.24778535962104797\n",
      "Epoch 12[198/625] Time:0.694, Train Loss:0.2641768157482147\n",
      "Epoch 12[199/625] Time:0.695, Train Loss:0.3491274416446686\n",
      "Epoch 12[200/625] Time:0.695, Train Loss:0.34677278995513916\n",
      "Epoch 12[201/625] Time:0.694, Train Loss:0.3434866964817047\n",
      "Epoch 12[202/625] Time:0.701, Train Loss:0.4088016152381897\n",
      "Epoch 12[203/625] Time:0.726, Train Loss:0.2631860375404358\n",
      "Epoch 12[204/625] Time:0.695, Train Loss:0.31595391035079956\n",
      "Epoch 12[205/625] Time:0.694, Train Loss:0.25323042273521423\n",
      "Epoch 12[206/625] Time:0.697, Train Loss:0.2894126772880554\n",
      "Epoch 12[207/625] Time:0.695, Train Loss:0.3678538203239441\n",
      "Epoch 12[208/625] Time:0.704, Train Loss:0.2929879426956177\n",
      "Epoch 12[209/625] Time:0.705, Train Loss:0.2664027214050293\n",
      "Epoch 12[210/625] Time:0.708, Train Loss:0.3158642649650574\n",
      "Epoch 12[211/625] Time:0.702, Train Loss:0.27041196823120117\n",
      "Epoch 12[212/625] Time:0.703, Train Loss:0.36775311827659607\n",
      "Epoch 12[213/625] Time:0.702, Train Loss:0.3239138424396515\n",
      "Epoch 12[214/625] Time:0.703, Train Loss:0.35734644532203674\n",
      "Epoch 12[215/625] Time:0.702, Train Loss:0.3476056158542633\n",
      "Epoch 12[216/625] Time:0.703, Train Loss:0.3376829922199249\n",
      "Epoch 12[217/625] Time:0.705, Train Loss:0.2540442645549774\n",
      "Epoch 12[218/625] Time:0.724, Train Loss:0.24295759201049805\n",
      "Epoch 12[219/625] Time:0.701, Train Loss:0.29833680391311646\n",
      "Epoch 12[220/625] Time:0.71, Train Loss:0.31991085410118103\n",
      "Epoch 12[221/625] Time:0.702, Train Loss:0.25998884439468384\n",
      "Epoch 12[222/625] Time:0.7, Train Loss:0.3015187382698059\n",
      "Epoch 12[223/625] Time:0.702, Train Loss:0.3190910518169403\n",
      "Epoch 12[224/625] Time:0.702, Train Loss:0.29778754711151123\n",
      "Epoch 12[225/625] Time:0.702, Train Loss:0.3958250880241394\n",
      "Epoch 12[226/625] Time:0.702, Train Loss:0.31109490990638733\n",
      "Epoch 12[227/625] Time:0.704, Train Loss:0.2825011909008026\n",
      "Epoch 12[228/625] Time:0.702, Train Loss:0.2718541920185089\n",
      "Epoch 12[229/625] Time:0.703, Train Loss:0.2717643976211548\n",
      "Epoch 12[230/625] Time:0.702, Train Loss:0.2740458846092224\n",
      "Epoch 12[231/625] Time:0.703, Train Loss:0.2569856643676758\n",
      "Epoch 12[232/625] Time:0.703, Train Loss:0.21615111827850342\n",
      "Epoch 12[233/625] Time:0.703, Train Loss:0.229359969496727\n",
      "Epoch 12[234/625] Time:0.704, Train Loss:0.25507909059524536\n",
      "Epoch 12[235/625] Time:0.706, Train Loss:0.41331425309181213\n",
      "Epoch 12[236/625] Time:0.695, Train Loss:0.22889958322048187\n",
      "Epoch 12[237/625] Time:0.694, Train Loss:0.273201048374176\n",
      "Epoch 12[238/625] Time:0.694, Train Loss:0.27315953373908997\n",
      "Epoch 12[239/625] Time:0.715, Train Loss:0.2286723405122757\n",
      "Epoch 12[240/625] Time:0.701, Train Loss:0.2640957236289978\n",
      "Epoch 12[241/625] Time:0.702, Train Loss:0.41821932792663574\n",
      "Epoch 12[242/625] Time:0.704, Train Loss:0.33334264159202576\n",
      "Epoch 12[243/625] Time:0.7, Train Loss:0.29880696535110474\n",
      "Epoch 12[244/625] Time:0.702, Train Loss:0.3713063597679138\n",
      "Epoch 12[245/625] Time:0.703, Train Loss:0.20552334189414978\n",
      "Epoch 12[246/625] Time:0.702, Train Loss:0.2670121490955353\n",
      "Epoch 12[247/625] Time:0.702, Train Loss:0.27821600437164307\n",
      "Epoch 12[248/625] Time:0.702, Train Loss:0.2836291193962097\n",
      "Epoch 12[249/625] Time:0.702, Train Loss:0.3302800953388214\n",
      "Epoch 12[250/625] Time:0.703, Train Loss:0.3415524959564209\n",
      "Epoch 12[251/625] Time:0.703, Train Loss:0.2805042564868927\n",
      "Epoch 12[252/625] Time:0.701, Train Loss:0.2506658732891083\n",
      "Epoch 12[253/625] Time:0.703, Train Loss:0.251656711101532\n",
      "Epoch 12[254/625] Time:0.703, Train Loss:0.27708756923675537\n",
      "Epoch 12[255/625] Time:0.704, Train Loss:0.39574235677719116\n",
      "Epoch 12[256/625] Time:0.702, Train Loss:0.28946229815483093\n",
      "Epoch 12[257/625] Time:0.703, Train Loss:0.2820952832698822\n",
      "Epoch 12[258/625] Time:0.704, Train Loss:0.32615044713020325\n",
      "Epoch 12[259/625] Time:0.701, Train Loss:0.3082437217235565\n",
      "Epoch 12[260/625] Time:0.704, Train Loss:0.2068907469511032\n",
      "Epoch 12[261/625] Time:0.703, Train Loss:0.22639934718608856\n",
      "Epoch 12[262/625] Time:0.703, Train Loss:0.3909361958503723\n",
      "Epoch 12[263/625] Time:0.706, Train Loss:0.3172511160373688\n",
      "Epoch 12[264/625] Time:0.703, Train Loss:0.29072898626327515\n",
      "Epoch 12[265/625] Time:0.703, Train Loss:0.25838997960090637\n",
      "Epoch 12[266/625] Time:0.702, Train Loss:0.2991688549518585\n",
      "Epoch 12[267/625] Time:0.702, Train Loss:0.3346063792705536\n",
      "Epoch 12[268/625] Time:0.704, Train Loss:0.3889250159263611\n",
      "Epoch 12[269/625] Time:0.703, Train Loss:0.29717832803726196\n",
      "Epoch 12[270/625] Time:0.703, Train Loss:0.3117240071296692\n",
      "Epoch 12[271/625] Time:0.703, Train Loss:0.2702091336250305\n",
      "Epoch 12[272/625] Time:0.704, Train Loss:0.32861027121543884\n",
      "Epoch 12[273/625] Time:0.71, Train Loss:0.31559643149375916\n",
      "Epoch 12[274/625] Time:0.702, Train Loss:0.27637261152267456\n",
      "Epoch 12[275/625] Time:0.703, Train Loss:0.35711365938186646\n",
      "Epoch 12[276/625] Time:0.704, Train Loss:0.2843356132507324\n",
      "Epoch 12[277/625] Time:0.703, Train Loss:0.2514854669570923\n",
      "Epoch 12[278/625] Time:0.704, Train Loss:0.21203771233558655\n",
      "Epoch 12[279/625] Time:0.701, Train Loss:0.2832116484642029\n",
      "Epoch 12[280/625] Time:0.705, Train Loss:0.2513805031776428\n",
      "Epoch 12[281/625] Time:0.704, Train Loss:0.32437968254089355\n",
      "Epoch 12[282/625] Time:0.702, Train Loss:0.28976181149482727\n",
      "Epoch 12[283/625] Time:0.704, Train Loss:0.34141871333122253\n",
      "Epoch 12[284/625] Time:0.705, Train Loss:0.26956477761268616\n",
      "Epoch 12[285/625] Time:0.703, Train Loss:0.2881407141685486\n",
      "Epoch 12[286/625] Time:0.703, Train Loss:0.26512330770492554\n",
      "Epoch 12[287/625] Time:0.703, Train Loss:0.3506670296192169\n",
      "Epoch 12[288/625] Time:0.702, Train Loss:0.2697604298591614\n",
      "Epoch 12[289/625] Time:0.709, Train Loss:0.26786547899246216\n",
      "Epoch 12[290/625] Time:0.702, Train Loss:0.30885809659957886\n",
      "Epoch 12[291/625] Time:0.703, Train Loss:0.2921448349952698\n",
      "Epoch 12[292/625] Time:0.703, Train Loss:0.29444923996925354\n",
      "Epoch 12[293/625] Time:0.704, Train Loss:0.2338239997625351\n",
      "Epoch 12[294/625] Time:0.703, Train Loss:0.28988656401634216\n",
      "Epoch 12[295/625] Time:0.703, Train Loss:0.35907772183418274\n",
      "Epoch 12[296/625] Time:0.703, Train Loss:0.2658034563064575\n",
      "Epoch 12[297/625] Time:0.703, Train Loss:0.40474769473075867\n",
      "Epoch 12[298/625] Time:0.703, Train Loss:0.3580761253833771\n",
      "Epoch 12[299/625] Time:0.704, Train Loss:0.37468963861465454\n",
      "Epoch 12[300/625] Time:0.693, Train Loss:0.3011530339717865\n",
      "Epoch 12[301/625] Time:0.696, Train Loss:0.3431328237056732\n",
      "Epoch 12[302/625] Time:0.695, Train Loss:0.23757559061050415\n",
      "Epoch 12[303/625] Time:0.694, Train Loss:0.3173041343688965\n",
      "Epoch 12[304/625] Time:0.693, Train Loss:0.2790127694606781\n",
      "Epoch 12[305/625] Time:0.694, Train Loss:0.27994632720947266\n",
      "Epoch 12[306/625] Time:0.702, Train Loss:0.341241717338562\n",
      "Epoch 12[307/625] Time:0.708, Train Loss:0.3271210789680481\n",
      "Epoch 12[308/625] Time:0.703, Train Loss:0.30384957790374756\n",
      "Epoch 12[309/625] Time:0.703, Train Loss:0.258847713470459\n",
      "Epoch 12[310/625] Time:0.703, Train Loss:0.3077983260154724\n",
      "Epoch 12[311/625] Time:0.703, Train Loss:0.29683369398117065\n",
      "Epoch 12[312/625] Time:0.703, Train Loss:0.2843156158924103\n",
      "Epoch 12[313/625] Time:0.704, Train Loss:0.29135605692863464\n",
      "Epoch 12[314/625] Time:0.704, Train Loss:0.2629677653312683\n",
      "Epoch 12[315/625] Time:0.703, Train Loss:0.2899198830127716\n",
      "Epoch 12[316/625] Time:0.707, Train Loss:0.2890549302101135\n",
      "Epoch 12[317/625] Time:0.704, Train Loss:0.2559870183467865\n",
      "Epoch 12[318/625] Time:0.702, Train Loss:0.2871042788028717\n",
      "Epoch 12[319/625] Time:0.702, Train Loss:0.28038594126701355\n",
      "Epoch 12[320/625] Time:0.703, Train Loss:0.3080005645751953\n",
      "Epoch 12[321/625] Time:0.704, Train Loss:0.2903125286102295\n",
      "Epoch 12[322/625] Time:0.703, Train Loss:0.24615222215652466\n",
      "Epoch 12[323/625] Time:0.703, Train Loss:0.34063971042633057\n",
      "Epoch 12[324/625] Time:0.704, Train Loss:0.3139679729938507\n",
      "Epoch 12[325/625] Time:0.703, Train Loss:0.30947059392929077\n",
      "Epoch 12[326/625] Time:0.704, Train Loss:0.2566154897212982\n",
      "Epoch 12[327/625] Time:0.703, Train Loss:0.36126604676246643\n",
      "Epoch 12[328/625] Time:0.703, Train Loss:0.27727627754211426\n",
      "Epoch 12[329/625] Time:0.704, Train Loss:0.33931612968444824\n",
      "Epoch 12[330/625] Time:0.703, Train Loss:0.268764466047287\n",
      "Epoch 12[331/625] Time:0.703, Train Loss:0.27110931277275085\n",
      "Epoch 12[332/625] Time:0.703, Train Loss:0.2431405782699585\n",
      "Epoch 12[333/625] Time:0.703, Train Loss:0.24760371446609497\n",
      "Epoch 12[334/625] Time:0.703, Train Loss:0.21373265981674194\n",
      "Epoch 12[335/625] Time:0.703, Train Loss:0.298166424036026\n",
      "Epoch 12[336/625] Time:0.703, Train Loss:0.23514828085899353\n",
      "Epoch 12[337/625] Time:0.703, Train Loss:0.2512478530406952\n",
      "Epoch 12[338/625] Time:0.704, Train Loss:0.31572750210762024\n",
      "Epoch 12[339/625] Time:0.704, Train Loss:0.28528979420661926\n",
      "Epoch 12[340/625] Time:0.703, Train Loss:0.2672155201435089\n",
      "Epoch 12[341/625] Time:0.705, Train Loss:0.260404109954834\n",
      "Epoch 12[342/625] Time:0.703, Train Loss:0.27331820130348206\n",
      "Epoch 12[343/625] Time:0.705, Train Loss:0.32865285873413086\n",
      "Epoch 12[344/625] Time:0.703, Train Loss:0.28174635767936707\n",
      "Epoch 12[345/625] Time:0.704, Train Loss:0.33524754643440247\n",
      "Epoch 12[346/625] Time:0.704, Train Loss:0.2727498710155487\n",
      "Epoch 12[347/625] Time:0.702, Train Loss:0.28709349036216736\n",
      "Epoch 12[348/625] Time:0.703, Train Loss:0.31185442209243774\n",
      "Epoch 12[349/625] Time:0.703, Train Loss:0.28227677941322327\n",
      "Epoch 12[350/625] Time:0.703, Train Loss:0.2684309780597687\n",
      "Epoch 12[351/625] Time:0.738, Train Loss:0.3106142580509186\n",
      "Epoch 12[352/625] Time:0.698, Train Loss:0.2693483531475067\n",
      "Epoch 12[353/625] Time:0.702, Train Loss:0.269511878490448\n",
      "Epoch 12[354/625] Time:0.703, Train Loss:0.38510721921920776\n",
      "Epoch 12[355/625] Time:0.733, Train Loss:0.3454872965812683\n",
      "Epoch 12[356/625] Time:0.704, Train Loss:0.3079273998737335\n",
      "Epoch 12[357/625] Time:0.724, Train Loss:0.2470916509628296\n",
      "Epoch 12[358/625] Time:0.703, Train Loss:0.2815673351287842\n",
      "Epoch 12[359/625] Time:0.703, Train Loss:0.24855183064937592\n",
      "Epoch 12[360/625] Time:0.705, Train Loss:0.296591579914093\n",
      "Epoch 12[361/625] Time:0.702, Train Loss:0.2768953740596771\n",
      "Epoch 12[362/625] Time:0.701, Train Loss:0.27875614166259766\n",
      "Epoch 12[363/625] Time:0.702, Train Loss:0.26143890619277954\n",
      "Epoch 12[364/625] Time:0.702, Train Loss:0.39029660820961\n",
      "Epoch 12[365/625] Time:0.704, Train Loss:0.26144132018089294\n",
      "Epoch 12[366/625] Time:0.703, Train Loss:0.31570056080818176\n",
      "Epoch 12[367/625] Time:0.704, Train Loss:0.28120067715644836\n",
      "Epoch 12[368/625] Time:0.702, Train Loss:0.3929895758628845\n",
      "Epoch 12[369/625] Time:0.702, Train Loss:0.2763642966747284\n",
      "Epoch 12[370/625] Time:0.706, Train Loss:0.2847070097923279\n",
      "Epoch 12[371/625] Time:0.703, Train Loss:0.3083430528640747\n",
      "Epoch 12[372/625] Time:0.736, Train Loss:0.350823312997818\n",
      "Epoch 12[373/625] Time:0.701, Train Loss:0.24906013906002045\n",
      "Epoch 12[374/625] Time:0.703, Train Loss:0.37260717153549194\n",
      "Epoch 12[375/625] Time:0.713, Train Loss:0.2693069875240326\n",
      "Epoch 12[376/625] Time:0.73, Train Loss:0.26601871848106384\n",
      "Epoch 12[377/625] Time:0.733, Train Loss:0.32501837611198425\n",
      "Epoch 12[378/625] Time:0.693, Train Loss:0.2837539613246918\n",
      "Epoch 12[379/625] Time:0.704, Train Loss:0.3442743122577667\n",
      "Epoch 12[380/625] Time:0.704, Train Loss:0.35733842849731445\n",
      "Epoch 12[381/625] Time:0.703, Train Loss:0.24709922075271606\n",
      "Epoch 12[382/625] Time:0.703, Train Loss:0.3578503727912903\n",
      "Epoch 12[383/625] Time:0.705, Train Loss:0.22057747840881348\n",
      "Epoch 12[384/625] Time:0.71, Train Loss:0.33456626534461975\n",
      "Epoch 12[385/625] Time:0.697, Train Loss:0.32372426986694336\n",
      "Epoch 12[386/625] Time:0.695, Train Loss:0.2857668995857239\n",
      "Epoch 12[387/625] Time:0.696, Train Loss:0.31322649121284485\n",
      "Epoch 12[388/625] Time:0.694, Train Loss:0.360021710395813\n",
      "Epoch 12[389/625] Time:0.702, Train Loss:0.31528201699256897\n",
      "Epoch 12[390/625] Time:0.703, Train Loss:0.3108566701412201\n",
      "Epoch 12[391/625] Time:0.703, Train Loss:0.3037055432796478\n",
      "Epoch 12[392/625] Time:0.702, Train Loss:0.3010806143283844\n",
      "Epoch 12[393/625] Time:0.702, Train Loss:0.36587807536125183\n",
      "Epoch 12[394/625] Time:0.703, Train Loss:0.20831511914730072\n",
      "Epoch 12[395/625] Time:0.693, Train Loss:0.2853613495826721\n",
      "Epoch 12[396/625] Time:0.702, Train Loss:0.25306499004364014\n",
      "Epoch 12[397/625] Time:0.705, Train Loss:0.3832196295261383\n",
      "Epoch 12[398/625] Time:0.695, Train Loss:0.24820199608802795\n",
      "Epoch 12[399/625] Time:0.696, Train Loss:0.284827321767807\n",
      "Epoch 12[400/625] Time:0.695, Train Loss:0.23896944522857666\n",
      "Epoch 12[401/625] Time:0.694, Train Loss:0.3330376446247101\n",
      "Epoch 12[402/625] Time:0.699, Train Loss:0.24621623754501343\n",
      "Epoch 12[403/625] Time:0.697, Train Loss:0.27880436182022095\n",
      "Epoch 12[404/625] Time:0.7, Train Loss:0.2942071557044983\n",
      "Epoch 12[405/625] Time:0.708, Train Loss:0.2929069995880127\n",
      "Epoch 12[406/625] Time:0.702, Train Loss:0.32725125551223755\n",
      "Epoch 12[407/625] Time:0.702, Train Loss:0.3655351996421814\n",
      "Epoch 12[408/625] Time:0.695, Train Loss:0.29703018069267273\n",
      "Epoch 12[409/625] Time:0.694, Train Loss:0.3400112986564636\n",
      "Epoch 12[410/625] Time:0.694, Train Loss:0.39496511220932007\n",
      "Epoch 12[411/625] Time:0.693, Train Loss:0.34731587767601013\n",
      "Epoch 12[412/625] Time:0.693, Train Loss:0.3262289762496948\n",
      "Epoch 12[413/625] Time:0.722, Train Loss:0.2730587422847748\n",
      "Epoch 12[414/625] Time:0.737, Train Loss:0.3204330503940582\n",
      "Epoch 12[415/625] Time:0.703, Train Loss:0.2715055048465729\n",
      "Epoch 12[416/625] Time:0.702, Train Loss:0.28626903891563416\n",
      "Epoch 12[417/625] Time:0.705, Train Loss:0.20391996204853058\n",
      "Epoch 12[418/625] Time:0.704, Train Loss:0.3358590006828308\n",
      "Epoch 12[419/625] Time:0.73, Train Loss:0.2238004505634308\n",
      "Epoch 12[420/625] Time:0.703, Train Loss:0.29858142137527466\n",
      "Epoch 12[421/625] Time:0.703, Train Loss:0.26520779728889465\n",
      "Epoch 12[422/625] Time:0.704, Train Loss:0.30354416370391846\n",
      "Epoch 12[423/625] Time:0.702, Train Loss:0.3909650146961212\n",
      "Epoch 12[424/625] Time:0.729, Train Loss:0.302282452583313\n",
      "Epoch 12[425/625] Time:0.692, Train Loss:0.37262436747550964\n",
      "Epoch 12[426/625] Time:0.694, Train Loss:0.2625289857387543\n",
      "Epoch 12[427/625] Time:0.696, Train Loss:0.29112258553504944\n",
      "Epoch 12[428/625] Time:0.696, Train Loss:0.2920691967010498\n",
      "Epoch 12[429/625] Time:0.694, Train Loss:0.2926296591758728\n",
      "Epoch 12[430/625] Time:0.697, Train Loss:0.3091076910495758\n",
      "Epoch 12[431/625] Time:0.693, Train Loss:0.2573350965976715\n",
      "Epoch 12[432/625] Time:0.694, Train Loss:0.24613484740257263\n",
      "Epoch 12[433/625] Time:0.693, Train Loss:0.2975580394268036\n",
      "Epoch 12[434/625] Time:0.696, Train Loss:0.378747820854187\n",
      "Epoch 12[435/625] Time:0.694, Train Loss:0.2897946238517761\n",
      "Epoch 12[436/625] Time:0.702, Train Loss:0.3005251884460449\n",
      "Epoch 12[437/625] Time:0.703, Train Loss:0.2779780924320221\n",
      "Epoch 12[438/625] Time:0.702, Train Loss:0.34077560901641846\n",
      "Epoch 12[439/625] Time:0.702, Train Loss:0.3511126935482025\n",
      "Epoch 12[440/625] Time:0.702, Train Loss:0.28747254610061646\n",
      "Epoch 12[441/625] Time:0.701, Train Loss:0.30476146936416626\n",
      "Epoch 12[442/625] Time:0.702, Train Loss:0.29461634159088135\n",
      "Epoch 12[443/625] Time:0.703, Train Loss:0.3559877276420593\n",
      "Epoch 12[444/625] Time:0.701, Train Loss:0.28641995787620544\n",
      "Epoch 12[445/625] Time:0.701, Train Loss:0.2763576805591583\n",
      "Epoch 12[446/625] Time:0.703, Train Loss:0.28717169165611267\n",
      "Epoch 12[447/625] Time:0.701, Train Loss:0.3512583374977112\n",
      "Epoch 12[448/625] Time:0.704, Train Loss:0.23213693499565125\n",
      "Epoch 12[449/625] Time:0.704, Train Loss:0.37975412607192993\n",
      "Epoch 12[450/625] Time:0.716, Train Loss:0.2888548970222473\n",
      "Epoch 12[451/625] Time:0.694, Train Loss:0.30035579204559326\n",
      "Epoch 12[452/625] Time:0.694, Train Loss:0.2829490602016449\n",
      "Epoch 12[453/625] Time:0.702, Train Loss:0.3023759722709656\n",
      "Epoch 12[454/625] Time:0.703, Train Loss:0.2812363803386688\n",
      "Epoch 12[455/625] Time:0.703, Train Loss:0.2642207741737366\n",
      "Epoch 12[456/625] Time:0.703, Train Loss:0.2516274154186249\n",
      "Epoch 12[457/625] Time:0.703, Train Loss:0.35999637842178345\n",
      "Epoch 12[458/625] Time:0.702, Train Loss:0.3252648413181305\n",
      "Epoch 12[459/625] Time:0.705, Train Loss:0.2847708463668823\n",
      "Epoch 12[460/625] Time:0.703, Train Loss:0.3114219009876251\n",
      "Epoch 12[461/625] Time:0.704, Train Loss:0.23063324391841888\n",
      "Epoch 12[462/625] Time:0.702, Train Loss:0.3581523001194\n",
      "Epoch 12[463/625] Time:0.704, Train Loss:0.29068535566329956\n",
      "Epoch 12[464/625] Time:0.705, Train Loss:0.3241640627384186\n",
      "Epoch 12[465/625] Time:0.707, Train Loss:0.32952219247817993\n",
      "Epoch 12[466/625] Time:0.708, Train Loss:0.26766344904899597\n",
      "Epoch 12[467/625] Time:0.72, Train Loss:0.28370508551597595\n",
      "Epoch 12[468/625] Time:0.702, Train Loss:0.22242511808872223\n",
      "Epoch 12[469/625] Time:0.705, Train Loss:0.28166818618774414\n",
      "Epoch 12[470/625] Time:0.702, Train Loss:0.29060098528862\n",
      "Epoch 12[471/625] Time:0.701, Train Loss:0.2744481563568115\n",
      "Epoch 12[472/625] Time:0.725, Train Loss:0.25258398056030273\n",
      "Epoch 12[473/625] Time:0.694, Train Loss:0.40302401781082153\n",
      "Epoch 12[474/625] Time:0.695, Train Loss:0.25742068886756897\n",
      "Epoch 12[475/625] Time:0.702, Train Loss:0.21500834822654724\n",
      "Epoch 12[476/625] Time:0.702, Train Loss:0.2855769693851471\n",
      "Epoch 12[477/625] Time:0.706, Train Loss:0.30228832364082336\n",
      "Epoch 12[478/625] Time:0.703, Train Loss:0.25416746735572815\n",
      "Epoch 12[479/625] Time:0.705, Train Loss:0.28051117062568665\n",
      "Epoch 12[480/625] Time:0.724, Train Loss:0.29508280754089355\n",
      "Epoch 12[481/625] Time:0.695, Train Loss:0.273798406124115\n",
      "Epoch 12[482/625] Time:0.695, Train Loss:0.31904274225234985\n",
      "Epoch 12[483/625] Time:0.702, Train Loss:0.3359021842479706\n",
      "Epoch 12[484/625] Time:0.702, Train Loss:0.28425922989845276\n",
      "Epoch 12[485/625] Time:0.711, Train Loss:0.28648892045021057\n",
      "Epoch 12[486/625] Time:0.693, Train Loss:0.2514953017234802\n",
      "Epoch 12[487/625] Time:0.693, Train Loss:0.3050990402698517\n",
      "Epoch 12[488/625] Time:0.694, Train Loss:0.23623071610927582\n",
      "Epoch 12[489/625] Time:0.695, Train Loss:0.24552494287490845\n",
      "Epoch 12[490/625] Time:0.694, Train Loss:0.23695382475852966\n",
      "Epoch 12[491/625] Time:0.694, Train Loss:0.35768601298332214\n",
      "Epoch 12[492/625] Time:0.694, Train Loss:0.18638192117214203\n",
      "Epoch 12[493/625] Time:0.696, Train Loss:0.21590861678123474\n",
      "Epoch 12[494/625] Time:0.704, Train Loss:0.2939331829547882\n",
      "Epoch 12[495/625] Time:0.701, Train Loss:0.24586638808250427\n",
      "Epoch 12[496/625] Time:0.708, Train Loss:0.4112377464771271\n",
      "Epoch 12[497/625] Time:0.695, Train Loss:0.3170299232006073\n",
      "Epoch 12[498/625] Time:0.693, Train Loss:0.2983033359050751\n",
      "Epoch 12[499/625] Time:0.703, Train Loss:0.2587309181690216\n",
      "Epoch 12[500/625] Time:0.703, Train Loss:0.22303248941898346\n",
      "Epoch 12[501/625] Time:0.702, Train Loss:0.2683434784412384\n",
      "Epoch 12[502/625] Time:0.705, Train Loss:0.2855873107910156\n",
      "Epoch 12[503/625] Time:0.703, Train Loss:0.26393017172813416\n",
      "Epoch 12[504/625] Time:0.703, Train Loss:0.2631324827671051\n",
      "Epoch 12[505/625] Time:0.707, Train Loss:0.31261032819747925\n",
      "Epoch 12[506/625] Time:0.702, Train Loss:0.33513718843460083\n",
      "Epoch 12[507/625] Time:0.702, Train Loss:0.3278561234474182\n",
      "Epoch 12[508/625] Time:0.708, Train Loss:0.3097582161426544\n",
      "Epoch 12[509/625] Time:0.703, Train Loss:0.2975304424762726\n",
      "Epoch 12[510/625] Time:0.703, Train Loss:0.2791782021522522\n",
      "Epoch 12[511/625] Time:0.703, Train Loss:0.2973553538322449\n",
      "Epoch 12[512/625] Time:0.705, Train Loss:0.35881543159484863\n",
      "Epoch 12[513/625] Time:0.703, Train Loss:0.25422918796539307\n",
      "Epoch 12[514/625] Time:0.704, Train Loss:0.26309990882873535\n",
      "Epoch 12[515/625] Time:0.704, Train Loss:0.2770536541938782\n",
      "Epoch 12[516/625] Time:0.707, Train Loss:0.33454760909080505\n",
      "Epoch 12[517/625] Time:0.705, Train Loss:0.25195905566215515\n",
      "Epoch 12[518/625] Time:0.703, Train Loss:0.36350885033607483\n",
      "Epoch 12[519/625] Time:0.703, Train Loss:0.2900535464286804\n",
      "Epoch 12[520/625] Time:0.703, Train Loss:0.3432801365852356\n",
      "Epoch 12[521/625] Time:0.703, Train Loss:0.30981117486953735\n",
      "Epoch 12[522/625] Time:0.703, Train Loss:0.3475036323070526\n",
      "Epoch 12[523/625] Time:0.703, Train Loss:0.34329527616500854\n",
      "Epoch 12[524/625] Time:0.709, Train Loss:0.33841609954833984\n",
      "Epoch 12[525/625] Time:0.704, Train Loss:0.36051785945892334\n",
      "Epoch 12[526/625] Time:0.703, Train Loss:0.34929290413856506\n",
      "Epoch 12[527/625] Time:0.703, Train Loss:0.3533971905708313\n",
      "Epoch 12[528/625] Time:0.702, Train Loss:0.3103965222835541\n",
      "Epoch 12[529/625] Time:0.705, Train Loss:0.23597189784049988\n",
      "Epoch 12[530/625] Time:0.704, Train Loss:0.29934078454971313\n",
      "Epoch 12[531/625] Time:0.706, Train Loss:0.36678561568260193\n",
      "Epoch 12[532/625] Time:0.709, Train Loss:0.36196115612983704\n",
      "Epoch 12[533/625] Time:0.703, Train Loss:0.3061925172805786\n",
      "Epoch 12[534/625] Time:0.703, Train Loss:0.23042720556259155\n",
      "Epoch 12[535/625] Time:0.702, Train Loss:0.31308993697166443\n",
      "Epoch 12[536/625] Time:0.702, Train Loss:0.26673653721809387\n",
      "Epoch 12[537/625] Time:0.702, Train Loss:0.25133010745048523\n",
      "Epoch 12[538/625] Time:0.703, Train Loss:0.298530250787735\n",
      "Epoch 12[539/625] Time:0.702, Train Loss:0.2679503858089447\n",
      "Epoch 12[540/625] Time:0.704, Train Loss:0.30219897627830505\n",
      "Epoch 12[541/625] Time:0.702, Train Loss:0.2923536002635956\n",
      "Epoch 12[542/625] Time:0.703, Train Loss:0.30102938413619995\n",
      "Epoch 12[543/625] Time:0.704, Train Loss:0.28807929158210754\n",
      "Epoch 12[544/625] Time:0.704, Train Loss:0.290358304977417\n",
      "Epoch 12[545/625] Time:0.705, Train Loss:0.317007452249527\n",
      "Epoch 12[546/625] Time:0.702, Train Loss:0.2904863655567169\n",
      "Epoch 12[547/625] Time:0.704, Train Loss:0.3060324490070343\n",
      "Epoch 12[548/625] Time:0.698, Train Loss:0.25428634881973267\n",
      "Epoch 12[549/625] Time:0.702, Train Loss:0.27403247356414795\n",
      "Epoch 12[550/625] Time:0.702, Train Loss:0.3120863437652588\n",
      "Epoch 12[551/625] Time:0.702, Train Loss:0.25964221358299255\n",
      "Epoch 12[552/625] Time:0.702, Train Loss:0.2254762202501297\n",
      "Epoch 12[553/625] Time:0.703, Train Loss:0.2147102952003479\n",
      "Epoch 12[554/625] Time:0.704, Train Loss:0.3605601191520691\n",
      "Epoch 12[555/625] Time:0.703, Train Loss:0.2780880928039551\n",
      "Epoch 12[556/625] Time:0.706, Train Loss:0.23461773991584778\n",
      "Epoch 12[557/625] Time:0.7, Train Loss:0.245147243142128\n",
      "Epoch 12[558/625] Time:0.702, Train Loss:0.28356170654296875\n",
      "Epoch 12[559/625] Time:0.701, Train Loss:0.2718318998813629\n",
      "Epoch 12[560/625] Time:0.704, Train Loss:0.21522289514541626\n",
      "Epoch 12[561/625] Time:0.704, Train Loss:0.36049753427505493\n",
      "Epoch 12[562/625] Time:0.705, Train Loss:0.24008159339427948\n",
      "Epoch 12[563/625] Time:0.696, Train Loss:0.24785874783992767\n",
      "Epoch 12[564/625] Time:0.712, Train Loss:0.2910640835762024\n",
      "Epoch 12[565/625] Time:0.704, Train Loss:0.20671169459819794\n",
      "Epoch 12[566/625] Time:0.701, Train Loss:0.3267711102962494\n",
      "Epoch 12[567/625] Time:0.703, Train Loss:0.24201291799545288\n",
      "Epoch 12[568/625] Time:0.701, Train Loss:0.2476632297039032\n",
      "Epoch 12[569/625] Time:0.703, Train Loss:0.2780546247959137\n",
      "Epoch 12[570/625] Time:0.703, Train Loss:0.3022196888923645\n",
      "Epoch 12[571/625] Time:0.702, Train Loss:0.34407779574394226\n",
      "Epoch 12[572/625] Time:0.703, Train Loss:0.24003814160823822\n",
      "Epoch 12[573/625] Time:0.703, Train Loss:0.2610166668891907\n",
      "Epoch 12[574/625] Time:0.702, Train Loss:0.2792355716228485\n",
      "Epoch 12[575/625] Time:0.703, Train Loss:0.4189678132534027\n",
      "Epoch 12[576/625] Time:0.703, Train Loss:0.2383318990468979\n",
      "Epoch 12[577/625] Time:0.702, Train Loss:0.26374131441116333\n",
      "Epoch 12[578/625] Time:0.701, Train Loss:0.2810748815536499\n",
      "Epoch 12[579/625] Time:0.704, Train Loss:0.24322468042373657\n",
      "Epoch 12[580/625] Time:0.703, Train Loss:0.31351110339164734\n",
      "Epoch 12[581/625] Time:0.704, Train Loss:0.2482549101114273\n",
      "Epoch 12[582/625] Time:0.704, Train Loss:0.2854433059692383\n",
      "Epoch 12[583/625] Time:0.705, Train Loss:0.3252766728401184\n",
      "Epoch 12[584/625] Time:0.704, Train Loss:0.2621101140975952\n",
      "Epoch 12[585/625] Time:0.704, Train Loss:0.29784175753593445\n",
      "Epoch 12[586/625] Time:0.703, Train Loss:0.24490825831890106\n",
      "Epoch 12[587/625] Time:0.703, Train Loss:0.2973976135253906\n",
      "Epoch 12[588/625] Time:0.703, Train Loss:0.23673750460147858\n",
      "Epoch 12[589/625] Time:0.703, Train Loss:0.35342520475387573\n",
      "Epoch 12[590/625] Time:0.704, Train Loss:0.31984636187553406\n",
      "Epoch 12[591/625] Time:0.704, Train Loss:0.2557961344718933\n",
      "Epoch 12[592/625] Time:0.706, Train Loss:0.2520351707935333\n",
      "Epoch 12[593/625] Time:0.724, Train Loss:0.30988502502441406\n",
      "Epoch 12[594/625] Time:0.694, Train Loss:0.28076592087745667\n",
      "Epoch 12[595/625] Time:0.703, Train Loss:0.29743778705596924\n",
      "Epoch 12[596/625] Time:0.702, Train Loss:0.27517881989479065\n",
      "Epoch 12[597/625] Time:0.703, Train Loss:0.3414139449596405\n",
      "Epoch 12[598/625] Time:0.702, Train Loss:0.2900458574295044\n",
      "Epoch 12[599/625] Time:0.703, Train Loss:0.3155622184276581\n",
      "Epoch 12[600/625] Time:0.703, Train Loss:0.29990267753601074\n",
      "Epoch 12[601/625] Time:0.703, Train Loss:0.29198625683784485\n",
      "Epoch 12[602/625] Time:0.702, Train Loss:0.24760079383850098\n",
      "Epoch 12[603/625] Time:0.703, Train Loss:0.3808988630771637\n",
      "Epoch 12[604/625] Time:0.703, Train Loss:0.27642250061035156\n",
      "Epoch 12[605/625] Time:0.703, Train Loss:0.2875993251800537\n",
      "Epoch 12[606/625] Time:0.703, Train Loss:0.39537712931632996\n",
      "Epoch 12[607/625] Time:0.702, Train Loss:0.34851786494255066\n",
      "Epoch 12[608/625] Time:0.703, Train Loss:0.2507587671279907\n",
      "Epoch 12[609/625] Time:0.703, Train Loss:0.2849295735359192\n",
      "Epoch 12[610/625] Time:0.715, Train Loss:0.2921702265739441\n",
      "Epoch 12[611/625] Time:0.706, Train Loss:0.3643428087234497\n",
      "Epoch 12[612/625] Time:0.705, Train Loss:0.2748163640499115\n",
      "Epoch 12[613/625] Time:0.703, Train Loss:0.3269006013870239\n",
      "Epoch 12[614/625] Time:0.702, Train Loss:0.37097129225730896\n",
      "Epoch 12[615/625] Time:0.703, Train Loss:0.30886757373809814\n",
      "Epoch 12[616/625] Time:0.703, Train Loss:0.31086358428001404\n",
      "Epoch 12[617/625] Time:0.703, Train Loss:0.2791873812675476\n",
      "Epoch 12[618/625] Time:0.704, Train Loss:0.2999315857887268\n",
      "Epoch 12[619/625] Time:0.705, Train Loss:0.33494600653648376\n",
      "Epoch 12[620/625] Time:0.703, Train Loss:0.30285876989364624\n",
      "Epoch 12[621/625] Time:0.703, Train Loss:0.2561807334423065\n",
      "Epoch 12[622/625] Time:0.703, Train Loss:0.20978209376335144\n",
      "Epoch 12[623/625] Time:0.702, Train Loss:0.2856769263744354\n",
      "Epoch 12[624/625] Time:0.705, Train Loss:0.2337450385093689\n",
      "Epoch 12[0/78] Val Loss:0.26027509570121765\n",
      "Epoch 12[1/78] Val Loss:0.25487378239631653\n",
      "Epoch 12[2/78] Val Loss:0.2731890082359314\n",
      "Epoch 12[3/78] Val Loss:0.2625651955604553\n",
      "Epoch 12[4/78] Val Loss:0.3702445328235626\n",
      "Epoch 12[5/78] Val Loss:0.3331032395362854\n",
      "Epoch 12[6/78] Val Loss:0.32087212800979614\n",
      "Epoch 12[7/78] Val Loss:0.40666237473487854\n",
      "Epoch 12[8/78] Val Loss:0.24664196372032166\n",
      "Epoch 12[9/78] Val Loss:0.13931286334991455\n",
      "Epoch 12[10/78] Val Loss:0.09554335474967957\n",
      "Epoch 12[11/78] Val Loss:0.1473761945962906\n",
      "Epoch 12[12/78] Val Loss:0.1220228299498558\n",
      "Epoch 12[13/78] Val Loss:0.09723396599292755\n",
      "Epoch 12[14/78] Val Loss:0.24752680957317352\n",
      "Epoch 12[15/78] Val Loss:0.1892337054014206\n",
      "Epoch 12[16/78] Val Loss:0.22054414451122284\n",
      "Epoch 12[17/78] Val Loss:0.16031144559383392\n",
      "Epoch 12[18/78] Val Loss:0.2921741306781769\n",
      "Epoch 12[19/78] Val Loss:0.3719026446342468\n",
      "Epoch 12[20/78] Val Loss:0.25165000557899475\n",
      "Epoch 12[21/78] Val Loss:0.5214003324508667\n",
      "Epoch 12[22/78] Val Loss:0.7109047174453735\n",
      "Epoch 12[23/78] Val Loss:0.4523460865020752\n",
      "Epoch 12[24/78] Val Loss:0.2921639084815979\n",
      "Epoch 12[25/78] Val Loss:0.3829886019229889\n",
      "Epoch 12[26/78] Val Loss:0.36694058775901794\n",
      "Epoch 12[27/78] Val Loss:0.3441278040409088\n",
      "Epoch 12[28/78] Val Loss:0.3402740955352783\n",
      "Epoch 12[29/78] Val Loss:0.475789874792099\n",
      "Epoch 12[30/78] Val Loss:1.458260178565979\n",
      "Epoch 12[31/78] Val Loss:1.3032160997390747\n",
      "Epoch 12[32/78] Val Loss:1.1098201274871826\n",
      "Epoch 12[33/78] Val Loss:0.48583343625068665\n",
      "Epoch 12[34/78] Val Loss:0.3879123628139496\n",
      "Epoch 12[35/78] Val Loss:0.4122641682624817\n",
      "Epoch 12[36/78] Val Loss:0.3786580562591553\n",
      "Epoch 12[37/78] Val Loss:0.43807217478752136\n",
      "Epoch 12[38/78] Val Loss:0.23837167024612427\n",
      "Epoch 12[39/78] Val Loss:0.2558310627937317\n",
      "Epoch 12[40/78] Val Loss:0.271870881319046\n",
      "Epoch 12[41/78] Val Loss:0.2774238884449005\n",
      "Epoch 12[42/78] Val Loss:0.27941086888313293\n",
      "Epoch 12[43/78] Val Loss:0.1724238246679306\n",
      "Epoch 12[44/78] Val Loss:0.1200796365737915\n",
      "Epoch 12[45/78] Val Loss:0.11046437919139862\n",
      "Epoch 12[46/78] Val Loss:0.11599192023277283\n",
      "Epoch 12[47/78] Val Loss:0.11410558223724365\n",
      "Epoch 12[48/78] Val Loss:0.1469578593969345\n",
      "Epoch 12[49/78] Val Loss:0.10449063777923584\n",
      "Epoch 12[50/78] Val Loss:0.10229861736297607\n",
      "Epoch 12[51/78] Val Loss:0.11226799339056015\n",
      "Epoch 12[52/78] Val Loss:0.14303864538669586\n",
      "Epoch 12[53/78] Val Loss:0.10535869002342224\n",
      "Epoch 12[54/78] Val Loss:0.10161110758781433\n",
      "Epoch 12[55/78] Val Loss:0.1335485577583313\n",
      "Epoch 12[56/78] Val Loss:0.4075205624103546\n",
      "Epoch 12[57/78] Val Loss:0.3907875418663025\n",
      "Epoch 12[58/78] Val Loss:0.373611181974411\n",
      "Epoch 12[59/78] Val Loss:0.327323317527771\n",
      "Epoch 12[60/78] Val Loss:0.3350810706615448\n",
      "Epoch 12[61/78] Val Loss:0.35500067472457886\n",
      "Epoch 12[62/78] Val Loss:0.42468011379241943\n",
      "Epoch 12[63/78] Val Loss:0.2297578603029251\n",
      "Epoch 12[64/78] Val Loss:0.18880526721477509\n",
      "Epoch 12[65/78] Val Loss:0.1429172158241272\n",
      "Epoch 12[66/78] Val Loss:0.20668520033359528\n",
      "Epoch 12[67/78] Val Loss:0.19966724514961243\n",
      "Epoch 12[68/78] Val Loss:0.5497193336486816\n",
      "Epoch 12[69/78] Val Loss:0.4966736137866974\n",
      "Epoch 12[70/78] Val Loss:0.4859946370124817\n",
      "Epoch 12[71/78] Val Loss:0.409258633852005\n",
      "Epoch 12[72/78] Val Loss:0.2200034260749817\n",
      "Epoch 12[73/78] Val Loss:0.2022944986820221\n",
      "Epoch 12[74/78] Val Loss:0.5297286510467529\n",
      "Epoch 12[75/78] Val Loss:0.5964621305465698\n",
      "Epoch 12[76/78] Val Loss:0.594119131565094\n",
      "Epoch 12[77/78] Val Loss:0.5990709662437439\n",
      "Epoch 12[78/78] Val Loss:0.7431828379631042\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.87      0.91     15691\n",
      "           1       0.64      0.83      0.72      4309\n",
      "\n",
      "    accuracy                           0.86     20000\n",
      "   macro avg       0.79      0.85      0.82     20000\n",
      "weighted avg       0.88      0.86      0.87     20000\n",
      "\n",
      "Epoch 12: Train Loss 0.29612362394332886, Val Loss 0.3440784995372479, Train Time 792.4021203517914, Val Time 38.08084273338318\n",
      "Epoch 13[0/625] Time:0.689, Train Loss:0.347430557012558\n",
      "Epoch 13[1/625] Time:0.703, Train Loss:0.29884764552116394\n",
      "Epoch 13[2/625] Time:0.702, Train Loss:0.23992986977100372\n",
      "Epoch 13[3/625] Time:0.707, Train Loss:0.25315678119659424\n",
      "Epoch 13[4/625] Time:0.702, Train Loss:0.29153239727020264\n",
      "Epoch 13[5/625] Time:0.703, Train Loss:0.31583648920059204\n",
      "Epoch 13[6/625] Time:0.703, Train Loss:0.42702293395996094\n",
      "Epoch 13[7/625] Time:0.702, Train Loss:0.3582667112350464\n",
      "Epoch 13[8/625] Time:0.694, Train Loss:0.24472399055957794\n",
      "Epoch 13[9/625] Time:0.693, Train Loss:0.28006434440612793\n",
      "Epoch 13[10/625] Time:0.702, Train Loss:0.271706759929657\n",
      "Epoch 13[11/625] Time:0.705, Train Loss:0.23242421448230743\n",
      "Epoch 13[12/625] Time:0.702, Train Loss:0.2736475467681885\n",
      "Epoch 13[13/625] Time:0.703, Train Loss:0.2872854471206665\n",
      "Epoch 13[14/625] Time:0.702, Train Loss:0.28003793954849243\n",
      "Epoch 13[15/625] Time:0.704, Train Loss:0.29465633630752563\n",
      "Epoch 13[16/625] Time:0.702, Train Loss:0.35857921838760376\n",
      "Epoch 13[17/625] Time:0.731, Train Loss:0.2851991653442383\n",
      "Epoch 13[18/625] Time:0.695, Train Loss:0.293997585773468\n",
      "Epoch 13[19/625] Time:0.708, Train Loss:0.2234373539686203\n",
      "Epoch 13[20/625] Time:0.705, Train Loss:0.2931082844734192\n",
      "Epoch 13[21/625] Time:0.704, Train Loss:0.2989400029182434\n",
      "Epoch 13[22/625] Time:0.742, Train Loss:0.2773447036743164\n",
      "Epoch 13[23/625] Time:0.702, Train Loss:0.2716325521469116\n",
      "Epoch 13[24/625] Time:0.703, Train Loss:0.2279757857322693\n",
      "Epoch 13[25/625] Time:0.704, Train Loss:0.32041674852371216\n",
      "Epoch 13[26/625] Time:0.703, Train Loss:0.29222220182418823\n",
      "Epoch 13[27/625] Time:0.704, Train Loss:0.2426859438419342\n",
      "Epoch 13[28/625] Time:0.703, Train Loss:0.37726515531539917\n",
      "Epoch 13[29/625] Time:0.703, Train Loss:0.26983216404914856\n",
      "Epoch 13[30/625] Time:0.705, Train Loss:0.3271070420742035\n",
      "Epoch 13[31/625] Time:0.717, Train Loss:0.4572041630744934\n",
      "Epoch 13[32/625] Time:0.694, Train Loss:0.1981959044933319\n",
      "Epoch 13[33/625] Time:0.693, Train Loss:0.33199790120124817\n",
      "Epoch 13[34/625] Time:0.694, Train Loss:0.25347092747688293\n",
      "Epoch 13[35/625] Time:0.695, Train Loss:0.28169897198677063\n",
      "Epoch 13[36/625] Time:0.697, Train Loss:0.2551697790622711\n",
      "Epoch 13[37/625] Time:0.694, Train Loss:0.23657448589801788\n",
      "Epoch 13[38/625] Time:0.693, Train Loss:0.2723868787288666\n",
      "Epoch 13[39/625] Time:0.703, Train Loss:0.28687772154808044\n",
      "Epoch 13[40/625] Time:0.704, Train Loss:0.24374641478061676\n",
      "Epoch 13[41/625] Time:0.703, Train Loss:0.23114745318889618\n",
      "Epoch 13[42/625] Time:0.705, Train Loss:0.21671605110168457\n",
      "Epoch 13[43/625] Time:0.704, Train Loss:0.2332569658756256\n",
      "Epoch 13[44/625] Time:0.703, Train Loss:0.26536887884140015\n",
      "Epoch 13[45/625] Time:0.702, Train Loss:0.3130365014076233\n",
      "Epoch 13[46/625] Time:0.704, Train Loss:0.34696540236473083\n",
      "Epoch 13[47/625] Time:0.703, Train Loss:0.23599407076835632\n",
      "Epoch 13[48/625] Time:0.704, Train Loss:0.32953158020973206\n",
      "Epoch 13[49/625] Time:0.729, Train Loss:0.2932910621166229\n",
      "Epoch 13[50/625] Time:0.695, Train Loss:0.24957862496376038\n",
      "Epoch 13[51/625] Time:0.703, Train Loss:0.24928948283195496\n",
      "Epoch 13[52/625] Time:0.703, Train Loss:0.2831723690032959\n",
      "Epoch 13[53/625] Time:0.702, Train Loss:0.2722965478897095\n",
      "Epoch 13[54/625] Time:0.703, Train Loss:0.23331566154956818\n",
      "Epoch 13[55/625] Time:0.704, Train Loss:0.2704871892929077\n",
      "Epoch 13[56/625] Time:0.706, Train Loss:0.24801675975322723\n",
      "Epoch 13[57/625] Time:0.693, Train Loss:0.27545684576034546\n",
      "Epoch 13[58/625] Time:0.709, Train Loss:0.2524884343147278\n",
      "Epoch 13[59/625] Time:0.703, Train Loss:0.29388925433158875\n",
      "Epoch 13[60/625] Time:0.703, Train Loss:0.23089981079101562\n",
      "Epoch 13[61/625] Time:0.703, Train Loss:0.3673505485057831\n",
      "Epoch 13[62/625] Time:0.704, Train Loss:0.2793346345424652\n",
      "Epoch 13[63/625] Time:0.704, Train Loss:0.3539685308933258\n",
      "Epoch 13[64/625] Time:0.703, Train Loss:0.25039854645729065\n",
      "Epoch 13[65/625] Time:0.704, Train Loss:0.2739044725894928\n",
      "Epoch 13[66/625] Time:0.701, Train Loss:0.23121513426303864\n",
      "Epoch 13[67/625] Time:0.694, Train Loss:0.24531711637973785\n",
      "Epoch 13[68/625] Time:0.695, Train Loss:0.2619965672492981\n",
      "Epoch 13[69/625] Time:0.696, Train Loss:0.32899972796440125\n",
      "Epoch 13[70/625] Time:0.694, Train Loss:0.196990504860878\n",
      "Epoch 13[71/625] Time:0.696, Train Loss:0.3437085747718811\n",
      "Epoch 13[72/625] Time:0.696, Train Loss:0.23889052867889404\n",
      "Epoch 13[73/625] Time:0.694, Train Loss:0.28265997767448425\n",
      "Epoch 13[74/625] Time:0.705, Train Loss:0.22224004566669464\n",
      "Epoch 13[75/625] Time:0.702, Train Loss:0.32592085003852844\n",
      "Epoch 13[76/625] Time:0.702, Train Loss:0.31722384691238403\n",
      "Epoch 13[77/625] Time:0.703, Train Loss:0.36737003922462463\n",
      "Epoch 13[78/625] Time:0.704, Train Loss:0.3251977264881134\n",
      "Epoch 13[79/625] Time:0.703, Train Loss:0.3622347116470337\n",
      "Epoch 13[80/625] Time:0.703, Train Loss:0.304076611995697\n",
      "Epoch 13[81/625] Time:0.711, Train Loss:0.32516151666641235\n",
      "Epoch 13[82/625] Time:0.707, Train Loss:0.2847580909729004\n",
      "Epoch 13[83/625] Time:0.706, Train Loss:0.29494866728782654\n",
      "Epoch 13[84/625] Time:0.704, Train Loss:0.24556870758533478\n",
      "Epoch 13[85/625] Time:0.704, Train Loss:0.22385361790657043\n",
      "Epoch 13[86/625] Time:0.703, Train Loss:0.2249414026737213\n",
      "Epoch 13[87/625] Time:0.705, Train Loss:0.30851274728775024\n",
      "Epoch 13[88/625] Time:0.717, Train Loss:0.2642706632614136\n",
      "Epoch 13[89/625] Time:0.696, Train Loss:0.24613037705421448\n",
      "Epoch 13[90/625] Time:0.704, Train Loss:0.25731027126312256\n",
      "Epoch 13[91/625] Time:0.704, Train Loss:0.22172300517559052\n",
      "Epoch 13[92/625] Time:0.704, Train Loss:0.29660341143608093\n",
      "Epoch 13[93/625] Time:0.705, Train Loss:0.28488755226135254\n",
      "Epoch 13[94/625] Time:0.704, Train Loss:0.25315049290657043\n",
      "Epoch 13[95/625] Time:0.705, Train Loss:0.34472018480300903\n",
      "Epoch 13[96/625] Time:0.704, Train Loss:0.24883590638637543\n",
      "Epoch 13[97/625] Time:0.691, Train Loss:0.23542355000972748\n",
      "Epoch 13[98/625] Time:0.703, Train Loss:0.37159913778305054\n",
      "Epoch 13[99/625] Time:0.704, Train Loss:0.2557290196418762\n",
      "Epoch 13[100/625] Time:0.704, Train Loss:0.33879944682121277\n",
      "Epoch 13[101/625] Time:0.705, Train Loss:0.2657211720943451\n",
      "Epoch 13[102/625] Time:0.705, Train Loss:0.38797473907470703\n",
      "Epoch 13[103/625] Time:0.705, Train Loss:0.26133865118026733\n",
      "Epoch 13[104/625] Time:0.705, Train Loss:0.30476540327072144\n",
      "Epoch 13[105/625] Time:0.705, Train Loss:0.2301497906446457\n",
      "Epoch 13[106/625] Time:0.706, Train Loss:0.3075005114078522\n",
      "Epoch 13[107/625] Time:0.704, Train Loss:0.35696280002593994\n",
      "Epoch 13[108/625] Time:0.707, Train Loss:0.265228271484375\n",
      "Epoch 13[109/625] Time:0.704, Train Loss:0.23363153636455536\n",
      "Epoch 13[110/625] Time:0.704, Train Loss:0.2563376724720001\n",
      "Epoch 13[111/625] Time:0.705, Train Loss:0.21708303689956665\n",
      "Epoch 13[112/625] Time:0.705, Train Loss:0.2903677523136139\n",
      "Epoch 13[113/625] Time:0.705, Train Loss:0.25840452313423157\n",
      "Epoch 13[114/625] Time:0.705, Train Loss:0.3234831988811493\n",
      "Epoch 13[115/625] Time:0.704, Train Loss:0.22294434905052185\n",
      "Epoch 13[116/625] Time:0.748, Train Loss:0.2332683950662613\n",
      "Epoch 13[117/625] Time:0.693, Train Loss:0.3771485686302185\n",
      "Epoch 13[118/625] Time:0.694, Train Loss:0.3031293749809265\n",
      "Epoch 13[119/625] Time:0.694, Train Loss:0.20820681750774384\n",
      "Epoch 13[120/625] Time:0.694, Train Loss:0.2849505841732025\n",
      "Epoch 13[121/625] Time:0.694, Train Loss:0.3299903869628906\n",
      "Epoch 13[122/625] Time:0.693, Train Loss:0.31859979033470154\n",
      "Epoch 13[123/625] Time:0.692, Train Loss:0.2842681407928467\n",
      "Epoch 13[124/625] Time:0.693, Train Loss:0.22188080847263336\n",
      "Epoch 13[125/625] Time:0.704, Train Loss:0.304257333278656\n",
      "Epoch 13[126/625] Time:0.703, Train Loss:0.39545270800590515\n",
      "Epoch 13[127/625] Time:0.705, Train Loss:0.4480908513069153\n",
      "Epoch 13[128/625] Time:0.73, Train Loss:0.27127352356910706\n",
      "Epoch 13[129/625] Time:0.692, Train Loss:0.39548373222351074\n",
      "Epoch 13[130/625] Time:0.705, Train Loss:0.26050081849098206\n",
      "Epoch 13[131/625] Time:0.743, Train Loss:0.24153690040111542\n",
      "Epoch 13[132/625] Time:0.703, Train Loss:0.2675763666629791\n",
      "Epoch 13[133/625] Time:0.703, Train Loss:0.4263860285282135\n",
      "Epoch 13[134/625] Time:0.703, Train Loss:0.2715017795562744\n",
      "Epoch 13[135/625] Time:0.704, Train Loss:0.26894333958625793\n",
      "Epoch 13[136/625] Time:0.702, Train Loss:0.34860217571258545\n",
      "Epoch 13[137/625] Time:0.733, Train Loss:0.390079140663147\n",
      "Epoch 13[138/625] Time:0.693, Train Loss:0.2396751493215561\n",
      "Epoch 13[139/625] Time:0.693, Train Loss:0.4372035264968872\n",
      "Epoch 13[140/625] Time:0.693, Train Loss:0.27081504464149475\n",
      "Epoch 13[141/625] Time:0.693, Train Loss:0.30270761251449585\n",
      "Epoch 13[142/625] Time:0.694, Train Loss:0.2586243748664856\n",
      "Epoch 13[143/625] Time:0.691, Train Loss:0.31146562099456787\n",
      "Epoch 13[144/625] Time:0.694, Train Loss:0.3683290183544159\n",
      "Epoch 13[145/625] Time:0.695, Train Loss:0.24889418482780457\n",
      "Epoch 13[146/625] Time:0.694, Train Loss:0.3382687270641327\n",
      "Epoch 13[147/625] Time:0.694, Train Loss:0.3347719609737396\n",
      "Epoch 13[148/625] Time:0.693, Train Loss:0.34841403365135193\n",
      "Epoch 13[149/625] Time:0.706, Train Loss:0.26699694991111755\n",
      "Epoch 13[150/625] Time:0.706, Train Loss:0.31015273928642273\n",
      "Epoch 13[151/625] Time:0.705, Train Loss:0.24318702518939972\n",
      "Epoch 13[152/625] Time:0.704, Train Loss:0.23011253774166107\n",
      "Epoch 13[153/625] Time:0.703, Train Loss:0.25549057126045227\n",
      "Epoch 13[154/625] Time:0.694, Train Loss:0.3205999732017517\n",
      "Epoch 13[155/625] Time:0.704, Train Loss:0.275621622800827\n",
      "Epoch 13[156/625] Time:0.705, Train Loss:0.2695673704147339\n",
      "Epoch 13[157/625] Time:0.706, Train Loss:0.3270428478717804\n",
      "Epoch 13[158/625] Time:0.704, Train Loss:0.3356320559978485\n",
      "Epoch 13[159/625] Time:0.705, Train Loss:0.31752580404281616\n",
      "Epoch 13[160/625] Time:0.704, Train Loss:0.3474281132221222\n",
      "Epoch 13[161/625] Time:0.704, Train Loss:0.2593413293361664\n",
      "Epoch 13[162/625] Time:0.704, Train Loss:0.28759631514549255\n",
      "Epoch 13[163/625] Time:0.705, Train Loss:0.3131710886955261\n",
      "Epoch 13[164/625] Time:0.705, Train Loss:0.32625848054885864\n",
      "Epoch 13[165/625] Time:0.704, Train Loss:0.30146583914756775\n",
      "Epoch 13[166/625] Time:0.704, Train Loss:0.3222707509994507\n",
      "Epoch 13[167/625] Time:0.707, Train Loss:0.3164198100566864\n",
      "Epoch 13[168/625] Time:0.703, Train Loss:0.38642022013664246\n",
      "Epoch 13[169/625] Time:0.704, Train Loss:0.2971051037311554\n",
      "Epoch 13[170/625] Time:0.704, Train Loss:0.34392914175987244\n",
      "Epoch 13[171/625] Time:0.704, Train Loss:0.26282593607902527\n",
      "Epoch 13[172/625] Time:0.704, Train Loss:0.29916489124298096\n",
      "Epoch 13[173/625] Time:0.705, Train Loss:0.2517929673194885\n",
      "Epoch 13[174/625] Time:0.703, Train Loss:0.3157225549221039\n",
      "Epoch 13[175/625] Time:0.705, Train Loss:0.2819563150405884\n",
      "Epoch 13[176/625] Time:0.703, Train Loss:0.3360249698162079\n",
      "Epoch 13[177/625] Time:0.705, Train Loss:0.32637766003608704\n",
      "Epoch 13[178/625] Time:0.705, Train Loss:0.27604779601097107\n",
      "Epoch 13[179/625] Time:0.704, Train Loss:0.31241223216056824\n",
      "Epoch 13[180/625] Time:0.707, Train Loss:0.29751482605934143\n",
      "Epoch 13[181/625] Time:0.705, Train Loss:0.26309067010879517\n",
      "Epoch 13[182/625] Time:0.704, Train Loss:0.23643262684345245\n",
      "Epoch 13[183/625] Time:0.704, Train Loss:0.29228469729423523\n",
      "Epoch 13[184/625] Time:0.705, Train Loss:0.2941146492958069\n",
      "Epoch 13[185/625] Time:0.719, Train Loss:0.302175372838974\n",
      "Epoch 13[186/625] Time:0.69, Train Loss:0.22939755022525787\n",
      "Epoch 13[187/625] Time:0.691, Train Loss:0.2563782334327698\n",
      "Epoch 13[188/625] Time:0.69, Train Loss:0.26613885164260864\n",
      "Epoch 13[189/625] Time:0.692, Train Loss:0.24862325191497803\n",
      "Epoch 13[190/625] Time:0.69, Train Loss:0.21488267183303833\n",
      "Epoch 13[191/625] Time:0.694, Train Loss:0.3336983621120453\n",
      "Epoch 13[192/625] Time:0.69, Train Loss:0.24252885580062866\n",
      "Epoch 13[193/625] Time:0.692, Train Loss:0.27652668952941895\n",
      "Epoch 13[194/625] Time:0.69, Train Loss:0.2634362578392029\n",
      "Epoch 13[195/625] Time:0.691, Train Loss:0.27243494987487793\n",
      "Epoch 13[196/625] Time:0.693, Train Loss:0.2690697908401489\n",
      "Epoch 13[197/625] Time:0.692, Train Loss:0.2888128459453583\n",
      "Epoch 13[198/625] Time:0.711, Train Loss:0.23994863033294678\n",
      "Epoch 13[199/625] Time:0.709, Train Loss:0.2431192845106125\n",
      "Epoch 13[200/625] Time:0.714, Train Loss:0.28188464045524597\n",
      "Epoch 13[201/625] Time:0.694, Train Loss:0.28131887316703796\n",
      "Epoch 13[202/625] Time:0.704, Train Loss:0.33680638670921326\n",
      "Epoch 13[203/625] Time:0.704, Train Loss:0.31523266434669495\n",
      "Epoch 13[204/625] Time:0.705, Train Loss:0.2824884355068207\n",
      "Epoch 13[205/625] Time:0.705, Train Loss:0.31547197699546814\n",
      "Epoch 13[206/625] Time:0.705, Train Loss:0.34872686862945557\n",
      "Epoch 13[207/625] Time:0.705, Train Loss:0.276570200920105\n",
      "Epoch 13[208/625] Time:0.707, Train Loss:0.23567764461040497\n",
      "Epoch 13[209/625] Time:0.726, Train Loss:0.3227202296257019\n",
      "Epoch 13[210/625] Time:0.69, Train Loss:0.2878638803958893\n",
      "Epoch 13[211/625] Time:0.698, Train Loss:0.29200485348701477\n",
      "Epoch 13[212/625] Time:0.692, Train Loss:0.3274083137512207\n",
      "Epoch 13[213/625] Time:0.691, Train Loss:0.30237823724746704\n",
      "Epoch 13[214/625] Time:0.73, Train Loss:0.29112952947616577\n",
      "Epoch 13[215/625] Time:0.704, Train Loss:0.2347242534160614\n",
      "Epoch 13[216/625] Time:0.704, Train Loss:0.2543662488460541\n",
      "Epoch 13[217/625] Time:0.704, Train Loss:0.23326708376407623\n",
      "Epoch 13[218/625] Time:0.704, Train Loss:0.2835242450237274\n",
      "Epoch 13[219/625] Time:0.704, Train Loss:0.26816481351852417\n",
      "Epoch 13[220/625] Time:0.704, Train Loss:0.2652387022972107\n",
      "Epoch 13[221/625] Time:0.707, Train Loss:0.295180082321167\n",
      "Epoch 13[222/625] Time:0.707, Train Loss:0.2524074912071228\n",
      "Epoch 13[223/625] Time:0.704, Train Loss:0.27051135897636414\n",
      "Epoch 13[224/625] Time:0.705, Train Loss:0.2767581343650818\n",
      "Epoch 13[225/625] Time:0.706, Train Loss:0.22320248186588287\n",
      "Epoch 13[226/625] Time:0.704, Train Loss:0.28329771757125854\n",
      "Epoch 13[227/625] Time:0.705, Train Loss:0.24794606864452362\n",
      "Epoch 13[228/625] Time:0.705, Train Loss:0.3114393651485443\n",
      "Epoch 13[229/625] Time:0.704, Train Loss:0.25918108224868774\n",
      "Epoch 13[230/625] Time:0.704, Train Loss:0.24313613772392273\n",
      "Epoch 13[231/625] Time:0.704, Train Loss:0.3076087236404419\n",
      "Epoch 13[232/625] Time:0.706, Train Loss:0.3850703537464142\n",
      "Epoch 13[233/625] Time:0.706, Train Loss:0.2779541313648224\n",
      "Epoch 13[234/625] Time:0.704, Train Loss:0.29606446623802185\n",
      "Epoch 13[235/625] Time:0.705, Train Loss:0.4222742021083832\n",
      "Epoch 13[236/625] Time:0.708, Train Loss:0.3080732226371765\n",
      "Epoch 13[237/625] Time:0.693, Train Loss:0.2567565441131592\n",
      "Epoch 13[238/625] Time:0.704, Train Loss:0.2337680608034134\n",
      "Epoch 13[239/625] Time:0.727, Train Loss:0.26743456721305847\n",
      "Epoch 13[240/625] Time:0.704, Train Loss:0.27609723806381226\n",
      "Epoch 13[241/625] Time:0.703, Train Loss:0.2502996623516083\n",
      "Epoch 13[242/625] Time:0.704, Train Loss:0.26705291867256165\n",
      "Epoch 13[243/625] Time:0.705, Train Loss:0.3060934245586395\n",
      "Epoch 13[244/625] Time:0.705, Train Loss:0.3388570547103882\n",
      "Epoch 13[245/625] Time:0.704, Train Loss:0.2988811135292053\n",
      "Epoch 13[246/625] Time:0.706, Train Loss:0.2373351752758026\n",
      "Epoch 13[247/625] Time:0.709, Train Loss:0.27850663661956787\n",
      "Epoch 13[248/625] Time:0.733, Train Loss:0.27092528343200684\n",
      "Epoch 13[249/625] Time:0.704, Train Loss:0.21958400309085846\n",
      "Epoch 13[250/625] Time:0.706, Train Loss:0.38223981857299805\n",
      "Epoch 13[251/625] Time:0.705, Train Loss:0.42020541429519653\n",
      "Epoch 13[252/625] Time:0.705, Train Loss:0.2546427249908447\n",
      "Epoch 13[253/625] Time:0.704, Train Loss:0.24262313544750214\n",
      "Epoch 13[254/625] Time:0.703, Train Loss:0.2591326832771301\n",
      "Epoch 13[255/625] Time:0.703, Train Loss:0.23350484669208527\n",
      "Epoch 13[256/625] Time:0.707, Train Loss:0.3394804298877716\n",
      "Epoch 13[257/625] Time:0.706, Train Loss:0.3428219258785248\n",
      "Epoch 13[258/625] Time:0.707, Train Loss:0.17753903567790985\n",
      "Epoch 13[259/625] Time:0.704, Train Loss:0.3622751235961914\n",
      "Epoch 13[260/625] Time:0.704, Train Loss:0.24641677737236023\n",
      "Epoch 13[261/625] Time:0.705, Train Loss:0.22664833068847656\n",
      "Epoch 13[262/625] Time:0.705, Train Loss:0.31269657611846924\n",
      "Epoch 13[263/625] Time:0.705, Train Loss:0.29480400681495667\n",
      "Epoch 13[264/625] Time:0.704, Train Loss:0.20860907435417175\n",
      "Epoch 13[265/625] Time:0.704, Train Loss:0.23493309319019318\n",
      "Epoch 13[266/625] Time:0.703, Train Loss:0.3268285095691681\n",
      "Epoch 13[267/625] Time:0.706, Train Loss:0.28519394993782043\n",
      "Epoch 13[268/625] Time:0.704, Train Loss:0.24855220317840576\n",
      "Epoch 13[269/625] Time:0.704, Train Loss:0.24302713572978973\n",
      "Epoch 13[270/625] Time:0.703, Train Loss:0.2567555010318756\n",
      "Epoch 13[271/625] Time:0.703, Train Loss:0.23014765977859497\n",
      "Epoch 13[272/625] Time:0.727, Train Loss:0.33749568462371826\n",
      "Epoch 13[273/625] Time:0.694, Train Loss:0.31081369519233704\n",
      "Epoch 13[274/625] Time:0.694, Train Loss:0.29989057779312134\n",
      "Epoch 13[275/625] Time:0.694, Train Loss:0.27242282032966614\n",
      "Epoch 13[276/625] Time:0.695, Train Loss:0.3120745122432709\n",
      "Epoch 13[277/625] Time:0.706, Train Loss:0.25151145458221436\n",
      "Epoch 13[278/625] Time:0.703, Train Loss:0.25521042943000793\n",
      "Epoch 13[279/625] Time:0.704, Train Loss:0.29489848017692566\n",
      "Epoch 13[280/625] Time:0.705, Train Loss:0.4054946005344391\n",
      "Epoch 13[281/625] Time:0.704, Train Loss:0.31938090920448303\n",
      "Epoch 13[282/625] Time:0.703, Train Loss:0.2019847333431244\n",
      "Epoch 13[283/625] Time:0.704, Train Loss:0.18675032258033752\n",
      "Epoch 13[284/625] Time:0.704, Train Loss:0.2874082624912262\n",
      "Epoch 13[285/625] Time:0.703, Train Loss:0.2885258197784424\n",
      "Epoch 13[286/625] Time:0.703, Train Loss:0.28039929270744324\n",
      "Epoch 13[287/625] Time:0.703, Train Loss:0.3134102523326874\n",
      "Epoch 13[288/625] Time:0.704, Train Loss:0.26989057660102844\n",
      "Epoch 13[289/625] Time:0.704, Train Loss:0.3198016881942749\n",
      "Epoch 13[290/625] Time:0.705, Train Loss:0.31067657470703125\n",
      "Epoch 13[291/625] Time:0.704, Train Loss:0.22022496163845062\n",
      "Epoch 13[292/625] Time:0.704, Train Loss:0.2896270751953125\n",
      "Epoch 13[293/625] Time:0.703, Train Loss:0.3333352208137512\n",
      "Epoch 13[294/625] Time:0.705, Train Loss:0.31205645203590393\n",
      "Epoch 13[295/625] Time:0.704, Train Loss:0.31821319460868835\n",
      "Epoch 13[296/625] Time:0.705, Train Loss:0.2920036017894745\n",
      "Epoch 13[297/625] Time:0.741, Train Loss:0.3186529278755188\n",
      "Epoch 13[298/625] Time:0.692, Train Loss:0.3126082122325897\n",
      "Epoch 13[299/625] Time:0.706, Train Loss:0.2525230646133423\n",
      "Epoch 13[300/625] Time:0.721, Train Loss:0.31078290939331055\n",
      "Epoch 13[301/625] Time:0.693, Train Loss:0.2272120863199234\n",
      "Epoch 13[302/625] Time:0.704, Train Loss:0.1961219608783722\n",
      "Epoch 13[303/625] Time:0.704, Train Loss:0.27756255865097046\n",
      "Epoch 13[304/625] Time:0.709, Train Loss:0.2622847855091095\n",
      "Epoch 13[305/625] Time:0.702, Train Loss:0.18543611466884613\n",
      "Epoch 13[306/625] Time:0.702, Train Loss:0.2882062792778015\n",
      "Epoch 13[307/625] Time:0.703, Train Loss:0.3284096419811249\n",
      "Epoch 13[308/625] Time:0.704, Train Loss:0.19091583788394928\n",
      "Epoch 13[309/625] Time:0.704, Train Loss:0.249245747923851\n",
      "Epoch 13[310/625] Time:0.703, Train Loss:0.26028797030448914\n",
      "Epoch 13[311/625] Time:0.706, Train Loss:0.3182019591331482\n",
      "Epoch 13[312/625] Time:0.704, Train Loss:0.29057666659355164\n",
      "Epoch 13[313/625] Time:0.704, Train Loss:0.27544477581977844\n",
      "Epoch 13[314/625] Time:0.704, Train Loss:0.3953779339790344\n",
      "Epoch 13[315/625] Time:0.707, Train Loss:0.3138340413570404\n",
      "Epoch 13[316/625] Time:0.706, Train Loss:0.33546319603919983\n",
      "Epoch 13[317/625] Time:0.703, Train Loss:0.27689576148986816\n",
      "Epoch 13[318/625] Time:0.703, Train Loss:0.26618751883506775\n",
      "Epoch 13[319/625] Time:0.703, Train Loss:0.2530718147754669\n",
      "Epoch 13[320/625] Time:0.704, Train Loss:0.25312748551368713\n",
      "Epoch 13[321/625] Time:0.703, Train Loss:0.30180326104164124\n",
      "Epoch 13[322/625] Time:0.703, Train Loss:0.320843368768692\n",
      "Epoch 13[323/625] Time:0.702, Train Loss:0.34564661979675293\n",
      "Epoch 13[324/625] Time:0.702, Train Loss:0.3621733784675598\n",
      "Epoch 13[325/625] Time:0.704, Train Loss:0.26983842253685\n",
      "Epoch 13[326/625] Time:0.704, Train Loss:0.24334587156772614\n",
      "Epoch 13[327/625] Time:0.705, Train Loss:0.3038060665130615\n",
      "Epoch 13[328/625] Time:0.703, Train Loss:0.27399569749832153\n",
      "Epoch 13[329/625] Time:0.703, Train Loss:0.3080006241798401\n",
      "Epoch 13[330/625] Time:0.692, Train Loss:0.28078293800354004\n",
      "Epoch 13[331/625] Time:0.694, Train Loss:0.23865634202957153\n",
      "Epoch 13[332/625] Time:0.695, Train Loss:0.2775517702102661\n",
      "Epoch 13[333/625] Time:0.704, Train Loss:0.31149154901504517\n",
      "Epoch 13[334/625] Time:0.705, Train Loss:0.26987722516059875\n",
      "Epoch 13[335/625] Time:0.704, Train Loss:0.2299361526966095\n",
      "Epoch 13[336/625] Time:0.706, Train Loss:0.3000221252441406\n",
      "Epoch 13[337/625] Time:0.704, Train Loss:0.3945014178752899\n",
      "Epoch 13[338/625] Time:0.706, Train Loss:0.30042728781700134\n",
      "Epoch 13[339/625] Time:0.702, Train Loss:0.31036892533302307\n",
      "Epoch 13[340/625] Time:0.705, Train Loss:0.28867846727371216\n",
      "Epoch 13[341/625] Time:0.704, Train Loss:0.29795926809310913\n",
      "Epoch 13[342/625] Time:0.706, Train Loss:0.2627706825733185\n",
      "Epoch 13[343/625] Time:0.704, Train Loss:0.2820987105369568\n",
      "Epoch 13[344/625] Time:0.705, Train Loss:0.24392278492450714\n",
      "Epoch 13[345/625] Time:0.703, Train Loss:0.3983583450317383\n",
      "Epoch 13[346/625] Time:0.705, Train Loss:0.3470297157764435\n",
      "Epoch 13[347/625] Time:0.704, Train Loss:0.2778075635433197\n",
      "Epoch 13[348/625] Time:0.704, Train Loss:0.2542596161365509\n",
      "Epoch 13[349/625] Time:0.704, Train Loss:0.31817445158958435\n",
      "Epoch 13[350/625] Time:0.707, Train Loss:0.3754914402961731\n",
      "Epoch 13[351/625] Time:0.706, Train Loss:0.2730913758277893\n",
      "Epoch 13[352/625] Time:0.705, Train Loss:0.2686241865158081\n",
      "Epoch 13[353/625] Time:0.704, Train Loss:0.36283695697784424\n",
      "Epoch 13[354/625] Time:0.705, Train Loss:0.29066166281700134\n",
      "Epoch 13[355/625] Time:0.71, Train Loss:0.3594290614128113\n",
      "Epoch 13[356/625] Time:0.703, Train Loss:0.3308504819869995\n",
      "Epoch 13[357/625] Time:0.704, Train Loss:0.4121338129043579\n",
      "Epoch 13[358/625] Time:0.703, Train Loss:0.35610878467559814\n",
      "Epoch 13[359/625] Time:0.705, Train Loss:0.31805965304374695\n",
      "Epoch 13[360/625] Time:0.705, Train Loss:0.23068691790103912\n",
      "Epoch 13[361/625] Time:0.705, Train Loss:0.2740209102630615\n",
      "Epoch 13[362/625] Time:0.704, Train Loss:0.2900569438934326\n",
      "Epoch 13[363/625] Time:0.706, Train Loss:0.21052949130535126\n",
      "Epoch 13[364/625] Time:0.703, Train Loss:0.22354833781719208\n",
      "Epoch 13[365/625] Time:0.703, Train Loss:0.33933088183403015\n",
      "Epoch 13[366/625] Time:0.704, Train Loss:0.32371121644973755\n",
      "Epoch 13[367/625] Time:0.704, Train Loss:0.2896426022052765\n",
      "Epoch 13[368/625] Time:0.704, Train Loss:0.25127163529396057\n",
      "Epoch 13[369/625] Time:0.705, Train Loss:0.35829848051071167\n",
      "Epoch 13[370/625] Time:0.71, Train Loss:0.3253321945667267\n",
      "Epoch 13[371/625] Time:0.703, Train Loss:0.28544554114341736\n",
      "Epoch 13[372/625] Time:0.703, Train Loss:0.26847511529922485\n",
      "Epoch 13[373/625] Time:0.705, Train Loss:0.2646193504333496\n",
      "Epoch 13[374/625] Time:0.707, Train Loss:0.21477694809436798\n",
      "Epoch 13[375/625] Time:0.709, Train Loss:0.24044059216976166\n",
      "Epoch 13[376/625] Time:0.693, Train Loss:0.3883335292339325\n",
      "Epoch 13[377/625] Time:0.705, Train Loss:0.393113374710083\n",
      "Epoch 13[378/625] Time:0.705, Train Loss:0.26518672704696655\n",
      "Epoch 13[379/625] Time:0.704, Train Loss:0.25680240988731384\n",
      "Epoch 13[380/625] Time:0.706, Train Loss:0.24917301535606384\n",
      "Epoch 13[381/625] Time:0.705, Train Loss:0.2917337417602539\n",
      "Epoch 13[382/625] Time:0.739, Train Loss:0.3155091404914856\n",
      "Epoch 13[383/625] Time:0.692, Train Loss:0.24244879186153412\n",
      "Epoch 13[384/625] Time:0.694, Train Loss:0.32254520058631897\n",
      "Epoch 13[385/625] Time:0.693, Train Loss:0.3080410361289978\n",
      "Epoch 13[386/625] Time:0.694, Train Loss:0.24014204740524292\n",
      "Epoch 13[387/625] Time:0.693, Train Loss:0.38232722878456116\n",
      "Epoch 13[388/625] Time:0.715, Train Loss:0.31678110361099243\n",
      "Epoch 13[389/625] Time:0.703, Train Loss:0.3268524408340454\n",
      "Epoch 13[390/625] Time:0.704, Train Loss:0.1774996966123581\n",
      "Epoch 13[391/625] Time:0.704, Train Loss:0.30105143785476685\n",
      "Epoch 13[392/625] Time:0.708, Train Loss:0.2612568736076355\n",
      "Epoch 13[393/625] Time:0.695, Train Loss:0.27680110931396484\n",
      "Epoch 13[394/625] Time:0.705, Train Loss:0.339449405670166\n",
      "Epoch 13[395/625] Time:0.704, Train Loss:0.2897699773311615\n",
      "Epoch 13[396/625] Time:0.704, Train Loss:0.33400124311447144\n",
      "Epoch 13[397/625] Time:0.728, Train Loss:0.3027583658695221\n",
      "Epoch 13[398/625] Time:0.689, Train Loss:0.3789672255516052\n",
      "Epoch 13[399/625] Time:0.693, Train Loss:0.2982610762119293\n",
      "Epoch 13[400/625] Time:0.692, Train Loss:0.32781487703323364\n",
      "Epoch 13[401/625] Time:0.709, Train Loss:0.34997397661209106\n",
      "Epoch 13[402/625] Time:0.693, Train Loss:0.280975341796875\n",
      "Epoch 13[403/625] Time:0.704, Train Loss:0.2571476399898529\n",
      "Epoch 13[404/625] Time:0.705, Train Loss:0.24711646139621735\n",
      "Epoch 13[405/625] Time:0.705, Train Loss:0.21775659918785095\n",
      "Epoch 13[406/625] Time:0.705, Train Loss:0.28161415457725525\n",
      "Epoch 13[407/625] Time:0.691, Train Loss:0.29814690351486206\n",
      "Epoch 13[408/625] Time:0.692, Train Loss:0.34676021337509155\n",
      "Epoch 13[409/625] Time:0.695, Train Loss:0.24968238174915314\n",
      "Epoch 13[410/625] Time:0.693, Train Loss:0.3589828908443451\n",
      "Epoch 13[411/625] Time:0.693, Train Loss:0.3374258577823639\n",
      "Epoch 13[412/625] Time:0.693, Train Loss:0.3362134099006653\n",
      "Epoch 13[413/625] Time:0.694, Train Loss:0.3684627115726471\n",
      "Epoch 13[414/625] Time:0.693, Train Loss:0.3169635832309723\n",
      "Epoch 13[415/625] Time:0.703, Train Loss:0.25155720114707947\n",
      "Epoch 13[416/625] Time:0.696, Train Loss:0.24859048426151276\n",
      "Epoch 13[417/625] Time:0.704, Train Loss:0.2958489954471588\n",
      "Epoch 13[418/625] Time:0.701, Train Loss:0.3746315836906433\n",
      "Epoch 13[419/625] Time:0.702, Train Loss:0.32227277755737305\n",
      "Epoch 13[420/625] Time:0.706, Train Loss:0.24521036446094513\n",
      "Epoch 13[421/625] Time:0.711, Train Loss:0.24171240627765656\n",
      "Epoch 13[422/625] Time:0.705, Train Loss:0.3785615563392639\n",
      "Epoch 13[423/625] Time:0.703, Train Loss:0.31507158279418945\n",
      "Epoch 13[424/625] Time:0.707, Train Loss:0.3001941740512848\n",
      "Epoch 13[425/625] Time:0.708, Train Loss:0.30993935465812683\n",
      "Epoch 13[426/625] Time:0.704, Train Loss:0.3826926350593567\n",
      "Epoch 13[427/625] Time:0.705, Train Loss:0.34778881072998047\n",
      "Epoch 13[428/625] Time:0.707, Train Loss:0.2502840757369995\n",
      "Epoch 13[429/625] Time:0.738, Train Loss:0.23304031789302826\n",
      "Epoch 13[430/625] Time:0.693, Train Loss:0.35156071186065674\n",
      "Epoch 13[431/625] Time:0.693, Train Loss:0.2812640070915222\n",
      "Epoch 13[432/625] Time:0.694, Train Loss:0.31107088923454285\n",
      "Epoch 13[433/625] Time:0.692, Train Loss:0.2958846688270569\n",
      "Epoch 13[434/625] Time:0.693, Train Loss:0.2494765818119049\n",
      "Epoch 13[435/625] Time:0.692, Train Loss:0.3442264497280121\n",
      "Epoch 13[436/625] Time:0.693, Train Loss:0.3898373246192932\n",
      "Epoch 13[437/625] Time:0.704, Train Loss:0.2865459620952606\n",
      "Epoch 13[438/625] Time:0.735, Train Loss:0.2782783806324005\n",
      "Epoch 13[439/625] Time:0.704, Train Loss:0.27937400341033936\n",
      "Epoch 13[440/625] Time:0.703, Train Loss:0.21019157767295837\n",
      "Epoch 13[441/625] Time:0.702, Train Loss:0.3622588515281677\n",
      "Epoch 13[442/625] Time:0.704, Train Loss:0.2543673515319824\n",
      "Epoch 13[443/625] Time:0.702, Train Loss:0.31016436219215393\n",
      "Epoch 13[444/625] Time:0.703, Train Loss:0.2546918988227844\n",
      "Epoch 13[445/625] Time:0.704, Train Loss:0.39000460505485535\n",
      "Epoch 13[446/625] Time:0.703, Train Loss:0.27122771739959717\n",
      "Epoch 13[447/625] Time:0.706, Train Loss:0.29676592350006104\n",
      "Epoch 13[448/625] Time:0.703, Train Loss:0.3514346182346344\n",
      "Epoch 13[449/625] Time:0.703, Train Loss:0.33676934242248535\n",
      "Epoch 13[450/625] Time:0.702, Train Loss:0.28756988048553467\n",
      "Epoch 13[451/625] Time:0.702, Train Loss:0.2167699635028839\n",
      "Epoch 13[452/625] Time:0.714, Train Loss:0.2855003774166107\n",
      "Epoch 13[453/625] Time:0.695, Train Loss:0.22698575258255005\n",
      "Epoch 13[454/625] Time:0.695, Train Loss:0.23073215782642365\n",
      "Epoch 13[455/625] Time:0.695, Train Loss:0.3340502381324768\n",
      "Epoch 13[456/625] Time:0.712, Train Loss:0.30444949865341187\n",
      "Epoch 13[457/625] Time:0.703, Train Loss:0.27768129110336304\n",
      "Epoch 13[458/625] Time:0.725, Train Loss:0.20661288499832153\n",
      "Epoch 13[459/625] Time:0.693, Train Loss:0.31011417508125305\n",
      "Epoch 13[460/625] Time:0.696, Train Loss:0.28587597608566284\n",
      "Epoch 13[461/625] Time:0.71, Train Loss:0.28121674060821533\n",
      "Epoch 13[462/625] Time:0.692, Train Loss:0.23129446804523468\n",
      "Epoch 13[463/625] Time:0.696, Train Loss:0.36831390857696533\n",
      "Epoch 13[464/625] Time:0.694, Train Loss:0.2359856367111206\n",
      "Epoch 13[465/625] Time:0.696, Train Loss:0.2868480980396271\n",
      "Epoch 13[466/625] Time:0.707, Train Loss:0.26265254616737366\n",
      "Epoch 13[467/625] Time:0.702, Train Loss:0.3943481147289276\n",
      "Epoch 13[468/625] Time:0.703, Train Loss:0.24971908330917358\n",
      "Epoch 13[469/625] Time:0.703, Train Loss:0.42509305477142334\n",
      "Epoch 13[470/625] Time:0.71, Train Loss:0.2682065963745117\n",
      "Epoch 13[471/625] Time:0.702, Train Loss:0.20724326372146606\n",
      "Epoch 13[472/625] Time:0.703, Train Loss:0.331589937210083\n",
      "Epoch 13[473/625] Time:0.703, Train Loss:0.25050321221351624\n",
      "Epoch 13[474/625] Time:0.703, Train Loss:0.3573002517223358\n",
      "Epoch 13[475/625] Time:0.703, Train Loss:0.397661030292511\n",
      "Epoch 13[476/625] Time:0.702, Train Loss:0.2666775584220886\n",
      "Epoch 13[477/625] Time:0.706, Train Loss:0.3061036169528961\n",
      "Epoch 13[478/625] Time:0.704, Train Loss:0.26205646991729736\n",
      "Epoch 13[479/625] Time:0.702, Train Loss:0.2690110206604004\n",
      "Epoch 13[480/625] Time:0.703, Train Loss:0.24274729192256927\n",
      "Epoch 13[481/625] Time:0.706, Train Loss:0.2905143201351166\n",
      "Epoch 13[482/625] Time:0.705, Train Loss:0.34419500827789307\n",
      "Epoch 13[483/625] Time:0.706, Train Loss:0.2443585991859436\n",
      "Epoch 13[484/625] Time:0.705, Train Loss:0.3857388198375702\n",
      "Epoch 13[485/625] Time:0.703, Train Loss:0.3070850968360901\n",
      "Epoch 13[486/625] Time:0.705, Train Loss:0.26648926734924316\n",
      "Epoch 13[487/625] Time:0.704, Train Loss:0.27390027046203613\n",
      "Epoch 13[488/625] Time:0.704, Train Loss:0.2330915778875351\n",
      "Epoch 13[489/625] Time:0.705, Train Loss:0.23085328936576843\n",
      "Epoch 13[490/625] Time:0.704, Train Loss:0.4014653265476227\n",
      "Epoch 13[491/625] Time:0.704, Train Loss:0.30505073070526123\n",
      "Epoch 13[492/625] Time:0.705, Train Loss:0.2874050736427307\n",
      "Epoch 13[493/625] Time:0.705, Train Loss:0.27211636304855347\n",
      "Epoch 13[494/625] Time:0.711, Train Loss:0.21536657214164734\n",
      "Epoch 13[495/625] Time:0.705, Train Loss:0.2817041873931885\n",
      "Epoch 13[496/625] Time:0.704, Train Loss:0.21486720442771912\n",
      "Epoch 13[497/625] Time:0.705, Train Loss:0.23616233468055725\n",
      "Epoch 13[498/625] Time:0.704, Train Loss:0.2775556147098541\n",
      "Epoch 13[499/625] Time:0.705, Train Loss:0.2713870108127594\n",
      "Epoch 13[500/625] Time:0.705, Train Loss:0.27768397331237793\n",
      "Epoch 13[501/625] Time:0.706, Train Loss:0.24968321621418\n",
      "Epoch 13[502/625] Time:0.705, Train Loss:0.20622509717941284\n",
      "Epoch 13[503/625] Time:0.737, Train Loss:0.2964157462120056\n",
      "Epoch 13[504/625] Time:0.691, Train Loss:0.2821984589099884\n",
      "Epoch 13[505/625] Time:0.708, Train Loss:0.3149472177028656\n",
      "Epoch 13[506/625] Time:0.745, Train Loss:0.2055586278438568\n",
      "Epoch 13[507/625] Time:0.702, Train Loss:0.21731941401958466\n",
      "Epoch 13[508/625] Time:0.703, Train Loss:0.2532441318035126\n",
      "Epoch 13[509/625] Time:0.703, Train Loss:0.29815763235092163\n",
      "Epoch 13[510/625] Time:0.704, Train Loss:0.38859623670578003\n",
      "Epoch 13[511/625] Time:0.733, Train Loss:0.2572515904903412\n",
      "Epoch 13[512/625] Time:0.692, Train Loss:0.34432944655418396\n",
      "Epoch 13[513/625] Time:0.691, Train Loss:0.3307052552700043\n",
      "Epoch 13[514/625] Time:0.691, Train Loss:0.35338401794433594\n",
      "Epoch 13[515/625] Time:0.696, Train Loss:0.28948336839675903\n",
      "Epoch 13[516/625] Time:0.694, Train Loss:0.2419772446155548\n",
      "Epoch 13[517/625] Time:0.697, Train Loss:0.2616170644760132\n",
      "Epoch 13[518/625] Time:0.704, Train Loss:0.3404064476490021\n",
      "Epoch 13[519/625] Time:0.704, Train Loss:0.23476962745189667\n",
      "Epoch 13[520/625] Time:0.738, Train Loss:0.24741621315479279\n",
      "Epoch 13[521/625] Time:0.695, Train Loss:0.34663864970207214\n",
      "Epoch 13[522/625] Time:0.693, Train Loss:0.2562021315097809\n",
      "Epoch 13[523/625] Time:0.709, Train Loss:0.29555198550224304\n",
      "Epoch 13[524/625] Time:0.704, Train Loss:0.38879987597465515\n",
      "Epoch 13[525/625] Time:0.704, Train Loss:0.26897212862968445\n",
      "Epoch 13[526/625] Time:0.703, Train Loss:0.24619615077972412\n",
      "Epoch 13[527/625] Time:0.704, Train Loss:0.32964783906936646\n",
      "Epoch 13[528/625] Time:0.705, Train Loss:0.21407140791416168\n",
      "Epoch 13[529/625] Time:0.706, Train Loss:0.2515309154987335\n",
      "Epoch 13[530/625] Time:0.705, Train Loss:0.2565824091434479\n",
      "Epoch 13[531/625] Time:0.704, Train Loss:0.34638360142707825\n",
      "Epoch 13[532/625] Time:0.707, Train Loss:0.2547875940799713\n",
      "Epoch 13[533/625] Time:0.705, Train Loss:0.28851646184921265\n",
      "Epoch 13[534/625] Time:0.705, Train Loss:0.3400397300720215\n",
      "Epoch 13[535/625] Time:0.709, Train Loss:0.25558406114578247\n",
      "Epoch 13[536/625] Time:0.729, Train Loss:0.24479994177818298\n",
      "Epoch 13[537/625] Time:0.7, Train Loss:0.2565016746520996\n",
      "Epoch 13[538/625] Time:0.705, Train Loss:0.37167513370513916\n",
      "Epoch 13[539/625] Time:0.705, Train Loss:0.3323354125022888\n",
      "Epoch 13[540/625] Time:0.711, Train Loss:0.3286169171333313\n",
      "Epoch 13[541/625] Time:0.738, Train Loss:0.23820506036281586\n",
      "Epoch 13[542/625] Time:0.692, Train Loss:0.2680331766605377\n",
      "Epoch 13[543/625] Time:0.703, Train Loss:0.3017614781856537\n",
      "Epoch 13[544/625] Time:0.704, Train Loss:0.35595014691352844\n",
      "Epoch 13[545/625] Time:0.703, Train Loss:0.31013262271881104\n",
      "Epoch 13[546/625] Time:0.703, Train Loss:0.2779574990272522\n",
      "Epoch 13[547/625] Time:0.705, Train Loss:0.24565763771533966\n",
      "Epoch 13[548/625] Time:0.706, Train Loss:0.3078352212905884\n",
      "Epoch 13[549/625] Time:0.704, Train Loss:0.32335689663887024\n",
      "Epoch 13[550/625] Time:0.704, Train Loss:0.3008124828338623\n",
      "Epoch 13[551/625] Time:0.707, Train Loss:0.2690642476081848\n",
      "Epoch 13[552/625] Time:0.705, Train Loss:0.2986898422241211\n",
      "Epoch 13[553/625] Time:0.705, Train Loss:0.332447350025177\n",
      "Epoch 13[554/625] Time:0.704, Train Loss:0.2913317382335663\n",
      "Epoch 13[555/625] Time:0.706, Train Loss:0.2734682857990265\n",
      "Epoch 13[556/625] Time:0.702, Train Loss:0.3101191222667694\n",
      "Epoch 13[557/625] Time:0.704, Train Loss:0.19735193252563477\n",
      "Epoch 13[558/625] Time:0.704, Train Loss:0.39752197265625\n",
      "Epoch 13[559/625] Time:0.704, Train Loss:0.28969505429267883\n",
      "Epoch 13[560/625] Time:0.703, Train Loss:0.26905444264411926\n",
      "Epoch 13[561/625] Time:0.703, Train Loss:0.2659907341003418\n",
      "Epoch 13[562/625] Time:0.704, Train Loss:0.24400204420089722\n",
      "Epoch 13[563/625] Time:0.703, Train Loss:0.3126389980316162\n",
      "Epoch 13[564/625] Time:0.703, Train Loss:0.3808879852294922\n",
      "Epoch 13[565/625] Time:0.703, Train Loss:0.34352943301200867\n",
      "Epoch 13[566/625] Time:0.703, Train Loss:0.38156235218048096\n",
      "Epoch 13[567/625] Time:0.703, Train Loss:0.24747206270694733\n",
      "Epoch 13[568/625] Time:0.704, Train Loss:0.2619067132472992\n",
      "Epoch 13[569/625] Time:0.705, Train Loss:0.29976534843444824\n",
      "Epoch 13[570/625] Time:0.692, Train Loss:0.27069273591041565\n",
      "Epoch 13[571/625] Time:0.694, Train Loss:0.29948291182518005\n",
      "Epoch 13[572/625] Time:0.694, Train Loss:0.30560755729675293\n",
      "Epoch 13[573/625] Time:0.693, Train Loss:0.2740050256252289\n",
      "Epoch 13[574/625] Time:0.693, Train Loss:0.29112040996551514\n",
      "Epoch 13[575/625] Time:0.713, Train Loss:0.29619383811950684\n",
      "Epoch 13[576/625] Time:0.703, Train Loss:0.32786381244659424\n",
      "Epoch 13[577/625] Time:0.703, Train Loss:0.31619662046432495\n",
      "Epoch 13[578/625] Time:0.703, Train Loss:0.35310259461402893\n",
      "Epoch 13[579/625] Time:0.705, Train Loss:0.38435834646224976\n",
      "Epoch 13[580/625] Time:0.704, Train Loss:0.2677637040615082\n",
      "Epoch 13[581/625] Time:0.703, Train Loss:0.32445573806762695\n",
      "Epoch 13[582/625] Time:0.695, Train Loss:0.2694566547870636\n",
      "Epoch 13[583/625] Time:0.693, Train Loss:0.30292990803718567\n",
      "Epoch 13[584/625] Time:0.694, Train Loss:0.3428327739238739\n",
      "Epoch 13[585/625] Time:0.693, Train Loss:0.3255169093608856\n",
      "Epoch 13[586/625] Time:0.693, Train Loss:0.30557820200920105\n",
      "Epoch 13[587/625] Time:0.711, Train Loss:0.2500865161418915\n",
      "Epoch 13[588/625] Time:0.705, Train Loss:0.3184968829154968\n",
      "Epoch 13[589/625] Time:0.705, Train Loss:0.2847776412963867\n",
      "Epoch 13[590/625] Time:0.693, Train Loss:0.33704161643981934\n",
      "Epoch 13[591/625] Time:0.703, Train Loss:0.28747084736824036\n",
      "Epoch 13[592/625] Time:0.704, Train Loss:0.2408549189567566\n",
      "Epoch 13[593/625] Time:0.705, Train Loss:0.2710115611553192\n",
      "Epoch 13[594/625] Time:0.704, Train Loss:0.3392805755138397\n",
      "Epoch 13[595/625] Time:0.704, Train Loss:0.24759578704833984\n",
      "Epoch 13[596/625] Time:0.704, Train Loss:0.27821269631385803\n",
      "Epoch 13[597/625] Time:0.731, Train Loss:0.2591027617454529\n",
      "Epoch 13[598/625] Time:0.693, Train Loss:0.3240635097026825\n",
      "Epoch 13[599/625] Time:0.693, Train Loss:0.30354881286621094\n",
      "Epoch 13[600/625] Time:0.693, Train Loss:0.2887820303440094\n",
      "Epoch 13[601/625] Time:0.693, Train Loss:0.24477683007717133\n",
      "Epoch 13[602/625] Time:0.718, Train Loss:0.3678559362888336\n",
      "Epoch 13[603/625] Time:0.702, Train Loss:0.2787734568119049\n",
      "Epoch 13[604/625] Time:0.704, Train Loss:0.2711639106273651\n",
      "Epoch 13[605/625] Time:0.705, Train Loss:0.2644985318183899\n",
      "Epoch 13[606/625] Time:0.703, Train Loss:0.23134122788906097\n",
      "Epoch 13[607/625] Time:0.705, Train Loss:0.25276005268096924\n",
      "Epoch 13[608/625] Time:0.705, Train Loss:0.2915751338005066\n",
      "Epoch 13[609/625] Time:0.733, Train Loss:0.30430200695991516\n",
      "Epoch 13[610/625] Time:0.703, Train Loss:0.3347087800502777\n",
      "Epoch 13[611/625] Time:0.704, Train Loss:0.2420106828212738\n",
      "Epoch 13[612/625] Time:0.703, Train Loss:0.29699498414993286\n",
      "Epoch 13[613/625] Time:0.705, Train Loss:0.38671737909317017\n",
      "Epoch 13[614/625] Time:0.706, Train Loss:0.20082877576351166\n",
      "Epoch 13[615/625] Time:0.704, Train Loss:0.27270424365997314\n",
      "Epoch 13[616/625] Time:0.704, Train Loss:0.2552560567855835\n",
      "Epoch 13[617/625] Time:0.705, Train Loss:0.3016999363899231\n",
      "Epoch 13[618/625] Time:0.705, Train Loss:0.3154037594795227\n",
      "Epoch 13[619/625] Time:0.704, Train Loss:0.26985964179039\n",
      "Epoch 13[620/625] Time:0.704, Train Loss:0.3867235779762268\n",
      "Epoch 13[621/625] Time:0.704, Train Loss:0.21140417456626892\n",
      "Epoch 13[622/625] Time:0.704, Train Loss:0.27781298756599426\n",
      "Epoch 13[623/625] Time:0.707, Train Loss:0.30523407459259033\n",
      "Epoch 13[624/625] Time:0.705, Train Loss:0.2761825621128082\n",
      "Epoch 13[0/78] Val Loss:0.3288789689540863\n",
      "Epoch 13[1/78] Val Loss:0.26051950454711914\n",
      "Epoch 13[2/78] Val Loss:0.3190915584564209\n",
      "Epoch 13[3/78] Val Loss:0.3269239068031311\n",
      "Epoch 13[4/78] Val Loss:0.48944956064224243\n",
      "Epoch 13[5/78] Val Loss:0.3849354684352875\n",
      "Epoch 13[6/78] Val Loss:0.4787270128726959\n",
      "Epoch 13[7/78] Val Loss:0.5400601029396057\n",
      "Epoch 13[8/78] Val Loss:0.29404330253601074\n",
      "Epoch 13[9/78] Val Loss:0.1784210056066513\n",
      "Epoch 13[10/78] Val Loss:0.15548542141914368\n",
      "Epoch 13[11/78] Val Loss:0.1792948842048645\n",
      "Epoch 13[12/78] Val Loss:0.1620323359966278\n",
      "Epoch 13[13/78] Val Loss:0.13881124556064606\n",
      "Epoch 13[14/78] Val Loss:0.4117583632469177\n",
      "Epoch 13[15/78] Val Loss:0.3480132222175598\n",
      "Epoch 13[16/78] Val Loss:0.4293835759162903\n",
      "Epoch 13[17/78] Val Loss:0.41967740654945374\n",
      "Epoch 13[18/78] Val Loss:0.6909698247909546\n",
      "Epoch 13[19/78] Val Loss:0.7173281908035278\n",
      "Epoch 13[20/78] Val Loss:0.6531151533126831\n",
      "Epoch 13[21/78] Val Loss:0.5084288716316223\n",
      "Epoch 13[22/78] Val Loss:0.6556530594825745\n",
      "Epoch 13[23/78] Val Loss:0.530098021030426\n",
      "Epoch 13[24/78] Val Loss:0.36554446816444397\n",
      "Epoch 13[25/78] Val Loss:0.43369826674461365\n",
      "Epoch 13[26/78] Val Loss:0.44180846214294434\n",
      "Epoch 13[27/78] Val Loss:0.40073922276496887\n",
      "Epoch 13[28/78] Val Loss:0.338396281003952\n",
      "Epoch 13[29/78] Val Loss:0.5160872340202332\n",
      "Epoch 13[30/78] Val Loss:1.0534330606460571\n",
      "Epoch 13[31/78] Val Loss:0.9193903207778931\n",
      "Epoch 13[32/78] Val Loss:0.8347644805908203\n",
      "Epoch 13[33/78] Val Loss:0.46944141387939453\n",
      "Epoch 13[34/78] Val Loss:0.4014565348625183\n",
      "Epoch 13[35/78] Val Loss:0.44506651163101196\n",
      "Epoch 13[36/78] Val Loss:0.40470942854881287\n",
      "Epoch 13[37/78] Val Loss:0.5115647315979004\n",
      "Epoch 13[38/78] Val Loss:0.38997387886047363\n",
      "Epoch 13[39/78] Val Loss:0.3581465184688568\n",
      "Epoch 13[40/78] Val Loss:0.3811163902282715\n",
      "Epoch 13[41/78] Val Loss:0.4108966886997223\n",
      "Epoch 13[42/78] Val Loss:0.3969959616661072\n",
      "Epoch 13[43/78] Val Loss:0.27201661467552185\n",
      "Epoch 13[44/78] Val Loss:0.16342471539974213\n",
      "Epoch 13[45/78] Val Loss:0.16453735530376434\n",
      "Epoch 13[46/78] Val Loss:0.1642606556415558\n",
      "Epoch 13[47/78] Val Loss:0.18043851852416992\n",
      "Epoch 13[48/78] Val Loss:0.21046605706214905\n",
      "Epoch 13[49/78] Val Loss:0.16104862093925476\n",
      "Epoch 13[50/78] Val Loss:0.1141011044383049\n",
      "Epoch 13[51/78] Val Loss:0.1336643248796463\n",
      "Epoch 13[52/78] Val Loss:0.18579550087451935\n",
      "Epoch 13[53/78] Val Loss:0.12009578943252563\n",
      "Epoch 13[54/78] Val Loss:0.13982081413269043\n",
      "Epoch 13[55/78] Val Loss:0.15873782336711884\n",
      "Epoch 13[56/78] Val Loss:0.4455879330635071\n",
      "Epoch 13[57/78] Val Loss:0.39024779200553894\n",
      "Epoch 13[58/78] Val Loss:0.4049029052257538\n",
      "Epoch 13[59/78] Val Loss:0.4238531291484833\n",
      "Epoch 13[60/78] Val Loss:0.4380056858062744\n",
      "Epoch 13[61/78] Val Loss:0.437036395072937\n",
      "Epoch 13[62/78] Val Loss:0.5019040703773499\n",
      "Epoch 13[63/78] Val Loss:0.35995927453041077\n",
      "Epoch 13[64/78] Val Loss:0.36277613043785095\n",
      "Epoch 13[65/78] Val Loss:0.37317708134651184\n",
      "Epoch 13[66/78] Val Loss:0.44662538170814514\n",
      "Epoch 13[67/78] Val Loss:0.41980424523353577\n",
      "Epoch 13[68/78] Val Loss:0.47540777921676636\n",
      "Epoch 13[69/78] Val Loss:0.48711639642715454\n",
      "Epoch 13[70/78] Val Loss:0.48580655455589294\n",
      "Epoch 13[71/78] Val Loss:0.46348652243614197\n",
      "Epoch 13[72/78] Val Loss:0.4057607352733612\n",
      "Epoch 13[73/78] Val Loss:0.4052126407623291\n",
      "Epoch 13[74/78] Val Loss:0.5153328776359558\n",
      "Epoch 13[75/78] Val Loss:0.6013885140419006\n",
      "Epoch 13[76/78] Val Loss:0.5809750556945801\n",
      "Epoch 13[77/78] Val Loss:0.6171411275863647\n",
      "Epoch 13[78/78] Val Loss:0.7233697175979614\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.81      0.88     15691\n",
      "           1       0.55      0.88      0.68      4309\n",
      "\n",
      "    accuracy                           0.82     20000\n",
      "   macro avg       0.76      0.84      0.78     20000\n",
      "weighted avg       0.87      0.82      0.83     20000\n",
      "\n",
      "Epoch 13: Train Loss 0.29035109632015227, Val Loss 0.41035404696296424, Train Time 803.3663637638092, Val Time 37.299782037734985\n",
      "Epoch 14[0/625] Time:0.69, Train Loss:0.29566875100135803\n",
      "Epoch 14[1/625] Time:0.705, Train Loss:0.2370045930147171\n",
      "Epoch 14[2/625] Time:0.707, Train Loss:0.27701428532600403\n",
      "Epoch 14[3/625] Time:0.704, Train Loss:0.2548571228981018\n",
      "Epoch 14[4/625] Time:0.703, Train Loss:0.2644718885421753\n",
      "Epoch 14[5/625] Time:0.703, Train Loss:0.3669169247150421\n",
      "Epoch 14[6/625] Time:0.705, Train Loss:0.21884791553020477\n",
      "Epoch 14[7/625] Time:0.704, Train Loss:0.4393438398838043\n",
      "Epoch 14[8/625] Time:0.704, Train Loss:0.2919442653656006\n",
      "Epoch 14[9/625] Time:0.705, Train Loss:0.29010990262031555\n",
      "Epoch 14[10/625] Time:0.693, Train Loss:0.31416282057762146\n",
      "Epoch 14[11/625] Time:0.704, Train Loss:0.32359638810157776\n",
      "Epoch 14[12/625] Time:0.703, Train Loss:0.29390400648117065\n",
      "Epoch 14[13/625] Time:0.703, Train Loss:0.26752954721450806\n",
      "Epoch 14[14/625] Time:0.704, Train Loss:0.3137860596179962\n",
      "Epoch 14[15/625] Time:0.703, Train Loss:0.2844100892543793\n",
      "Epoch 14[16/625] Time:0.702, Train Loss:0.3019210696220398\n",
      "Epoch 14[17/625] Time:0.703, Train Loss:0.31635919213294983\n",
      "Epoch 14[18/625] Time:0.703, Train Loss:0.263690710067749\n",
      "Epoch 14[19/625] Time:0.703, Train Loss:0.305846244096756\n",
      "Epoch 14[20/625] Time:0.704, Train Loss:0.3084605634212494\n",
      "Epoch 14[21/625] Time:0.704, Train Loss:0.2661646604537964\n",
      "Epoch 14[22/625] Time:0.703, Train Loss:0.2561398148536682\n",
      "Epoch 14[23/625] Time:0.749, Train Loss:0.2787338197231293\n",
      "Epoch 14[24/625] Time:0.695, Train Loss:0.30544862151145935\n",
      "Epoch 14[25/625] Time:0.706, Train Loss:0.24264594912528992\n",
      "Epoch 14[26/625] Time:0.71, Train Loss:0.2740420401096344\n",
      "Epoch 14[27/625] Time:0.704, Train Loss:0.3398544490337372\n",
      "Epoch 14[28/625] Time:0.705, Train Loss:0.3514191210269928\n",
      "Epoch 14[29/625] Time:0.707, Train Loss:0.2469886839389801\n",
      "Epoch 14[30/625] Time:0.703, Train Loss:0.32412147521972656\n",
      "Epoch 14[31/625] Time:0.703, Train Loss:0.26174798607826233\n",
      "Epoch 14[32/625] Time:0.704, Train Loss:0.3015374541282654\n",
      "Epoch 14[33/625] Time:0.703, Train Loss:0.24441932141780853\n",
      "Epoch 14[34/625] Time:0.711, Train Loss:0.2632350027561188\n",
      "Epoch 14[35/625] Time:0.728, Train Loss:0.1959892213344574\n",
      "Epoch 14[36/625] Time:0.692, Train Loss:0.25911563634872437\n",
      "Epoch 14[37/625] Time:0.692, Train Loss:0.30815404653549194\n",
      "Epoch 14[38/625] Time:0.693, Train Loss:0.33734849095344543\n",
      "Epoch 14[39/625] Time:0.692, Train Loss:0.25655892491340637\n",
      "Epoch 14[40/625] Time:0.695, Train Loss:0.30345070362091064\n",
      "Epoch 14[41/625] Time:0.692, Train Loss:0.28200700879096985\n",
      "Epoch 14[42/625] Time:0.693, Train Loss:0.3124862313270569\n",
      "Epoch 14[43/625] Time:0.693, Train Loss:0.3206152021884918\n",
      "Epoch 14[44/625] Time:0.695, Train Loss:0.21034370362758636\n",
      "Epoch 14[45/625] Time:0.692, Train Loss:0.32073137164115906\n",
      "Epoch 14[46/625] Time:0.696, Train Loss:0.2742936313152313\n",
      "Epoch 14[47/625] Time:0.693, Train Loss:0.24637091159820557\n",
      "Epoch 14[48/625] Time:0.704, Train Loss:0.4064212143421173\n",
      "Epoch 14[49/625] Time:0.704, Train Loss:0.23044045269489288\n",
      "Epoch 14[50/625] Time:0.704, Train Loss:0.3098750412464142\n",
      "Epoch 14[51/625] Time:0.704, Train Loss:0.33715224266052246\n",
      "Epoch 14[52/625] Time:0.704, Train Loss:0.23990797996520996\n",
      "Epoch 14[53/625] Time:0.703, Train Loss:0.3737196624279022\n",
      "Epoch 14[54/625] Time:0.704, Train Loss:0.30469658970832825\n",
      "Epoch 14[55/625] Time:0.704, Train Loss:0.24070872366428375\n",
      "Epoch 14[56/625] Time:0.705, Train Loss:0.32950958609580994\n",
      "Epoch 14[57/625] Time:0.705, Train Loss:0.26610520482063293\n",
      "Epoch 14[58/625] Time:0.734, Train Loss:0.24604235589504242\n",
      "Epoch 14[59/625] Time:0.691, Train Loss:0.3499256372451782\n",
      "Epoch 14[60/625] Time:0.695, Train Loss:0.30353787541389465\n",
      "Epoch 14[61/625] Time:0.695, Train Loss:0.35342442989349365\n",
      "Epoch 14[62/625] Time:0.694, Train Loss:0.2791317403316498\n",
      "Epoch 14[63/625] Time:0.696, Train Loss:0.33750495314598083\n",
      "Epoch 14[64/625] Time:0.694, Train Loss:0.29958680272102356\n",
      "Epoch 14[65/625] Time:0.696, Train Loss:0.2636179029941559\n",
      "Epoch 14[66/625] Time:0.693, Train Loss:0.3149024546146393\n",
      "Epoch 14[67/625] Time:0.702, Train Loss:0.2676846981048584\n",
      "Epoch 14[68/625] Time:0.705, Train Loss:0.25123855471611023\n",
      "Epoch 14[69/625] Time:0.704, Train Loss:0.2800006866455078\n",
      "Epoch 14[70/625] Time:0.704, Train Loss:0.27131620049476624\n",
      "Epoch 14[71/625] Time:0.703, Train Loss:0.30402132868766785\n",
      "Epoch 14[72/625] Time:0.703, Train Loss:0.28666016459465027\n",
      "Epoch 14[73/625] Time:0.708, Train Loss:0.256987988948822\n",
      "Epoch 14[74/625] Time:0.734, Train Loss:0.23568980395793915\n",
      "Epoch 14[75/625] Time:0.692, Train Loss:0.2930843234062195\n",
      "Epoch 14[76/625] Time:0.693, Train Loss:0.31194204092025757\n",
      "Epoch 14[77/625] Time:0.693, Train Loss:0.25537171959877014\n",
      "Epoch 14[78/625] Time:0.694, Train Loss:0.23584285378456116\n",
      "Epoch 14[79/625] Time:0.694, Train Loss:0.22439368069171906\n",
      "Epoch 14[80/625] Time:0.693, Train Loss:0.2689167559146881\n",
      "Epoch 14[81/625] Time:0.704, Train Loss:0.22295589745044708\n",
      "Epoch 14[82/625] Time:0.703, Train Loss:0.40870237350463867\n",
      "Epoch 14[83/625] Time:0.703, Train Loss:0.2908489406108856\n",
      "Epoch 14[84/625] Time:0.703, Train Loss:0.26187893748283386\n",
      "Epoch 14[85/625] Time:0.703, Train Loss:0.2857220470905304\n",
      "Epoch 14[86/625] Time:0.703, Train Loss:0.27979177236557007\n",
      "Epoch 14[87/625] Time:0.705, Train Loss:0.26496151089668274\n",
      "Epoch 14[88/625] Time:0.706, Train Loss:0.3288237452507019\n",
      "Epoch 14[89/625] Time:0.705, Train Loss:0.3493695855140686\n",
      "Epoch 14[90/625] Time:0.708, Train Loss:0.18886196613311768\n",
      "Epoch 14[91/625] Time:0.704, Train Loss:0.27137547731399536\n",
      "Epoch 14[92/625] Time:0.705, Train Loss:0.2731257975101471\n",
      "Epoch 14[93/625] Time:0.704, Train Loss:0.293765127658844\n",
      "Epoch 14[94/625] Time:0.703, Train Loss:0.3525579273700714\n",
      "Epoch 14[95/625] Time:0.709, Train Loss:0.31070515513420105\n",
      "Epoch 14[96/625] Time:0.705, Train Loss:0.23007994890213013\n",
      "Epoch 14[97/625] Time:0.703, Train Loss:0.2619376480579376\n",
      "Epoch 14[98/625] Time:0.703, Train Loss:0.23976843059062958\n",
      "Epoch 14[99/625] Time:0.704, Train Loss:0.3830384910106659\n",
      "Epoch 14[100/625] Time:0.721, Train Loss:0.33438289165496826\n",
      "Epoch 14[101/625] Time:0.704, Train Loss:0.29739928245544434\n",
      "Epoch 14[102/625] Time:0.703, Train Loss:0.2928766906261444\n",
      "Epoch 14[103/625] Time:0.703, Train Loss:0.39903348684310913\n",
      "Epoch 14[104/625] Time:0.703, Train Loss:0.2707085907459259\n",
      "Epoch 14[105/625] Time:0.708, Train Loss:0.2761944532394409\n",
      "Epoch 14[106/625] Time:0.703, Train Loss:0.2951892018318176\n",
      "Epoch 14[107/625] Time:0.703, Train Loss:0.29209908843040466\n",
      "Epoch 14[108/625] Time:0.704, Train Loss:0.28973981738090515\n",
      "Epoch 14[109/625] Time:0.708, Train Loss:0.30906468629837036\n",
      "Epoch 14[110/625] Time:0.704, Train Loss:0.35425934195518494\n",
      "Epoch 14[111/625] Time:0.704, Train Loss:0.2638999819755554\n",
      "Epoch 14[112/625] Time:0.704, Train Loss:0.2912226915359497\n",
      "Epoch 14[113/625] Time:0.705, Train Loss:0.29195699095726013\n",
      "Epoch 14[114/625] Time:0.711, Train Loss:0.3220471739768982\n",
      "Epoch 14[115/625] Time:0.703, Train Loss:0.35151392221450806\n",
      "Epoch 14[116/625] Time:0.703, Train Loss:0.27335554361343384\n",
      "Epoch 14[117/625] Time:0.704, Train Loss:0.2913724482059479\n",
      "Epoch 14[118/625] Time:0.704, Train Loss:0.2811198830604553\n",
      "Epoch 14[119/625] Time:0.703, Train Loss:0.2919062376022339\n",
      "Epoch 14[120/625] Time:0.704, Train Loss:0.27792787551879883\n",
      "Epoch 14[121/625] Time:0.703, Train Loss:0.3033836781978607\n",
      "Epoch 14[122/625] Time:0.702, Train Loss:0.21564297378063202\n",
      "Epoch 14[123/625] Time:0.704, Train Loss:0.2837049663066864\n",
      "Epoch 14[124/625] Time:0.704, Train Loss:0.3050408363342285\n",
      "Epoch 14[125/625] Time:0.705, Train Loss:0.3835684359073639\n",
      "Epoch 14[126/625] Time:0.704, Train Loss:0.2857191264629364\n",
      "Epoch 14[127/625] Time:0.694, Train Loss:0.28773272037506104\n",
      "Epoch 14[128/625] Time:0.692, Train Loss:0.20413941144943237\n",
      "Epoch 14[129/625] Time:0.693, Train Loss:0.2783694565296173\n",
      "Epoch 14[130/625] Time:0.694, Train Loss:0.3019225001335144\n",
      "Epoch 14[131/625] Time:0.692, Train Loss:0.27785724401474\n",
      "Epoch 14[132/625] Time:0.694, Train Loss:0.3555271625518799\n",
      "Epoch 14[133/625] Time:0.692, Train Loss:0.25593993067741394\n",
      "Epoch 14[134/625] Time:0.692, Train Loss:0.3131266236305237\n",
      "Epoch 14[135/625] Time:0.693, Train Loss:0.21713444590568542\n",
      "Epoch 14[136/625] Time:0.696, Train Loss:0.2598475217819214\n",
      "Epoch 14[137/625] Time:0.693, Train Loss:0.3922262191772461\n",
      "Epoch 14[138/625] Time:0.702, Train Loss:0.22062014043331146\n",
      "Epoch 14[139/625] Time:0.704, Train Loss:0.267414391040802\n",
      "Epoch 14[140/625] Time:0.702, Train Loss:0.3109079599380493\n",
      "Epoch 14[141/625] Time:0.703, Train Loss:0.22846956551074982\n",
      "Epoch 14[142/625] Time:0.703, Train Loss:0.2276240587234497\n",
      "Epoch 14[143/625] Time:0.703, Train Loss:0.26882535219192505\n",
      "Epoch 14[144/625] Time:0.704, Train Loss:0.23993651568889618\n",
      "Epoch 14[145/625] Time:0.704, Train Loss:0.2689862549304962\n",
      "Epoch 14[146/625] Time:0.703, Train Loss:0.2459712028503418\n",
      "Epoch 14[147/625] Time:0.704, Train Loss:0.28841477632522583\n",
      "Epoch 14[148/625] Time:0.704, Train Loss:0.21038410067558289\n",
      "Epoch 14[149/625] Time:0.704, Train Loss:0.2376895397901535\n",
      "Epoch 14[150/625] Time:0.703, Train Loss:0.3070741593837738\n",
      "Epoch 14[151/625] Time:0.703, Train Loss:0.2595047652721405\n",
      "Epoch 14[152/625] Time:0.704, Train Loss:0.26929134130477905\n",
      "Epoch 14[153/625] Time:0.704, Train Loss:0.33672744035720825\n",
      "Epoch 14[154/625] Time:0.703, Train Loss:0.2300475686788559\n",
      "Epoch 14[155/625] Time:0.704, Train Loss:0.26265424489974976\n",
      "Epoch 14[156/625] Time:0.704, Train Loss:0.44614219665527344\n",
      "Epoch 14[157/625] Time:0.703, Train Loss:0.2963745594024658\n",
      "Epoch 14[158/625] Time:0.703, Train Loss:0.39063000679016113\n",
      "Epoch 14[159/625] Time:0.704, Train Loss:0.22261297702789307\n",
      "Epoch 14[160/625] Time:0.705, Train Loss:0.2919358015060425\n",
      "Epoch 14[161/625] Time:0.703, Train Loss:0.29287588596343994\n",
      "Epoch 14[162/625] Time:0.703, Train Loss:0.3217693269252777\n",
      "Epoch 14[163/625] Time:0.704, Train Loss:0.32070180773735046\n",
      "Epoch 14[164/625] Time:0.706, Train Loss:0.30078306794166565\n",
      "Epoch 14[165/625] Time:0.706, Train Loss:0.2524578869342804\n",
      "Epoch 14[166/625] Time:0.704, Train Loss:0.2970815896987915\n",
      "Epoch 14[167/625] Time:0.703, Train Loss:0.2733832895755768\n",
      "Epoch 14[168/625] Time:0.706, Train Loss:0.29914817214012146\n",
      "Epoch 14[169/625] Time:0.705, Train Loss:0.3158612549304962\n",
      "Epoch 14[170/625] Time:0.705, Train Loss:0.2964341938495636\n",
      "Epoch 14[171/625] Time:0.703, Train Loss:0.20683300495147705\n",
      "Epoch 14[172/625] Time:0.706, Train Loss:0.2562394142150879\n",
      "Epoch 14[173/625] Time:0.704, Train Loss:0.27887970209121704\n",
      "Epoch 14[174/625] Time:0.703, Train Loss:0.2843746244907379\n",
      "Epoch 14[175/625] Time:0.703, Train Loss:0.248208686709404\n",
      "Epoch 14[176/625] Time:0.705, Train Loss:0.3608803153038025\n",
      "Epoch 14[177/625] Time:0.702, Train Loss:0.2564549744129181\n",
      "Epoch 14[178/625] Time:0.703, Train Loss:0.23965734243392944\n",
      "Epoch 14[179/625] Time:0.705, Train Loss:0.18070025742053986\n",
      "Epoch 14[180/625] Time:0.704, Train Loss:0.29225245118141174\n",
      "Epoch 14[181/625] Time:0.704, Train Loss:0.28502920269966125\n",
      "Epoch 14[182/625] Time:0.703, Train Loss:0.2345452904701233\n",
      "Epoch 14[183/625] Time:0.704, Train Loss:0.2533506751060486\n",
      "Epoch 14[184/625] Time:0.705, Train Loss:0.32459062337875366\n",
      "Epoch 14[185/625] Time:0.704, Train Loss:0.2923968434333801\n",
      "Epoch 14[186/625] Time:0.706, Train Loss:0.2826223075389862\n",
      "Epoch 14[187/625] Time:0.703, Train Loss:0.24966853857040405\n",
      "Epoch 14[188/625] Time:0.704, Train Loss:0.20822523534297943\n",
      "Epoch 14[189/625] Time:0.704, Train Loss:0.2908410131931305\n",
      "Epoch 14[190/625] Time:0.703, Train Loss:0.21784909069538116\n",
      "Epoch 14[191/625] Time:0.703, Train Loss:0.23999261856079102\n",
      "Epoch 14[192/625] Time:0.708, Train Loss:0.27798593044281006\n",
      "Epoch 14[193/625] Time:0.702, Train Loss:0.31674009561538696\n",
      "Epoch 14[194/625] Time:0.703, Train Loss:0.251932829618454\n",
      "Epoch 14[195/625] Time:0.703, Train Loss:0.20414523780345917\n",
      "Epoch 14[196/625] Time:0.703, Train Loss:0.40340954065322876\n",
      "Epoch 14[197/625] Time:0.703, Train Loss:0.3151862919330597\n",
      "Epoch 14[198/625] Time:0.704, Train Loss:0.31420621275901794\n",
      "Epoch 14[199/625] Time:0.703, Train Loss:0.24213699996471405\n",
      "Epoch 14[200/625] Time:0.71, Train Loss:0.28375744819641113\n",
      "Epoch 14[201/625] Time:0.704, Train Loss:0.32380589842796326\n",
      "Epoch 14[202/625] Time:0.703, Train Loss:0.29864418506622314\n",
      "Epoch 14[203/625] Time:0.704, Train Loss:0.2263316810131073\n",
      "Epoch 14[204/625] Time:0.704, Train Loss:0.20635049045085907\n",
      "Epoch 14[205/625] Time:0.704, Train Loss:0.26507189869880676\n",
      "Epoch 14[206/625] Time:0.704, Train Loss:0.26586970686912537\n",
      "Epoch 14[207/625] Time:0.704, Train Loss:0.28875309228897095\n",
      "Epoch 14[208/625] Time:0.705, Train Loss:0.2328784018754959\n",
      "Epoch 14[209/625] Time:0.703, Train Loss:0.28783711791038513\n",
      "Epoch 14[210/625] Time:0.703, Train Loss:0.3080601394176483\n",
      "Epoch 14[211/625] Time:0.703, Train Loss:0.406943142414093\n",
      "Epoch 14[212/625] Time:0.74, Train Loss:0.23082169890403748\n",
      "Epoch 14[213/625] Time:0.7, Train Loss:0.2832236886024475\n",
      "Epoch 14[214/625] Time:0.702, Train Loss:0.2780857980251312\n",
      "Epoch 14[215/625] Time:0.703, Train Loss:0.2615949511528015\n",
      "Epoch 14[216/625] Time:0.704, Train Loss:0.3463020324707031\n",
      "Epoch 14[217/625] Time:0.703, Train Loss:0.2809964418411255\n",
      "Epoch 14[218/625] Time:0.702, Train Loss:0.2901948392391205\n",
      "Epoch 14[219/625] Time:0.702, Train Loss:0.25910329818725586\n",
      "Epoch 14[220/625] Time:0.702, Train Loss:0.3012341856956482\n",
      "Epoch 14[221/625] Time:0.703, Train Loss:0.3135358691215515\n",
      "Epoch 14[222/625] Time:0.703, Train Loss:0.353523313999176\n",
      "Epoch 14[223/625] Time:0.705, Train Loss:0.36220234632492065\n",
      "Epoch 14[224/625] Time:0.703, Train Loss:0.3582771420478821\n",
      "Epoch 14[225/625] Time:0.702, Train Loss:0.29040849208831787\n",
      "Epoch 14[226/625] Time:0.703, Train Loss:0.24428021907806396\n",
      "Epoch 14[227/625] Time:0.703, Train Loss:0.28927502036094666\n",
      "Epoch 14[228/625] Time:0.703, Train Loss:0.3504559099674225\n",
      "Epoch 14[229/625] Time:0.702, Train Loss:0.3060411810874939\n",
      "Epoch 14[230/625] Time:0.703, Train Loss:0.3974635601043701\n",
      "Epoch 14[231/625] Time:0.703, Train Loss:0.25089579820632935\n",
      "Epoch 14[232/625] Time:0.703, Train Loss:0.28307390213012695\n",
      "Epoch 14[233/625] Time:0.702, Train Loss:0.31623780727386475\n",
      "Epoch 14[234/625] Time:0.703, Train Loss:0.3343204855918884\n",
      "Epoch 14[235/625] Time:0.704, Train Loss:0.3049885630607605\n",
      "Epoch 14[236/625] Time:0.703, Train Loss:0.3076721727848053\n",
      "Epoch 14[237/625] Time:0.705, Train Loss:0.3164222836494446\n",
      "Epoch 14[238/625] Time:0.703, Train Loss:0.2955891191959381\n",
      "Epoch 14[239/625] Time:0.703, Train Loss:0.29973649978637695\n",
      "Epoch 14[240/625] Time:0.704, Train Loss:0.2673371434211731\n",
      "Epoch 14[241/625] Time:0.703, Train Loss:0.2791841924190521\n",
      "Epoch 14[242/625] Time:0.703, Train Loss:0.2892027199268341\n",
      "Epoch 14[243/625] Time:0.703, Train Loss:0.2715899348258972\n",
      "Epoch 14[244/625] Time:0.703, Train Loss:0.32671037316322327\n",
      "Epoch 14[245/625] Time:0.704, Train Loss:0.23517030477523804\n",
      "Epoch 14[246/625] Time:0.704, Train Loss:0.235106959939003\n",
      "Epoch 14[247/625] Time:0.703, Train Loss:0.2354472130537033\n",
      "Epoch 14[248/625] Time:0.704, Train Loss:0.3376322388648987\n",
      "Epoch 14[249/625] Time:0.702, Train Loss:0.25626131892204285\n",
      "Epoch 14[250/625] Time:0.702, Train Loss:0.28112149238586426\n",
      "Epoch 14[251/625] Time:0.702, Train Loss:0.23481819033622742\n",
      "Epoch 14[252/625] Time:0.703, Train Loss:0.34545236825942993\n",
      "Epoch 14[253/625] Time:0.703, Train Loss:0.28050145506858826\n",
      "Epoch 14[254/625] Time:0.702, Train Loss:0.2099427580833435\n",
      "Epoch 14[255/625] Time:0.704, Train Loss:0.31959471106529236\n",
      "Epoch 14[256/625] Time:0.706, Train Loss:0.27955132722854614\n",
      "Epoch 14[257/625] Time:0.704, Train Loss:0.3140750527381897\n",
      "Epoch 14[258/625] Time:0.704, Train Loss:0.25172680616378784\n",
      "Epoch 14[259/625] Time:0.703, Train Loss:0.2453741580247879\n",
      "Epoch 14[260/625] Time:0.704, Train Loss:0.25386911630630493\n",
      "Epoch 14[261/625] Time:0.704, Train Loss:0.26031914353370667\n",
      "Epoch 14[262/625] Time:0.703, Train Loss:0.32238274812698364\n",
      "Epoch 14[263/625] Time:0.703, Train Loss:0.32367104291915894\n",
      "Epoch 14[264/625] Time:0.703, Train Loss:0.32824522256851196\n",
      "Epoch 14[265/625] Time:0.703, Train Loss:0.29722175002098083\n",
      "Epoch 14[266/625] Time:0.704, Train Loss:0.29508933424949646\n",
      "Epoch 14[267/625] Time:0.703, Train Loss:0.24482479691505432\n",
      "Epoch 14[268/625] Time:0.703, Train Loss:0.2855888903141022\n",
      "Epoch 14[269/625] Time:0.702, Train Loss:0.2754688262939453\n",
      "Epoch 14[270/625] Time:0.702, Train Loss:0.3187190592288971\n",
      "Epoch 14[271/625] Time:0.702, Train Loss:0.22906921803951263\n",
      "Epoch 14[272/625] Time:0.704, Train Loss:0.23205438256263733\n",
      "Epoch 14[273/625] Time:0.702, Train Loss:0.28473636507987976\n",
      "Epoch 14[274/625] Time:0.702, Train Loss:0.2808590531349182\n",
      "Epoch 14[275/625] Time:0.712, Train Loss:0.2602302134037018\n",
      "Epoch 14[276/625] Time:0.704, Train Loss:0.3003927767276764\n",
      "Epoch 14[277/625] Time:0.704, Train Loss:0.31083208322525024\n",
      "Epoch 14[278/625] Time:0.703, Train Loss:0.2201290875673294\n",
      "Epoch 14[279/625] Time:0.693, Train Loss:0.24947603046894073\n",
      "Epoch 14[280/625] Time:0.694, Train Loss:0.2813108265399933\n",
      "Epoch 14[281/625] Time:0.694, Train Loss:0.28866246342658997\n",
      "Epoch 14[282/625] Time:0.693, Train Loss:0.34225964546203613\n",
      "Epoch 14[283/625] Time:0.703, Train Loss:0.30249321460723877\n",
      "Epoch 14[284/625] Time:0.702, Train Loss:0.2193281650543213\n",
      "Epoch 14[285/625] Time:0.703, Train Loss:0.31324657797813416\n",
      "Epoch 14[286/625] Time:0.706, Train Loss:0.27084895968437195\n",
      "Epoch 14[287/625] Time:0.705, Train Loss:0.21370328962802887\n",
      "Epoch 14[288/625] Time:0.703, Train Loss:0.23505330085754395\n",
      "Epoch 14[289/625] Time:0.704, Train Loss:0.3617734909057617\n",
      "Epoch 14[290/625] Time:0.704, Train Loss:0.25448375940322876\n",
      "Epoch 14[291/625] Time:0.704, Train Loss:0.2759838402271271\n",
      "Epoch 14[292/625] Time:0.704, Train Loss:0.2946065068244934\n",
      "Epoch 14[293/625] Time:0.704, Train Loss:0.1960621029138565\n",
      "Epoch 14[294/625] Time:0.704, Train Loss:0.23187637329101562\n",
      "Epoch 14[295/625] Time:0.708, Train Loss:0.27891016006469727\n",
      "Epoch 14[296/625] Time:0.703, Train Loss:0.3190698027610779\n",
      "Epoch 14[297/625] Time:0.703, Train Loss:0.2746182382106781\n",
      "Epoch 14[298/625] Time:0.704, Train Loss:0.29200124740600586\n",
      "Epoch 14[299/625] Time:0.704, Train Loss:0.2899157404899597\n",
      "Epoch 14[300/625] Time:0.704, Train Loss:0.3799948990345001\n",
      "Epoch 14[301/625] Time:0.703, Train Loss:0.3427527844905853\n",
      "Epoch 14[302/625] Time:0.703, Train Loss:0.3211376965045929\n",
      "Epoch 14[303/625] Time:0.702, Train Loss:0.32097452878952026\n",
      "Epoch 14[304/625] Time:0.702, Train Loss:0.32779303193092346\n",
      "Epoch 14[305/625] Time:0.703, Train Loss:0.3389877676963806\n",
      "Epoch 14[306/625] Time:0.703, Train Loss:0.28412601351737976\n",
      "Epoch 14[307/625] Time:0.703, Train Loss:0.3560221195220947\n",
      "Epoch 14[308/625] Time:0.704, Train Loss:0.3484991192817688\n",
      "Epoch 14[309/625] Time:0.703, Train Loss:0.284598708152771\n",
      "Epoch 14[310/625] Time:0.704, Train Loss:0.2841719090938568\n",
      "Epoch 14[311/625] Time:0.703, Train Loss:0.28855234384536743\n",
      "Epoch 14[312/625] Time:0.703, Train Loss:0.2703154981136322\n",
      "Epoch 14[313/625] Time:0.701, Train Loss:0.27907198667526245\n",
      "Epoch 14[314/625] Time:0.704, Train Loss:0.23912528157234192\n",
      "Epoch 14[315/625] Time:0.702, Train Loss:0.2727484703063965\n",
      "Epoch 14[316/625] Time:0.702, Train Loss:0.2924187481403351\n",
      "Epoch 14[317/625] Time:0.702, Train Loss:0.22050665318965912\n",
      "Epoch 14[318/625] Time:0.709, Train Loss:0.3021676242351532\n",
      "Epoch 14[319/625] Time:0.703, Train Loss:0.2612849771976471\n",
      "Epoch 14[320/625] Time:0.702, Train Loss:0.2405121773481369\n",
      "Epoch 14[321/625] Time:0.702, Train Loss:0.3000892996788025\n",
      "Epoch 14[322/625] Time:0.703, Train Loss:0.3007417917251587\n",
      "Epoch 14[323/625] Time:0.702, Train Loss:0.3087766170501709\n",
      "Epoch 14[324/625] Time:0.701, Train Loss:0.29311424493789673\n",
      "Epoch 14[325/625] Time:0.704, Train Loss:0.3413672149181366\n",
      "Epoch 14[326/625] Time:0.703, Train Loss:0.2928859293460846\n",
      "Epoch 14[327/625] Time:0.702, Train Loss:0.3018636107444763\n",
      "Epoch 14[328/625] Time:0.702, Train Loss:0.307932049036026\n",
      "Epoch 14[329/625] Time:0.703, Train Loss:0.2903822064399719\n",
      "Epoch 14[330/625] Time:0.703, Train Loss:0.2654612958431244\n",
      "Epoch 14[331/625] Time:0.704, Train Loss:0.281767338514328\n",
      "Epoch 14[332/625] Time:0.703, Train Loss:0.2774505019187927\n",
      "Epoch 14[333/625] Time:0.703, Train Loss:0.29861772060394287\n",
      "Epoch 14[334/625] Time:0.744, Train Loss:0.20681753754615784\n",
      "Epoch 14[335/625] Time:0.692, Train Loss:0.2626175582408905\n",
      "Epoch 14[336/625] Time:0.702, Train Loss:0.2956252098083496\n",
      "Epoch 14[337/625] Time:0.71, Train Loss:0.24334383010864258\n",
      "Epoch 14[338/625] Time:0.692, Train Loss:0.24012038111686707\n",
      "Epoch 14[339/625] Time:0.694, Train Loss:0.29742464423179626\n",
      "Epoch 14[340/625] Time:0.694, Train Loss:0.288468599319458\n",
      "Epoch 14[341/625] Time:0.691, Train Loss:0.22182944416999817\n",
      "Epoch 14[342/625] Time:0.692, Train Loss:0.3473391830921173\n",
      "Epoch 14[343/625] Time:0.696, Train Loss:0.33081158995628357\n",
      "Epoch 14[344/625] Time:0.702, Train Loss:0.30292513966560364\n",
      "Epoch 14[345/625] Time:0.713, Train Loss:0.32298848032951355\n",
      "Epoch 14[346/625] Time:0.702, Train Loss:0.35806354880332947\n",
      "Epoch 14[347/625] Time:0.703, Train Loss:0.3191525936126709\n",
      "Epoch 14[348/625] Time:0.703, Train Loss:0.30399617552757263\n",
      "Epoch 14[349/625] Time:0.704, Train Loss:0.38368576765060425\n",
      "Epoch 14[350/625] Time:0.703, Train Loss:0.30146971344947815\n",
      "Epoch 14[351/625] Time:0.704, Train Loss:0.20888055860996246\n",
      "Epoch 14[352/625] Time:0.703, Train Loss:0.32340842485427856\n",
      "Epoch 14[353/625] Time:0.703, Train Loss:0.35031524300575256\n",
      "Epoch 14[354/625] Time:0.703, Train Loss:0.32920753955841064\n",
      "Epoch 14[355/625] Time:0.703, Train Loss:0.3312585651874542\n",
      "Epoch 14[356/625] Time:0.703, Train Loss:0.275944322347641\n",
      "Epoch 14[357/625] Time:0.703, Train Loss:0.25812453031539917\n",
      "Epoch 14[358/625] Time:0.702, Train Loss:0.26805832982063293\n",
      "Epoch 14[359/625] Time:0.705, Train Loss:0.37117651104927063\n",
      "Epoch 14[360/625] Time:0.702, Train Loss:0.30067896842956543\n",
      "Epoch 14[361/625] Time:0.703, Train Loss:0.2828448414802551\n",
      "Epoch 14[362/625] Time:0.703, Train Loss:0.28158295154571533\n",
      "Epoch 14[363/625] Time:0.732, Train Loss:0.30283522605895996\n",
      "Epoch 14[364/625] Time:0.692, Train Loss:0.2506324350833893\n",
      "Epoch 14[365/625] Time:0.692, Train Loss:0.23577828705310822\n",
      "Epoch 14[366/625] Time:0.693, Train Loss:0.23402366042137146\n",
      "Epoch 14[367/625] Time:0.691, Train Loss:0.2736566364765167\n",
      "Epoch 14[368/625] Time:0.703, Train Loss:0.21351845562458038\n",
      "Epoch 14[369/625] Time:0.703, Train Loss:0.2700421214103699\n",
      "Epoch 14[370/625] Time:0.703, Train Loss:0.31333112716674805\n",
      "Epoch 14[371/625] Time:0.703, Train Loss:0.3241875469684601\n",
      "Epoch 14[372/625] Time:0.703, Train Loss:0.3543989062309265\n",
      "Epoch 14[373/625] Time:0.703, Train Loss:0.23256944119930267\n",
      "Epoch 14[374/625] Time:0.703, Train Loss:0.25488588213920593\n",
      "Epoch 14[375/625] Time:0.703, Train Loss:0.2744697332382202\n",
      "Epoch 14[376/625] Time:0.706, Train Loss:0.38502371311187744\n",
      "Epoch 14[377/625] Time:0.702, Train Loss:0.3636282980442047\n",
      "Epoch 14[378/625] Time:0.694, Train Loss:0.34291619062423706\n",
      "Epoch 14[379/625] Time:0.693, Train Loss:0.2646334767341614\n",
      "Epoch 14[380/625] Time:0.697, Train Loss:0.26575133204460144\n",
      "Epoch 14[381/625] Time:0.702, Train Loss:0.3812691867351532\n",
      "Epoch 14[382/625] Time:0.703, Train Loss:0.23479314148426056\n",
      "Epoch 14[383/625] Time:0.703, Train Loss:0.2877136170864105\n",
      "Epoch 14[384/625] Time:0.702, Train Loss:0.28375110030174255\n",
      "Epoch 14[385/625] Time:0.702, Train Loss:0.2568974196910858\n",
      "Epoch 14[386/625] Time:0.704, Train Loss:0.2064732164144516\n",
      "Epoch 14[387/625] Time:0.704, Train Loss:0.24842491745948792\n",
      "Epoch 14[388/625] Time:0.706, Train Loss:0.2419976145029068\n",
      "Epoch 14[389/625] Time:0.704, Train Loss:0.2683645188808441\n",
      "Epoch 14[390/625] Time:0.703, Train Loss:0.3237054944038391\n",
      "Epoch 14[391/625] Time:0.703, Train Loss:0.27739912271499634\n",
      "Epoch 14[392/625] Time:0.704, Train Loss:0.30404675006866455\n",
      "Epoch 14[393/625] Time:0.702, Train Loss:0.32420065999031067\n",
      "Epoch 14[394/625] Time:0.704, Train Loss:0.26815304160118103\n",
      "Epoch 14[395/625] Time:0.703, Train Loss:0.20302364230155945\n",
      "Epoch 14[396/625] Time:0.702, Train Loss:0.23168078064918518\n",
      "Epoch 14[397/625] Time:0.703, Train Loss:0.18005616962909698\n",
      "Epoch 14[398/625] Time:0.748, Train Loss:0.3344234824180603\n",
      "Epoch 14[399/625] Time:0.7, Train Loss:0.19626830518245697\n",
      "Epoch 14[400/625] Time:0.704, Train Loss:0.3293628990650177\n",
      "Epoch 14[401/625] Time:0.703, Train Loss:0.193586528301239\n",
      "Epoch 14[402/625] Time:0.703, Train Loss:0.3796032965183258\n",
      "Epoch 14[403/625] Time:0.704, Train Loss:0.20772512257099152\n",
      "Epoch 14[404/625] Time:0.705, Train Loss:0.3126732409000397\n",
      "Epoch 14[405/625] Time:0.703, Train Loss:0.3157873749732971\n",
      "Epoch 14[406/625] Time:0.711, Train Loss:0.23881787061691284\n",
      "Epoch 14[407/625] Time:0.702, Train Loss:0.22948057949543\n",
      "Epoch 14[408/625] Time:0.702, Train Loss:0.2879950702190399\n",
      "Epoch 14[409/625] Time:0.703, Train Loss:0.3183167576789856\n",
      "Epoch 14[410/625] Time:0.704, Train Loss:0.34898683428764343\n",
      "Epoch 14[411/625] Time:0.705, Train Loss:0.22166338562965393\n",
      "Epoch 14[412/625] Time:0.705, Train Loss:0.3295117914676666\n",
      "Epoch 14[413/625] Time:0.703, Train Loss:0.2703109681606293\n",
      "Epoch 14[414/625] Time:0.707, Train Loss:0.2780955135822296\n",
      "Epoch 14[415/625] Time:0.703, Train Loss:0.2492983192205429\n",
      "Epoch 14[416/625] Time:0.703, Train Loss:0.23215745389461517\n",
      "Epoch 14[417/625] Time:0.703, Train Loss:0.4131397306919098\n",
      "Epoch 14[418/625] Time:0.702, Train Loss:0.2079843133687973\n",
      "Epoch 14[419/625] Time:0.703, Train Loss:0.28372547030448914\n",
      "Epoch 14[420/625] Time:0.703, Train Loss:0.39301595091819763\n",
      "Epoch 14[421/625] Time:0.703, Train Loss:0.33454105257987976\n",
      "Epoch 14[422/625] Time:0.704, Train Loss:0.3784516751766205\n",
      "Epoch 14[423/625] Time:0.703, Train Loss:0.30047011375427246\n",
      "Epoch 14[424/625] Time:0.704, Train Loss:0.30374836921691895\n",
      "Epoch 14[425/625] Time:0.703, Train Loss:0.3410097658634186\n",
      "Epoch 14[426/625] Time:0.703, Train Loss:0.27647659182548523\n",
      "Epoch 14[427/625] Time:0.725, Train Loss:0.26121997833251953\n",
      "Epoch 14[428/625] Time:0.693, Train Loss:0.24552902579307556\n",
      "Epoch 14[429/625] Time:0.694, Train Loss:0.26046261191368103\n",
      "Epoch 14[430/625] Time:0.696, Train Loss:0.33716753125190735\n",
      "Epoch 14[431/625] Time:0.704, Train Loss:0.3444919288158417\n",
      "Epoch 14[432/625] Time:0.706, Train Loss:0.23732887208461761\n",
      "Epoch 14[433/625] Time:0.704, Train Loss:0.30265751481056213\n",
      "Epoch 14[434/625] Time:0.705, Train Loss:0.291431725025177\n",
      "Epoch 14[435/625] Time:0.704, Train Loss:0.28430140018463135\n",
      "Epoch 14[436/625] Time:0.704, Train Loss:0.2852054536342621\n",
      "Epoch 14[437/625] Time:0.704, Train Loss:0.33510342240333557\n",
      "Epoch 14[438/625] Time:0.703, Train Loss:0.27372127771377563\n",
      "Epoch 14[439/625] Time:0.703, Train Loss:0.30182915925979614\n",
      "Epoch 14[440/625] Time:0.704, Train Loss:0.20540741086006165\n",
      "Epoch 14[441/625] Time:0.703, Train Loss:0.26041075587272644\n",
      "Epoch 14[442/625] Time:0.703, Train Loss:0.24512037634849548\n",
      "Epoch 14[443/625] Time:0.703, Train Loss:0.3412591516971588\n",
      "Epoch 14[444/625] Time:0.704, Train Loss:0.2883690297603607\n",
      "Epoch 14[445/625] Time:0.704, Train Loss:0.3196682929992676\n",
      "Epoch 14[446/625] Time:0.748, Train Loss:0.2812957763671875\n",
      "Epoch 14[447/625] Time:0.692, Train Loss:0.316651850938797\n",
      "Epoch 14[448/625] Time:0.708, Train Loss:0.2859116792678833\n",
      "Epoch 14[449/625] Time:0.703, Train Loss:0.2359817773103714\n",
      "Epoch 14[450/625] Time:0.704, Train Loss:0.3209061622619629\n",
      "Epoch 14[451/625] Time:0.703, Train Loss:0.21770833432674408\n",
      "Epoch 14[452/625] Time:0.703, Train Loss:0.3471943438053131\n",
      "Epoch 14[453/625] Time:0.705, Train Loss:0.26841533184051514\n",
      "Epoch 14[454/625] Time:0.704, Train Loss:0.30778971314430237\n",
      "Epoch 14[455/625] Time:0.703, Train Loss:0.34072503447532654\n",
      "Epoch 14[456/625] Time:0.704, Train Loss:0.26820433139801025\n",
      "Epoch 14[457/625] Time:0.704, Train Loss:0.21801027655601501\n",
      "Epoch 14[458/625] Time:0.702, Train Loss:0.18759340047836304\n",
      "Epoch 14[459/625] Time:0.703, Train Loss:0.24738191068172455\n",
      "Epoch 14[460/625] Time:0.704, Train Loss:0.2197132259607315\n",
      "Epoch 14[461/625] Time:0.702, Train Loss:0.23329614102840424\n",
      "Epoch 14[462/625] Time:0.703, Train Loss:0.3272445797920227\n",
      "Epoch 14[463/625] Time:0.703, Train Loss:0.28090208768844604\n",
      "Epoch 14[464/625] Time:0.703, Train Loss:0.22427737712860107\n",
      "Epoch 14[465/625] Time:0.703, Train Loss:0.2652377784252167\n",
      "Epoch 14[466/625] Time:0.704, Train Loss:0.1978524774312973\n",
      "Epoch 14[467/625] Time:0.704, Train Loss:0.20951148867607117\n",
      "Epoch 14[468/625] Time:0.705, Train Loss:0.2701362371444702\n",
      "Epoch 14[469/625] Time:0.703, Train Loss:0.2584056854248047\n",
      "Epoch 14[470/625] Time:0.703, Train Loss:0.30146604776382446\n",
      "Epoch 14[471/625] Time:0.704, Train Loss:0.24571508169174194\n",
      "Epoch 14[472/625] Time:0.703, Train Loss:0.22268834710121155\n",
      "Epoch 14[473/625] Time:0.703, Train Loss:0.3189155161380768\n",
      "Epoch 14[474/625] Time:0.704, Train Loss:0.2891307473182678\n",
      "Epoch 14[475/625] Time:0.703, Train Loss:0.26417070627212524\n",
      "Epoch 14[476/625] Time:0.703, Train Loss:0.2628921568393707\n",
      "Epoch 14[477/625] Time:0.703, Train Loss:0.32975777983665466\n",
      "Epoch 14[478/625] Time:0.702, Train Loss:0.24937665462493896\n",
      "Epoch 14[479/625] Time:0.7, Train Loss:0.26204365491867065\n",
      "Epoch 14[480/625] Time:0.702, Train Loss:0.25413909554481506\n",
      "Epoch 14[481/625] Time:0.703, Train Loss:0.19457991421222687\n",
      "Epoch 14[482/625] Time:0.702, Train Loss:0.3066052198410034\n",
      "Epoch 14[483/625] Time:0.702, Train Loss:0.30643171072006226\n",
      "Epoch 14[484/625] Time:0.704, Train Loss:0.3070080876350403\n",
      "Epoch 14[485/625] Time:0.703, Train Loss:0.2452506721019745\n",
      "Epoch 14[486/625] Time:0.703, Train Loss:0.27049699425697327\n",
      "Epoch 14[487/625] Time:0.702, Train Loss:0.38739052414894104\n",
      "Epoch 14[488/625] Time:0.703, Train Loss:0.3272715210914612\n",
      "Epoch 14[489/625] Time:0.735, Train Loss:0.3505558669567108\n",
      "Epoch 14[490/625] Time:0.692, Train Loss:0.3213738799095154\n",
      "Epoch 14[491/625] Time:0.704, Train Loss:0.25169244408607483\n",
      "Epoch 14[492/625] Time:0.702, Train Loss:0.2940307557582855\n",
      "Epoch 14[493/625] Time:0.702, Train Loss:0.26075947284698486\n",
      "Epoch 14[494/625] Time:0.703, Train Loss:0.3041859269142151\n",
      "Epoch 14[495/625] Time:0.705, Train Loss:0.3180866241455078\n",
      "Epoch 14[496/625] Time:0.703, Train Loss:0.26380258798599243\n",
      "Epoch 14[497/625] Time:0.703, Train Loss:0.2994579076766968\n",
      "Epoch 14[498/625] Time:0.704, Train Loss:0.36591875553131104\n",
      "Epoch 14[499/625] Time:0.704, Train Loss:0.2725498080253601\n",
      "Epoch 14[500/625] Time:0.704, Train Loss:0.22823558747768402\n",
      "Epoch 14[501/625] Time:0.703, Train Loss:0.22614458203315735\n",
      "Epoch 14[502/625] Time:0.703, Train Loss:0.2889462113380432\n",
      "Epoch 14[503/625] Time:0.703, Train Loss:0.21207405626773834\n",
      "Epoch 14[504/625] Time:0.704, Train Loss:0.2617591619491577\n",
      "Epoch 14[505/625] Time:0.704, Train Loss:0.24991384148597717\n",
      "Epoch 14[506/625] Time:0.703, Train Loss:0.30786043405532837\n",
      "Epoch 14[507/625] Time:0.705, Train Loss:0.2712012827396393\n",
      "Epoch 14[508/625] Time:0.703, Train Loss:0.35748323798179626\n",
      "Epoch 14[509/625] Time:0.704, Train Loss:0.3497934341430664\n",
      "Epoch 14[510/625] Time:0.706, Train Loss:0.25980332493782043\n",
      "Epoch 14[511/625] Time:0.703, Train Loss:0.23342476785182953\n",
      "Epoch 14[512/625] Time:0.703, Train Loss:0.2976406514644623\n",
      "Epoch 14[513/625] Time:0.704, Train Loss:0.24777847528457642\n",
      "Epoch 14[514/625] Time:0.706, Train Loss:0.23176367580890656\n",
      "Epoch 14[515/625] Time:0.704, Train Loss:0.2970752418041229\n",
      "Epoch 14[516/625] Time:0.703, Train Loss:0.25761285424232483\n",
      "Epoch 14[517/625] Time:0.703, Train Loss:0.2510015070438385\n",
      "Epoch 14[518/625] Time:0.703, Train Loss:0.2787036895751953\n",
      "Epoch 14[519/625] Time:0.703, Train Loss:0.22980976104736328\n",
      "Epoch 14[520/625] Time:0.702, Train Loss:0.27200374007225037\n",
      "Epoch 14[521/625] Time:0.703, Train Loss:0.25889262557029724\n",
      "Epoch 14[522/625] Time:0.702, Train Loss:0.2039119303226471\n",
      "Epoch 14[523/625] Time:0.703, Train Loss:0.24407660961151123\n",
      "Epoch 14[524/625] Time:0.703, Train Loss:0.30741241574287415\n",
      "Epoch 14[525/625] Time:0.702, Train Loss:0.2813023030757904\n",
      "Epoch 14[526/625] Time:0.702, Train Loss:0.2814463675022125\n",
      "Epoch 14[527/625] Time:0.702, Train Loss:0.4762878715991974\n",
      "Epoch 14[528/625] Time:0.702, Train Loss:0.32273077964782715\n",
      "Epoch 14[529/625] Time:0.703, Train Loss:0.259636253118515\n",
      "Epoch 14[530/625] Time:0.704, Train Loss:0.29731765389442444\n",
      "Epoch 14[531/625] Time:0.704, Train Loss:0.23643721640110016\n",
      "Epoch 14[532/625] Time:0.705, Train Loss:0.33948734402656555\n",
      "Epoch 14[533/625] Time:0.703, Train Loss:0.3099699020385742\n",
      "Epoch 14[534/625] Time:0.705, Train Loss:0.272902250289917\n",
      "Epoch 14[535/625] Time:0.703, Train Loss:0.28878527879714966\n",
      "Epoch 14[536/625] Time:0.743, Train Loss:0.2780008614063263\n",
      "Epoch 14[537/625] Time:0.692, Train Loss:0.2950429320335388\n",
      "Epoch 14[538/625] Time:0.693, Train Loss:0.3203813135623932\n",
      "Epoch 14[539/625] Time:0.693, Train Loss:0.23433905839920044\n",
      "Epoch 14[540/625] Time:0.693, Train Loss:0.3875938951969147\n",
      "Epoch 14[541/625] Time:0.711, Train Loss:0.3108486831188202\n",
      "Epoch 14[542/625] Time:0.703, Train Loss:0.28866997361183167\n",
      "Epoch 14[543/625] Time:0.703, Train Loss:0.24064674973487854\n",
      "Epoch 14[544/625] Time:0.703, Train Loss:0.30952703952789307\n",
      "Epoch 14[545/625] Time:0.702, Train Loss:0.25203779339790344\n",
      "Epoch 14[546/625] Time:0.702, Train Loss:0.40279626846313477\n",
      "Epoch 14[547/625] Time:0.704, Train Loss:0.2265675961971283\n",
      "Epoch 14[548/625] Time:0.703, Train Loss:0.29484084248542786\n",
      "Epoch 14[549/625] Time:0.703, Train Loss:0.25579214096069336\n",
      "Epoch 14[550/625] Time:0.702, Train Loss:0.39432722330093384\n",
      "Epoch 14[551/625] Time:0.702, Train Loss:0.3243492543697357\n",
      "Epoch 14[552/625] Time:0.703, Train Loss:0.29846757650375366\n",
      "Epoch 14[553/625] Time:0.703, Train Loss:0.3484591543674469\n",
      "Epoch 14[554/625] Time:0.704, Train Loss:0.25015977025032043\n",
      "Epoch 14[555/625] Time:0.74, Train Loss:0.28415513038635254\n",
      "Epoch 14[556/625] Time:0.726, Train Loss:0.3211711645126343\n",
      "Epoch 14[557/625] Time:0.708, Train Loss:0.24388913810253143\n",
      "Epoch 14[558/625] Time:0.702, Train Loss:0.2945278286933899\n",
      "Epoch 14[559/625] Time:0.703, Train Loss:0.3573937714099884\n",
      "Epoch 14[560/625] Time:0.702, Train Loss:0.2752998471260071\n",
      "Epoch 14[561/625] Time:0.702, Train Loss:0.2756962478160858\n",
      "Epoch 14[562/625] Time:0.704, Train Loss:0.2772238254547119\n",
      "Epoch 14[563/625] Time:0.703, Train Loss:0.3044430911540985\n",
      "Epoch 14[564/625] Time:0.702, Train Loss:0.2945491075515747\n",
      "Epoch 14[565/625] Time:0.704, Train Loss:0.25260815024375916\n",
      "Epoch 14[566/625] Time:0.707, Train Loss:0.28465965390205383\n",
      "Epoch 14[567/625] Time:0.703, Train Loss:0.2124965488910675\n",
      "Epoch 14[568/625] Time:0.703, Train Loss:0.3789089322090149\n",
      "Epoch 14[569/625] Time:0.701, Train Loss:0.31724101305007935\n",
      "Epoch 14[570/625] Time:0.704, Train Loss:0.2413032352924347\n",
      "Epoch 14[571/625] Time:0.704, Train Loss:0.30893629789352417\n",
      "Epoch 14[572/625] Time:0.704, Train Loss:0.23859357833862305\n",
      "Epoch 14[573/625] Time:0.705, Train Loss:0.2266608327627182\n",
      "Epoch 14[574/625] Time:0.704, Train Loss:0.3014059364795685\n",
      "Epoch 14[575/625] Time:0.702, Train Loss:0.2808595299720764\n",
      "Epoch 14[576/625] Time:0.692, Train Loss:0.3363319933414459\n",
      "Epoch 14[577/625] Time:0.693, Train Loss:0.25603222846984863\n",
      "Epoch 14[578/625] Time:0.693, Train Loss:0.3090601861476898\n",
      "Epoch 14[579/625] Time:0.693, Train Loss:0.29919737577438354\n",
      "Epoch 14[580/625] Time:0.726, Train Loss:0.2739241421222687\n",
      "Epoch 14[581/625] Time:0.704, Train Loss:0.351986289024353\n",
      "Epoch 14[582/625] Time:0.703, Train Loss:0.2461865246295929\n",
      "Epoch 14[583/625] Time:0.702, Train Loss:0.34001922607421875\n",
      "Epoch 14[584/625] Time:0.702, Train Loss:0.23178386688232422\n",
      "Epoch 14[585/625] Time:0.705, Train Loss:0.24764765799045563\n",
      "Epoch 14[586/625] Time:0.704, Train Loss:0.25183194875717163\n",
      "Epoch 14[587/625] Time:0.704, Train Loss:0.31615138053894043\n",
      "Epoch 14[588/625] Time:0.703, Train Loss:0.27237123250961304\n",
      "Epoch 14[589/625] Time:0.704, Train Loss:0.3459904193878174\n",
      "Epoch 14[590/625] Time:0.703, Train Loss:0.25252094864845276\n",
      "Epoch 14[591/625] Time:0.702, Train Loss:0.2635354697704315\n",
      "Epoch 14[592/625] Time:0.703, Train Loss:0.3814913034439087\n",
      "Epoch 14[593/625] Time:0.702, Train Loss:0.28044915199279785\n",
      "Epoch 14[594/625] Time:0.703, Train Loss:0.3210408389568329\n",
      "Epoch 14[595/625] Time:0.707, Train Loss:0.2746541202068329\n",
      "Epoch 14[596/625] Time:0.703, Train Loss:0.30114683508872986\n",
      "Epoch 14[597/625] Time:0.704, Train Loss:0.30131104588508606\n",
      "Epoch 14[598/625] Time:0.702, Train Loss:0.2893334925174713\n",
      "Epoch 14[599/625] Time:0.704, Train Loss:0.25338879227638245\n",
      "Epoch 14[600/625] Time:0.704, Train Loss:0.3087144196033478\n",
      "Epoch 14[601/625] Time:0.702, Train Loss:0.2808965742588043\n",
      "Epoch 14[602/625] Time:0.703, Train Loss:0.26883941888809204\n",
      "Epoch 14[603/625] Time:0.705, Train Loss:0.29253169894218445\n",
      "Epoch 14[604/625] Time:0.703, Train Loss:0.29620352387428284\n",
      "Epoch 14[605/625] Time:0.705, Train Loss:0.26526594161987305\n",
      "Epoch 14[606/625] Time:0.704, Train Loss:0.2644402086734772\n",
      "Epoch 14[607/625] Time:0.703, Train Loss:0.2624160349369049\n",
      "Epoch 14[608/625] Time:0.703, Train Loss:0.39127156138420105\n",
      "Epoch 14[609/625] Time:0.703, Train Loss:0.26588085293769836\n",
      "Epoch 14[610/625] Time:0.702, Train Loss:0.3440573513507843\n",
      "Epoch 14[611/625] Time:0.703, Train Loss:0.222752183675766\n",
      "Epoch 14[612/625] Time:0.7, Train Loss:0.2960459887981415\n",
      "Epoch 14[613/625] Time:0.702, Train Loss:0.38590207695961\n",
      "Epoch 14[614/625] Time:0.703, Train Loss:0.2296406626701355\n",
      "Epoch 14[615/625] Time:0.703, Train Loss:0.24686428904533386\n",
      "Epoch 14[616/625] Time:0.702, Train Loss:0.27700725197792053\n",
      "Epoch 14[617/625] Time:0.705, Train Loss:0.23772647976875305\n",
      "Epoch 14[618/625] Time:0.703, Train Loss:0.2809339165687561\n",
      "Epoch 14[619/625] Time:0.703, Train Loss:0.2047269493341446\n",
      "Epoch 14[620/625] Time:0.707, Train Loss:0.3019621670246124\n",
      "Epoch 14[621/625] Time:0.704, Train Loss:0.2704654633998871\n",
      "Epoch 14[622/625] Time:0.704, Train Loss:0.23721414804458618\n",
      "Epoch 14[623/625] Time:0.703, Train Loss:0.3208237886428833\n",
      "Epoch 14[624/625] Time:0.706, Train Loss:0.27597856521606445\n",
      "Epoch 14[0/78] Val Loss:0.33756503462791443\n",
      "Epoch 14[1/78] Val Loss:0.29833903908729553\n",
      "Epoch 14[2/78] Val Loss:0.3498997092247009\n",
      "Epoch 14[3/78] Val Loss:0.3811115622520447\n",
      "Epoch 14[4/78] Val Loss:0.6705397963523865\n",
      "Epoch 14[5/78] Val Loss:0.686303436756134\n",
      "Epoch 14[6/78] Val Loss:0.6745688319206238\n",
      "Epoch 14[7/78] Val Loss:0.7835031747817993\n",
      "Epoch 14[8/78] Val Loss:0.46658143401145935\n",
      "Epoch 14[9/78] Val Loss:0.1978720724582672\n",
      "Epoch 14[10/78] Val Loss:0.188100665807724\n",
      "Epoch 14[11/78] Val Loss:0.19967976212501526\n",
      "Epoch 14[12/78] Val Loss:0.1733125001192093\n",
      "Epoch 14[13/78] Val Loss:0.1465245485305786\n",
      "Epoch 14[14/78] Val Loss:0.4790382385253906\n",
      "Epoch 14[15/78] Val Loss:0.41883689165115356\n",
      "Epoch 14[16/78] Val Loss:0.4552810490131378\n",
      "Epoch 14[17/78] Val Loss:0.3461724817752838\n",
      "Epoch 14[18/78] Val Loss:0.6325020790100098\n",
      "Epoch 14[19/78] Val Loss:0.7343003153800964\n",
      "Epoch 14[20/78] Val Loss:0.5438116192817688\n",
      "Epoch 14[21/78] Val Loss:0.7424446940422058\n",
      "Epoch 14[22/78] Val Loss:0.9557750821113586\n",
      "Epoch 14[23/78] Val Loss:0.6266782283782959\n",
      "Epoch 14[24/78] Val Loss:0.40068185329437256\n",
      "Epoch 14[25/78] Val Loss:0.4938160181045532\n",
      "Epoch 14[26/78] Val Loss:0.5469263792037964\n",
      "Epoch 14[27/78] Val Loss:0.4636584520339966\n",
      "Epoch 14[28/78] Val Loss:0.4572327733039856\n",
      "Epoch 14[29/78] Val Loss:0.46113789081573486\n",
      "Epoch 14[30/78] Val Loss:0.9878845810890198\n",
      "Epoch 14[31/78] Val Loss:0.8371312022209167\n",
      "Epoch 14[32/78] Val Loss:1.0226373672485352\n",
      "Epoch 14[33/78] Val Loss:0.8885953426361084\n",
      "Epoch 14[34/78] Val Loss:0.77203369140625\n",
      "Epoch 14[35/78] Val Loss:0.7188616394996643\n",
      "Epoch 14[36/78] Val Loss:0.720042884349823\n",
      "Epoch 14[37/78] Val Loss:0.8631851673126221\n",
      "Epoch 14[38/78] Val Loss:0.385194331407547\n",
      "Epoch 14[39/78] Val Loss:0.4651522934436798\n",
      "Epoch 14[40/78] Val Loss:0.40304362773895264\n",
      "Epoch 14[41/78] Val Loss:0.3816131353378296\n",
      "Epoch 14[42/78] Val Loss:0.46613574028015137\n",
      "Epoch 14[43/78] Val Loss:0.2731718122959137\n",
      "Epoch 14[44/78] Val Loss:0.16965341567993164\n",
      "Epoch 14[45/78] Val Loss:0.17066369950771332\n",
      "Epoch 14[46/78] Val Loss:0.15792202949523926\n",
      "Epoch 14[47/78] Val Loss:0.2033722847700119\n",
      "Epoch 14[48/78] Val Loss:0.19536687433719635\n",
      "Epoch 14[49/78] Val Loss:0.12437307089567184\n",
      "Epoch 14[50/78] Val Loss:0.14029371738433838\n",
      "Epoch 14[51/78] Val Loss:0.13462698459625244\n",
      "Epoch 14[52/78] Val Loss:0.2034481316804886\n",
      "Epoch 14[53/78] Val Loss:0.12241801619529724\n",
      "Epoch 14[54/78] Val Loss:0.1552818864583969\n",
      "Epoch 14[55/78] Val Loss:0.18852688372135162\n",
      "Epoch 14[56/78] Val Loss:1.0332926511764526\n",
      "Epoch 14[57/78] Val Loss:0.898664653301239\n",
      "Epoch 14[58/78] Val Loss:0.8892496824264526\n",
      "Epoch 14[59/78] Val Loss:0.4760758876800537\n",
      "Epoch 14[60/78] Val Loss:0.3777530789375305\n",
      "Epoch 14[61/78] Val Loss:0.42340973019599915\n",
      "Epoch 14[62/78] Val Loss:0.5496827363967896\n",
      "Epoch 14[63/78] Val Loss:0.36345145106315613\n",
      "Epoch 14[64/78] Val Loss:0.39736539125442505\n",
      "Epoch 14[65/78] Val Loss:0.4069373905658722\n",
      "Epoch 14[66/78] Val Loss:0.5012996196746826\n",
      "Epoch 14[67/78] Val Loss:0.4590102732181549\n",
      "Epoch 14[68/78] Val Loss:0.8848363757133484\n",
      "Epoch 14[69/78] Val Loss:0.8208895921707153\n",
      "Epoch 14[70/78] Val Loss:0.8154575824737549\n",
      "Epoch 14[71/78] Val Loss:0.7173541188240051\n",
      "Epoch 14[72/78] Val Loss:0.5322327613830566\n",
      "Epoch 14[73/78] Val Loss:0.5028932094573975\n",
      "Epoch 14[74/78] Val Loss:0.919140100479126\n",
      "Epoch 14[75/78] Val Loss:1.0306146144866943\n",
      "Epoch 14[76/78] Val Loss:1.073103666305542\n",
      "Epoch 14[77/78] Val Loss:1.0446372032165527\n",
      "Epoch 14[78/78] Val Loss:1.3619375228881836\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.77      0.86     15691\n",
      "           1       0.52      0.92      0.66      4309\n",
      "\n",
      "    accuracy                           0.80     20000\n",
      "   macro avg       0.75      0.84      0.76     20000\n",
      "weighted avg       0.87      0.80      0.82     20000\n",
      "\n",
      "Epoch 14: Train Loss 0.2859721095561981, Val Loss 0.537719419464851, Train Time 789.6914439201355, Val Time 36.78086042404175\n",
      "Epoch 15[0/625] Time:0.688, Train Loss:0.2981591522693634\n",
      "Epoch 15[1/625] Time:0.703, Train Loss:0.24593713879585266\n",
      "Epoch 15[2/625] Time:0.702, Train Loss:0.24046601355075836\n",
      "Epoch 15[3/625] Time:0.702, Train Loss:0.28744640946388245\n",
      "Epoch 15[4/625] Time:0.703, Train Loss:0.27698442339897156\n",
      "Epoch 15[5/625] Time:0.703, Train Loss:0.2825923562049866\n",
      "Epoch 15[6/625] Time:0.703, Train Loss:0.27005255222320557\n",
      "Epoch 15[7/625] Time:0.709, Train Loss:0.31163111329078674\n",
      "Epoch 15[8/625] Time:0.702, Train Loss:0.2658957540988922\n",
      "Epoch 15[9/625] Time:0.703, Train Loss:0.37103408575057983\n",
      "Epoch 15[10/625] Time:0.703, Train Loss:0.25299587845802307\n",
      "Epoch 15[11/625] Time:0.702, Train Loss:0.317283034324646\n",
      "Epoch 15[12/625] Time:0.703, Train Loss:0.24224939942359924\n",
      "Epoch 15[13/625] Time:0.702, Train Loss:0.28985634446144104\n",
      "Epoch 15[14/625] Time:0.701, Train Loss:0.3394167423248291\n",
      "Epoch 15[15/625] Time:0.702, Train Loss:0.33129405975341797\n",
      "Epoch 15[16/625] Time:0.703, Train Loss:0.33644938468933105\n",
      "Epoch 15[17/625] Time:0.706, Train Loss:0.2769623398780823\n",
      "Epoch 15[18/625] Time:0.702, Train Loss:0.3788151144981384\n",
      "Epoch 15[19/625] Time:0.705, Train Loss:0.296258807182312\n",
      "Epoch 15[20/625] Time:0.703, Train Loss:0.2975408732891083\n",
      "Epoch 15[21/625] Time:0.703, Train Loss:0.3363671898841858\n",
      "Epoch 15[22/625] Time:0.702, Train Loss:0.26434144377708435\n",
      "Epoch 15[23/625] Time:0.703, Train Loss:0.2767532765865326\n",
      "Epoch 15[24/625] Time:0.708, Train Loss:0.302386611700058\n",
      "Epoch 15[25/625] Time:0.705, Train Loss:0.24035955965518951\n",
      "Epoch 15[26/625] Time:0.703, Train Loss:0.289260596036911\n",
      "Epoch 15[27/625] Time:0.704, Train Loss:0.28812023997306824\n",
      "Epoch 15[28/625] Time:0.703, Train Loss:0.3252578377723694\n",
      "Epoch 15[29/625] Time:0.703, Train Loss:0.25788548588752747\n",
      "Epoch 15[30/625] Time:0.704, Train Loss:0.33231621980667114\n",
      "Epoch 15[31/625] Time:0.702, Train Loss:0.21898333728313446\n",
      "Epoch 15[32/625] Time:0.702, Train Loss:0.3045313060283661\n",
      "Epoch 15[33/625] Time:0.702, Train Loss:0.270332008600235\n",
      "Epoch 15[34/625] Time:0.704, Train Loss:0.22339996695518494\n",
      "Epoch 15[35/625] Time:0.694, Train Loss:0.30854013562202454\n",
      "Epoch 15[36/625] Time:0.703, Train Loss:0.2608059346675873\n",
      "Epoch 15[37/625] Time:0.707, Train Loss:0.32906776666641235\n",
      "Epoch 15[38/625] Time:0.705, Train Loss:0.3443562388420105\n",
      "Epoch 15[39/625] Time:0.701, Train Loss:0.24086563289165497\n",
      "Epoch 15[40/625] Time:0.704, Train Loss:0.26614829897880554\n",
      "Epoch 15[41/625] Time:0.702, Train Loss:0.23370081186294556\n",
      "Epoch 15[42/625] Time:0.702, Train Loss:0.25245410203933716\n",
      "Epoch 15[43/625] Time:0.743, Train Loss:0.3187965452671051\n",
      "Epoch 15[44/625] Time:0.693, Train Loss:0.23438924551010132\n",
      "Epoch 15[45/625] Time:0.696, Train Loss:0.24015988409519196\n",
      "Epoch 15[46/625] Time:0.694, Train Loss:0.21598078310489655\n",
      "Epoch 15[47/625] Time:0.706, Train Loss:0.26373785734176636\n",
      "Epoch 15[48/625] Time:0.748, Train Loss:0.32136785984039307\n",
      "Epoch 15[49/625] Time:0.693, Train Loss:0.314182847738266\n",
      "Epoch 15[50/625] Time:0.694, Train Loss:0.2701757848262787\n",
      "Epoch 15[51/625] Time:0.693, Train Loss:0.291849821805954\n",
      "Epoch 15[52/625] Time:0.694, Train Loss:0.2625429332256317\n",
      "Epoch 15[53/625] Time:0.699, Train Loss:0.3564889430999756\n",
      "Epoch 15[54/625] Time:0.705, Train Loss:0.2774602174758911\n",
      "Epoch 15[55/625] Time:0.702, Train Loss:0.24528217315673828\n",
      "Epoch 15[56/625] Time:0.703, Train Loss:0.29662594199180603\n",
      "Epoch 15[57/625] Time:0.75, Train Loss:0.21310585737228394\n",
      "Epoch 15[58/625] Time:0.694, Train Loss:0.25133222341537476\n",
      "Epoch 15[59/625] Time:0.696, Train Loss:0.2224411517381668\n",
      "Epoch 15[60/625] Time:0.702, Train Loss:0.32521823048591614\n",
      "Epoch 15[61/625] Time:0.697, Train Loss:0.24395616352558136\n",
      "Epoch 15[62/625] Time:0.745, Train Loss:0.2291422188282013\n",
      "Epoch 15[63/625] Time:0.702, Train Loss:0.33964452147483826\n",
      "Epoch 15[64/625] Time:0.704, Train Loss:0.36909744143486023\n",
      "Epoch 15[65/625] Time:0.704, Train Loss:0.26910242438316345\n",
      "Epoch 15[66/625] Time:0.704, Train Loss:0.3264857828617096\n",
      "Epoch 15[67/625] Time:0.704, Train Loss:0.21291758120059967\n",
      "Epoch 15[68/625] Time:0.704, Train Loss:0.20896728336811066\n",
      "Epoch 15[69/625] Time:0.703, Train Loss:0.28511399030685425\n",
      "Epoch 15[70/625] Time:0.705, Train Loss:0.26857924461364746\n",
      "Epoch 15[71/625] Time:0.706, Train Loss:0.22789564728736877\n",
      "Epoch 15[72/625] Time:0.705, Train Loss:0.2477007657289505\n",
      "Epoch 15[73/625] Time:0.703, Train Loss:0.19493867456912994\n",
      "Epoch 15[74/625] Time:0.703, Train Loss:0.2580166161060333\n",
      "Epoch 15[75/625] Time:0.703, Train Loss:0.2505911886692047\n",
      "Epoch 15[76/625] Time:0.704, Train Loss:0.26147404313087463\n",
      "Epoch 15[77/625] Time:0.701, Train Loss:0.39599543809890747\n",
      "Epoch 15[78/625] Time:0.705, Train Loss:0.19412921369075775\n",
      "Epoch 15[79/625] Time:0.706, Train Loss:0.30221933126449585\n",
      "Epoch 15[80/625] Time:0.705, Train Loss:0.36046308279037476\n",
      "Epoch 15[81/625] Time:0.704, Train Loss:0.2987251281738281\n",
      "Epoch 15[82/625] Time:0.704, Train Loss:0.2794110178947449\n",
      "Epoch 15[83/625] Time:0.704, Train Loss:0.2416841834783554\n",
      "Epoch 15[84/625] Time:0.705, Train Loss:0.25681671500205994\n",
      "Epoch 15[85/625] Time:0.704, Train Loss:0.35590776801109314\n",
      "Epoch 15[86/625] Time:0.704, Train Loss:0.2507702112197876\n",
      "Epoch 15[87/625] Time:0.704, Train Loss:0.2768184244632721\n",
      "Epoch 15[88/625] Time:0.705, Train Loss:0.28896668553352356\n",
      "Epoch 15[89/625] Time:0.705, Train Loss:0.3284599184989929\n",
      "Epoch 15[90/625] Time:0.704, Train Loss:0.24483108520507812\n",
      "Epoch 15[91/625] Time:0.705, Train Loss:0.2388467788696289\n",
      "Epoch 15[92/625] Time:0.706, Train Loss:0.25795111060142517\n",
      "Epoch 15[93/625] Time:0.705, Train Loss:0.2613600194454193\n",
      "Epoch 15[94/625] Time:0.705, Train Loss:0.3088644742965698\n",
      "Epoch 15[95/625] Time:0.707, Train Loss:0.25689736008644104\n",
      "Epoch 15[96/625] Time:0.708, Train Loss:0.21810998022556305\n",
      "Epoch 15[97/625] Time:0.704, Train Loss:0.2366575449705124\n",
      "Epoch 15[98/625] Time:0.703, Train Loss:0.33903762698173523\n",
      "Epoch 15[99/625] Time:0.704, Train Loss:0.2329827845096588\n",
      "Epoch 15[100/625] Time:0.694, Train Loss:0.29055845737457275\n",
      "Epoch 15[101/625] Time:0.703, Train Loss:0.21729646623134613\n",
      "Epoch 15[102/625] Time:0.702, Train Loss:0.26500385999679565\n",
      "Epoch 15[103/625] Time:0.704, Train Loss:0.30114924907684326\n",
      "Epoch 15[104/625] Time:0.704, Train Loss:0.27085429430007935\n",
      "Epoch 15[105/625] Time:0.704, Train Loss:0.2522262632846832\n",
      "Epoch 15[106/625] Time:0.703, Train Loss:0.2955132722854614\n",
      "Epoch 15[107/625] Time:0.703, Train Loss:0.24817724525928497\n",
      "Epoch 15[108/625] Time:0.71, Train Loss:0.2828054130077362\n",
      "Epoch 15[109/625] Time:0.703, Train Loss:0.19333294034004211\n",
      "Epoch 15[110/625] Time:0.704, Train Loss:0.27451327443122864\n",
      "Epoch 15[111/625] Time:0.704, Train Loss:0.2924152910709381\n",
      "Epoch 15[112/625] Time:0.703, Train Loss:0.22610674798488617\n",
      "Epoch 15[113/625] Time:0.704, Train Loss:0.30961859226226807\n",
      "Epoch 15[114/625] Time:0.703, Train Loss:0.29433026909828186\n",
      "Epoch 15[115/625] Time:0.703, Train Loss:0.27830925583839417\n",
      "Epoch 15[116/625] Time:0.711, Train Loss:0.2913687229156494\n",
      "Epoch 15[117/625] Time:0.703, Train Loss:0.2320355474948883\n",
      "Epoch 15[118/625] Time:0.704, Train Loss:0.2238069623708725\n",
      "Epoch 15[119/625] Time:0.704, Train Loss:0.26343369483947754\n",
      "Epoch 15[120/625] Time:0.704, Train Loss:0.2277679592370987\n",
      "Epoch 15[121/625] Time:0.703, Train Loss:0.23327608406543732\n",
      "Epoch 15[122/625] Time:0.704, Train Loss:0.2958897352218628\n",
      "Epoch 15[123/625] Time:0.703, Train Loss:0.2775581479072571\n",
      "Epoch 15[124/625] Time:0.711, Train Loss:0.2561829686164856\n",
      "Epoch 15[125/625] Time:0.704, Train Loss:0.2654094099998474\n",
      "Epoch 15[126/625] Time:0.734, Train Loss:0.33365505933761597\n",
      "Epoch 15[127/625] Time:0.692, Train Loss:0.3333463966846466\n",
      "Epoch 15[128/625] Time:0.694, Train Loss:0.35944756865501404\n",
      "Epoch 15[129/625] Time:0.693, Train Loss:0.27474457025527954\n",
      "Epoch 15[130/625] Time:0.716, Train Loss:0.2104347050189972\n",
      "Epoch 15[131/625] Time:0.702, Train Loss:0.2360258400440216\n",
      "Epoch 15[132/625] Time:0.703, Train Loss:0.2916692793369293\n",
      "Epoch 15[133/625] Time:0.704, Train Loss:0.33028098940849304\n",
      "Epoch 15[134/625] Time:0.702, Train Loss:0.4009426236152649\n",
      "Epoch 15[135/625] Time:0.702, Train Loss:0.26283568143844604\n",
      "Epoch 15[136/625] Time:0.704, Train Loss:0.24661867320537567\n",
      "Epoch 15[137/625] Time:0.703, Train Loss:0.24667273461818695\n",
      "Epoch 15[138/625] Time:0.702, Train Loss:0.34449416399002075\n",
      "Epoch 15[139/625] Time:0.703, Train Loss:0.3726739287376404\n",
      "Epoch 15[140/625] Time:0.704, Train Loss:0.26641765236854553\n",
      "Epoch 15[141/625] Time:0.704, Train Loss:0.2828640341758728\n",
      "Epoch 15[142/625] Time:0.704, Train Loss:0.22933542728424072\n",
      "Epoch 15[143/625] Time:0.704, Train Loss:0.25970587134361267\n",
      "Epoch 15[144/625] Time:0.703, Train Loss:0.27661895751953125\n",
      "Epoch 15[145/625] Time:0.703, Train Loss:0.3371853828430176\n",
      "Epoch 15[146/625] Time:0.704, Train Loss:0.3151489794254303\n",
      "Epoch 15[147/625] Time:0.708, Train Loss:0.24836567044258118\n",
      "Epoch 15[148/625] Time:0.703, Train Loss:0.2516399621963501\n",
      "Epoch 15[149/625] Time:0.702, Train Loss:0.25078636407852173\n",
      "Epoch 15[150/625] Time:0.704, Train Loss:0.2790564000606537\n",
      "Epoch 15[151/625] Time:0.703, Train Loss:0.2955598831176758\n",
      "Epoch 15[152/625] Time:0.705, Train Loss:0.30585286021232605\n",
      "Epoch 15[153/625] Time:0.704, Train Loss:0.28486910462379456\n",
      "Epoch 15[154/625] Time:0.705, Train Loss:0.20656612515449524\n",
      "Epoch 15[155/625] Time:0.703, Train Loss:0.2934300899505615\n",
      "Epoch 15[156/625] Time:0.706, Train Loss:0.24213509261608124\n",
      "Epoch 15[157/625] Time:0.703, Train Loss:0.28227517008781433\n",
      "Epoch 15[158/625] Time:0.705, Train Loss:0.20778557658195496\n",
      "Epoch 15[159/625] Time:0.705, Train Loss:0.3666783273220062\n",
      "Epoch 15[160/625] Time:0.704, Train Loss:0.2877124547958374\n",
      "Epoch 15[161/625] Time:0.703, Train Loss:0.23972411453723907\n",
      "Epoch 15[162/625] Time:0.704, Train Loss:0.277304083108902\n",
      "Epoch 15[163/625] Time:0.703, Train Loss:0.20615726709365845\n",
      "Epoch 15[164/625] Time:0.711, Train Loss:0.24434921145439148\n",
      "Epoch 15[165/625] Time:0.703, Train Loss:0.3200555145740509\n",
      "Epoch 15[166/625] Time:0.703, Train Loss:0.22482647001743317\n",
      "Epoch 15[167/625] Time:0.705, Train Loss:0.25821566581726074\n",
      "Epoch 15[168/625] Time:0.703, Train Loss:0.3076857328414917\n",
      "Epoch 15[169/625] Time:0.702, Train Loss:0.26276829838752747\n",
      "Epoch 15[170/625] Time:0.707, Train Loss:0.23366069793701172\n",
      "Epoch 15[171/625] Time:0.703, Train Loss:0.2800799310207367\n",
      "Epoch 15[172/625] Time:0.708, Train Loss:0.3197323977947235\n",
      "Epoch 15[173/625] Time:0.703, Train Loss:0.3819087743759155\n",
      "Epoch 15[174/625] Time:0.703, Train Loss:0.3105425238609314\n",
      "Epoch 15[175/625] Time:0.703, Train Loss:0.3017491400241852\n",
      "Epoch 15[176/625] Time:0.704, Train Loss:0.2713969647884369\n",
      "Epoch 15[177/625] Time:0.701, Train Loss:0.2772812247276306\n",
      "Epoch 15[178/625] Time:0.702, Train Loss:0.2620825171470642\n",
      "Epoch 15[179/625] Time:0.703, Train Loss:0.2433687001466751\n",
      "Epoch 15[180/625] Time:0.709, Train Loss:0.2601580023765564\n",
      "Epoch 15[181/625] Time:0.705, Train Loss:0.24717289209365845\n",
      "Epoch 15[182/625] Time:0.711, Train Loss:0.2766299247741699\n",
      "Epoch 15[183/625] Time:0.703, Train Loss:0.2856594920158386\n",
      "Epoch 15[184/625] Time:0.705, Train Loss:0.29989972710609436\n",
      "Epoch 15[185/625] Time:0.705, Train Loss:0.2596660852432251\n",
      "Epoch 15[186/625] Time:0.705, Train Loss:0.2738247513771057\n",
      "Epoch 15[187/625] Time:0.708, Train Loss:0.27521517872810364\n",
      "Epoch 15[188/625] Time:0.708, Train Loss:0.2990655303001404\n",
      "Epoch 15[189/625] Time:0.703, Train Loss:0.2568851113319397\n",
      "Epoch 15[190/625] Time:0.7, Train Loss:0.31679630279541016\n",
      "Epoch 15[191/625] Time:0.702, Train Loss:0.28344663977622986\n",
      "Epoch 15[192/625] Time:0.694, Train Loss:0.33919766545295715\n",
      "Epoch 15[193/625] Time:0.694, Train Loss:0.2701469361782074\n",
      "Epoch 15[194/625] Time:0.693, Train Loss:0.31184789538383484\n",
      "Epoch 15[195/625] Time:0.694, Train Loss:0.2779168486595154\n",
      "Epoch 15[196/625] Time:0.697, Train Loss:0.288525253534317\n",
      "Epoch 15[197/625] Time:0.703, Train Loss:0.3408859968185425\n",
      "Epoch 15[198/625] Time:0.702, Train Loss:0.2567535638809204\n",
      "Epoch 15[199/625] Time:0.702, Train Loss:0.29692503809928894\n",
      "Epoch 15[200/625] Time:0.703, Train Loss:0.2901850640773773\n",
      "Epoch 15[201/625] Time:0.703, Train Loss:0.32193881273269653\n",
      "Epoch 15[202/625] Time:0.705, Train Loss:0.2185174822807312\n",
      "Epoch 15[203/625] Time:0.703, Train Loss:0.2889401614665985\n",
      "Epoch 15[204/625] Time:0.708, Train Loss:0.28185248374938965\n",
      "Epoch 15[205/625] Time:0.701, Train Loss:0.29864153265953064\n",
      "Epoch 15[206/625] Time:0.703, Train Loss:0.30183500051498413\n",
      "Epoch 15[207/625] Time:0.702, Train Loss:0.31870418787002563\n",
      "Epoch 15[208/625] Time:0.747, Train Loss:0.3157780170440674\n",
      "Epoch 15[209/625] Time:0.692, Train Loss:0.28451839089393616\n",
      "Epoch 15[210/625] Time:0.694, Train Loss:0.18510648608207703\n",
      "Epoch 15[211/625] Time:0.693, Train Loss:0.29590675234794617\n",
      "Epoch 15[212/625] Time:0.695, Train Loss:0.2063959240913391\n",
      "Epoch 15[213/625] Time:0.692, Train Loss:0.31612399220466614\n",
      "Epoch 15[214/625] Time:0.693, Train Loss:0.26137271523475647\n",
      "Epoch 15[215/625] Time:0.693, Train Loss:0.3935871124267578\n",
      "Epoch 15[216/625] Time:0.691, Train Loss:0.3005672097206116\n",
      "Epoch 15[217/625] Time:0.69, Train Loss:0.21329949796199799\n",
      "Epoch 15[218/625] Time:0.693, Train Loss:0.2679382264614105\n",
      "Epoch 15[219/625] Time:0.695, Train Loss:0.19722287356853485\n",
      "Epoch 15[220/625] Time:0.696, Train Loss:0.32921677827835083\n",
      "Epoch 15[221/625] Time:0.702, Train Loss:0.27685123682022095\n",
      "Epoch 15[222/625] Time:0.711, Train Loss:0.36902859807014465\n",
      "Epoch 15[223/625] Time:0.703, Train Loss:0.25516393780708313\n",
      "Epoch 15[224/625] Time:0.703, Train Loss:0.2703517973423004\n",
      "Epoch 15[225/625] Time:0.703, Train Loss:0.3415919542312622\n",
      "Epoch 15[226/625] Time:0.704, Train Loss:0.2502020597457886\n",
      "Epoch 15[227/625] Time:0.704, Train Loss:0.2770863175392151\n",
      "Epoch 15[228/625] Time:0.737, Train Loss:0.22755488753318787\n",
      "Epoch 15[229/625] Time:0.692, Train Loss:0.22693181037902832\n",
      "Epoch 15[230/625] Time:0.694, Train Loss:0.21831467747688293\n",
      "Epoch 15[231/625] Time:0.693, Train Loss:0.2609005868434906\n",
      "Epoch 15[232/625] Time:0.692, Train Loss:0.23563067615032196\n",
      "Epoch 15[233/625] Time:0.693, Train Loss:0.2284538596868515\n",
      "Epoch 15[234/625] Time:0.692, Train Loss:0.2686860263347626\n",
      "Epoch 15[235/625] Time:0.711, Train Loss:0.2792147397994995\n",
      "Epoch 15[236/625] Time:0.694, Train Loss:0.3108590841293335\n",
      "Epoch 15[237/625] Time:0.722, Train Loss:0.2718108296394348\n",
      "Epoch 15[238/625] Time:0.7, Train Loss:0.3044266402721405\n",
      "Epoch 15[239/625] Time:0.704, Train Loss:0.30238738656044006\n",
      "Epoch 15[240/625] Time:0.703, Train Loss:0.2866838872432709\n",
      "Epoch 15[241/625] Time:0.704, Train Loss:0.30665314197540283\n",
      "Epoch 15[242/625] Time:0.703, Train Loss:0.25141045451164246\n",
      "Epoch 15[243/625] Time:0.704, Train Loss:0.2606155276298523\n",
      "Epoch 15[244/625] Time:0.705, Train Loss:0.2436734288930893\n",
      "Epoch 15[245/625] Time:0.704, Train Loss:0.3285563290119171\n",
      "Epoch 15[246/625] Time:0.704, Train Loss:0.36927899718284607\n",
      "Epoch 15[247/625] Time:0.703, Train Loss:0.26407358050346375\n",
      "Epoch 15[248/625] Time:0.704, Train Loss:0.26990631222724915\n",
      "Epoch 15[249/625] Time:0.703, Train Loss:0.25800755620002747\n",
      "Epoch 15[250/625] Time:0.703, Train Loss:0.2542663514614105\n",
      "Epoch 15[251/625] Time:0.706, Train Loss:0.21669913828372955\n",
      "Epoch 15[252/625] Time:0.704, Train Loss:0.29158321022987366\n",
      "Epoch 15[253/625] Time:0.703, Train Loss:0.4013965129852295\n",
      "Epoch 15[254/625] Time:0.704, Train Loss:0.2237929254770279\n",
      "Epoch 15[255/625] Time:0.705, Train Loss:0.2160329520702362\n",
      "Epoch 15[256/625] Time:0.704, Train Loss:0.29373714327812195\n",
      "Epoch 15[257/625] Time:0.707, Train Loss:0.2894977033138275\n",
      "Epoch 15[258/625] Time:0.704, Train Loss:0.22838228940963745\n",
      "Epoch 15[259/625] Time:0.709, Train Loss:0.2512838542461395\n",
      "Epoch 15[260/625] Time:0.702, Train Loss:0.34617576003074646\n",
      "Epoch 15[261/625] Time:0.704, Train Loss:0.2529941201210022\n",
      "Epoch 15[262/625] Time:0.703, Train Loss:0.28046882152557373\n",
      "Epoch 15[263/625] Time:0.702, Train Loss:0.3149081766605377\n",
      "Epoch 15[264/625] Time:0.701, Train Loss:0.27544212341308594\n",
      "Epoch 15[265/625] Time:0.702, Train Loss:0.25401949882507324\n",
      "Epoch 15[266/625] Time:0.704, Train Loss:0.23941799998283386\n",
      "Epoch 15[267/625] Time:0.704, Train Loss:0.2535911798477173\n",
      "Epoch 15[268/625] Time:0.703, Train Loss:0.2767902612686157\n",
      "Epoch 15[269/625] Time:0.703, Train Loss:0.3269955515861511\n",
      "Epoch 15[270/625] Time:0.703, Train Loss:0.29375627636909485\n",
      "Epoch 15[271/625] Time:0.703, Train Loss:0.23000231385231018\n",
      "Epoch 15[272/625] Time:0.703, Train Loss:0.3747544288635254\n",
      "Epoch 15[273/625] Time:0.705, Train Loss:0.30743321776390076\n",
      "Epoch 15[274/625] Time:0.691, Train Loss:0.2409716695547104\n",
      "Epoch 15[275/625] Time:0.692, Train Loss:0.3053279519081116\n",
      "Epoch 15[276/625] Time:0.694, Train Loss:0.3098476827144623\n",
      "Epoch 15[277/625] Time:0.693, Train Loss:0.25201350450515747\n",
      "Epoch 15[278/625] Time:0.694, Train Loss:0.3222803473472595\n",
      "Epoch 15[279/625] Time:0.693, Train Loss:0.280303031206131\n",
      "Epoch 15[280/625] Time:0.694, Train Loss:0.26373666524887085\n",
      "Epoch 15[281/625] Time:0.693, Train Loss:0.18918216228485107\n",
      "Epoch 15[282/625] Time:0.721, Train Loss:0.27554282546043396\n",
      "Epoch 15[283/625] Time:0.704, Train Loss:0.26997172832489014\n",
      "Epoch 15[284/625] Time:0.703, Train Loss:0.25257718563079834\n",
      "Epoch 15[285/625] Time:0.704, Train Loss:0.3079593777656555\n",
      "Epoch 15[286/625] Time:0.703, Train Loss:0.24129687249660492\n",
      "Epoch 15[287/625] Time:0.703, Train Loss:0.2973534166812897\n",
      "Epoch 15[288/625] Time:0.703, Train Loss:0.28230956196784973\n",
      "Epoch 15[289/625] Time:0.703, Train Loss:0.2923099398612976\n",
      "Epoch 15[290/625] Time:0.704, Train Loss:0.26048851013183594\n",
      "Epoch 15[291/625] Time:0.703, Train Loss:0.265886515378952\n",
      "Epoch 15[292/625] Time:0.703, Train Loss:0.3829381763935089\n",
      "Epoch 15[293/625] Time:0.703, Train Loss:0.3372500240802765\n",
      "Epoch 15[294/625] Time:0.704, Train Loss:0.23240455985069275\n",
      "Epoch 15[295/625] Time:0.703, Train Loss:0.2856028079986572\n",
      "Epoch 15[296/625] Time:0.705, Train Loss:0.2872626483440399\n",
      "Epoch 15[297/625] Time:0.703, Train Loss:0.22064727544784546\n",
      "Epoch 15[298/625] Time:0.705, Train Loss:0.2949087619781494\n",
      "Epoch 15[299/625] Time:0.702, Train Loss:0.34716272354125977\n",
      "Epoch 15[300/625] Time:0.702, Train Loss:0.2742643356323242\n",
      "Epoch 15[301/625] Time:0.703, Train Loss:0.28373822569847107\n",
      "Epoch 15[302/625] Time:0.702, Train Loss:0.2415345013141632\n",
      "Epoch 15[303/625] Time:0.706, Train Loss:0.2731265127658844\n",
      "Epoch 15[304/625] Time:0.702, Train Loss:0.317695677280426\n",
      "Epoch 15[305/625] Time:0.704, Train Loss:0.35631635785102844\n",
      "Epoch 15[306/625] Time:0.706, Train Loss:0.30449339747428894\n",
      "Epoch 15[307/625] Time:0.703, Train Loss:0.23983612656593323\n",
      "Epoch 15[308/625] Time:0.703, Train Loss:0.2547169625759125\n",
      "Epoch 15[309/625] Time:0.703, Train Loss:0.3915974497795105\n",
      "Epoch 15[310/625] Time:0.704, Train Loss:0.2768633961677551\n",
      "Epoch 15[311/625] Time:0.702, Train Loss:0.31534335017204285\n",
      "Epoch 15[312/625] Time:0.701, Train Loss:0.2749505937099457\n",
      "Epoch 15[313/625] Time:0.702, Train Loss:0.2363460808992386\n",
      "Epoch 15[314/625] Time:0.71, Train Loss:0.3501852750778198\n",
      "Epoch 15[315/625] Time:0.703, Train Loss:0.3184519112110138\n",
      "Epoch 15[316/625] Time:0.701, Train Loss:0.3462059795856476\n",
      "Epoch 15[317/625] Time:0.718, Train Loss:0.29350680112838745\n",
      "Epoch 15[318/625] Time:0.694, Train Loss:0.281902551651001\n",
      "Epoch 15[319/625] Time:0.694, Train Loss:0.3782847821712494\n",
      "Epoch 15[320/625] Time:0.694, Train Loss:0.28770342469215393\n",
      "Epoch 15[321/625] Time:0.694, Train Loss:0.34058767557144165\n",
      "Epoch 15[322/625] Time:0.695, Train Loss:0.21382887661457062\n",
      "Epoch 15[323/625] Time:0.695, Train Loss:0.3406428098678589\n",
      "Epoch 15[324/625] Time:0.694, Train Loss:0.33574360609054565\n",
      "Epoch 15[325/625] Time:0.694, Train Loss:0.287672221660614\n",
      "Epoch 15[326/625] Time:0.695, Train Loss:0.2785364091396332\n",
      "Epoch 15[327/625] Time:0.694, Train Loss:0.30055293440818787\n",
      "Epoch 15[328/625] Time:0.694, Train Loss:0.32240796089172363\n",
      "Epoch 15[329/625] Time:0.696, Train Loss:0.27544787526130676\n",
      "Epoch 15[330/625] Time:0.698, Train Loss:0.2180565595626831\n",
      "Epoch 15[331/625] Time:0.702, Train Loss:0.22627583146095276\n",
      "Epoch 15[332/625] Time:0.711, Train Loss:0.2734682559967041\n",
      "Epoch 15[333/625] Time:0.731, Train Loss:0.252248615026474\n",
      "Epoch 15[334/625] Time:0.693, Train Loss:0.31262969970703125\n",
      "Epoch 15[335/625] Time:0.694, Train Loss:0.3260800540447235\n",
      "Epoch 15[336/625] Time:0.695, Train Loss:0.2701718807220459\n",
      "Epoch 15[337/625] Time:0.694, Train Loss:0.2645111680030823\n",
      "Epoch 15[338/625] Time:0.694, Train Loss:0.27445974946022034\n",
      "Epoch 15[339/625] Time:0.694, Train Loss:0.2669582664966583\n",
      "Epoch 15[340/625] Time:0.695, Train Loss:0.2872474491596222\n",
      "Epoch 15[341/625] Time:0.694, Train Loss:0.24494625627994537\n",
      "Epoch 15[342/625] Time:0.695, Train Loss:0.32446447014808655\n",
      "Epoch 15[343/625] Time:0.731, Train Loss:0.3162913918495178\n",
      "Epoch 15[344/625] Time:0.701, Train Loss:0.2875957787036896\n",
      "Epoch 15[345/625] Time:0.707, Train Loss:0.3235308825969696\n",
      "Epoch 15[346/625] Time:0.705, Train Loss:0.22217358648777008\n",
      "Epoch 15[347/625] Time:0.703, Train Loss:0.31465306878089905\n",
      "Epoch 15[348/625] Time:0.703, Train Loss:0.3317243456840515\n",
      "Epoch 15[349/625] Time:0.703, Train Loss:0.1980341672897339\n",
      "Epoch 15[350/625] Time:0.702, Train Loss:0.18763285875320435\n",
      "Epoch 15[351/625] Time:0.704, Train Loss:0.20945973694324493\n",
      "Epoch 15[352/625] Time:0.702, Train Loss:0.35437604784965515\n",
      "Epoch 15[353/625] Time:0.705, Train Loss:0.2861672639846802\n",
      "Epoch 15[354/625] Time:0.702, Train Loss:0.29040876030921936\n",
      "Epoch 15[355/625] Time:0.702, Train Loss:0.2509850859642029\n",
      "Epoch 15[356/625] Time:0.704, Train Loss:0.23847174644470215\n",
      "Epoch 15[357/625] Time:0.694, Train Loss:0.21631112694740295\n",
      "Epoch 15[358/625] Time:0.696, Train Loss:0.27307015657424927\n",
      "Epoch 15[359/625] Time:0.732, Train Loss:0.32268640398979187\n",
      "Epoch 15[360/625] Time:0.703, Train Loss:0.31995174288749695\n",
      "Epoch 15[361/625] Time:0.704, Train Loss:0.36707261204719543\n",
      "Epoch 15[362/625] Time:0.704, Train Loss:0.2519751489162445\n",
      "Epoch 15[363/625] Time:0.703, Train Loss:0.29687920212745667\n",
      "Epoch 15[364/625] Time:0.702, Train Loss:0.32763004302978516\n",
      "Epoch 15[365/625] Time:0.703, Train Loss:0.2785724401473999\n",
      "Epoch 15[366/625] Time:0.702, Train Loss:0.21712549030780792\n",
      "Epoch 15[367/625] Time:0.702, Train Loss:0.20190496742725372\n",
      "Epoch 15[368/625] Time:0.703, Train Loss:0.3106135129928589\n",
      "Epoch 15[369/625] Time:0.712, Train Loss:0.2746562063694\n",
      "Epoch 15[370/625] Time:0.702, Train Loss:0.3537939190864563\n",
      "Epoch 15[371/625] Time:0.703, Train Loss:0.23224186897277832\n",
      "Epoch 15[372/625] Time:0.704, Train Loss:0.28183791041374207\n",
      "Epoch 15[373/625] Time:0.704, Train Loss:0.3213893473148346\n",
      "Epoch 15[374/625] Time:0.702, Train Loss:0.24814055860042572\n",
      "Epoch 15[375/625] Time:0.703, Train Loss:0.1979016810655594\n",
      "Epoch 15[376/625] Time:0.702, Train Loss:0.30632954835891724\n",
      "Epoch 15[377/625] Time:0.703, Train Loss:0.2942935824394226\n",
      "Epoch 15[378/625] Time:0.703, Train Loss:0.3392679691314697\n",
      "Epoch 15[379/625] Time:0.703, Train Loss:0.2529544532299042\n",
      "Epoch 15[380/625] Time:0.704, Train Loss:0.30623847246170044\n",
      "Epoch 15[381/625] Time:0.704, Train Loss:0.22590671479701996\n",
      "Epoch 15[382/625] Time:0.704, Train Loss:0.27170151472091675\n",
      "Epoch 15[383/625] Time:0.703, Train Loss:0.2962719798088074\n",
      "Epoch 15[384/625] Time:0.702, Train Loss:0.3280188739299774\n",
      "Epoch 15[385/625] Time:0.704, Train Loss:0.3438767194747925\n",
      "Epoch 15[386/625] Time:0.703, Train Loss:0.3186230957508087\n",
      "Epoch 15[387/625] Time:0.702, Train Loss:0.23889370262622833\n",
      "Epoch 15[388/625] Time:0.703, Train Loss:0.2614447772502899\n",
      "Epoch 15[389/625] Time:0.703, Train Loss:0.30094823241233826\n",
      "Epoch 15[390/625] Time:0.703, Train Loss:0.3090592920780182\n",
      "Epoch 15[391/625] Time:0.703, Train Loss:0.2144516110420227\n",
      "Epoch 15[392/625] Time:0.702, Train Loss:0.27824243903160095\n",
      "Epoch 15[393/625] Time:0.718, Train Loss:0.2677987813949585\n",
      "Epoch 15[394/625] Time:0.695, Train Loss:0.28004634380340576\n",
      "Epoch 15[395/625] Time:0.694, Train Loss:0.25136977434158325\n",
      "Epoch 15[396/625] Time:0.693, Train Loss:0.24289029836654663\n",
      "Epoch 15[397/625] Time:0.697, Train Loss:0.2756537199020386\n",
      "Epoch 15[398/625] Time:0.695, Train Loss:0.24983541667461395\n",
      "Epoch 15[399/625] Time:0.695, Train Loss:0.3599792420864105\n",
      "Epoch 15[400/625] Time:0.695, Train Loss:0.24474899470806122\n",
      "Epoch 15[401/625] Time:0.696, Train Loss:0.25310465693473816\n",
      "Epoch 15[402/625] Time:0.693, Train Loss:0.2790983021259308\n",
      "Epoch 15[403/625] Time:0.707, Train Loss:0.22013312578201294\n",
      "Epoch 15[404/625] Time:0.705, Train Loss:0.2485082745552063\n",
      "Epoch 15[405/625] Time:0.702, Train Loss:0.30364322662353516\n",
      "Epoch 15[406/625] Time:0.702, Train Loss:0.2525371015071869\n",
      "Epoch 15[407/625] Time:0.704, Train Loss:0.3400952219963074\n",
      "Epoch 15[408/625] Time:0.705, Train Loss:0.36003169417381287\n",
      "Epoch 15[409/625] Time:0.704, Train Loss:0.366462767124176\n",
      "Epoch 15[410/625] Time:0.702, Train Loss:0.3266902565956116\n",
      "Epoch 15[411/625] Time:0.702, Train Loss:0.17910902202129364\n",
      "Epoch 15[412/625] Time:0.701, Train Loss:0.22211283445358276\n",
      "Epoch 15[413/625] Time:0.704, Train Loss:0.1907159388065338\n",
      "Epoch 15[414/625] Time:0.702, Train Loss:0.3426345884799957\n",
      "Epoch 15[415/625] Time:0.703, Train Loss:0.28114786744117737\n",
      "Epoch 15[416/625] Time:0.711, Train Loss:0.2014814168214798\n",
      "Epoch 15[417/625] Time:0.705, Train Loss:0.2835213243961334\n",
      "Epoch 15[418/625] Time:0.708, Train Loss:0.26775774359703064\n",
      "Epoch 15[419/625] Time:0.702, Train Loss:0.26410624384880066\n",
      "Epoch 15[420/625] Time:0.704, Train Loss:0.2529560625553131\n",
      "Epoch 15[421/625] Time:0.702, Train Loss:0.3014741837978363\n",
      "Epoch 15[422/625] Time:0.701, Train Loss:0.3247976303100586\n",
      "Epoch 15[423/625] Time:0.703, Train Loss:0.2564746141433716\n",
      "Epoch 15[424/625] Time:0.704, Train Loss:0.3171593248844147\n",
      "Epoch 15[425/625] Time:0.702, Train Loss:0.304639607667923\n",
      "Epoch 15[426/625] Time:0.703, Train Loss:0.2863314747810364\n",
      "Epoch 15[427/625] Time:0.703, Train Loss:0.26920974254608154\n",
      "Epoch 15[428/625] Time:0.703, Train Loss:0.2923792600631714\n",
      "Epoch 15[429/625] Time:0.702, Train Loss:0.32741326093673706\n",
      "Epoch 15[430/625] Time:0.704, Train Loss:0.27349287271499634\n",
      "Epoch 15[431/625] Time:0.703, Train Loss:0.3457793593406677\n",
      "Epoch 15[432/625] Time:0.706, Train Loss:0.20866072177886963\n",
      "Epoch 15[433/625] Time:0.703, Train Loss:0.3649873435497284\n",
      "Epoch 15[434/625] Time:0.702, Train Loss:0.27681517601013184\n",
      "Epoch 15[435/625] Time:0.704, Train Loss:0.3585280179977417\n",
      "Epoch 15[436/625] Time:0.703, Train Loss:0.24571822583675385\n",
      "Epoch 15[437/625] Time:0.703, Train Loss:0.27107900381088257\n",
      "Epoch 15[438/625] Time:0.706, Train Loss:0.19938792288303375\n",
      "Epoch 15[439/625] Time:0.704, Train Loss:0.31413429975509644\n",
      "Epoch 15[440/625] Time:0.705, Train Loss:0.25982263684272766\n",
      "Epoch 15[441/625] Time:0.703, Train Loss:0.29333406686782837\n",
      "Epoch 15[442/625] Time:0.702, Train Loss:0.2944428026676178\n",
      "Epoch 15[443/625] Time:0.702, Train Loss:0.21381700038909912\n",
      "Epoch 15[444/625] Time:0.702, Train Loss:0.2722194790840149\n",
      "Epoch 15[445/625] Time:0.702, Train Loss:0.2338746339082718\n",
      "Epoch 15[446/625] Time:0.703, Train Loss:0.30504700541496277\n",
      "Epoch 15[447/625] Time:0.704, Train Loss:0.24769867956638336\n",
      "Epoch 15[448/625] Time:0.704, Train Loss:0.2840719223022461\n",
      "Epoch 15[449/625] Time:0.702, Train Loss:0.2537615895271301\n",
      "Epoch 15[450/625] Time:0.703, Train Loss:0.2918279469013214\n",
      "Epoch 15[451/625] Time:0.703, Train Loss:0.2911718785762787\n",
      "Epoch 15[452/625] Time:0.702, Train Loss:0.22669799625873566\n",
      "Epoch 15[453/625] Time:0.705, Train Loss:0.2380099594593048\n",
      "Epoch 15[454/625] Time:0.704, Train Loss:0.2520362138748169\n",
      "Epoch 15[455/625] Time:0.704, Train Loss:0.28615617752075195\n",
      "Epoch 15[456/625] Time:0.704, Train Loss:0.3543908894062042\n",
      "Epoch 15[457/625] Time:0.703, Train Loss:0.28584519028663635\n",
      "Epoch 15[458/625] Time:0.704, Train Loss:0.27989649772644043\n",
      "Epoch 15[459/625] Time:0.704, Train Loss:0.2314598709344864\n",
      "Epoch 15[460/625] Time:0.703, Train Loss:0.30767178535461426\n",
      "Epoch 15[461/625] Time:0.703, Train Loss:0.2998078167438507\n",
      "Epoch 15[462/625] Time:0.704, Train Loss:0.25461074709892273\n",
      "Epoch 15[463/625] Time:0.703, Train Loss:0.4182543456554413\n",
      "Epoch 15[464/625] Time:0.703, Train Loss:0.42087069153785706\n",
      "Epoch 15[465/625] Time:0.703, Train Loss:0.31041452288627625\n",
      "Epoch 15[466/625] Time:0.702, Train Loss:0.30563652515411377\n",
      "Epoch 15[467/625] Time:0.703, Train Loss:0.2297520935535431\n",
      "Epoch 15[468/625] Time:0.702, Train Loss:0.3021196126937866\n",
      "Epoch 15[469/625] Time:0.703, Train Loss:0.30727419257164\n",
      "Epoch 15[470/625] Time:0.703, Train Loss:0.2995523512363434\n",
      "Epoch 15[471/625] Time:0.702, Train Loss:0.2632875442504883\n",
      "Epoch 15[472/625] Time:0.703, Train Loss:0.29155734181404114\n",
      "Epoch 15[473/625] Time:0.703, Train Loss:0.358058899641037\n",
      "Epoch 15[474/625] Time:0.722, Train Loss:0.23334066569805145\n",
      "Epoch 15[475/625] Time:0.692, Train Loss:0.19979698956012726\n",
      "Epoch 15[476/625] Time:0.694, Train Loss:0.2717829644680023\n",
      "Epoch 15[477/625] Time:0.693, Train Loss:0.2750329077243805\n",
      "Epoch 15[478/625] Time:0.695, Train Loss:0.24421197175979614\n",
      "Epoch 15[479/625] Time:0.694, Train Loss:0.23412972688674927\n",
      "Epoch 15[480/625] Time:0.697, Train Loss:0.2268567979335785\n",
      "Epoch 15[481/625] Time:0.703, Train Loss:0.28693437576293945\n",
      "Epoch 15[482/625] Time:0.703, Train Loss:0.2778128981590271\n",
      "Epoch 15[483/625] Time:0.703, Train Loss:0.28223761916160583\n",
      "Epoch 15[484/625] Time:0.703, Train Loss:0.35992631316185\n",
      "Epoch 15[485/625] Time:0.701, Train Loss:0.2536042630672455\n",
      "Epoch 15[486/625] Time:0.702, Train Loss:0.272975891828537\n",
      "Epoch 15[487/625] Time:0.702, Train Loss:0.25053292512893677\n",
      "Epoch 15[488/625] Time:0.704, Train Loss:0.28017330169677734\n",
      "Epoch 15[489/625] Time:0.747, Train Loss:0.26617562770843506\n",
      "Epoch 15[490/625] Time:0.694, Train Loss:0.37529414892196655\n",
      "Epoch 15[491/625] Time:0.695, Train Loss:0.2981441915035248\n",
      "Epoch 15[492/625] Time:0.704, Train Loss:0.32596707344055176\n",
      "Epoch 15[493/625] Time:0.703, Train Loss:0.3512575626373291\n",
      "Epoch 15[494/625] Time:0.703, Train Loss:0.3341798484325409\n",
      "Epoch 15[495/625] Time:0.703, Train Loss:0.2938414514064789\n",
      "Epoch 15[496/625] Time:0.705, Train Loss:0.2811097204685211\n",
      "Epoch 15[497/625] Time:0.702, Train Loss:0.2935686707496643\n",
      "Epoch 15[498/625] Time:0.704, Train Loss:0.2919675409793854\n",
      "Epoch 15[499/625] Time:0.706, Train Loss:0.18507277965545654\n",
      "Epoch 15[500/625] Time:0.695, Train Loss:0.29113680124282837\n",
      "Epoch 15[501/625] Time:0.694, Train Loss:0.29405272006988525\n",
      "Epoch 15[502/625] Time:0.696, Train Loss:0.2504425644874573\n",
      "Epoch 15[503/625] Time:0.733, Train Loss:0.29161134362220764\n",
      "Epoch 15[504/625] Time:0.701, Train Loss:0.27800649404525757\n",
      "Epoch 15[505/625] Time:0.703, Train Loss:0.28823617100715637\n",
      "Epoch 15[506/625] Time:0.704, Train Loss:0.25144532322883606\n",
      "Epoch 15[507/625] Time:0.703, Train Loss:0.2976033389568329\n",
      "Epoch 15[508/625] Time:0.703, Train Loss:0.23178604245185852\n",
      "Epoch 15[509/625] Time:0.703, Train Loss:0.2676115930080414\n",
      "Epoch 15[510/625] Time:0.703, Train Loss:0.24280819296836853\n",
      "Epoch 15[511/625] Time:0.702, Train Loss:0.32748177647590637\n",
      "Epoch 15[512/625] Time:0.704, Train Loss:0.39605140686035156\n",
      "Epoch 15[513/625] Time:0.703, Train Loss:0.2988314628601074\n",
      "Epoch 15[514/625] Time:0.704, Train Loss:0.29705920815467834\n",
      "Epoch 15[515/625] Time:0.693, Train Loss:0.3227051794528961\n",
      "Epoch 15[516/625] Time:0.696, Train Loss:0.2652510404586792\n",
      "Epoch 15[517/625] Time:0.704, Train Loss:0.28613096475601196\n",
      "Epoch 15[518/625] Time:0.702, Train Loss:0.2979600727558136\n",
      "Epoch 15[519/625] Time:0.706, Train Loss:0.3544343411922455\n",
      "Epoch 15[520/625] Time:0.702, Train Loss:0.40497493743896484\n",
      "Epoch 15[521/625] Time:0.703, Train Loss:0.2173566371202469\n",
      "Epoch 15[522/625] Time:0.704, Train Loss:0.2989456355571747\n",
      "Epoch 15[523/625] Time:0.706, Train Loss:0.26532670855522156\n",
      "Epoch 15[524/625] Time:0.703, Train Loss:0.28885191679000854\n",
      "Epoch 15[525/625] Time:0.704, Train Loss:0.2807309031486511\n",
      "Epoch 15[526/625] Time:0.702, Train Loss:0.27847757935523987\n",
      "Epoch 15[527/625] Time:0.703, Train Loss:0.21842871606349945\n",
      "Epoch 15[528/625] Time:0.703, Train Loss:0.2983427047729492\n",
      "Epoch 15[529/625] Time:0.702, Train Loss:0.2666279077529907\n",
      "Epoch 15[530/625] Time:0.703, Train Loss:0.24962423741817474\n",
      "Epoch 15[531/625] Time:0.702, Train Loss:0.26034703850746155\n",
      "Epoch 15[532/625] Time:0.702, Train Loss:0.2515154480934143\n",
      "Epoch 15[533/625] Time:0.702, Train Loss:0.33558544516563416\n",
      "Epoch 15[534/625] Time:0.703, Train Loss:0.25550639629364014\n",
      "Epoch 15[535/625] Time:0.703, Train Loss:0.2255251556634903\n",
      "Epoch 15[536/625] Time:0.702, Train Loss:0.2628304958343506\n",
      "Epoch 15[537/625] Time:0.702, Train Loss:0.2454555779695511\n",
      "Epoch 15[538/625] Time:0.702, Train Loss:0.3476203978061676\n",
      "Epoch 15[539/625] Time:0.704, Train Loss:0.21919527649879456\n",
      "Epoch 15[540/625] Time:0.704, Train Loss:0.2385469526052475\n",
      "Epoch 15[541/625] Time:0.703, Train Loss:0.2916584014892578\n",
      "Epoch 15[542/625] Time:0.702, Train Loss:0.3607262372970581\n",
      "Epoch 15[543/625] Time:0.702, Train Loss:0.35440415143966675\n",
      "Epoch 15[544/625] Time:0.704, Train Loss:0.30793020129203796\n",
      "Epoch 15[545/625] Time:0.703, Train Loss:0.21533425152301788\n",
      "Epoch 15[546/625] Time:0.702, Train Loss:0.18916530907154083\n",
      "Epoch 15[547/625] Time:0.702, Train Loss:0.27417105436325073\n",
      "Epoch 15[548/625] Time:0.703, Train Loss:0.29740700125694275\n",
      "Epoch 15[549/625] Time:0.702, Train Loss:0.2542499303817749\n",
      "Epoch 15[550/625] Time:0.703, Train Loss:0.2769283652305603\n",
      "Epoch 15[551/625] Time:0.702, Train Loss:0.2401568740606308\n",
      "Epoch 15[552/625] Time:0.703, Train Loss:0.22610355913639069\n",
      "Epoch 15[553/625] Time:0.703, Train Loss:0.27855509519577026\n",
      "Epoch 15[554/625] Time:0.702, Train Loss:0.2835880517959595\n",
      "Epoch 15[555/625] Time:0.702, Train Loss:0.31059369444847107\n",
      "Epoch 15[556/625] Time:0.703, Train Loss:0.2771592140197754\n",
      "Epoch 15[557/625] Time:0.704, Train Loss:0.3628294765949249\n",
      "Epoch 15[558/625] Time:0.702, Train Loss:0.27408674359321594\n",
      "Epoch 15[559/625] Time:0.704, Train Loss:0.20714083313941956\n",
      "Epoch 15[560/625] Time:0.703, Train Loss:0.29987430572509766\n",
      "Epoch 15[561/625] Time:0.702, Train Loss:0.3149811029434204\n",
      "Epoch 15[562/625] Time:0.703, Train Loss:0.2859649062156677\n",
      "Epoch 15[563/625] Time:0.703, Train Loss:0.35767000913619995\n",
      "Epoch 15[564/625] Time:0.704, Train Loss:0.26471051573753357\n",
      "Epoch 15[565/625] Time:0.704, Train Loss:0.30282512307167053\n",
      "Epoch 15[566/625] Time:0.703, Train Loss:0.3148384392261505\n",
      "Epoch 15[567/625] Time:0.704, Train Loss:0.3121184706687927\n",
      "Epoch 15[568/625] Time:0.705, Train Loss:0.31740105152130127\n",
      "Epoch 15[569/625] Time:0.703, Train Loss:0.32390010356903076\n",
      "Epoch 15[570/625] Time:0.703, Train Loss:0.27866289019584656\n",
      "Epoch 15[571/625] Time:0.702, Train Loss:0.2510651648044586\n",
      "Epoch 15[572/625] Time:0.703, Train Loss:0.32823655009269714\n",
      "Epoch 15[573/625] Time:0.702, Train Loss:0.2528283894062042\n",
      "Epoch 15[574/625] Time:0.702, Train Loss:0.3251491189002991\n",
      "Epoch 15[575/625] Time:0.702, Train Loss:0.3205997347831726\n",
      "Epoch 15[576/625] Time:0.717, Train Loss:0.29112401604652405\n",
      "Epoch 15[577/625] Time:0.695, Train Loss:0.2825658321380615\n",
      "Epoch 15[578/625] Time:0.694, Train Loss:0.3862980604171753\n",
      "Epoch 15[579/625] Time:0.702, Train Loss:0.31416597962379456\n",
      "Epoch 15[580/625] Time:0.701, Train Loss:0.24384699761867523\n",
      "Epoch 15[581/625] Time:0.706, Train Loss:0.2818868160247803\n",
      "Epoch 15[582/625] Time:0.709, Train Loss:0.22867417335510254\n",
      "Epoch 15[583/625] Time:0.693, Train Loss:0.23918986320495605\n",
      "Epoch 15[584/625] Time:0.705, Train Loss:0.23956243693828583\n",
      "Epoch 15[585/625] Time:0.704, Train Loss:0.355999231338501\n",
      "Epoch 15[586/625] Time:0.704, Train Loss:0.24952024221420288\n",
      "Epoch 15[587/625] Time:0.705, Train Loss:0.31974080204963684\n",
      "Epoch 15[588/625] Time:0.703, Train Loss:0.27302420139312744\n",
      "Epoch 15[589/625] Time:0.703, Train Loss:0.21951378881931305\n",
      "Epoch 15[590/625] Time:0.703, Train Loss:0.24192632734775543\n",
      "Epoch 15[591/625] Time:0.704, Train Loss:0.26085561513900757\n",
      "Epoch 15[592/625] Time:0.702, Train Loss:0.2790575325489044\n",
      "Epoch 15[593/625] Time:0.702, Train Loss:0.2752368152141571\n",
      "Epoch 15[594/625] Time:0.703, Train Loss:0.27289828658103943\n",
      "Epoch 15[595/625] Time:0.703, Train Loss:0.2905426621437073\n",
      "Epoch 15[596/625] Time:0.703, Train Loss:0.26679155230522156\n",
      "Epoch 15[597/625] Time:0.703, Train Loss:0.2862209379673004\n",
      "Epoch 15[598/625] Time:0.703, Train Loss:0.32978278398513794\n",
      "Epoch 15[599/625] Time:0.703, Train Loss:0.24603042006492615\n",
      "Epoch 15[600/625] Time:0.703, Train Loss:0.24084597826004028\n",
      "Epoch 15[601/625] Time:0.702, Train Loss:0.317149817943573\n",
      "Epoch 15[602/625] Time:0.702, Train Loss:0.2504780888557434\n",
      "Epoch 15[603/625] Time:0.702, Train Loss:0.25760966539382935\n",
      "Epoch 15[604/625] Time:0.702, Train Loss:0.2655187249183655\n",
      "Epoch 15[605/625] Time:0.704, Train Loss:0.26441434025764465\n",
      "Epoch 15[606/625] Time:0.704, Train Loss:0.2566095292568207\n",
      "Epoch 15[607/625] Time:0.702, Train Loss:0.28165483474731445\n",
      "Epoch 15[608/625] Time:0.704, Train Loss:0.29036450386047363\n",
      "Epoch 15[609/625] Time:0.703, Train Loss:0.2879837453365326\n",
      "Epoch 15[610/625] Time:0.703, Train Loss:0.23561283946037292\n",
      "Epoch 15[611/625] Time:0.704, Train Loss:0.2803477644920349\n",
      "Epoch 15[612/625] Time:0.703, Train Loss:0.23753568530082703\n",
      "Epoch 15[613/625] Time:0.703, Train Loss:0.23895013332366943\n",
      "Epoch 15[614/625] Time:0.703, Train Loss:0.21516075730323792\n",
      "Epoch 15[615/625] Time:0.705, Train Loss:0.3841937184333801\n",
      "Epoch 15[616/625] Time:0.703, Train Loss:0.28886884450912476\n",
      "Epoch 15[617/625] Time:0.702, Train Loss:0.32029223442077637\n",
      "Epoch 15[618/625] Time:0.703, Train Loss:0.21278521418571472\n",
      "Epoch 15[619/625] Time:0.703, Train Loss:0.24696126580238342\n",
      "Epoch 15[620/625] Time:0.703, Train Loss:0.3488011956214905\n",
      "Epoch 15[621/625] Time:0.703, Train Loss:0.3435942530632019\n",
      "Epoch 15[622/625] Time:0.703, Train Loss:0.2356482893228531\n",
      "Epoch 15[623/625] Time:0.709, Train Loss:0.3167184293270111\n",
      "Epoch 15[624/625] Time:0.703, Train Loss:0.2909235954284668\n",
      "Epoch 15[0/78] Val Loss:0.0998595654964447\n",
      "Epoch 15[1/78] Val Loss:0.10796404629945755\n",
      "Epoch 15[2/78] Val Loss:0.11626148223876953\n",
      "Epoch 15[3/78] Val Loss:0.09707760065793991\n",
      "Epoch 15[4/78] Val Loss:0.16707193851470947\n",
      "Epoch 15[5/78] Val Loss:0.09774000197649002\n",
      "Epoch 15[6/78] Val Loss:0.09536965936422348\n",
      "Epoch 15[7/78] Val Loss:0.14352481067180634\n",
      "Epoch 15[8/78] Val Loss:0.11083155870437622\n",
      "Epoch 15[9/78] Val Loss:0.15746480226516724\n",
      "Epoch 15[10/78] Val Loss:0.12091471254825592\n",
      "Epoch 15[11/78] Val Loss:0.19686384499073029\n",
      "Epoch 15[12/78] Val Loss:0.14071869850158691\n",
      "Epoch 15[13/78] Val Loss:0.11779245734214783\n",
      "Epoch 15[14/78] Val Loss:0.09898710250854492\n",
      "Epoch 15[15/78] Val Loss:0.07356245070695877\n",
      "Epoch 15[16/78] Val Loss:0.1208016499876976\n",
      "Epoch 15[17/78] Val Loss:0.10340318828821182\n",
      "Epoch 15[18/78] Val Loss:0.10930386930704117\n",
      "Epoch 15[19/78] Val Loss:0.16294826567173004\n",
      "Epoch 15[20/78] Val Loss:0.08863772451877594\n",
      "Epoch 15[21/78] Val Loss:0.47122034430503845\n",
      "Epoch 15[22/78] Val Loss:0.7168896198272705\n",
      "Epoch 15[23/78] Val Loss:0.47086384892463684\n",
      "Epoch 15[24/78] Val Loss:0.32272228598594666\n",
      "Epoch 15[25/78] Val Loss:0.4246942400932312\n",
      "Epoch 15[26/78] Val Loss:0.3965054452419281\n",
      "Epoch 15[27/78] Val Loss:0.35895204544067383\n",
      "Epoch 15[28/78] Val Loss:0.37191975116729736\n",
      "Epoch 15[29/78] Val Loss:0.48241519927978516\n",
      "Epoch 15[30/78] Val Loss:1.9985406398773193\n",
      "Epoch 15[31/78] Val Loss:1.7861510515213013\n",
      "Epoch 15[32/78] Val Loss:1.2467091083526611\n",
      "Epoch 15[33/78] Val Loss:0.36244997382164\n",
      "Epoch 15[34/78] Val Loss:0.2854527533054352\n",
      "Epoch 15[35/78] Val Loss:0.3758563995361328\n",
      "Epoch 15[36/78] Val Loss:0.2976692318916321\n",
      "Epoch 15[37/78] Val Loss:0.4076237976551056\n",
      "Epoch 15[38/78] Val Loss:0.1463652402162552\n",
      "Epoch 15[39/78] Val Loss:0.13559269905090332\n",
      "Epoch 15[40/78] Val Loss:0.15206216275691986\n",
      "Epoch 15[41/78] Val Loss:0.15152506530284882\n",
      "Epoch 15[42/78] Val Loss:0.13782083988189697\n",
      "Epoch 15[43/78] Val Loss:0.08977357298135757\n",
      "Epoch 15[44/78] Val Loss:0.09729047864675522\n",
      "Epoch 15[45/78] Val Loss:0.10066457092761993\n",
      "Epoch 15[46/78] Val Loss:0.1260867863893509\n",
      "Epoch 15[47/78] Val Loss:0.09738107025623322\n",
      "Epoch 15[48/78] Val Loss:0.14628413319587708\n",
      "Epoch 15[49/78] Val Loss:0.10940193384885788\n",
      "Epoch 15[50/78] Val Loss:0.0900474563241005\n",
      "Epoch 15[51/78] Val Loss:0.09675831347703934\n",
      "Epoch 15[52/78] Val Loss:0.11333853006362915\n",
      "Epoch 15[53/78] Val Loss:0.11911970376968384\n",
      "Epoch 15[54/78] Val Loss:0.08703522384166718\n",
      "Epoch 15[55/78] Val Loss:0.07794688642024994\n",
      "Epoch 15[56/78] Val Loss:0.3258580267429352\n",
      "Epoch 15[57/78] Val Loss:0.2989887595176697\n",
      "Epoch 15[58/78] Val Loss:0.27652835845947266\n",
      "Epoch 15[59/78] Val Loss:0.3808737099170685\n",
      "Epoch 15[60/78] Val Loss:0.3678457736968994\n",
      "Epoch 15[61/78] Val Loss:0.4254942536354065\n",
      "Epoch 15[62/78] Val Loss:0.40116754174232483\n",
      "Epoch 15[63/78] Val Loss:0.2136729210615158\n",
      "Epoch 15[64/78] Val Loss:0.08328257501125336\n",
      "Epoch 15[65/78] Val Loss:0.06828498840332031\n",
      "Epoch 15[66/78] Val Loss:0.08243576437234879\n",
      "Epoch 15[67/78] Val Loss:0.09875050187110901\n",
      "Epoch 15[68/78] Val Loss:0.3717418909072876\n",
      "Epoch 15[69/78] Val Loss:0.3601404130458832\n",
      "Epoch 15[70/78] Val Loss:0.35513630509376526\n",
      "Epoch 15[71/78] Val Loss:0.23210273683071136\n",
      "Epoch 15[72/78] Val Loss:0.07123760879039764\n",
      "Epoch 15[73/78] Val Loss:0.06378982216119766\n",
      "Epoch 15[74/78] Val Loss:0.3635597229003906\n",
      "Epoch 15[75/78] Val Loss:0.464059442281723\n",
      "Epoch 15[76/78] Val Loss:0.4247937500476837\n",
      "Epoch 15[77/78] Val Loss:0.45077791810035706\n",
      "Epoch 15[78/78] Val Loss:0.46355628967285156\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93     15691\n",
      "           1       0.78      0.71      0.74      4309\n",
      "\n",
      "    accuracy                           0.89     20000\n",
      "   macro avg       0.85      0.83      0.84     20000\n",
      "weighted avg       0.89      0.89      0.89     20000\n",
      "\n",
      "Epoch 15: Train Loss 0.28089786612987516, Val Loss 0.28233729369747335, Train Time 789.9261965751648, Val Time 36.24199628829956\n",
      "New best model at epoch 15\n",
      "Epoch 16[0/625] Time:0.692, Train Loss:0.31139716506004333\n",
      "Epoch 16[1/625] Time:0.703, Train Loss:0.2407362461090088\n",
      "Epoch 16[2/625] Time:0.703, Train Loss:0.3797019124031067\n",
      "Epoch 16[3/625] Time:0.7, Train Loss:0.35484418272972107\n",
      "Epoch 16[4/625] Time:0.717, Train Loss:0.32565224170684814\n",
      "Epoch 16[5/625] Time:0.694, Train Loss:0.2592037618160248\n",
      "Epoch 16[6/625] Time:0.693, Train Loss:0.2605521082878113\n",
      "Epoch 16[7/625] Time:0.703, Train Loss:0.24434871971607208\n",
      "Epoch 16[8/625] Time:0.703, Train Loss:0.349103718996048\n",
      "Epoch 16[9/625] Time:0.702, Train Loss:0.3354383409023285\n",
      "Epoch 16[10/625] Time:0.703, Train Loss:0.28233274817466736\n",
      "Epoch 16[11/625] Time:0.703, Train Loss:0.24146056175231934\n",
      "Epoch 16[12/625] Time:0.726, Train Loss:0.2760237753391266\n",
      "Epoch 16[13/625] Time:0.694, Train Loss:0.26906293630599976\n",
      "Epoch 16[14/625] Time:0.694, Train Loss:0.25455379486083984\n",
      "Epoch 16[15/625] Time:0.694, Train Loss:0.24240465462207794\n",
      "Epoch 16[16/625] Time:0.695, Train Loss:0.30149999260902405\n",
      "Epoch 16[17/625] Time:0.696, Train Loss:0.3282986581325531\n",
      "Epoch 16[18/625] Time:0.694, Train Loss:0.284212201833725\n",
      "Epoch 16[19/625] Time:0.694, Train Loss:0.2409607619047165\n",
      "Epoch 16[20/625] Time:0.703, Train Loss:0.2525872588157654\n",
      "Epoch 16[21/625] Time:0.703, Train Loss:0.3157554864883423\n",
      "Epoch 16[22/625] Time:0.704, Train Loss:0.24468909204006195\n",
      "Epoch 16[23/625] Time:0.704, Train Loss:0.2939542531967163\n",
      "Epoch 16[24/625] Time:0.703, Train Loss:0.23326192796230316\n",
      "Epoch 16[25/625] Time:0.703, Train Loss:0.3001258075237274\n",
      "Epoch 16[26/625] Time:0.702, Train Loss:0.23743969202041626\n",
      "Epoch 16[27/625] Time:0.702, Train Loss:0.2590571343898773\n",
      "Epoch 16[28/625] Time:0.702, Train Loss:0.21175797283649445\n",
      "Epoch 16[29/625] Time:0.703, Train Loss:0.25995156168937683\n",
      "Epoch 16[30/625] Time:0.702, Train Loss:0.28499606251716614\n",
      "Epoch 16[31/625] Time:0.713, Train Loss:0.3341120779514313\n",
      "Epoch 16[32/625] Time:0.713, Train Loss:0.2868489623069763\n",
      "Epoch 16[33/625] Time:0.703, Train Loss:0.25521910190582275\n",
      "Epoch 16[34/625] Time:0.703, Train Loss:0.2037021368741989\n",
      "Epoch 16[35/625] Time:0.705, Train Loss:0.26951101422309875\n",
      "Epoch 16[36/625] Time:0.704, Train Loss:0.258230984210968\n",
      "Epoch 16[37/625] Time:0.703, Train Loss:0.2856771945953369\n",
      "Epoch 16[38/625] Time:0.704, Train Loss:0.3281835615634918\n",
      "Epoch 16[39/625] Time:0.705, Train Loss:0.27259665727615356\n",
      "Epoch 16[40/625] Time:0.702, Train Loss:0.18297415971755981\n",
      "Epoch 16[41/625] Time:0.703, Train Loss:0.28273099660873413\n",
      "Epoch 16[42/625] Time:0.703, Train Loss:0.3432093858718872\n",
      "Epoch 16[43/625] Time:0.702, Train Loss:0.22555634379386902\n",
      "Epoch 16[44/625] Time:0.703, Train Loss:0.29353097081184387\n",
      "Epoch 16[45/625] Time:0.705, Train Loss:0.30105090141296387\n",
      "Epoch 16[46/625] Time:0.704, Train Loss:0.3470953404903412\n",
      "Epoch 16[47/625] Time:0.706, Train Loss:0.2999074161052704\n",
      "Epoch 16[48/625] Time:0.703, Train Loss:0.2898685336112976\n",
      "Epoch 16[49/625] Time:0.705, Train Loss:0.28292691707611084\n",
      "Epoch 16[50/625] Time:0.705, Train Loss:0.2473633885383606\n",
      "Epoch 16[51/625] Time:0.703, Train Loss:0.22771145403385162\n",
      "Epoch 16[52/625] Time:0.705, Train Loss:0.3093206286430359\n",
      "Epoch 16[53/625] Time:0.704, Train Loss:0.26553356647491455\n",
      "Epoch 16[54/625] Time:0.703, Train Loss:0.23667113482952118\n",
      "Epoch 16[55/625] Time:0.705, Train Loss:0.2511431574821472\n",
      "Epoch 16[56/625] Time:0.705, Train Loss:0.226164773106575\n",
      "Epoch 16[57/625] Time:0.689, Train Loss:0.2695753574371338\n",
      "Epoch 16[58/625] Time:0.692, Train Loss:0.32540473341941833\n",
      "Epoch 16[59/625] Time:0.693, Train Loss:0.26089516282081604\n",
      "Epoch 16[60/625] Time:0.692, Train Loss:0.28621259331703186\n",
      "Epoch 16[61/625] Time:0.694, Train Loss:0.1903638392686844\n",
      "Epoch 16[62/625] Time:0.694, Train Loss:0.2236609309911728\n",
      "Epoch 16[63/625] Time:0.696, Train Loss:0.23551245033740997\n",
      "Epoch 16[64/625] Time:0.694, Train Loss:0.26800623536109924\n",
      "Epoch 16[65/625] Time:0.71, Train Loss:0.2509828209877014\n",
      "Epoch 16[66/625] Time:0.703, Train Loss:0.2502535283565521\n",
      "Epoch 16[67/625] Time:0.702, Train Loss:0.2509249448776245\n",
      "Epoch 16[68/625] Time:0.703, Train Loss:0.2717776298522949\n",
      "Epoch 16[69/625] Time:0.703, Train Loss:0.3089047074317932\n",
      "Epoch 16[70/625] Time:0.745, Train Loss:0.2695554792881012\n",
      "Epoch 16[71/625] Time:0.692, Train Loss:0.4192803204059601\n",
      "Epoch 16[72/625] Time:0.694, Train Loss:0.2817651927471161\n",
      "Epoch 16[73/625] Time:0.692, Train Loss:0.30628320574760437\n",
      "Epoch 16[74/625] Time:0.691, Train Loss:0.28472480177879333\n",
      "Epoch 16[75/625] Time:0.693, Train Loss:0.22776877880096436\n",
      "Epoch 16[76/625] Time:0.693, Train Loss:0.2655523121356964\n",
      "Epoch 16[77/625] Time:0.704, Train Loss:0.21872152388095856\n",
      "Epoch 16[78/625] Time:0.71, Train Loss:0.3004194498062134\n",
      "Epoch 16[79/625] Time:0.701, Train Loss:0.24547305703163147\n",
      "Epoch 16[80/625] Time:0.703, Train Loss:0.26941534876823425\n",
      "Epoch 16[81/625] Time:0.704, Train Loss:0.38043656945228577\n",
      "Epoch 16[82/625] Time:0.705, Train Loss:0.35756829380989075\n",
      "Epoch 16[83/625] Time:0.703, Train Loss:0.2245323807001114\n",
      "Epoch 16[84/625] Time:0.702, Train Loss:0.2565913498401642\n",
      "Epoch 16[85/625] Time:0.702, Train Loss:0.26048988103866577\n",
      "Epoch 16[86/625] Time:0.712, Train Loss:0.2948337495326996\n",
      "Epoch 16[87/625] Time:0.702, Train Loss:0.30894577503204346\n",
      "Epoch 16[88/625] Time:0.702, Train Loss:0.24356038868427277\n",
      "Epoch 16[89/625] Time:0.703, Train Loss:0.3615032136440277\n",
      "Epoch 16[90/625] Time:0.702, Train Loss:0.32072293758392334\n",
      "Epoch 16[91/625] Time:0.702, Train Loss:0.20445887744426727\n",
      "Epoch 16[92/625] Time:0.704, Train Loss:0.36103636026382446\n",
      "Epoch 16[93/625] Time:0.702, Train Loss:0.38325169682502747\n",
      "Epoch 16[94/625] Time:0.704, Train Loss:0.2312442809343338\n",
      "Epoch 16[95/625] Time:0.702, Train Loss:0.2004973143339157\n",
      "Epoch 16[96/625] Time:0.703, Train Loss:0.35328441858291626\n",
      "Epoch 16[97/625] Time:0.703, Train Loss:0.28665316104888916\n",
      "Epoch 16[98/625] Time:0.703, Train Loss:0.26460540294647217\n",
      "Epoch 16[99/625] Time:0.731, Train Loss:0.2770588994026184\n",
      "Epoch 16[100/625] Time:0.695, Train Loss:0.2783163785934448\n",
      "Epoch 16[101/625] Time:0.694, Train Loss:0.28013062477111816\n",
      "Epoch 16[102/625] Time:0.695, Train Loss:0.3306528329849243\n",
      "Epoch 16[103/625] Time:0.694, Train Loss:0.2385280430316925\n",
      "Epoch 16[104/625] Time:0.702, Train Loss:0.3126661777496338\n",
      "Epoch 16[105/625] Time:0.702, Train Loss:0.2031896710395813\n",
      "Epoch 16[106/625] Time:0.703, Train Loss:0.4038779139518738\n",
      "Epoch 16[107/625] Time:0.702, Train Loss:0.3110297620296478\n",
      "Epoch 16[108/625] Time:0.703, Train Loss:0.2987145483493805\n",
      "Epoch 16[109/625] Time:0.702, Train Loss:0.26345914602279663\n",
      "Epoch 16[110/625] Time:0.703, Train Loss:0.20261797308921814\n",
      "Epoch 16[111/625] Time:0.706, Train Loss:0.25562238693237305\n",
      "Epoch 16[112/625] Time:0.705, Train Loss:0.26266762614250183\n",
      "Epoch 16[113/625] Time:0.704, Train Loss:0.2967836856842041\n",
      "Epoch 16[114/625] Time:0.704, Train Loss:0.2816191613674164\n",
      "Epoch 16[115/625] Time:0.703, Train Loss:0.2531575858592987\n",
      "Epoch 16[116/625] Time:0.704, Train Loss:0.2559548616409302\n",
      "Epoch 16[117/625] Time:0.706, Train Loss:0.22181245684623718\n",
      "Epoch 16[118/625] Time:0.707, Train Loss:0.23216478526592255\n",
      "Epoch 16[119/625] Time:0.702, Train Loss:0.3179311752319336\n",
      "Epoch 16[120/625] Time:0.703, Train Loss:0.21864093840122223\n",
      "Epoch 16[121/625] Time:0.704, Train Loss:0.3040538430213928\n",
      "Epoch 16[122/625] Time:0.742, Train Loss:0.32781532406806946\n",
      "Epoch 16[123/625] Time:0.692, Train Loss:0.28061312437057495\n",
      "Epoch 16[124/625] Time:0.694, Train Loss:0.2174605429172516\n",
      "Epoch 16[125/625] Time:0.694, Train Loss:0.28825974464416504\n",
      "Epoch 16[126/625] Time:0.693, Train Loss:0.1998404860496521\n",
      "Epoch 16[127/625] Time:0.717, Train Loss:0.32401472330093384\n",
      "Epoch 16[128/625] Time:0.703, Train Loss:0.3751346170902252\n",
      "Epoch 16[129/625] Time:0.703, Train Loss:0.207087442278862\n",
      "Epoch 16[130/625] Time:0.705, Train Loss:0.2042023092508316\n",
      "Epoch 16[131/625] Time:0.704, Train Loss:0.22189563512802124\n",
      "Epoch 16[132/625] Time:0.704, Train Loss:0.2926879823207855\n",
      "Epoch 16[133/625] Time:0.703, Train Loss:0.23966771364212036\n",
      "Epoch 16[134/625] Time:0.704, Train Loss:0.4123275876045227\n",
      "Epoch 16[135/625] Time:0.704, Train Loss:0.30273813009262085\n",
      "Epoch 16[136/625] Time:0.703, Train Loss:0.27522796392440796\n",
      "Epoch 16[137/625] Time:0.703, Train Loss:0.28558847308158875\n",
      "Epoch 16[138/625] Time:0.703, Train Loss:0.2361266016960144\n",
      "Epoch 16[139/625] Time:0.704, Train Loss:0.22556905448436737\n",
      "Epoch 16[140/625] Time:0.704, Train Loss:0.2975379526615143\n",
      "Epoch 16[141/625] Time:0.703, Train Loss:0.29430967569351196\n",
      "Epoch 16[142/625] Time:0.704, Train Loss:0.28812432289123535\n",
      "Epoch 16[143/625] Time:0.705, Train Loss:0.2977430820465088\n",
      "Epoch 16[144/625] Time:0.705, Train Loss:0.21932531893253326\n",
      "Epoch 16[145/625] Time:0.704, Train Loss:0.24531519412994385\n",
      "Epoch 16[146/625] Time:0.705, Train Loss:0.29450035095214844\n",
      "Epoch 16[147/625] Time:0.706, Train Loss:0.3341044485569\n",
      "Epoch 16[148/625] Time:0.704, Train Loss:0.24269483983516693\n",
      "Epoch 16[149/625] Time:0.704, Train Loss:0.20144376158714294\n",
      "Epoch 16[150/625] Time:0.704, Train Loss:0.34779882431030273\n",
      "Epoch 16[151/625] Time:0.708, Train Loss:0.2598326504230499\n",
      "Epoch 16[152/625] Time:0.706, Train Loss:0.3092218339443207\n",
      "Epoch 16[153/625] Time:0.706, Train Loss:0.2649649381637573\n",
      "Epoch 16[154/625] Time:0.708, Train Loss:0.28254634141921997\n",
      "Epoch 16[155/625] Time:0.705, Train Loss:0.2670222222805023\n",
      "Epoch 16[156/625] Time:0.705, Train Loss:0.3343058228492737\n",
      "Epoch 16[157/625] Time:0.705, Train Loss:0.21348921954631805\n",
      "Epoch 16[158/625] Time:0.703, Train Loss:0.2530888319015503\n",
      "Epoch 16[159/625] Time:0.704, Train Loss:0.4466300308704376\n",
      "Epoch 16[160/625] Time:0.703, Train Loss:0.20332986116409302\n",
      "Epoch 16[161/625] Time:0.704, Train Loss:0.26446864008903503\n",
      "Epoch 16[162/625] Time:0.703, Train Loss:0.2056647539138794\n",
      "Epoch 16[163/625] Time:0.703, Train Loss:0.16158314049243927\n",
      "Epoch 16[164/625] Time:0.703, Train Loss:0.26309916377067566\n",
      "Epoch 16[165/625] Time:0.702, Train Loss:0.2646304965019226\n",
      "Epoch 16[166/625] Time:0.704, Train Loss:0.2673894762992859\n",
      "Epoch 16[167/625] Time:0.705, Train Loss:0.28135353326797485\n",
      "Epoch 16[168/625] Time:0.703, Train Loss:0.2857590913772583\n",
      "Epoch 16[169/625] Time:0.703, Train Loss:0.20132502913475037\n",
      "Epoch 16[170/625] Time:0.704, Train Loss:0.28452351689338684\n",
      "Epoch 16[171/625] Time:0.703, Train Loss:0.2638736069202423\n",
      "Epoch 16[172/625] Time:0.701, Train Loss:0.24487873911857605\n",
      "Epoch 16[173/625] Time:0.71, Train Loss:0.3008314371109009\n",
      "Epoch 16[174/625] Time:0.703, Train Loss:0.18159916996955872\n",
      "Epoch 16[175/625] Time:0.704, Train Loss:0.2456442415714264\n",
      "Epoch 16[176/625] Time:0.703, Train Loss:0.2789566218852997\n",
      "Epoch 16[177/625] Time:0.704, Train Loss:0.29341766238212585\n",
      "Epoch 16[178/625] Time:0.703, Train Loss:0.24184325337409973\n",
      "Epoch 16[179/625] Time:0.703, Train Loss:0.2750200927257538\n",
      "Epoch 16[180/625] Time:0.703, Train Loss:0.23602987825870514\n",
      "Epoch 16[181/625] Time:0.709, Train Loss:0.26776713132858276\n",
      "Epoch 16[182/625] Time:0.703, Train Loss:0.2790631353855133\n",
      "Epoch 16[183/625] Time:0.703, Train Loss:0.21924389898777008\n",
      "Epoch 16[184/625] Time:0.702, Train Loss:0.21681441366672516\n",
      "Epoch 16[185/625] Time:0.702, Train Loss:0.3099294602870941\n",
      "Epoch 16[186/625] Time:0.702, Train Loss:0.26645541191101074\n",
      "Epoch 16[187/625] Time:0.694, Train Loss:0.23249079287052155\n",
      "Epoch 16[188/625] Time:0.695, Train Loss:0.22464051842689514\n",
      "Epoch 16[189/625] Time:0.698, Train Loss:0.2756255269050598\n",
      "Epoch 16[190/625] Time:0.697, Train Loss:0.3167179524898529\n",
      "Epoch 16[191/625] Time:0.694, Train Loss:0.2897278368473053\n",
      "Epoch 16[192/625] Time:0.694, Train Loss:0.24750275909900665\n",
      "Epoch 16[193/625] Time:0.693, Train Loss:0.2718914747238159\n",
      "Epoch 16[194/625] Time:0.694, Train Loss:0.2254520207643509\n",
      "Epoch 16[195/625] Time:0.693, Train Loss:0.2474229633808136\n",
      "Epoch 16[196/625] Time:0.696, Train Loss:0.2196512371301651\n",
      "Epoch 16[197/625] Time:0.695, Train Loss:0.32693201303482056\n",
      "Epoch 16[198/625] Time:0.705, Train Loss:0.2220359891653061\n",
      "Epoch 16[199/625] Time:0.703, Train Loss:0.3434743583202362\n",
      "Epoch 16[200/625] Time:0.703, Train Loss:0.3501549959182739\n",
      "Epoch 16[201/625] Time:0.703, Train Loss:0.33529409766197205\n",
      "Epoch 16[202/625] Time:0.703, Train Loss:0.21271611750125885\n",
      "Epoch 16[203/625] Time:0.703, Train Loss:0.18152332305908203\n",
      "Epoch 16[204/625] Time:0.693, Train Loss:0.25869375467300415\n",
      "Epoch 16[205/625] Time:0.695, Train Loss:0.23765608668327332\n",
      "Epoch 16[206/625] Time:0.693, Train Loss:0.23737090826034546\n",
      "Epoch 16[207/625] Time:0.695, Train Loss:0.29565757513046265\n",
      "Epoch 16[208/625] Time:0.703, Train Loss:0.23148678243160248\n",
      "Epoch 16[209/625] Time:0.704, Train Loss:0.2978520095348358\n",
      "Epoch 16[210/625] Time:0.702, Train Loss:0.22506500780582428\n",
      "Epoch 16[211/625] Time:0.704, Train Loss:0.25236040353775024\n",
      "Epoch 16[212/625] Time:0.702, Train Loss:0.28203636407852173\n",
      "Epoch 16[213/625] Time:0.704, Train Loss:0.29671069979667664\n",
      "Epoch 16[214/625] Time:0.703, Train Loss:0.31647735834121704\n",
      "Epoch 16[215/625] Time:0.704, Train Loss:0.2085556983947754\n",
      "Epoch 16[216/625] Time:0.704, Train Loss:0.21052774786949158\n",
      "Epoch 16[217/625] Time:0.703, Train Loss:0.2700534760951996\n",
      "Epoch 16[218/625] Time:0.705, Train Loss:0.2872534394264221\n",
      "Epoch 16[219/625] Time:0.703, Train Loss:0.31981271505355835\n",
      "Epoch 16[220/625] Time:0.702, Train Loss:0.2880426049232483\n",
      "Epoch 16[221/625] Time:0.704, Train Loss:0.2784964144229889\n",
      "Epoch 16[222/625] Time:0.705, Train Loss:0.207758367061615\n",
      "Epoch 16[223/625] Time:0.704, Train Loss:0.23263712227344513\n",
      "Epoch 16[224/625] Time:0.702, Train Loss:0.2329282909631729\n",
      "Epoch 16[225/625] Time:0.706, Train Loss:0.3764083981513977\n",
      "Epoch 16[226/625] Time:0.703, Train Loss:0.3748498558998108\n",
      "Epoch 16[227/625] Time:0.704, Train Loss:0.23597650229930878\n",
      "Epoch 16[228/625] Time:0.702, Train Loss:0.19897060096263885\n",
      "Epoch 16[229/625] Time:0.704, Train Loss:0.29836997389793396\n",
      "Epoch 16[230/625] Time:0.703, Train Loss:0.3242761492729187\n",
      "Epoch 16[231/625] Time:0.703, Train Loss:0.2977869212627411\n",
      "Epoch 16[232/625] Time:0.703, Train Loss:0.2894153594970703\n",
      "Epoch 16[233/625] Time:0.704, Train Loss:0.3269699513912201\n",
      "Epoch 16[234/625] Time:0.703, Train Loss:0.2287265658378601\n",
      "Epoch 16[235/625] Time:0.702, Train Loss:0.30934014916419983\n",
      "Epoch 16[236/625] Time:0.703, Train Loss:0.2701641619205475\n",
      "Epoch 16[237/625] Time:0.705, Train Loss:0.28345897793769836\n",
      "Epoch 16[238/625] Time:0.702, Train Loss:0.17758624255657196\n",
      "Epoch 16[239/625] Time:0.704, Train Loss:0.24298693239688873\n",
      "Epoch 16[240/625] Time:0.727, Train Loss:0.27074477076530457\n",
      "Epoch 16[241/625] Time:0.692, Train Loss:0.19022533297538757\n",
      "Epoch 16[242/625] Time:0.693, Train Loss:0.31707677245140076\n",
      "Epoch 16[243/625] Time:0.694, Train Loss:0.26768919825553894\n",
      "Epoch 16[244/625] Time:0.697, Train Loss:0.30933788418769836\n",
      "Epoch 16[245/625] Time:0.712, Train Loss:0.24963432550430298\n",
      "Epoch 16[246/625] Time:0.702, Train Loss:0.2465895116329193\n",
      "Epoch 16[247/625] Time:0.739, Train Loss:0.27202367782592773\n",
      "Epoch 16[248/625] Time:0.702, Train Loss:0.2297915667295456\n",
      "Epoch 16[249/625] Time:0.702, Train Loss:0.2830769717693329\n",
      "Epoch 16[250/625] Time:0.702, Train Loss:0.26458805799484253\n",
      "Epoch 16[251/625] Time:0.705, Train Loss:0.3101576268672943\n",
      "Epoch 16[252/625] Time:0.702, Train Loss:0.25108930468559265\n",
      "Epoch 16[253/625] Time:0.703, Train Loss:0.24677060544490814\n",
      "Epoch 16[254/625] Time:0.694, Train Loss:0.3232004642486572\n",
      "Epoch 16[255/625] Time:0.693, Train Loss:0.3232569396495819\n",
      "Epoch 16[256/625] Time:0.704, Train Loss:0.20909850299358368\n",
      "Epoch 16[257/625] Time:0.703, Train Loss:0.2871328592300415\n",
      "Epoch 16[258/625] Time:0.703, Train Loss:0.23757967352867126\n",
      "Epoch 16[259/625] Time:0.703, Train Loss:0.34821757674217224\n",
      "Epoch 16[260/625] Time:0.703, Train Loss:0.2633679211139679\n",
      "Epoch 16[261/625] Time:0.706, Train Loss:0.44721630215644836\n",
      "Epoch 16[262/625] Time:0.705, Train Loss:0.35116592049598694\n",
      "Epoch 16[263/625] Time:0.703, Train Loss:0.31288039684295654\n",
      "Epoch 16[264/625] Time:0.704, Train Loss:0.21045057475566864\n",
      "Epoch 16[265/625] Time:0.703, Train Loss:0.31353455781936646\n",
      "Epoch 16[266/625] Time:0.703, Train Loss:0.2340531200170517\n",
      "Epoch 16[267/625] Time:0.704, Train Loss:0.24583153426647186\n",
      "Epoch 16[268/625] Time:0.704, Train Loss:0.2547992765903473\n",
      "Epoch 16[269/625] Time:0.706, Train Loss:0.3022485375404358\n",
      "Epoch 16[270/625] Time:0.732, Train Loss:0.30464598536491394\n",
      "Epoch 16[271/625] Time:0.701, Train Loss:0.2688310444355011\n",
      "Epoch 16[272/625] Time:0.703, Train Loss:0.2529591917991638\n",
      "Epoch 16[273/625] Time:0.702, Train Loss:0.22684039175510406\n",
      "Epoch 16[274/625] Time:0.703, Train Loss:0.3080216646194458\n",
      "Epoch 16[275/625] Time:0.703, Train Loss:0.33444875478744507\n",
      "Epoch 16[276/625] Time:0.703, Train Loss:0.29771021008491516\n",
      "Epoch 16[277/625] Time:0.703, Train Loss:0.30681395530700684\n",
      "Epoch 16[278/625] Time:0.704, Train Loss:0.33229324221611023\n",
      "Epoch 16[279/625] Time:0.703, Train Loss:0.34850406646728516\n",
      "Epoch 16[280/625] Time:0.706, Train Loss:0.33348390460014343\n",
      "Epoch 16[281/625] Time:0.703, Train Loss:0.2841218113899231\n",
      "Epoch 16[282/625] Time:0.704, Train Loss:0.2940584123134613\n",
      "Epoch 16[283/625] Time:0.703, Train Loss:0.250460684299469\n",
      "Epoch 16[284/625] Time:0.708, Train Loss:0.2748318612575531\n",
      "Epoch 16[285/625] Time:0.704, Train Loss:0.31770288944244385\n",
      "Epoch 16[286/625] Time:0.703, Train Loss:0.2722056806087494\n",
      "Epoch 16[287/625] Time:0.702, Train Loss:0.24690423905849457\n",
      "Epoch 16[288/625] Time:0.702, Train Loss:0.32413747906684875\n",
      "Epoch 16[289/625] Time:0.703, Train Loss:0.3032844364643097\n",
      "Epoch 16[290/625] Time:0.702, Train Loss:0.26488739252090454\n",
      "Epoch 16[291/625] Time:0.704, Train Loss:0.3123241066932678\n",
      "Epoch 16[292/625] Time:0.702, Train Loss:0.20136266946792603\n",
      "Epoch 16[293/625] Time:0.702, Train Loss:0.21625161170959473\n",
      "Epoch 16[294/625] Time:0.703, Train Loss:0.36065176129341125\n",
      "Epoch 16[295/625] Time:0.702, Train Loss:0.29652512073516846\n",
      "Epoch 16[296/625] Time:0.704, Train Loss:0.2713474929332733\n",
      "Epoch 16[297/625] Time:0.703, Train Loss:0.25046640634536743\n",
      "Epoch 16[298/625] Time:0.702, Train Loss:0.24159321188926697\n",
      "Epoch 16[299/625] Time:0.705, Train Loss:0.2680339515209198\n",
      "Epoch 16[300/625] Time:0.705, Train Loss:0.27919453382492065\n",
      "Epoch 16[301/625] Time:0.703, Train Loss:0.3040553331375122\n",
      "Epoch 16[302/625] Time:0.702, Train Loss:0.376414030790329\n",
      "Epoch 16[303/625] Time:0.702, Train Loss:0.30168789625167847\n",
      "Epoch 16[304/625] Time:0.702, Train Loss:0.3074803054332733\n",
      "Epoch 16[305/625] Time:0.702, Train Loss:0.26880791783332825\n",
      "Epoch 16[306/625] Time:0.703, Train Loss:0.24476368725299835\n",
      "Epoch 16[307/625] Time:0.702, Train Loss:0.2931872308254242\n",
      "Epoch 16[308/625] Time:0.703, Train Loss:0.20448008179664612\n",
      "Epoch 16[309/625] Time:0.703, Train Loss:0.22892269492149353\n",
      "Epoch 16[310/625] Time:0.703, Train Loss:0.2680794894695282\n",
      "Epoch 16[311/625] Time:0.703, Train Loss:0.2268744707107544\n",
      "Epoch 16[312/625] Time:0.703, Train Loss:0.25884976983070374\n",
      "Epoch 16[313/625] Time:0.703, Train Loss:0.20522835850715637\n",
      "Epoch 16[314/625] Time:0.703, Train Loss:0.25573137402534485\n",
      "Epoch 16[315/625] Time:0.704, Train Loss:0.27986904978752136\n",
      "Epoch 16[316/625] Time:0.705, Train Loss:0.371359646320343\n",
      "Epoch 16[317/625] Time:0.703, Train Loss:0.24213294684886932\n",
      "Epoch 16[318/625] Time:0.703, Train Loss:0.2898576259613037\n",
      "Epoch 16[319/625] Time:0.704, Train Loss:0.22893962264060974\n",
      "Epoch 16[320/625] Time:0.703, Train Loss:0.2576911151409149\n",
      "Epoch 16[321/625] Time:0.703, Train Loss:0.281940758228302\n",
      "Epoch 16[322/625] Time:0.703, Train Loss:0.33513879776000977\n",
      "Epoch 16[323/625] Time:0.705, Train Loss:0.24561671912670135\n",
      "Epoch 16[324/625] Time:0.711, Train Loss:0.20306570827960968\n",
      "Epoch 16[325/625] Time:0.704, Train Loss:0.3071681559085846\n",
      "Epoch 16[326/625] Time:0.705, Train Loss:0.2417057454586029\n",
      "Epoch 16[327/625] Time:0.704, Train Loss:0.33665549755096436\n",
      "Epoch 16[328/625] Time:0.704, Train Loss:0.23463307321071625\n",
      "Epoch 16[329/625] Time:0.705, Train Loss:0.29124948382377625\n",
      "Epoch 16[330/625] Time:0.704, Train Loss:0.29525813460350037\n",
      "Epoch 16[331/625] Time:0.703, Train Loss:0.24518349766731262\n",
      "Epoch 16[332/625] Time:0.705, Train Loss:0.3090059161186218\n",
      "Epoch 16[333/625] Time:0.707, Train Loss:0.25114521384239197\n",
      "Epoch 16[334/625] Time:0.703, Train Loss:0.23679308593273163\n",
      "Epoch 16[335/625] Time:0.703, Train Loss:0.2592319846153259\n",
      "Epoch 16[336/625] Time:0.693, Train Loss:0.2554561495780945\n",
      "Epoch 16[337/625] Time:0.695, Train Loss:0.2605327367782593\n",
      "Epoch 16[338/625] Time:0.696, Train Loss:0.31940558552742004\n",
      "Epoch 16[339/625] Time:0.695, Train Loss:0.27527081966400146\n",
      "Epoch 16[340/625] Time:0.695, Train Loss:0.33978381752967834\n",
      "Epoch 16[341/625] Time:0.696, Train Loss:0.2887937128543854\n",
      "Epoch 16[342/625] Time:0.696, Train Loss:0.2838651239871979\n",
      "Epoch 16[343/625] Time:0.704, Train Loss:0.27832967042922974\n",
      "Epoch 16[344/625] Time:0.707, Train Loss:0.24579913914203644\n",
      "Epoch 16[345/625] Time:0.703, Train Loss:0.30944567918777466\n",
      "Epoch 16[346/625] Time:0.696, Train Loss:0.2693233788013458\n",
      "Epoch 16[347/625] Time:0.692, Train Loss:0.26220589876174927\n",
      "Epoch 16[348/625] Time:0.692, Train Loss:0.2566273510456085\n",
      "Epoch 16[349/625] Time:0.691, Train Loss:0.24817541241645813\n",
      "Epoch 16[350/625] Time:0.691, Train Loss:0.3448967933654785\n",
      "Epoch 16[351/625] Time:0.691, Train Loss:0.33190029859542847\n",
      "Epoch 16[352/625] Time:0.69, Train Loss:0.242422416806221\n",
      "Epoch 16[353/625] Time:0.692, Train Loss:0.30979129672050476\n",
      "Epoch 16[354/625] Time:0.693, Train Loss:0.2600579559803009\n",
      "Epoch 16[355/625] Time:0.691, Train Loss:0.3714565932750702\n",
      "Epoch 16[356/625] Time:0.693, Train Loss:0.2617760896682739\n",
      "Epoch 16[357/625] Time:0.705, Train Loss:0.3113043010234833\n",
      "Epoch 16[358/625] Time:0.704, Train Loss:0.4439452290534973\n",
      "Epoch 16[359/625] Time:0.704, Train Loss:0.18718789517879486\n",
      "Epoch 16[360/625] Time:0.704, Train Loss:0.2914377748966217\n",
      "Epoch 16[361/625] Time:0.703, Train Loss:0.2933671772480011\n",
      "Epoch 16[362/625] Time:0.703, Train Loss:0.31998714804649353\n",
      "Epoch 16[363/625] Time:0.703, Train Loss:0.32351744174957275\n",
      "Epoch 16[364/625] Time:0.703, Train Loss:0.31457436084747314\n",
      "Epoch 16[365/625] Time:0.703, Train Loss:0.29093706607818604\n",
      "Epoch 16[366/625] Time:0.713, Train Loss:0.2906973659992218\n",
      "Epoch 16[367/625] Time:0.705, Train Loss:0.24905599653720856\n",
      "Epoch 16[368/625] Time:0.703, Train Loss:0.3197529911994934\n",
      "Epoch 16[369/625] Time:0.702, Train Loss:0.38204285502433777\n",
      "Epoch 16[370/625] Time:0.703, Train Loss:0.3125975728034973\n",
      "Epoch 16[371/625] Time:0.703, Train Loss:0.2473028153181076\n",
      "Epoch 16[372/625] Time:0.705, Train Loss:0.2396240085363388\n",
      "Epoch 16[373/625] Time:0.703, Train Loss:0.26960289478302\n",
      "Epoch 16[374/625] Time:0.704, Train Loss:0.2900756597518921\n",
      "Epoch 16[375/625] Time:0.706, Train Loss:0.2873503863811493\n",
      "Epoch 16[376/625] Time:0.703, Train Loss:0.2860240042209625\n",
      "Epoch 16[377/625] Time:0.704, Train Loss:0.28620678186416626\n",
      "Epoch 16[378/625] Time:0.702, Train Loss:0.2646518349647522\n",
      "Epoch 16[379/625] Time:0.703, Train Loss:0.3147669732570648\n",
      "Epoch 16[380/625] Time:0.703, Train Loss:0.29635411500930786\n",
      "Epoch 16[381/625] Time:0.703, Train Loss:0.2780764698982239\n",
      "Epoch 16[382/625] Time:0.703, Train Loss:0.24988099932670593\n",
      "Epoch 16[383/625] Time:0.702, Train Loss:0.2591586112976074\n",
      "Epoch 16[384/625] Time:0.702, Train Loss:0.24393559992313385\n",
      "Epoch 16[385/625] Time:0.703, Train Loss:0.2767971456050873\n",
      "Epoch 16[386/625] Time:0.703, Train Loss:0.3124658763408661\n",
      "Epoch 16[387/625] Time:0.703, Train Loss:0.21600396931171417\n",
      "Epoch 16[388/625] Time:0.704, Train Loss:0.24484318494796753\n",
      "Epoch 16[389/625] Time:0.708, Train Loss:0.30770477652549744\n",
      "Epoch 16[390/625] Time:0.703, Train Loss:0.2686963379383087\n",
      "Epoch 16[391/625] Time:0.703, Train Loss:0.23102210462093353\n",
      "Epoch 16[392/625] Time:0.704, Train Loss:0.2721400856971741\n",
      "Epoch 16[393/625] Time:0.703, Train Loss:0.25632545351982117\n",
      "Epoch 16[394/625] Time:0.703, Train Loss:0.2651616334915161\n",
      "Epoch 16[395/625] Time:0.707, Train Loss:0.30379316210746765\n",
      "Epoch 16[396/625] Time:0.703, Train Loss:0.2804693281650543\n",
      "Epoch 16[397/625] Time:0.703, Train Loss:0.3145218789577484\n",
      "Epoch 16[398/625] Time:0.703, Train Loss:0.23008985817432404\n",
      "Epoch 16[399/625] Time:0.702, Train Loss:0.3042599856853485\n",
      "Epoch 16[400/625] Time:0.702, Train Loss:0.28831204771995544\n",
      "Epoch 16[401/625] Time:0.704, Train Loss:0.25130555033683777\n",
      "Epoch 16[402/625] Time:0.704, Train Loss:0.32330119609832764\n",
      "Epoch 16[403/625] Time:0.703, Train Loss:0.31541988253593445\n",
      "Epoch 16[404/625] Time:0.703, Train Loss:0.3087792694568634\n",
      "Epoch 16[405/625] Time:0.703, Train Loss:0.2536337077617645\n",
      "Epoch 16[406/625] Time:0.702, Train Loss:0.2325545698404312\n",
      "Epoch 16[407/625] Time:0.702, Train Loss:0.28502723574638367\n",
      "Epoch 16[408/625] Time:0.703, Train Loss:0.27156221866607666\n",
      "Epoch 16[409/625] Time:0.703, Train Loss:0.2921387553215027\n",
      "Epoch 16[410/625] Time:0.703, Train Loss:0.2211945354938507\n",
      "Epoch 16[411/625] Time:0.703, Train Loss:0.2872803211212158\n",
      "Epoch 16[412/625] Time:0.703, Train Loss:0.2682013511657715\n",
      "Epoch 16[413/625] Time:0.733, Train Loss:0.19646021723747253\n",
      "Epoch 16[414/625] Time:0.693, Train Loss:0.31313303112983704\n",
      "Epoch 16[415/625] Time:0.693, Train Loss:0.357189416885376\n",
      "Epoch 16[416/625] Time:0.694, Train Loss:0.25641998648643494\n",
      "Epoch 16[417/625] Time:0.694, Train Loss:0.2633213996887207\n",
      "Epoch 16[418/625] Time:0.694, Train Loss:0.25893864035606384\n",
      "Epoch 16[419/625] Time:0.711, Train Loss:0.31878456473350525\n",
      "Epoch 16[420/625] Time:0.702, Train Loss:0.28064867854118347\n",
      "Epoch 16[421/625] Time:0.703, Train Loss:0.3354209065437317\n",
      "Epoch 16[422/625] Time:0.703, Train Loss:0.292033851146698\n",
      "Epoch 16[423/625] Time:0.703, Train Loss:0.25869104266166687\n",
      "Epoch 16[424/625] Time:0.728, Train Loss:0.35241785645484924\n",
      "Epoch 16[425/625] Time:0.693, Train Loss:0.22247366607189178\n",
      "Epoch 16[426/625] Time:0.693, Train Loss:0.3332137167453766\n",
      "Epoch 16[427/625] Time:0.695, Train Loss:0.41547346115112305\n",
      "Epoch 16[428/625] Time:0.705, Train Loss:0.2959834933280945\n",
      "Epoch 16[429/625] Time:0.703, Train Loss:0.26223328709602356\n",
      "Epoch 16[430/625] Time:0.715, Train Loss:0.24751482903957367\n",
      "Epoch 16[431/625] Time:0.703, Train Loss:0.40113258361816406\n",
      "Epoch 16[432/625] Time:0.706, Train Loss:0.35749414563179016\n",
      "Epoch 16[433/625] Time:0.703, Train Loss:0.27263349294662476\n",
      "Epoch 16[434/625] Time:0.705, Train Loss:0.3307437300682068\n",
      "Epoch 16[435/625] Time:0.713, Train Loss:0.36396679282188416\n",
      "Epoch 16[436/625] Time:0.747, Train Loss:0.24247020483016968\n",
      "Epoch 16[437/625] Time:0.693, Train Loss:0.2571951746940613\n",
      "Epoch 16[438/625] Time:0.695, Train Loss:0.23245255649089813\n",
      "Epoch 16[439/625] Time:0.695, Train Loss:0.24532023072242737\n",
      "Epoch 16[440/625] Time:0.694, Train Loss:0.2416471689939499\n",
      "Epoch 16[441/625] Time:0.697, Train Loss:0.2635744512081146\n",
      "Epoch 16[442/625] Time:0.705, Train Loss:0.29563236236572266\n",
      "Epoch 16[443/625] Time:0.705, Train Loss:0.2845097482204437\n",
      "Epoch 16[444/625] Time:0.701, Train Loss:0.2773783504962921\n",
      "Epoch 16[445/625] Time:0.712, Train Loss:0.3031613528728485\n",
      "Epoch 16[446/625] Time:0.693, Train Loss:0.27957630157470703\n",
      "Epoch 16[447/625] Time:0.692, Train Loss:0.2995859384536743\n",
      "Epoch 16[448/625] Time:0.731, Train Loss:0.36175256967544556\n",
      "Epoch 16[449/625] Time:0.702, Train Loss:0.38798773288726807\n",
      "Epoch 16[450/625] Time:0.704, Train Loss:0.27323246002197266\n",
      "Epoch 16[451/625] Time:0.703, Train Loss:0.24457500874996185\n",
      "Epoch 16[452/625] Time:0.704, Train Loss:0.38053667545318604\n",
      "Epoch 16[453/625] Time:0.703, Train Loss:0.23906630277633667\n",
      "Epoch 16[454/625] Time:0.702, Train Loss:0.3033107817173004\n",
      "Epoch 16[455/625] Time:0.704, Train Loss:0.2754262387752533\n",
      "Epoch 16[456/625] Time:0.704, Train Loss:0.23198139667510986\n",
      "Epoch 16[457/625] Time:0.703, Train Loss:0.3323788344860077\n",
      "Epoch 16[458/625] Time:0.706, Train Loss:0.2517841160297394\n",
      "Epoch 16[459/625] Time:0.703, Train Loss:0.3054368197917938\n",
      "Epoch 16[460/625] Time:0.703, Train Loss:0.32966095209121704\n",
      "Epoch 16[461/625] Time:0.703, Train Loss:0.22536982595920563\n",
      "Epoch 16[462/625] Time:0.693, Train Loss:0.29261860251426697\n",
      "Epoch 16[463/625] Time:0.696, Train Loss:0.22696252167224884\n",
      "Epoch 16[464/625] Time:0.693, Train Loss:0.19536489248275757\n",
      "Epoch 16[465/625] Time:0.694, Train Loss:0.3286261558532715\n",
      "Epoch 16[466/625] Time:0.694, Train Loss:0.3818683624267578\n",
      "Epoch 16[467/625] Time:0.694, Train Loss:0.24014471471309662\n",
      "Epoch 16[468/625] Time:0.694, Train Loss:0.2902173101902008\n",
      "Epoch 16[469/625] Time:0.695, Train Loss:0.3058764338493347\n",
      "Epoch 16[470/625] Time:0.693, Train Loss:0.2889738082885742\n",
      "Epoch 16[471/625] Time:0.703, Train Loss:0.20021049678325653\n",
      "Epoch 16[472/625] Time:0.704, Train Loss:0.2722826600074768\n",
      "Epoch 16[473/625] Time:0.704, Train Loss:0.2884187698364258\n",
      "Epoch 16[474/625] Time:0.705, Train Loss:0.2361193597316742\n",
      "Epoch 16[475/625] Time:0.703, Train Loss:0.32325664162635803\n",
      "Epoch 16[476/625] Time:0.704, Train Loss:0.30008548498153687\n",
      "Epoch 16[477/625] Time:0.704, Train Loss:0.28059154748916626\n",
      "Epoch 16[478/625] Time:0.705, Train Loss:0.31131893396377563\n",
      "Epoch 16[479/625] Time:0.704, Train Loss:0.256926953792572\n",
      "Epoch 16[480/625] Time:0.703, Train Loss:0.3200797140598297\n",
      "Epoch 16[481/625] Time:0.702, Train Loss:0.30910569429397583\n",
      "Epoch 16[482/625] Time:0.71, Train Loss:0.3013942837715149\n",
      "Epoch 16[483/625] Time:0.702, Train Loss:0.19461818039417267\n",
      "Epoch 16[484/625] Time:0.708, Train Loss:0.28993695974349976\n",
      "Epoch 16[485/625] Time:0.694, Train Loss:0.27063438296318054\n",
      "Epoch 16[486/625] Time:0.695, Train Loss:0.2219381183385849\n",
      "Epoch 16[487/625] Time:0.696, Train Loss:0.25933775305747986\n",
      "Epoch 16[488/625] Time:0.696, Train Loss:0.26127946376800537\n",
      "Epoch 16[489/625] Time:0.703, Train Loss:0.25319841504096985\n",
      "Epoch 16[490/625] Time:0.704, Train Loss:0.36327534914016724\n",
      "Epoch 16[491/625] Time:0.703, Train Loss:0.26086536049842834\n",
      "Epoch 16[492/625] Time:0.702, Train Loss:0.2649605870246887\n",
      "Epoch 16[493/625] Time:0.703, Train Loss:0.17786574363708496\n",
      "Epoch 16[494/625] Time:0.703, Train Loss:0.1987418532371521\n",
      "Epoch 16[495/625] Time:0.703, Train Loss:0.3065843880176544\n",
      "Epoch 16[496/625] Time:0.703, Train Loss:0.2436504364013672\n",
      "Epoch 16[497/625] Time:0.704, Train Loss:0.34342095255851746\n",
      "Epoch 16[498/625] Time:0.706, Train Loss:0.22250577807426453\n",
      "Epoch 16[499/625] Time:0.705, Train Loss:0.2650565505027771\n",
      "Epoch 16[500/625] Time:0.704, Train Loss:0.20509850978851318\n",
      "Epoch 16[501/625] Time:0.704, Train Loss:0.1740003526210785\n",
      "Epoch 16[502/625] Time:0.706, Train Loss:0.37090054154396057\n",
      "Epoch 16[503/625] Time:0.706, Train Loss:0.2298484891653061\n",
      "Epoch 16[504/625] Time:0.705, Train Loss:0.29378631711006165\n",
      "Epoch 16[505/625] Time:0.705, Train Loss:0.2610228657722473\n",
      "Epoch 16[506/625] Time:0.705, Train Loss:0.2726382911205292\n",
      "Epoch 16[507/625] Time:0.703, Train Loss:0.3070800006389618\n",
      "Epoch 16[508/625] Time:0.703, Train Loss:0.2823212444782257\n",
      "Epoch 16[509/625] Time:0.705, Train Loss:0.24008294939994812\n",
      "Epoch 16[510/625] Time:0.705, Train Loss:0.21480993926525116\n",
      "Epoch 16[511/625] Time:0.705, Train Loss:0.2666440010070801\n",
      "Epoch 16[512/625] Time:0.705, Train Loss:0.31034642457962036\n",
      "Epoch 16[513/625] Time:0.705, Train Loss:0.3005756139755249\n",
      "Epoch 16[514/625] Time:0.705, Train Loss:0.3091870844364166\n",
      "Epoch 16[515/625] Time:0.705, Train Loss:0.304240345954895\n",
      "Epoch 16[516/625] Time:0.706, Train Loss:0.2714897394180298\n",
      "Epoch 16[517/625] Time:0.723, Train Loss:0.28147709369659424\n",
      "Epoch 16[518/625] Time:0.691, Train Loss:0.23650483787059784\n",
      "Epoch 16[519/625] Time:0.707, Train Loss:0.24355334043502808\n",
      "Epoch 16[520/625] Time:0.706, Train Loss:0.29245850443840027\n",
      "Epoch 16[521/625] Time:0.706, Train Loss:0.2923904061317444\n",
      "Epoch 16[522/625] Time:0.708, Train Loss:0.29694125056266785\n",
      "Epoch 16[523/625] Time:0.691, Train Loss:0.254251092672348\n",
      "Epoch 16[524/625] Time:0.705, Train Loss:0.19205474853515625\n",
      "Epoch 16[525/625] Time:0.708, Train Loss:0.2408953756093979\n",
      "Epoch 16[526/625] Time:0.709, Train Loss:0.21491891145706177\n",
      "Epoch 16[527/625] Time:0.703, Train Loss:0.2311587929725647\n",
      "Epoch 16[528/625] Time:0.703, Train Loss:0.2284272015094757\n",
      "Epoch 16[529/625] Time:0.704, Train Loss:0.2539919316768646\n",
      "Epoch 16[530/625] Time:0.704, Train Loss:0.26249390840530396\n",
      "Epoch 16[531/625] Time:0.716, Train Loss:0.23732171952724457\n",
      "Epoch 16[532/625] Time:0.702, Train Loss:0.3035925328731537\n",
      "Epoch 16[533/625] Time:0.706, Train Loss:0.2524142265319824\n",
      "Epoch 16[534/625] Time:0.721, Train Loss:0.3503915071487427\n",
      "Epoch 16[535/625] Time:0.703, Train Loss:0.2565540373325348\n",
      "Epoch 16[536/625] Time:0.715, Train Loss:0.3183220624923706\n",
      "Epoch 16[537/625] Time:0.705, Train Loss:0.22036981582641602\n",
      "Epoch 16[538/625] Time:0.703, Train Loss:0.3143489360809326\n",
      "Epoch 16[539/625] Time:0.704, Train Loss:0.27115821838378906\n",
      "Epoch 16[540/625] Time:0.705, Train Loss:0.30495262145996094\n",
      "Epoch 16[541/625] Time:0.704, Train Loss:0.2122267633676529\n",
      "Epoch 16[542/625] Time:0.704, Train Loss:0.2335321307182312\n",
      "Epoch 16[543/625] Time:0.706, Train Loss:0.3450483977794647\n",
      "Epoch 16[544/625] Time:0.706, Train Loss:0.17984940111637115\n",
      "Epoch 16[545/625] Time:0.704, Train Loss:0.36342552304267883\n",
      "Epoch 16[546/625] Time:0.704, Train Loss:0.26161763072013855\n",
      "Epoch 16[547/625] Time:0.708, Train Loss:0.34239527583122253\n",
      "Epoch 16[548/625] Time:0.703, Train Loss:0.29546377062797546\n",
      "Epoch 16[549/625] Time:0.703, Train Loss:0.30053573846817017\n",
      "Epoch 16[550/625] Time:0.703, Train Loss:0.22398154437541962\n",
      "Epoch 16[551/625] Time:0.704, Train Loss:0.19182829558849335\n",
      "Epoch 16[552/625] Time:0.703, Train Loss:0.1999003291130066\n",
      "Epoch 16[553/625] Time:0.704, Train Loss:0.3046678602695465\n",
      "Epoch 16[554/625] Time:0.703, Train Loss:0.22269335389137268\n",
      "Epoch 16[555/625] Time:0.703, Train Loss:0.2575693726539612\n",
      "Epoch 16[556/625] Time:0.703, Train Loss:0.2907721996307373\n",
      "Epoch 16[557/625] Time:0.702, Train Loss:0.34402087330818176\n",
      "Epoch 16[558/625] Time:0.702, Train Loss:0.22533614933490753\n",
      "Epoch 16[559/625] Time:0.705, Train Loss:0.23488971590995789\n",
      "Epoch 16[560/625] Time:0.703, Train Loss:0.26977452635765076\n",
      "Epoch 16[561/625] Time:0.704, Train Loss:0.18323563039302826\n",
      "Epoch 16[562/625] Time:0.704, Train Loss:0.33723780512809753\n",
      "Epoch 16[563/625] Time:0.704, Train Loss:0.30711883306503296\n",
      "Epoch 16[564/625] Time:0.703, Train Loss:0.36707791686058044\n",
      "Epoch 16[565/625] Time:0.704, Train Loss:0.3546660542488098\n",
      "Epoch 16[566/625] Time:0.706, Train Loss:0.19124917685985565\n",
      "Epoch 16[567/625] Time:0.702, Train Loss:0.20612189173698425\n",
      "Epoch 16[568/625] Time:0.709, Train Loss:0.19829392433166504\n",
      "Epoch 16[569/625] Time:0.703, Train Loss:0.2881225645542145\n",
      "Epoch 16[570/625] Time:0.703, Train Loss:0.26687338948249817\n",
      "Epoch 16[571/625] Time:0.705, Train Loss:0.245211660861969\n",
      "Epoch 16[572/625] Time:0.693, Train Loss:0.31078043580055237\n",
      "Epoch 16[573/625] Time:0.693, Train Loss:0.2429630607366562\n",
      "Epoch 16[574/625] Time:0.708, Train Loss:0.2521173655986786\n",
      "Epoch 16[575/625] Time:0.703, Train Loss:0.2833046019077301\n",
      "Epoch 16[576/625] Time:0.703, Train Loss:0.3121682405471802\n",
      "Epoch 16[577/625] Time:0.707, Train Loss:0.22967985272407532\n",
      "Epoch 16[578/625] Time:0.693, Train Loss:0.2412554770708084\n",
      "Epoch 16[579/625] Time:0.703, Train Loss:0.1941339522600174\n",
      "Epoch 16[580/625] Time:0.703, Train Loss:0.2410205602645874\n",
      "Epoch 16[581/625] Time:0.705, Train Loss:0.3893907368183136\n",
      "Epoch 16[582/625] Time:0.704, Train Loss:0.27361682057380676\n",
      "Epoch 16[583/625] Time:0.703, Train Loss:0.2801922559738159\n",
      "Epoch 16[584/625] Time:0.705, Train Loss:0.2519792318344116\n",
      "Epoch 16[585/625] Time:0.703, Train Loss:0.297635942697525\n",
      "Epoch 16[586/625] Time:0.714, Train Loss:0.29779452085494995\n",
      "Epoch 16[587/625] Time:0.703, Train Loss:0.30069589614868164\n",
      "Epoch 16[588/625] Time:0.704, Train Loss:0.31803956627845764\n",
      "Epoch 16[589/625] Time:0.705, Train Loss:0.2553875148296356\n",
      "Epoch 16[590/625] Time:0.706, Train Loss:0.2562386691570282\n",
      "Epoch 16[591/625] Time:0.705, Train Loss:0.2307433784008026\n",
      "Epoch 16[592/625] Time:0.704, Train Loss:0.30289167165756226\n",
      "Epoch 16[593/625] Time:0.705, Train Loss:0.3769017159938812\n",
      "Epoch 16[594/625] Time:0.731, Train Loss:0.29447436332702637\n",
      "Epoch 16[595/625] Time:0.692, Train Loss:0.34405094385147095\n",
      "Epoch 16[596/625] Time:0.694, Train Loss:0.2546641230583191\n",
      "Epoch 16[597/625] Time:0.693, Train Loss:0.2937129735946655\n",
      "Epoch 16[598/625] Time:0.693, Train Loss:0.2838970720767975\n",
      "Epoch 16[599/625] Time:0.697, Train Loss:0.22772470116615295\n",
      "Epoch 16[600/625] Time:0.697, Train Loss:0.3352881669998169\n",
      "Epoch 16[601/625] Time:0.695, Train Loss:0.2785583734512329\n",
      "Epoch 16[602/625] Time:0.695, Train Loss:0.29216763377189636\n",
      "Epoch 16[603/625] Time:0.693, Train Loss:0.21947482228279114\n",
      "Epoch 16[604/625] Time:0.694, Train Loss:0.3084714114665985\n",
      "Epoch 16[605/625] Time:0.695, Train Loss:0.28656065464019775\n",
      "Epoch 16[606/625] Time:0.694, Train Loss:0.30214688181877136\n",
      "Epoch 16[607/625] Time:0.706, Train Loss:0.23364666104316711\n",
      "Epoch 16[608/625] Time:0.704, Train Loss:0.23676255345344543\n",
      "Epoch 16[609/625] Time:0.703, Train Loss:0.29422926902770996\n",
      "Epoch 16[610/625] Time:0.704, Train Loss:0.3723924160003662\n",
      "Epoch 16[611/625] Time:0.703, Train Loss:0.23035286366939545\n",
      "Epoch 16[612/625] Time:0.703, Train Loss:0.25902464985847473\n",
      "Epoch 16[613/625] Time:0.703, Train Loss:0.3143289089202881\n",
      "Epoch 16[614/625] Time:0.703, Train Loss:0.36513039469718933\n",
      "Epoch 16[615/625] Time:0.712, Train Loss:0.31840384006500244\n",
      "Epoch 16[616/625] Time:0.702, Train Loss:0.34120234847068787\n",
      "Epoch 16[617/625] Time:0.705, Train Loss:0.22834058105945587\n",
      "Epoch 16[618/625] Time:0.704, Train Loss:0.2712387442588806\n",
      "Epoch 16[619/625] Time:0.703, Train Loss:0.24368681013584137\n",
      "Epoch 16[620/625] Time:0.705, Train Loss:0.25612443685531616\n",
      "Epoch 16[621/625] Time:0.706, Train Loss:0.2983541190624237\n",
      "Epoch 16[622/625] Time:0.704, Train Loss:0.27456551790237427\n",
      "Epoch 16[623/625] Time:0.705, Train Loss:0.2835609018802643\n",
      "Epoch 16[624/625] Time:0.703, Train Loss:0.31883373856544495\n",
      "Epoch 16[0/78] Val Loss:0.26228204369544983\n",
      "Epoch 16[1/78] Val Loss:0.22316865622997284\n",
      "Epoch 16[2/78] Val Loss:0.2658625543117523\n",
      "Epoch 16[3/78] Val Loss:0.2532518208026886\n",
      "Epoch 16[4/78] Val Loss:0.29961246252059937\n",
      "Epoch 16[5/78] Val Loss:0.18580688536167145\n",
      "Epoch 16[6/78] Val Loss:0.2575923204421997\n",
      "Epoch 16[7/78] Val Loss:0.3115503191947937\n",
      "Epoch 16[8/78] Val Loss:0.16075754165649414\n",
      "Epoch 16[9/78] Val Loss:0.1682988703250885\n",
      "Epoch 16[10/78] Val Loss:0.10954718291759491\n",
      "Epoch 16[11/78] Val Loss:0.1806146204471588\n",
      "Epoch 16[12/78] Val Loss:0.17711757123470306\n",
      "Epoch 16[13/78] Val Loss:0.11461606621742249\n",
      "Epoch 16[14/78] Val Loss:0.21952730417251587\n",
      "Epoch 16[15/78] Val Loss:0.19808952510356903\n",
      "Epoch 16[16/78] Val Loss:0.25168943405151367\n",
      "Epoch 16[17/78] Val Loss:0.17196011543273926\n",
      "Epoch 16[18/78] Val Loss:0.17750543355941772\n",
      "Epoch 16[19/78] Val Loss:0.2146848887205124\n",
      "Epoch 16[20/78] Val Loss:0.1752549260854721\n",
      "Epoch 16[21/78] Val Loss:0.4264235496520996\n",
      "Epoch 16[22/78] Val Loss:0.6168659329414368\n",
      "Epoch 16[23/78] Val Loss:0.46991074085235596\n",
      "Epoch 16[24/78] Val Loss:0.3176839053630829\n",
      "Epoch 16[25/78] Val Loss:0.43008264899253845\n",
      "Epoch 16[26/78] Val Loss:0.376030832529068\n",
      "Epoch 16[27/78] Val Loss:0.3652162253856659\n",
      "Epoch 16[28/78] Val Loss:0.3241842985153198\n",
      "Epoch 16[29/78] Val Loss:0.524167537689209\n",
      "Epoch 16[30/78] Val Loss:1.468414545059204\n",
      "Epoch 16[31/78] Val Loss:1.3243491649627686\n",
      "Epoch 16[32/78] Val Loss:1.058381199836731\n",
      "Epoch 16[33/78] Val Loss:0.4060414731502533\n",
      "Epoch 16[34/78] Val Loss:0.3223164975643158\n",
      "Epoch 16[35/78] Val Loss:0.36695894598960876\n",
      "Epoch 16[36/78] Val Loss:0.32042360305786133\n",
      "Epoch 16[37/78] Val Loss:0.40808582305908203\n",
      "Epoch 16[38/78] Val Loss:0.2536996006965637\n",
      "Epoch 16[39/78] Val Loss:0.23051218688488007\n",
      "Epoch 16[40/78] Val Loss:0.26050040125846863\n",
      "Epoch 16[41/78] Val Loss:0.26832982897758484\n",
      "Epoch 16[42/78] Val Loss:0.24367281794548035\n",
      "Epoch 16[43/78] Val Loss:0.19512252509593964\n",
      "Epoch 16[44/78] Val Loss:0.18694674968719482\n",
      "Epoch 16[45/78] Val Loss:0.19414997100830078\n",
      "Epoch 16[46/78] Val Loss:0.2055506408214569\n",
      "Epoch 16[47/78] Val Loss:0.2022136002779007\n",
      "Epoch 16[48/78] Val Loss:0.200430229306221\n",
      "Epoch 16[49/78] Val Loss:0.1592836081981659\n",
      "Epoch 16[50/78] Val Loss:0.13967129588127136\n",
      "Epoch 16[51/78] Val Loss:0.13540959358215332\n",
      "Epoch 16[52/78] Val Loss:0.1818087100982666\n",
      "Epoch 16[53/78] Val Loss:0.1583224982023239\n",
      "Epoch 16[54/78] Val Loss:0.16272588074207306\n",
      "Epoch 16[55/78] Val Loss:0.141704261302948\n",
      "Epoch 16[56/78] Val Loss:0.29424843192100525\n",
      "Epoch 16[57/78] Val Loss:0.2867392599582672\n",
      "Epoch 16[58/78] Val Loss:0.2776170074939728\n",
      "Epoch 16[59/78] Val Loss:0.3757225275039673\n",
      "Epoch 16[60/78] Val Loss:0.3605174124240875\n",
      "Epoch 16[61/78] Val Loss:0.42239776253700256\n",
      "Epoch 16[62/78] Val Loss:0.40846964716911316\n",
      "Epoch 16[63/78] Val Loss:0.2653282880783081\n",
      "Epoch 16[64/78] Val Loss:0.16330242156982422\n",
      "Epoch 16[65/78] Val Loss:0.14405041933059692\n",
      "Epoch 16[66/78] Val Loss:0.1848682165145874\n",
      "Epoch 16[67/78] Val Loss:0.18455719947814941\n",
      "Epoch 16[68/78] Val Loss:0.3579079806804657\n",
      "Epoch 16[69/78] Val Loss:0.35720986127853394\n",
      "Epoch 16[70/78] Val Loss:0.35451123118400574\n",
      "Epoch 16[71/78] Val Loss:0.3336072564125061\n",
      "Epoch 16[72/78] Val Loss:0.2734857201576233\n",
      "Epoch 16[73/78] Val Loss:0.30396732687950134\n",
      "Epoch 16[74/78] Val Loss:0.3786800503730774\n",
      "Epoch 16[75/78] Val Loss:0.44506222009658813\n",
      "Epoch 16[76/78] Val Loss:0.40608543157577515\n",
      "Epoch 16[77/78] Val Loss:0.4482707977294922\n",
      "Epoch 16[78/78] Val Loss:0.4920264482498169\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.90      0.92     15691\n",
      "           1       0.68      0.79      0.73      4309\n",
      "\n",
      "    accuracy                           0.87     20000\n",
      "   macro avg       0.81      0.84      0.82     20000\n",
      "weighted avg       0.88      0.87      0.88     20000\n",
      "\n",
      "Epoch 16: Train Loss 0.2767305564165115, Val Loss 0.31978224084163326, Train Time 793.0175411701202, Val Time 36.3312828540802\n",
      "Epoch 17[0/625] Time:0.689, Train Loss:0.2527507543563843\n",
      "Epoch 17[1/625] Time:0.702, Train Loss:0.2573617696762085\n",
      "Epoch 17[2/625] Time:0.701, Train Loss:0.26984265446662903\n",
      "Epoch 17[3/625] Time:0.704, Train Loss:0.2689856290817261\n",
      "Epoch 17[4/625] Time:0.704, Train Loss:0.21681736409664154\n",
      "Epoch 17[5/625] Time:0.704, Train Loss:0.2497231811285019\n",
      "Epoch 17[6/625] Time:0.704, Train Loss:0.2892773151397705\n",
      "Epoch 17[7/625] Time:0.704, Train Loss:0.24915844202041626\n",
      "Epoch 17[8/625] Time:0.705, Train Loss:0.3280688226222992\n",
      "Epoch 17[9/625] Time:0.704, Train Loss:0.22707581520080566\n",
      "Epoch 17[10/625] Time:0.704, Train Loss:0.2548173666000366\n",
      "Epoch 17[11/625] Time:0.705, Train Loss:0.23169799149036407\n",
      "Epoch 17[12/625] Time:0.705, Train Loss:0.1965477615594864\n",
      "Epoch 17[13/625] Time:0.704, Train Loss:0.20518775284290314\n",
      "Epoch 17[14/625] Time:0.705, Train Loss:0.2991732358932495\n",
      "Epoch 17[15/625] Time:0.705, Train Loss:0.26591092348098755\n",
      "Epoch 17[16/625] Time:0.705, Train Loss:0.23241406679153442\n",
      "Epoch 17[17/625] Time:0.704, Train Loss:0.2787087559700012\n",
      "Epoch 17[18/625] Time:0.704, Train Loss:0.2871761918067932\n",
      "Epoch 17[19/625] Time:0.704, Train Loss:0.23471809923648834\n",
      "Epoch 17[20/625] Time:0.703, Train Loss:0.2485823780298233\n",
      "Epoch 17[21/625] Time:0.703, Train Loss:0.29334592819213867\n",
      "Epoch 17[22/625] Time:0.702, Train Loss:0.3580937683582306\n",
      "Epoch 17[23/625] Time:0.703, Train Loss:0.2710185647010803\n",
      "Epoch 17[24/625] Time:0.703, Train Loss:0.3236944377422333\n",
      "Epoch 17[25/625] Time:0.703, Train Loss:0.3581860065460205\n",
      "Epoch 17[26/625] Time:0.703, Train Loss:0.26722481846809387\n",
      "Epoch 17[27/625] Time:0.703, Train Loss:0.2969607412815094\n",
      "Epoch 17[28/625] Time:0.703, Train Loss:0.24826502799987793\n",
      "Epoch 17[29/625] Time:0.703, Train Loss:0.22323133051395416\n",
      "Epoch 17[30/625] Time:0.703, Train Loss:0.2582166790962219\n",
      "Epoch 17[31/625] Time:0.703, Train Loss:0.2390294224023819\n",
      "Epoch 17[32/625] Time:0.71, Train Loss:0.22659410536289215\n",
      "Epoch 17[33/625] Time:0.704, Train Loss:0.3033180236816406\n",
      "Epoch 17[34/625] Time:0.702, Train Loss:0.23266710340976715\n",
      "Epoch 17[35/625] Time:0.709, Train Loss:0.25492730736732483\n",
      "Epoch 17[36/625] Time:0.702, Train Loss:0.23919497430324554\n",
      "Epoch 17[37/625] Time:0.703, Train Loss:0.33142441511154175\n",
      "Epoch 17[38/625] Time:0.703, Train Loss:0.31778237223625183\n",
      "Epoch 17[39/625] Time:0.702, Train Loss:0.3528610169887543\n",
      "Epoch 17[40/625] Time:0.708, Train Loss:0.24329867959022522\n",
      "Epoch 17[41/625] Time:0.703, Train Loss:0.27691134810447693\n",
      "Epoch 17[42/625] Time:0.704, Train Loss:0.26672670245170593\n",
      "Epoch 17[43/625] Time:0.703, Train Loss:0.23270635306835175\n",
      "Epoch 17[44/625] Time:0.703, Train Loss:0.2771036922931671\n",
      "Epoch 17[45/625] Time:0.703, Train Loss:0.32150140404701233\n",
      "Epoch 17[46/625] Time:0.703, Train Loss:0.37714552879333496\n",
      "Epoch 17[47/625] Time:0.703, Train Loss:0.3182004988193512\n",
      "Epoch 17[48/625] Time:0.705, Train Loss:0.3561972379684448\n",
      "Epoch 17[49/625] Time:0.703, Train Loss:0.2721588611602783\n",
      "Epoch 17[50/625] Time:0.703, Train Loss:0.27770543098449707\n",
      "Epoch 17[51/625] Time:0.703, Train Loss:0.30086779594421387\n",
      "Epoch 17[52/625] Time:0.703, Train Loss:0.3689921200275421\n",
      "Epoch 17[53/625] Time:0.704, Train Loss:0.27638089656829834\n",
      "Epoch 17[54/625] Time:0.703, Train Loss:0.2757883071899414\n",
      "Epoch 17[55/625] Time:0.702, Train Loss:0.3434021472930908\n",
      "Epoch 17[56/625] Time:0.702, Train Loss:0.2982652485370636\n",
      "Epoch 17[57/625] Time:0.704, Train Loss:0.3013347089290619\n",
      "Epoch 17[58/625] Time:0.703, Train Loss:0.316906601190567\n",
      "Epoch 17[59/625] Time:0.704, Train Loss:0.3143931031227112\n",
      "Epoch 17[60/625] Time:0.703, Train Loss:0.28547561168670654\n",
      "Epoch 17[61/625] Time:0.703, Train Loss:0.2792435884475708\n",
      "Epoch 17[62/625] Time:0.703, Train Loss:0.27097204327583313\n",
      "Epoch 17[63/625] Time:0.703, Train Loss:0.1970873326063156\n",
      "Epoch 17[64/625] Time:0.708, Train Loss:0.2470938265323639\n",
      "Epoch 17[65/625] Time:0.704, Train Loss:0.3199726939201355\n",
      "Epoch 17[66/625] Time:0.703, Train Loss:0.27663496136665344\n",
      "Epoch 17[67/625] Time:0.702, Train Loss:0.19689211249351501\n",
      "Epoch 17[68/625] Time:0.704, Train Loss:0.17724637687206268\n",
      "Epoch 17[69/625] Time:0.702, Train Loss:0.25751733779907227\n",
      "Epoch 17[70/625] Time:0.728, Train Loss:0.3061399757862091\n",
      "Epoch 17[71/625] Time:0.702, Train Loss:0.23273557424545288\n",
      "Epoch 17[72/625] Time:0.709, Train Loss:0.2985026240348816\n",
      "Epoch 17[73/625] Time:0.703, Train Loss:0.29265275597572327\n",
      "Epoch 17[74/625] Time:0.703, Train Loss:0.32635775208473206\n",
      "Epoch 17[75/625] Time:0.693, Train Loss:0.3577153980731964\n",
      "Epoch 17[76/625] Time:0.694, Train Loss:0.2247854471206665\n",
      "Epoch 17[77/625] Time:0.694, Train Loss:0.2354574054479599\n",
      "Epoch 17[78/625] Time:0.703, Train Loss:0.2699650228023529\n",
      "Epoch 17[79/625] Time:0.703, Train Loss:0.27699872851371765\n",
      "Epoch 17[80/625] Time:0.71, Train Loss:0.274914026260376\n",
      "Epoch 17[81/625] Time:0.705, Train Loss:0.2705513834953308\n",
      "Epoch 17[82/625] Time:0.704, Train Loss:0.21257588267326355\n",
      "Epoch 17[83/625] Time:0.703, Train Loss:0.2814885377883911\n",
      "Epoch 17[84/625] Time:0.703, Train Loss:0.22766107320785522\n",
      "Epoch 17[85/625] Time:0.702, Train Loss:0.2220289409160614\n",
      "Epoch 17[86/625] Time:0.704, Train Loss:0.2959641218185425\n",
      "Epoch 17[87/625] Time:0.703, Train Loss:0.3584325611591339\n",
      "Epoch 17[88/625] Time:0.705, Train Loss:0.22396281361579895\n",
      "Epoch 17[89/625] Time:0.704, Train Loss:0.3479227125644684\n",
      "Epoch 17[90/625] Time:0.703, Train Loss:0.3809458911418915\n",
      "Epoch 17[91/625] Time:0.703, Train Loss:0.19767306745052338\n",
      "Epoch 17[92/625] Time:0.711, Train Loss:0.25990229845046997\n",
      "Epoch 17[93/625] Time:0.703, Train Loss:0.28231048583984375\n",
      "Epoch 17[94/625] Time:0.705, Train Loss:0.3643999695777893\n",
      "Epoch 17[95/625] Time:0.703, Train Loss:0.245728000998497\n",
      "Epoch 17[96/625] Time:0.705, Train Loss:0.3396797478199005\n",
      "Epoch 17[97/625] Time:0.702, Train Loss:0.2396898865699768\n",
      "Epoch 17[98/625] Time:0.704, Train Loss:0.19844850897789001\n",
      "Epoch 17[99/625] Time:0.704, Train Loss:0.348638653755188\n",
      "Epoch 17[100/625] Time:0.703, Train Loss:0.29891350865364075\n",
      "Epoch 17[101/625] Time:0.705, Train Loss:0.1954490840435028\n",
      "Epoch 17[102/625] Time:0.706, Train Loss:0.28138676285743713\n",
      "Epoch 17[103/625] Time:0.704, Train Loss:0.2890312671661377\n",
      "Epoch 17[104/625] Time:0.705, Train Loss:0.253860205411911\n",
      "Epoch 17[105/625] Time:0.704, Train Loss:0.31102848052978516\n",
      "Epoch 17[106/625] Time:0.703, Train Loss:0.27104443311691284\n",
      "Epoch 17[107/625] Time:0.744, Train Loss:0.2983440160751343\n",
      "Epoch 17[108/625] Time:0.701, Train Loss:0.32301533222198486\n",
      "Epoch 17[109/625] Time:0.703, Train Loss:0.290548175573349\n",
      "Epoch 17[110/625] Time:0.695, Train Loss:0.26422640681266785\n",
      "Epoch 17[111/625] Time:0.707, Train Loss:0.2863241732120514\n",
      "Epoch 17[112/625] Time:0.694, Train Loss:0.26855793595314026\n",
      "Epoch 17[113/625] Time:0.694, Train Loss:0.2601785659790039\n",
      "Epoch 17[114/625] Time:0.694, Train Loss:0.24454542994499207\n",
      "Epoch 17[115/625] Time:0.695, Train Loss:0.3203672468662262\n",
      "Epoch 17[116/625] Time:0.694, Train Loss:0.23641975224018097\n",
      "Epoch 17[117/625] Time:0.694, Train Loss:0.24350085854530334\n",
      "Epoch 17[118/625] Time:0.695, Train Loss:0.2636756896972656\n",
      "Epoch 17[119/625] Time:0.694, Train Loss:0.26380234956741333\n",
      "Epoch 17[120/625] Time:0.696, Train Loss:0.31430649757385254\n",
      "Epoch 17[121/625] Time:0.694, Train Loss:0.3071853816509247\n",
      "Epoch 17[122/625] Time:0.694, Train Loss:0.23410601913928986\n",
      "Epoch 17[123/625] Time:0.697, Train Loss:0.3627452254295349\n",
      "Epoch 17[124/625] Time:0.694, Train Loss:0.22651614248752594\n",
      "Epoch 17[125/625] Time:0.704, Train Loss:0.26287421584129333\n",
      "Epoch 17[126/625] Time:0.736, Train Loss:0.3002994954586029\n",
      "Epoch 17[127/625] Time:0.695, Train Loss:0.33660972118377686\n",
      "Epoch 17[128/625] Time:0.703, Train Loss:0.23106372356414795\n",
      "Epoch 17[129/625] Time:0.703, Train Loss:0.27102962136268616\n",
      "Epoch 17[130/625] Time:0.703, Train Loss:0.27302297949790955\n",
      "Epoch 17[131/625] Time:0.705, Train Loss:0.28637686371803284\n",
      "Epoch 17[132/625] Time:0.703, Train Loss:0.2896420657634735\n",
      "Epoch 17[133/625] Time:0.704, Train Loss:0.3095506429672241\n",
      "Epoch 17[134/625] Time:0.704, Train Loss:0.29798778891563416\n",
      "Epoch 17[135/625] Time:0.704, Train Loss:0.321942538022995\n",
      "Epoch 17[136/625] Time:0.702, Train Loss:0.20802679657936096\n",
      "Epoch 17[137/625] Time:0.703, Train Loss:0.25115230679512024\n",
      "Epoch 17[138/625] Time:0.706, Train Loss:0.20045937597751617\n",
      "Epoch 17[139/625] Time:0.704, Train Loss:0.2655933201313019\n",
      "Epoch 17[140/625] Time:0.703, Train Loss:0.2503257393836975\n",
      "Epoch 17[141/625] Time:0.704, Train Loss:0.21293392777442932\n",
      "Epoch 17[142/625] Time:0.704, Train Loss:0.3140104115009308\n",
      "Epoch 17[143/625] Time:0.711, Train Loss:0.3461383879184723\n",
      "Epoch 17[144/625] Time:0.703, Train Loss:0.26893025636672974\n",
      "Epoch 17[145/625] Time:0.691, Train Loss:0.2628225088119507\n",
      "Epoch 17[146/625] Time:0.703, Train Loss:0.22455324232578278\n",
      "Epoch 17[147/625] Time:0.702, Train Loss:0.37511420249938965\n",
      "Epoch 17[148/625] Time:0.703, Train Loss:0.2750118672847748\n",
      "Epoch 17[149/625] Time:0.703, Train Loss:0.22081062197685242\n",
      "Epoch 17[150/625] Time:0.702, Train Loss:0.24349944293498993\n",
      "Epoch 17[151/625] Time:0.709, Train Loss:0.24461723864078522\n",
      "Epoch 17[152/625] Time:0.704, Train Loss:0.2757189869880676\n",
      "Epoch 17[153/625] Time:0.703, Train Loss:0.27970635890960693\n",
      "Epoch 17[154/625] Time:0.703, Train Loss:0.21307042241096497\n",
      "Epoch 17[155/625] Time:0.703, Train Loss:0.36455726623535156\n",
      "Epoch 17[156/625] Time:0.704, Train Loss:0.2864591181278229\n",
      "Epoch 17[157/625] Time:0.707, Train Loss:0.22158336639404297\n",
      "Epoch 17[158/625] Time:0.703, Train Loss:0.217878058552742\n",
      "Epoch 17[159/625] Time:0.703, Train Loss:0.25562822818756104\n",
      "Epoch 17[160/625] Time:0.704, Train Loss:0.33258771896362305\n",
      "Epoch 17[161/625] Time:0.703, Train Loss:0.2846934199333191\n",
      "Epoch 17[162/625] Time:0.704, Train Loss:0.2825532853603363\n",
      "Epoch 17[163/625] Time:0.704, Train Loss:0.30044806003570557\n",
      "Epoch 17[164/625] Time:0.702, Train Loss:0.2865888178348541\n",
      "Epoch 17[165/625] Time:0.704, Train Loss:0.24285611510276794\n",
      "Epoch 17[166/625] Time:0.704, Train Loss:0.2300422340631485\n",
      "Epoch 17[167/625] Time:0.705, Train Loss:0.2673800587654114\n",
      "Epoch 17[168/625] Time:0.714, Train Loss:0.34198781847953796\n",
      "Epoch 17[169/625] Time:0.706, Train Loss:0.26562049984931946\n",
      "Epoch 17[170/625] Time:0.693, Train Loss:0.2449863702058792\n",
      "Epoch 17[171/625] Time:0.693, Train Loss:0.29416024684906006\n",
      "Epoch 17[172/625] Time:0.694, Train Loss:0.2712664008140564\n",
      "Epoch 17[173/625] Time:0.694, Train Loss:0.2842651903629303\n",
      "Epoch 17[174/625] Time:0.694, Train Loss:0.30162209272384644\n",
      "Epoch 17[175/625] Time:0.695, Train Loss:0.25327786803245544\n",
      "Epoch 17[176/625] Time:0.704, Train Loss:0.27771586179733276\n",
      "Epoch 17[177/625] Time:0.703, Train Loss:0.22117967903614044\n",
      "Epoch 17[178/625] Time:0.705, Train Loss:0.351677805185318\n",
      "Epoch 17[179/625] Time:0.704, Train Loss:0.34565916657447815\n",
      "Epoch 17[180/625] Time:0.702, Train Loss:0.2621118128299713\n",
      "Epoch 17[181/625] Time:0.703, Train Loss:0.2555946409702301\n",
      "Epoch 17[182/625] Time:0.703, Train Loss:0.343027800321579\n",
      "Epoch 17[183/625] Time:0.703, Train Loss:0.27759435772895813\n",
      "Epoch 17[184/625] Time:0.702, Train Loss:0.30198734998703003\n",
      "Epoch 17[185/625] Time:0.702, Train Loss:0.24595674872398376\n",
      "Epoch 17[186/625] Time:0.703, Train Loss:0.25407424569129944\n",
      "Epoch 17[187/625] Time:0.703, Train Loss:0.3312279284000397\n",
      "Epoch 17[188/625] Time:0.702, Train Loss:0.30176249146461487\n",
      "Epoch 17[189/625] Time:0.703, Train Loss:0.20308290421962738\n",
      "Epoch 17[190/625] Time:0.703, Train Loss:0.2371218502521515\n",
      "Epoch 17[191/625] Time:0.71, Train Loss:0.2806139290332794\n",
      "Epoch 17[192/625] Time:0.702, Train Loss:0.2750823199748993\n",
      "Epoch 17[193/625] Time:0.703, Train Loss:0.2546805441379547\n",
      "Epoch 17[194/625] Time:0.702, Train Loss:0.25351908802986145\n",
      "Epoch 17[195/625] Time:0.702, Train Loss:0.3077772557735443\n",
      "Epoch 17[196/625] Time:0.702, Train Loss:0.2920280694961548\n",
      "Epoch 17[197/625] Time:0.703, Train Loss:0.2619551420211792\n",
      "Epoch 17[198/625] Time:0.703, Train Loss:0.3019990622997284\n",
      "Epoch 17[199/625] Time:0.705, Train Loss:0.23488326370716095\n",
      "Epoch 17[200/625] Time:0.703, Train Loss:0.20424650609493256\n",
      "Epoch 17[201/625] Time:0.702, Train Loss:0.21116667985916138\n",
      "Epoch 17[202/625] Time:0.729, Train Loss:0.3152903616428375\n",
      "Epoch 17[203/625] Time:0.702, Train Loss:0.22449076175689697\n",
      "Epoch 17[204/625] Time:0.706, Train Loss:0.2511030435562134\n",
      "Epoch 17[205/625] Time:0.703, Train Loss:0.3245318830013275\n",
      "Epoch 17[206/625] Time:0.703, Train Loss:0.25292080640792847\n",
      "Epoch 17[207/625] Time:0.709, Train Loss:0.23901130259037018\n",
      "Epoch 17[208/625] Time:0.707, Train Loss:0.24767716228961945\n",
      "Epoch 17[209/625] Time:0.702, Train Loss:0.20747093856334686\n",
      "Epoch 17[210/625] Time:0.703, Train Loss:0.23115085065364838\n",
      "Epoch 17[211/625] Time:0.702, Train Loss:0.2401803582906723\n",
      "Epoch 17[212/625] Time:0.702, Train Loss:0.3662510812282562\n",
      "Epoch 17[213/625] Time:0.704, Train Loss:0.3545379936695099\n",
      "Epoch 17[214/625] Time:0.703, Train Loss:0.18224136531352997\n",
      "Epoch 17[215/625] Time:0.702, Train Loss:0.3491470515727997\n",
      "Epoch 17[216/625] Time:0.703, Train Loss:0.29518261551856995\n",
      "Epoch 17[217/625] Time:0.703, Train Loss:0.2843376696109772\n",
      "Epoch 17[218/625] Time:0.703, Train Loss:0.3012758195400238\n",
      "Epoch 17[219/625] Time:0.703, Train Loss:0.30508625507354736\n",
      "Epoch 17[220/625] Time:0.703, Train Loss:0.2625184953212738\n",
      "Epoch 17[221/625] Time:0.704, Train Loss:0.2565506398677826\n",
      "Epoch 17[222/625] Time:0.703, Train Loss:0.34425440430641174\n",
      "Epoch 17[223/625] Time:0.71, Train Loss:0.2888091504573822\n",
      "Epoch 17[224/625] Time:0.701, Train Loss:0.31396540999412537\n",
      "Epoch 17[225/625] Time:0.703, Train Loss:0.28722599148750305\n",
      "Epoch 17[226/625] Time:0.704, Train Loss:0.2984899580478668\n",
      "Epoch 17[227/625] Time:0.703, Train Loss:0.3212127983570099\n",
      "Epoch 17[228/625] Time:0.702, Train Loss:0.2296023666858673\n",
      "Epoch 17[229/625] Time:0.705, Train Loss:0.3353678584098816\n",
      "Epoch 17[230/625] Time:0.703, Train Loss:0.29468363523483276\n",
      "Epoch 17[231/625] Time:0.704, Train Loss:0.3003775179386139\n",
      "Epoch 17[232/625] Time:0.702, Train Loss:0.357975572347641\n",
      "Epoch 17[233/625] Time:0.704, Train Loss:0.28212466835975647\n",
      "Epoch 17[234/625] Time:0.702, Train Loss:0.2930294871330261\n",
      "Epoch 17[235/625] Time:0.703, Train Loss:0.2382751703262329\n",
      "Epoch 17[236/625] Time:0.703, Train Loss:0.21128402650356293\n",
      "Epoch 17[237/625] Time:0.703, Train Loss:0.26723340153694153\n",
      "Epoch 17[238/625] Time:0.704, Train Loss:0.34537097811698914\n",
      "Epoch 17[239/625] Time:0.706, Train Loss:0.25708383321762085\n",
      "Epoch 17[240/625] Time:0.703, Train Loss:0.22708667814731598\n",
      "Epoch 17[241/625] Time:0.704, Train Loss:0.29779842495918274\n",
      "Epoch 17[242/625] Time:0.703, Train Loss:0.22392217814922333\n",
      "Epoch 17[243/625] Time:0.703, Train Loss:0.2583068609237671\n",
      "Epoch 17[244/625] Time:0.703, Train Loss:0.26815590262413025\n",
      "Epoch 17[245/625] Time:0.703, Train Loss:0.19362856447696686\n",
      "Epoch 17[246/625] Time:0.704, Train Loss:0.24047353863716125\n",
      "Epoch 17[247/625] Time:0.705, Train Loss:0.21747365593910217\n",
      "Epoch 17[248/625] Time:0.705, Train Loss:0.2819778621196747\n",
      "Epoch 17[249/625] Time:0.702, Train Loss:0.21959176659584045\n",
      "Epoch 17[250/625] Time:0.705, Train Loss:0.20130567252635956\n",
      "Epoch 17[251/625] Time:0.703, Train Loss:0.24546904861927032\n",
      "Epoch 17[252/625] Time:0.703, Train Loss:0.2690242528915405\n",
      "Epoch 17[253/625] Time:0.704, Train Loss:0.17318174242973328\n",
      "Epoch 17[254/625] Time:0.704, Train Loss:0.18975940346717834\n",
      "Epoch 17[255/625] Time:0.709, Train Loss:0.24675026535987854\n",
      "Epoch 17[256/625] Time:0.705, Train Loss:0.35559743642807007\n",
      "Epoch 17[257/625] Time:0.704, Train Loss:0.1953376978635788\n",
      "Epoch 17[258/625] Time:0.703, Train Loss:0.23877719044685364\n",
      "Epoch 17[259/625] Time:0.704, Train Loss:0.18322983384132385\n",
      "Epoch 17[260/625] Time:0.703, Train Loss:0.2250378429889679\n",
      "Epoch 17[261/625] Time:0.704, Train Loss:0.238802969455719\n",
      "Epoch 17[262/625] Time:0.703, Train Loss:0.23993568122386932\n",
      "Epoch 17[263/625] Time:0.704, Train Loss:0.26502498984336853\n",
      "Epoch 17[264/625] Time:0.703, Train Loss:0.3538251221179962\n",
      "Epoch 17[265/625] Time:0.703, Train Loss:0.3447018563747406\n",
      "Epoch 17[266/625] Time:0.702, Train Loss:0.22031185030937195\n",
      "Epoch 17[267/625] Time:0.703, Train Loss:0.31308895349502563\n",
      "Epoch 17[268/625] Time:0.702, Train Loss:0.259937047958374\n",
      "Epoch 17[269/625] Time:0.703, Train Loss:0.2326725423336029\n",
      "Epoch 17[270/625] Time:0.703, Train Loss:0.3298957645893097\n",
      "Epoch 17[271/625] Time:0.704, Train Loss:0.21056154370307922\n",
      "Epoch 17[272/625] Time:0.703, Train Loss:0.26791635155677795\n",
      "Epoch 17[273/625] Time:0.703, Train Loss:0.3386673629283905\n",
      "Epoch 17[274/625] Time:0.703, Train Loss:0.2527489960193634\n",
      "Epoch 17[275/625] Time:0.704, Train Loss:0.2759055495262146\n",
      "Epoch 17[276/625] Time:0.706, Train Loss:0.22151900827884674\n",
      "Epoch 17[277/625] Time:0.703, Train Loss:0.24530357122421265\n",
      "Epoch 17[278/625] Time:0.705, Train Loss:0.26853030920028687\n",
      "Epoch 17[279/625] Time:0.706, Train Loss:0.22705470025539398\n",
      "Epoch 17[280/625] Time:0.702, Train Loss:0.2721792757511139\n",
      "Epoch 17[281/625] Time:0.703, Train Loss:0.21177443861961365\n",
      "Epoch 17[282/625] Time:0.703, Train Loss:0.291509211063385\n",
      "Epoch 17[283/625] Time:0.705, Train Loss:0.2982816696166992\n",
      "Epoch 17[284/625] Time:0.703, Train Loss:0.23597931861877441\n",
      "Epoch 17[285/625] Time:0.703, Train Loss:0.28720802068710327\n",
      "Epoch 17[286/625] Time:0.704, Train Loss:0.24398353695869446\n",
      "Epoch 17[287/625] Time:0.703, Train Loss:0.2252850979566574\n",
      "Epoch 17[288/625] Time:0.703, Train Loss:0.33393874764442444\n",
      "Epoch 17[289/625] Time:0.703, Train Loss:0.26458925008773804\n",
      "Epoch 17[290/625] Time:0.704, Train Loss:0.21036623418331146\n",
      "Epoch 17[291/625] Time:0.705, Train Loss:0.280133992433548\n",
      "Epoch 17[292/625] Time:0.705, Train Loss:0.2792099416255951\n",
      "Epoch 17[293/625] Time:0.704, Train Loss:0.2281583845615387\n",
      "Epoch 17[294/625] Time:0.704, Train Loss:0.27072057127952576\n",
      "Epoch 17[295/625] Time:0.704, Train Loss:0.2668161988258362\n",
      "Epoch 17[296/625] Time:0.704, Train Loss:0.3098885118961334\n",
      "Epoch 17[297/625] Time:0.704, Train Loss:0.327048659324646\n",
      "Epoch 17[298/625] Time:0.706, Train Loss:0.29600149393081665\n",
      "Epoch 17[299/625] Time:0.739, Train Loss:0.22820459306240082\n",
      "Epoch 17[300/625] Time:0.694, Train Loss:0.274637371301651\n",
      "Epoch 17[301/625] Time:0.694, Train Loss:0.28716278076171875\n",
      "Epoch 17[302/625] Time:0.691, Train Loss:0.26141157746315\n",
      "Epoch 17[303/625] Time:0.696, Train Loss:0.2434501051902771\n",
      "Epoch 17[304/625] Time:0.706, Train Loss:0.33225682377815247\n",
      "Epoch 17[305/625] Time:0.705, Train Loss:0.33087217807769775\n",
      "Epoch 17[306/625] Time:0.703, Train Loss:0.19865185022354126\n",
      "Epoch 17[307/625] Time:0.705, Train Loss:0.28398647904396057\n",
      "Epoch 17[308/625] Time:0.704, Train Loss:0.2005416601896286\n",
      "Epoch 17[309/625] Time:0.703, Train Loss:0.2431449294090271\n",
      "Epoch 17[310/625] Time:0.702, Train Loss:0.3339015245437622\n",
      "Epoch 17[311/625] Time:0.704, Train Loss:0.2387445718050003\n",
      "Epoch 17[312/625] Time:0.702, Train Loss:0.23275461792945862\n",
      "Epoch 17[313/625] Time:0.702, Train Loss:0.2815873920917511\n",
      "Epoch 17[314/625] Time:0.703, Train Loss:0.3300454020500183\n",
      "Epoch 17[315/625] Time:0.703, Train Loss:0.22508500516414642\n",
      "Epoch 17[316/625] Time:0.703, Train Loss:0.32455456256866455\n",
      "Epoch 17[317/625] Time:0.703, Train Loss:0.2839716076850891\n",
      "Epoch 17[318/625] Time:0.702, Train Loss:0.24412302672863007\n",
      "Epoch 17[319/625] Time:0.703, Train Loss:0.28034597635269165\n",
      "Epoch 17[320/625] Time:0.702, Train Loss:0.3068578839302063\n",
      "Epoch 17[321/625] Time:0.703, Train Loss:0.2852250337600708\n",
      "Epoch 17[322/625] Time:0.705, Train Loss:0.3353034257888794\n",
      "Epoch 17[323/625] Time:0.704, Train Loss:0.2609390914440155\n",
      "Epoch 17[324/625] Time:0.705, Train Loss:0.29397326707839966\n",
      "Epoch 17[325/625] Time:0.703, Train Loss:0.29992398619651794\n",
      "Epoch 17[326/625] Time:0.704, Train Loss:0.2641668915748596\n",
      "Epoch 17[327/625] Time:0.706, Train Loss:0.25992947816848755\n",
      "Epoch 17[328/625] Time:0.704, Train Loss:0.276629239320755\n",
      "Epoch 17[329/625] Time:0.702, Train Loss:0.23562954366207123\n",
      "Epoch 17[330/625] Time:0.704, Train Loss:0.24582131206989288\n",
      "Epoch 17[331/625] Time:0.703, Train Loss:0.32463255524635315\n",
      "Epoch 17[332/625] Time:0.703, Train Loss:0.23739051818847656\n",
      "Epoch 17[333/625] Time:0.703, Train Loss:0.21467073261737823\n",
      "Epoch 17[334/625] Time:0.703, Train Loss:0.27973276376724243\n",
      "Epoch 17[335/625] Time:0.704, Train Loss:0.30152058601379395\n",
      "Epoch 17[336/625] Time:0.703, Train Loss:0.320759654045105\n",
      "Epoch 17[337/625] Time:0.703, Train Loss:0.2841741740703583\n",
      "Epoch 17[338/625] Time:0.703, Train Loss:0.22870928049087524\n",
      "Epoch 17[339/625] Time:0.704, Train Loss:0.3063608407974243\n",
      "Epoch 17[340/625] Time:0.703, Train Loss:0.3006815016269684\n",
      "Epoch 17[341/625] Time:0.703, Train Loss:0.21274033188819885\n",
      "Epoch 17[342/625] Time:0.703, Train Loss:0.3086276054382324\n",
      "Epoch 17[343/625] Time:0.704, Train Loss:0.27622926235198975\n",
      "Epoch 17[344/625] Time:0.704, Train Loss:0.24994266033172607\n",
      "Epoch 17[345/625] Time:0.703, Train Loss:0.343151718378067\n",
      "Epoch 17[346/625] Time:0.702, Train Loss:0.31023767590522766\n",
      "Epoch 17[347/625] Time:0.702, Train Loss:0.24480368196964264\n",
      "Epoch 17[348/625] Time:0.731, Train Loss:0.2813456356525421\n",
      "Epoch 17[349/625] Time:0.693, Train Loss:0.2582196593284607\n",
      "Epoch 17[350/625] Time:0.705, Train Loss:0.2045341581106186\n",
      "Epoch 17[351/625] Time:0.705, Train Loss:0.25457140803337097\n",
      "Epoch 17[352/625] Time:0.702, Train Loss:0.20548665523529053\n",
      "Epoch 17[353/625] Time:0.703, Train Loss:0.24688173830509186\n",
      "Epoch 17[354/625] Time:0.703, Train Loss:0.25862252712249756\n",
      "Epoch 17[355/625] Time:0.702, Train Loss:0.3380850851535797\n",
      "Epoch 17[356/625] Time:0.703, Train Loss:0.244137242436409\n",
      "Epoch 17[357/625] Time:0.703, Train Loss:0.2976307272911072\n",
      "Epoch 17[358/625] Time:0.703, Train Loss:0.2858599126338959\n",
      "Epoch 17[359/625] Time:0.704, Train Loss:0.26857489347457886\n",
      "Epoch 17[360/625] Time:0.702, Train Loss:0.25120770931243896\n",
      "Epoch 17[361/625] Time:0.702, Train Loss:0.2343900054693222\n",
      "Epoch 17[362/625] Time:0.703, Train Loss:0.2685770094394684\n",
      "Epoch 17[363/625] Time:0.703, Train Loss:0.3510901629924774\n",
      "Epoch 17[364/625] Time:0.704, Train Loss:0.26343509554862976\n",
      "Epoch 17[365/625] Time:0.694, Train Loss:0.22133085131645203\n",
      "Epoch 17[366/625] Time:0.703, Train Loss:0.21302694082260132\n",
      "Epoch 17[367/625] Time:0.703, Train Loss:0.20973408222198486\n",
      "Epoch 17[368/625] Time:0.703, Train Loss:0.21642345190048218\n",
      "Epoch 17[369/625] Time:0.702, Train Loss:0.2737587094306946\n",
      "Epoch 17[370/625] Time:0.703, Train Loss:0.363155335187912\n",
      "Epoch 17[371/625] Time:0.702, Train Loss:0.24386419355869293\n",
      "Epoch 17[372/625] Time:0.702, Train Loss:0.28161442279815674\n",
      "Epoch 17[373/625] Time:0.704, Train Loss:0.1465461552143097\n",
      "Epoch 17[374/625] Time:0.703, Train Loss:0.31552448868751526\n",
      "Epoch 17[375/625] Time:0.705, Train Loss:0.24978092312812805\n",
      "Epoch 17[376/625] Time:0.706, Train Loss:0.3015008568763733\n",
      "Epoch 17[377/625] Time:0.704, Train Loss:0.24161721765995026\n",
      "Epoch 17[378/625] Time:0.703, Train Loss:0.2766323685646057\n",
      "Epoch 17[379/625] Time:0.702, Train Loss:0.31715449690818787\n",
      "Epoch 17[380/625] Time:0.703, Train Loss:0.3294282555580139\n",
      "Epoch 17[381/625] Time:0.703, Train Loss:0.2936909794807434\n",
      "Epoch 17[382/625] Time:0.705, Train Loss:0.2681686282157898\n",
      "Epoch 17[383/625] Time:0.703, Train Loss:0.2736954092979431\n",
      "Epoch 17[384/625] Time:0.703, Train Loss:0.3096620738506317\n",
      "Epoch 17[385/625] Time:0.704, Train Loss:0.3802696168422699\n",
      "Epoch 17[386/625] Time:0.704, Train Loss:0.2615272104740143\n",
      "Epoch 17[387/625] Time:0.703, Train Loss:0.33519187569618225\n",
      "Epoch 17[388/625] Time:0.703, Train Loss:0.19459713995456696\n",
      "Epoch 17[389/625] Time:0.703, Train Loss:0.2972637712955475\n",
      "Epoch 17[390/625] Time:0.703, Train Loss:0.28252163529396057\n",
      "Epoch 17[391/625] Time:0.704, Train Loss:0.2419639527797699\n",
      "Epoch 17[392/625] Time:0.703, Train Loss:0.2740592658519745\n",
      "Epoch 17[393/625] Time:0.703, Train Loss:0.28499582409858704\n",
      "Epoch 17[394/625] Time:0.704, Train Loss:0.23852387070655823\n",
      "Epoch 17[395/625] Time:0.703, Train Loss:0.4220348000526428\n",
      "Epoch 17[396/625] Time:0.703, Train Loss:0.391409695148468\n",
      "Epoch 17[397/625] Time:0.703, Train Loss:0.3751618266105652\n",
      "Epoch 17[398/625] Time:0.703, Train Loss:0.2741171717643738\n",
      "Epoch 17[399/625] Time:0.703, Train Loss:0.282502144575119\n",
      "Epoch 17[400/625] Time:0.703, Train Loss:0.21462859213352203\n",
      "Epoch 17[401/625] Time:0.693, Train Loss:0.2902999222278595\n",
      "Epoch 17[402/625] Time:0.692, Train Loss:0.2752142548561096\n",
      "Epoch 17[403/625] Time:0.691, Train Loss:0.3532217741012573\n",
      "Epoch 17[404/625] Time:0.694, Train Loss:0.23826240003108978\n",
      "Epoch 17[405/625] Time:0.691, Train Loss:0.24721358716487885\n",
      "Epoch 17[406/625] Time:0.692, Train Loss:0.2785980701446533\n",
      "Epoch 17[407/625] Time:0.703, Train Loss:0.36550384759902954\n",
      "Epoch 17[408/625] Time:0.702, Train Loss:0.2624092698097229\n",
      "Epoch 17[409/625] Time:0.702, Train Loss:0.24489329755306244\n",
      "Epoch 17[410/625] Time:0.705, Train Loss:0.39638495445251465\n",
      "Epoch 17[411/625] Time:0.703, Train Loss:0.3010236322879791\n",
      "Epoch 17[412/625] Time:0.703, Train Loss:0.2601369619369507\n",
      "Epoch 17[413/625] Time:0.703, Train Loss:0.2271285504102707\n",
      "Epoch 17[414/625] Time:0.704, Train Loss:0.2710472345352173\n",
      "Epoch 17[415/625] Time:0.704, Train Loss:0.224781796336174\n",
      "Epoch 17[416/625] Time:0.705, Train Loss:0.16097980737686157\n",
      "Epoch 17[417/625] Time:0.703, Train Loss:0.2136976569890976\n",
      "Epoch 17[418/625] Time:0.704, Train Loss:0.2597215473651886\n",
      "Epoch 17[419/625] Time:0.703, Train Loss:0.28935518860816956\n",
      "Epoch 17[420/625] Time:0.705, Train Loss:0.33089354634284973\n",
      "Epoch 17[421/625] Time:0.703, Train Loss:0.2670983374118805\n",
      "Epoch 17[422/625] Time:0.704, Train Loss:0.27347394824028015\n",
      "Epoch 17[423/625] Time:0.703, Train Loss:0.26163849234580994\n",
      "Epoch 17[424/625] Time:0.702, Train Loss:0.27495241165161133\n",
      "Epoch 17[425/625] Time:0.704, Train Loss:0.24245122075080872\n",
      "Epoch 17[426/625] Time:0.706, Train Loss:0.2616293132305145\n",
      "Epoch 17[427/625] Time:0.704, Train Loss:0.329139769077301\n",
      "Epoch 17[428/625] Time:0.704, Train Loss:0.376689076423645\n",
      "Epoch 17[429/625] Time:0.706, Train Loss:0.23818638920783997\n",
      "Epoch 17[430/625] Time:0.71, Train Loss:0.244439035654068\n",
      "Epoch 17[431/625] Time:0.707, Train Loss:0.3244148790836334\n",
      "Epoch 17[432/625] Time:0.707, Train Loss:0.3464970886707306\n",
      "Epoch 17[433/625] Time:0.706, Train Loss:0.29147085547447205\n",
      "Epoch 17[434/625] Time:0.703, Train Loss:0.310594767332077\n",
      "Epoch 17[435/625] Time:0.702, Train Loss:0.3041955828666687\n",
      "Epoch 17[436/625] Time:0.703, Train Loss:0.2280593067407608\n",
      "Epoch 17[437/625] Time:0.703, Train Loss:0.24981899559497833\n",
      "Epoch 17[438/625] Time:0.702, Train Loss:0.2999165654182434\n",
      "Epoch 17[439/625] Time:0.702, Train Loss:0.22108793258666992\n",
      "Epoch 17[440/625] Time:0.702, Train Loss:0.2657764256000519\n",
      "Epoch 17[441/625] Time:0.702, Train Loss:0.30384114384651184\n",
      "Epoch 17[442/625] Time:0.702, Train Loss:0.25897201895713806\n",
      "Epoch 17[443/625] Time:0.703, Train Loss:0.23254671692848206\n",
      "Epoch 17[444/625] Time:0.702, Train Loss:0.3456864058971405\n",
      "Epoch 17[445/625] Time:0.702, Train Loss:0.21367256343364716\n",
      "Epoch 17[446/625] Time:0.704, Train Loss:0.22049112617969513\n",
      "Epoch 17[447/625] Time:0.702, Train Loss:0.22236934304237366\n",
      "Epoch 17[448/625] Time:0.702, Train Loss:0.26028043031692505\n",
      "Epoch 17[449/625] Time:0.702, Train Loss:0.2505291700363159\n",
      "Epoch 17[450/625] Time:0.702, Train Loss:0.24117411673069\n",
      "Epoch 17[451/625] Time:0.703, Train Loss:0.3490632176399231\n",
      "Epoch 17[452/625] Time:0.728, Train Loss:0.2981703281402588\n",
      "Epoch 17[453/625] Time:0.703, Train Loss:0.2659013569355011\n",
      "Epoch 17[454/625] Time:0.705, Train Loss:0.24919722974300385\n",
      "Epoch 17[455/625] Time:0.703, Train Loss:0.24656406044960022\n",
      "Epoch 17[456/625] Time:0.704, Train Loss:0.23876440525054932\n",
      "Epoch 17[457/625] Time:0.703, Train Loss:0.23817458748817444\n",
      "Epoch 17[458/625] Time:0.703, Train Loss:0.2503111958503723\n",
      "Epoch 17[459/625] Time:0.702, Train Loss:0.3810417056083679\n",
      "Epoch 17[460/625] Time:0.702, Train Loss:0.2854498624801636\n",
      "Epoch 17[461/625] Time:0.703, Train Loss:0.3426509499549866\n",
      "Epoch 17[462/625] Time:0.702, Train Loss:0.27328184247016907\n",
      "Epoch 17[463/625] Time:0.704, Train Loss:0.18814846873283386\n",
      "Epoch 17[464/625] Time:0.717, Train Loss:0.3396507799625397\n",
      "Epoch 17[465/625] Time:0.703, Train Loss:0.22953587770462036\n",
      "Epoch 17[466/625] Time:0.702, Train Loss:0.21968209743499756\n",
      "Epoch 17[467/625] Time:0.703, Train Loss:0.27970215678215027\n",
      "Epoch 17[468/625] Time:0.705, Train Loss:0.2794624865055084\n",
      "Epoch 17[469/625] Time:0.703, Train Loss:0.22670303285121918\n",
      "Epoch 17[470/625] Time:0.706, Train Loss:0.2794318199157715\n",
      "Epoch 17[471/625] Time:0.704, Train Loss:0.40783020853996277\n",
      "Epoch 17[472/625] Time:0.705, Train Loss:0.321056604385376\n",
      "Epoch 17[473/625] Time:0.704, Train Loss:0.3114260137081146\n",
      "Epoch 17[474/625] Time:0.704, Train Loss:0.23752866685390472\n",
      "Epoch 17[475/625] Time:0.704, Train Loss:0.2319195717573166\n",
      "Epoch 17[476/625] Time:0.704, Train Loss:0.31106483936309814\n",
      "Epoch 17[477/625] Time:0.71, Train Loss:0.346748411655426\n",
      "Epoch 17[478/625] Time:0.714, Train Loss:0.23716002702713013\n",
      "Epoch 17[479/625] Time:0.705, Train Loss:0.3222607374191284\n",
      "Epoch 17[480/625] Time:0.705, Train Loss:0.33501654863357544\n",
      "Epoch 17[481/625] Time:0.704, Train Loss:0.26342347264289856\n",
      "Epoch 17[482/625] Time:0.705, Train Loss:0.3065892457962036\n",
      "Epoch 17[483/625] Time:0.721, Train Loss:0.35837239027023315\n",
      "Epoch 17[484/625] Time:0.691, Train Loss:0.28066328167915344\n",
      "Epoch 17[485/625] Time:0.692, Train Loss:0.24697506427764893\n",
      "Epoch 17[486/625] Time:0.707, Train Loss:0.26011523604393005\n",
      "Epoch 17[487/625] Time:0.703, Train Loss:0.2744593620300293\n",
      "Epoch 17[488/625] Time:0.703, Train Loss:0.33211585879325867\n",
      "Epoch 17[489/625] Time:0.705, Train Loss:0.3103720545768738\n",
      "Epoch 17[490/625] Time:0.713, Train Loss:0.17036172747612\n",
      "Epoch 17[491/625] Time:0.74, Train Loss:0.30882740020751953\n",
      "Epoch 17[492/625] Time:0.704, Train Loss:0.2744060456752777\n",
      "Epoch 17[493/625] Time:0.704, Train Loss:0.23069527745246887\n",
      "Epoch 17[494/625] Time:0.703, Train Loss:0.2862023413181305\n",
      "Epoch 17[495/625] Time:0.702, Train Loss:0.3472672402858734\n",
      "Epoch 17[496/625] Time:0.704, Train Loss:0.2662312984466553\n",
      "Epoch 17[497/625] Time:0.703, Train Loss:0.26331132650375366\n",
      "Epoch 17[498/625] Time:0.703, Train Loss:0.32170185446739197\n",
      "Epoch 17[499/625] Time:0.703, Train Loss:0.2891918122768402\n",
      "Epoch 17[500/625] Time:0.703, Train Loss:0.24917389452457428\n",
      "Epoch 17[501/625] Time:0.704, Train Loss:0.3235214948654175\n",
      "Epoch 17[502/625] Time:0.703, Train Loss:0.2989005148410797\n",
      "Epoch 17[503/625] Time:0.702, Train Loss:0.31123074889183044\n",
      "Epoch 17[504/625] Time:0.704, Train Loss:0.25542953610420227\n",
      "Epoch 17[505/625] Time:0.702, Train Loss:0.22197706997394562\n",
      "Epoch 17[506/625] Time:0.703, Train Loss:0.27156996726989746\n",
      "Epoch 17[507/625] Time:0.703, Train Loss:0.2985745966434479\n",
      "Epoch 17[508/625] Time:0.703, Train Loss:0.29790741205215454\n",
      "Epoch 17[509/625] Time:0.702, Train Loss:0.2696606516838074\n",
      "Epoch 17[510/625] Time:0.703, Train Loss:0.27559250593185425\n",
      "Epoch 17[511/625] Time:0.702, Train Loss:0.269767165184021\n",
      "Epoch 17[512/625] Time:0.705, Train Loss:0.21808913350105286\n",
      "Epoch 17[513/625] Time:0.703, Train Loss:0.259765088558197\n",
      "Epoch 17[514/625] Time:0.703, Train Loss:0.22223858535289764\n",
      "Epoch 17[515/625] Time:0.704, Train Loss:0.3518086075782776\n",
      "Epoch 17[516/625] Time:0.704, Train Loss:0.34524068236351013\n",
      "Epoch 17[517/625] Time:0.703, Train Loss:0.26006752252578735\n",
      "Epoch 17[518/625] Time:0.703, Train Loss:0.2669755816459656\n",
      "Epoch 17[519/625] Time:0.704, Train Loss:0.20286040008068085\n",
      "Epoch 17[520/625] Time:0.702, Train Loss:0.231766477227211\n",
      "Epoch 17[521/625] Time:0.703, Train Loss:0.2203233689069748\n",
      "Epoch 17[522/625] Time:0.703, Train Loss:0.27940472960472107\n",
      "Epoch 17[523/625] Time:0.702, Train Loss:0.32949769496917725\n",
      "Epoch 17[524/625] Time:0.703, Train Loss:0.29633718729019165\n",
      "Epoch 17[525/625] Time:0.702, Train Loss:0.3198152184486389\n",
      "Epoch 17[526/625] Time:0.705, Train Loss:0.3282248377799988\n",
      "Epoch 17[527/625] Time:0.704, Train Loss:0.3034176826477051\n",
      "Epoch 17[528/625] Time:0.703, Train Loss:0.21310727298259735\n",
      "Epoch 17[529/625] Time:0.702, Train Loss:0.199610635638237\n",
      "Epoch 17[530/625] Time:0.702, Train Loss:0.3193327784538269\n",
      "Epoch 17[531/625] Time:0.703, Train Loss:0.24622897803783417\n",
      "Epoch 17[532/625] Time:0.702, Train Loss:0.2721656858921051\n",
      "Epoch 17[533/625] Time:0.702, Train Loss:0.27757400274276733\n",
      "Epoch 17[534/625] Time:0.703, Train Loss:0.17035087943077087\n",
      "Epoch 17[535/625] Time:0.705, Train Loss:0.24557776749134064\n",
      "Epoch 17[536/625] Time:0.704, Train Loss:0.34296461939811707\n",
      "Epoch 17[537/625] Time:0.703, Train Loss:0.2504681944847107\n",
      "Epoch 17[538/625] Time:0.702, Train Loss:0.29085853695869446\n",
      "Epoch 17[539/625] Time:0.702, Train Loss:0.313882976770401\n",
      "Epoch 17[540/625] Time:0.703, Train Loss:0.2539690136909485\n",
      "Epoch 17[541/625] Time:0.705, Train Loss:0.2062399685382843\n",
      "Epoch 17[542/625] Time:0.703, Train Loss:0.29993435740470886\n",
      "Epoch 17[543/625] Time:0.718, Train Loss:0.24985449016094208\n",
      "Epoch 17[544/625] Time:0.702, Train Loss:0.23884114623069763\n",
      "Epoch 17[545/625] Time:0.703, Train Loss:0.315079927444458\n",
      "Epoch 17[546/625] Time:0.702, Train Loss:0.34479811787605286\n",
      "Epoch 17[547/625] Time:0.703, Train Loss:0.2650727927684784\n",
      "Epoch 17[548/625] Time:0.704, Train Loss:0.2683221101760864\n",
      "Epoch 17[549/625] Time:0.703, Train Loss:0.28464871644973755\n",
      "Epoch 17[550/625] Time:0.708, Train Loss:0.26583582162857056\n",
      "Epoch 17[551/625] Time:0.703, Train Loss:0.26202425360679626\n",
      "Epoch 17[552/625] Time:0.705, Train Loss:0.17765404284000397\n",
      "Epoch 17[553/625] Time:0.735, Train Loss:0.24209372699260712\n",
      "Epoch 17[554/625] Time:0.692, Train Loss:0.40220820903778076\n",
      "Epoch 17[555/625] Time:0.693, Train Loss:0.27586135268211365\n",
      "Epoch 17[556/625] Time:0.696, Train Loss:0.3051609396934509\n",
      "Epoch 17[557/625] Time:0.694, Train Loss:0.26914408802986145\n",
      "Epoch 17[558/625] Time:0.696, Train Loss:0.35326114296913147\n",
      "Epoch 17[559/625] Time:0.697, Train Loss:0.2193000167608261\n",
      "Epoch 17[560/625] Time:0.695, Train Loss:0.22262020409107208\n",
      "Epoch 17[561/625] Time:0.695, Train Loss:0.2400309443473816\n",
      "Epoch 17[562/625] Time:0.703, Train Loss:0.2851632237434387\n",
      "Epoch 17[563/625] Time:0.704, Train Loss:0.3362470269203186\n",
      "Epoch 17[564/625] Time:0.704, Train Loss:0.20619994401931763\n",
      "Epoch 17[565/625] Time:0.703, Train Loss:0.3245198130607605\n",
      "Epoch 17[566/625] Time:0.705, Train Loss:0.3686889410018921\n",
      "Epoch 17[567/625] Time:0.704, Train Loss:0.26817503571510315\n",
      "Epoch 17[568/625] Time:0.703, Train Loss:0.2899084687232971\n",
      "Epoch 17[569/625] Time:0.703, Train Loss:0.26163461804389954\n",
      "Epoch 17[570/625] Time:0.703, Train Loss:0.24733711779117584\n",
      "Epoch 17[571/625] Time:0.704, Train Loss:0.2193354368209839\n",
      "Epoch 17[572/625] Time:0.706, Train Loss:0.32599854469299316\n",
      "Epoch 17[573/625] Time:0.703, Train Loss:0.33251577615737915\n",
      "Epoch 17[574/625] Time:0.704, Train Loss:0.21640650928020477\n",
      "Epoch 17[575/625] Time:0.702, Train Loss:0.2725718021392822\n",
      "Epoch 17[576/625] Time:0.707, Train Loss:0.25396719574928284\n",
      "Epoch 17[577/625] Time:0.703, Train Loss:0.25592392683029175\n",
      "Epoch 17[578/625] Time:0.704, Train Loss:0.26470690965652466\n",
      "Epoch 17[579/625] Time:0.709, Train Loss:0.38178563117980957\n",
      "Epoch 17[580/625] Time:0.703, Train Loss:0.279458224773407\n",
      "Epoch 17[581/625] Time:0.702, Train Loss:0.24378322064876556\n",
      "Epoch 17[582/625] Time:0.703, Train Loss:0.29469674825668335\n",
      "Epoch 17[583/625] Time:0.703, Train Loss:0.23599189519882202\n",
      "Epoch 17[584/625] Time:0.702, Train Loss:0.27373337745666504\n",
      "Epoch 17[585/625] Time:0.703, Train Loss:0.26734909415245056\n",
      "Epoch 17[586/625] Time:0.703, Train Loss:0.324570894241333\n",
      "Epoch 17[587/625] Time:0.703, Train Loss:0.26041969656944275\n",
      "Epoch 17[588/625] Time:0.703, Train Loss:0.25192686915397644\n",
      "Epoch 17[589/625] Time:0.704, Train Loss:0.27207911014556885\n",
      "Epoch 17[590/625] Time:0.702, Train Loss:0.35471564531326294\n",
      "Epoch 17[591/625] Time:0.706, Train Loss:0.28574609756469727\n",
      "Epoch 17[592/625] Time:0.702, Train Loss:0.34061330556869507\n",
      "Epoch 17[593/625] Time:0.704, Train Loss:0.25363582372665405\n",
      "Epoch 17[594/625] Time:0.708, Train Loss:0.24947474896907806\n",
      "Epoch 17[595/625] Time:0.703, Train Loss:0.24550285935401917\n",
      "Epoch 17[596/625] Time:0.705, Train Loss:0.19327136874198914\n",
      "Epoch 17[597/625] Time:0.703, Train Loss:0.18851277232170105\n",
      "Epoch 17[598/625] Time:0.705, Train Loss:0.23834238946437836\n",
      "Epoch 17[599/625] Time:0.693, Train Loss:0.2506295144557953\n",
      "Epoch 17[600/625] Time:0.703, Train Loss:0.2349388599395752\n",
      "Epoch 17[601/625] Time:0.704, Train Loss:0.2631639838218689\n",
      "Epoch 17[602/625] Time:0.704, Train Loss:0.2822219431400299\n",
      "Epoch 17[603/625] Time:0.705, Train Loss:0.25736647844314575\n",
      "Epoch 17[604/625] Time:0.713, Train Loss:0.2500384449958801\n",
      "Epoch 17[605/625] Time:0.691, Train Loss:0.22153161466121674\n",
      "Epoch 17[606/625] Time:0.693, Train Loss:0.28014320135116577\n",
      "Epoch 17[607/625] Time:0.693, Train Loss:0.34621530771255493\n",
      "Epoch 17[608/625] Time:0.695, Train Loss:0.2796688675880432\n",
      "Epoch 17[609/625] Time:0.694, Train Loss:0.24253159761428833\n",
      "Epoch 17[610/625] Time:0.693, Train Loss:0.2450362592935562\n",
      "Epoch 17[611/625] Time:0.693, Train Loss:0.3175375163555145\n",
      "Epoch 17[612/625] Time:0.692, Train Loss:0.2573074996471405\n",
      "Epoch 17[613/625] Time:0.692, Train Loss:0.30494654178619385\n",
      "Epoch 17[614/625] Time:0.693, Train Loss:0.2490130215883255\n",
      "Epoch 17[615/625] Time:0.694, Train Loss:0.21102213859558105\n",
      "Epoch 17[616/625] Time:0.694, Train Loss:0.25571030378341675\n",
      "Epoch 17[617/625] Time:0.702, Train Loss:0.2616609036922455\n",
      "Epoch 17[618/625] Time:0.703, Train Loss:0.2037823349237442\n",
      "Epoch 17[619/625] Time:0.703, Train Loss:0.25531455874443054\n",
      "Epoch 17[620/625] Time:0.703, Train Loss:0.2659991979598999\n",
      "Epoch 17[621/625] Time:0.703, Train Loss:0.32714614272117615\n",
      "Epoch 17[622/625] Time:0.703, Train Loss:0.3463596701622009\n",
      "Epoch 17[623/625] Time:0.703, Train Loss:0.19409185647964478\n",
      "Epoch 17[624/625] Time:0.702, Train Loss:0.21642309427261353\n",
      "Epoch 17[0/78] Val Loss:0.20537801086902618\n",
      "Epoch 17[1/78] Val Loss:0.1887274831533432\n",
      "Epoch 17[2/78] Val Loss:0.16854873299598694\n",
      "Epoch 17[3/78] Val Loss:0.21886208653450012\n",
      "Epoch 17[4/78] Val Loss:0.2828560173511505\n",
      "Epoch 17[5/78] Val Loss:0.25239840149879456\n",
      "Epoch 17[6/78] Val Loss:0.24290259182453156\n",
      "Epoch 17[7/78] Val Loss:0.3196907341480255\n",
      "Epoch 17[8/78] Val Loss:0.14062927663326263\n",
      "Epoch 17[9/78] Val Loss:0.1306219846010208\n",
      "Epoch 17[10/78] Val Loss:0.08860442042350769\n",
      "Epoch 17[11/78] Val Loss:0.1277351975440979\n",
      "Epoch 17[12/78] Val Loss:0.11179589480161667\n",
      "Epoch 17[13/78] Val Loss:0.08510912954807281\n",
      "Epoch 17[14/78] Val Loss:0.18942224979400635\n",
      "Epoch 17[15/78] Val Loss:0.16798654198646545\n",
      "Epoch 17[16/78] Val Loss:0.15027087926864624\n",
      "Epoch 17[17/78] Val Loss:0.13684602081775665\n",
      "Epoch 17[18/78] Val Loss:0.17969480156898499\n",
      "Epoch 17[19/78] Val Loss:0.2222570925951004\n",
      "Epoch 17[20/78] Val Loss:0.14549310505390167\n",
      "Epoch 17[21/78] Val Loss:0.48590657114982605\n",
      "Epoch 17[22/78] Val Loss:0.6647030115127563\n",
      "Epoch 17[23/78] Val Loss:0.4597053825855255\n",
      "Epoch 17[24/78] Val Loss:0.328619122505188\n",
      "Epoch 17[25/78] Val Loss:0.41097551584243774\n",
      "Epoch 17[26/78] Val Loss:0.3734871745109558\n",
      "Epoch 17[27/78] Val Loss:0.33049649000167847\n",
      "Epoch 17[28/78] Val Loss:0.3226635456085205\n",
      "Epoch 17[29/78] Val Loss:0.5275043249130249\n",
      "Epoch 17[30/78] Val Loss:2.0690364837646484\n",
      "Epoch 17[31/78] Val Loss:1.7910215854644775\n",
      "Epoch 17[32/78] Val Loss:1.3673986196517944\n",
      "Epoch 17[33/78] Val Loss:0.40442946553230286\n",
      "Epoch 17[34/78] Val Loss:0.2789043188095093\n",
      "Epoch 17[35/78] Val Loss:0.31188729405403137\n",
      "Epoch 17[36/78] Val Loss:0.3016141653060913\n",
      "Epoch 17[37/78] Val Loss:0.3765707015991211\n",
      "Epoch 17[38/78] Val Loss:0.1817638874053955\n",
      "Epoch 17[39/78] Val Loss:0.19689352810382843\n",
      "Epoch 17[40/78] Val Loss:0.21111972630023956\n",
      "Epoch 17[41/78] Val Loss:0.21695758402347565\n",
      "Epoch 17[42/78] Val Loss:0.20168039202690125\n",
      "Epoch 17[43/78] Val Loss:0.11896445602178574\n",
      "Epoch 17[44/78] Val Loss:0.09446050226688385\n",
      "Epoch 17[45/78] Val Loss:0.07643965631723404\n",
      "Epoch 17[46/78] Val Loss:0.09998258203268051\n",
      "Epoch 17[47/78] Val Loss:0.08180780708789825\n",
      "Epoch 17[48/78] Val Loss:0.11307717114686966\n",
      "Epoch 17[49/78] Val Loss:0.07723431289196014\n",
      "Epoch 17[50/78] Val Loss:0.08348671346902847\n",
      "Epoch 17[51/78] Val Loss:0.07156780362129211\n",
      "Epoch 17[52/78] Val Loss:0.08451578766107559\n",
      "Epoch 17[53/78] Val Loss:0.0840563252568245\n",
      "Epoch 17[54/78] Val Loss:0.06127294898033142\n",
      "Epoch 17[55/78] Val Loss:0.09900307655334473\n",
      "Epoch 17[56/78] Val Loss:0.5655325055122375\n",
      "Epoch 17[57/78] Val Loss:0.5169603824615479\n",
      "Epoch 17[58/78] Val Loss:0.4977078437805176\n",
      "Epoch 17[59/78] Val Loss:0.35377728939056396\n",
      "Epoch 17[60/78] Val Loss:0.3471721112728119\n",
      "Epoch 17[61/78] Val Loss:0.45889008045196533\n",
      "Epoch 17[62/78] Val Loss:0.40547797083854675\n",
      "Epoch 17[63/78] Val Loss:0.22284835577011108\n",
      "Epoch 17[64/78] Val Loss:0.11283959448337555\n",
      "Epoch 17[65/78] Val Loss:0.10054673999547958\n",
      "Epoch 17[66/78] Val Loss:0.1458343267440796\n",
      "Epoch 17[67/78] Val Loss:0.13487766683101654\n",
      "Epoch 17[68/78] Val Loss:0.4528699219226837\n",
      "Epoch 17[69/78] Val Loss:0.43981507420539856\n",
      "Epoch 17[70/78] Val Loss:0.46859005093574524\n",
      "Epoch 17[71/78] Val Loss:0.3479030132293701\n",
      "Epoch 17[72/78] Val Loss:0.17085617780685425\n",
      "Epoch 17[73/78] Val Loss:0.15453968942165375\n",
      "Epoch 17[74/78] Val Loss:0.4147430658340454\n",
      "Epoch 17[75/78] Val Loss:0.5520956516265869\n",
      "Epoch 17[76/78] Val Loss:0.5301991701126099\n",
      "Epoch 17[77/78] Val Loss:0.5717141032218933\n",
      "Epoch 17[78/78] Val Loss:0.6970764398574829\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.89      0.92     15691\n",
      "           1       0.67      0.81      0.74      4309\n",
      "\n",
      "    accuracy                           0.88     20000\n",
      "   macro avg       0.81      0.85      0.83     20000\n",
      "weighted avg       0.89      0.88      0.88     20000\n",
      "\n",
      "Epoch 17: Train Loss 0.27369457244873047, Val Loss 0.325357768111504, Train Time 785.5258989334106, Val Time 36.75454306602478\n",
      "Epoch 18[0/625] Time:0.729, Train Loss:0.2605522572994232\n",
      "Epoch 18[1/625] Time:0.693, Train Loss:0.2893703281879425\n",
      "Epoch 18[2/625] Time:0.694, Train Loss:0.23515582084655762\n",
      "Epoch 18[3/625] Time:0.693, Train Loss:0.2664738595485687\n",
      "Epoch 18[4/625] Time:0.693, Train Loss:0.21128123998641968\n",
      "Epoch 18[5/625] Time:0.694, Train Loss:0.269845575094223\n",
      "Epoch 18[6/625] Time:0.694, Train Loss:0.2850661873817444\n",
      "Epoch 18[7/625] Time:0.694, Train Loss:0.2520339787006378\n",
      "Epoch 18[8/625] Time:0.694, Train Loss:0.24311766028404236\n",
      "Epoch 18[9/625] Time:0.694, Train Loss:0.23650583624839783\n",
      "Epoch 18[10/625] Time:0.694, Train Loss:0.337399423122406\n",
      "Epoch 18[11/625] Time:0.694, Train Loss:0.20279783010482788\n",
      "Epoch 18[12/625] Time:0.694, Train Loss:0.25309503078460693\n",
      "Epoch 18[13/625] Time:0.701, Train Loss:0.2861875295639038\n",
      "Epoch 18[14/625] Time:0.731, Train Loss:0.1848938912153244\n",
      "Epoch 18[15/625] Time:0.692, Train Loss:0.24194833636283875\n",
      "Epoch 18[16/625] Time:0.694, Train Loss:0.24385257065296173\n",
      "Epoch 18[17/625] Time:0.694, Train Loss:0.22573213279247284\n",
      "Epoch 18[18/625] Time:0.694, Train Loss:0.27833959460258484\n",
      "Epoch 18[19/625] Time:0.694, Train Loss:0.19786284863948822\n",
      "Epoch 18[20/625] Time:0.693, Train Loss:0.3506583571434021\n",
      "Epoch 18[21/625] Time:0.693, Train Loss:0.2655780017375946\n",
      "Epoch 18[22/625] Time:0.694, Train Loss:0.14199167490005493\n",
      "Epoch 18[23/625] Time:0.694, Train Loss:0.2980722486972809\n",
      "Epoch 18[24/625] Time:0.693, Train Loss:0.3236021101474762\n",
      "Epoch 18[25/625] Time:0.693, Train Loss:0.24861611425876617\n",
      "Epoch 18[26/625] Time:0.694, Train Loss:0.23105894029140472\n",
      "Epoch 18[27/625] Time:0.703, Train Loss:0.22817231714725494\n",
      "Epoch 18[28/625] Time:0.703, Train Loss:0.22039827704429626\n",
      "Epoch 18[29/625] Time:0.703, Train Loss:0.3060896098613739\n",
      "Epoch 18[30/625] Time:0.705, Train Loss:0.225620299577713\n",
      "Epoch 18[31/625] Time:0.701, Train Loss:0.30213284492492676\n",
      "Epoch 18[32/625] Time:0.703, Train Loss:0.2848837673664093\n",
      "Epoch 18[33/625] Time:0.704, Train Loss:0.28442153334617615\n",
      "Epoch 18[34/625] Time:0.706, Train Loss:0.3340889513492584\n",
      "Epoch 18[35/625] Time:0.704, Train Loss:0.2831171751022339\n",
      "Epoch 18[36/625] Time:0.704, Train Loss:0.24358762800693512\n",
      "Epoch 18[37/625] Time:0.704, Train Loss:0.2668361961841583\n",
      "Epoch 18[38/625] Time:0.704, Train Loss:0.23962169885635376\n",
      "Epoch 18[39/625] Time:0.705, Train Loss:0.2816862165927887\n",
      "Epoch 18[40/625] Time:0.702, Train Loss:0.24905888736248016\n",
      "Epoch 18[41/625] Time:0.707, Train Loss:0.26282840967178345\n",
      "Epoch 18[42/625] Time:0.702, Train Loss:0.22241716086864471\n",
      "Epoch 18[43/625] Time:0.703, Train Loss:0.30419477820396423\n",
      "Epoch 18[44/625] Time:0.703, Train Loss:0.2949429154396057\n",
      "Epoch 18[45/625] Time:0.704, Train Loss:0.30096301436424255\n",
      "Epoch 18[46/625] Time:0.704, Train Loss:0.2995942533016205\n",
      "Epoch 18[47/625] Time:0.703, Train Loss:0.14463289082050323\n",
      "Epoch 18[48/625] Time:0.704, Train Loss:0.36241424083709717\n",
      "Epoch 18[49/625] Time:0.703, Train Loss:0.20900166034698486\n",
      "Epoch 18[50/625] Time:0.698, Train Loss:0.19516482949256897\n",
      "Epoch 18[51/625] Time:0.694, Train Loss:0.2907658517360687\n",
      "Epoch 18[52/625] Time:0.695, Train Loss:0.2953788638114929\n",
      "Epoch 18[53/625] Time:0.695, Train Loss:0.2987855076789856\n",
      "Epoch 18[54/625] Time:0.699, Train Loss:0.25603440403938293\n",
      "Epoch 18[55/625] Time:0.694, Train Loss:0.2580513656139374\n",
      "Epoch 18[56/625] Time:0.695, Train Loss:0.28524261713027954\n",
      "Epoch 18[57/625] Time:0.694, Train Loss:0.20852471888065338\n",
      "Epoch 18[58/625] Time:0.695, Train Loss:0.26203635334968567\n",
      "Epoch 18[59/625] Time:0.694, Train Loss:0.26078858971595764\n",
      "Epoch 18[60/625] Time:0.695, Train Loss:0.2186928540468216\n",
      "Epoch 18[61/625] Time:0.694, Train Loss:0.2487199455499649\n",
      "Epoch 18[62/625] Time:0.703, Train Loss:0.2634955048561096\n",
      "Epoch 18[63/625] Time:0.691, Train Loss:0.28571826219558716\n",
      "Epoch 18[64/625] Time:0.716, Train Loss:0.2480052411556244\n",
      "Epoch 18[65/625] Time:0.704, Train Loss:0.2194095402956009\n",
      "Epoch 18[66/625] Time:0.705, Train Loss:0.2997497320175171\n",
      "Epoch 18[67/625] Time:0.704, Train Loss:0.231432244181633\n",
      "Epoch 18[68/625] Time:0.707, Train Loss:0.3804101049900055\n",
      "Epoch 18[69/625] Time:0.745, Train Loss:0.21412700414657593\n",
      "Epoch 18[70/625] Time:0.702, Train Loss:0.27217257022857666\n",
      "Epoch 18[71/625] Time:0.704, Train Loss:0.3146105706691742\n",
      "Epoch 18[72/625] Time:0.729, Train Loss:0.3187468945980072\n",
      "Epoch 18[73/625] Time:0.692, Train Loss:0.25708165764808655\n",
      "Epoch 18[74/625] Time:0.705, Train Loss:0.24350407719612122\n",
      "Epoch 18[75/625] Time:0.706, Train Loss:0.268293559551239\n",
      "Epoch 18[76/625] Time:0.707, Train Loss:0.29756665229797363\n",
      "Epoch 18[77/625] Time:0.707, Train Loss:0.24682605266571045\n",
      "Epoch 18[78/625] Time:0.705, Train Loss:0.22774258255958557\n",
      "Epoch 18[79/625] Time:0.704, Train Loss:0.33115753531455994\n",
      "Epoch 18[80/625] Time:0.704, Train Loss:0.23467151820659637\n",
      "Epoch 18[81/625] Time:0.704, Train Loss:0.2676152288913727\n",
      "Epoch 18[82/625] Time:0.706, Train Loss:0.24607045948505402\n",
      "Epoch 18[83/625] Time:0.706, Train Loss:0.22893646359443665\n",
      "Epoch 18[84/625] Time:0.705, Train Loss:0.33198511600494385\n",
      "Epoch 18[85/625] Time:0.705, Train Loss:0.19568593800067902\n",
      "Epoch 18[86/625] Time:0.706, Train Loss:0.1933317929506302\n",
      "Epoch 18[87/625] Time:0.706, Train Loss:0.25710925459861755\n",
      "Epoch 18[88/625] Time:0.705, Train Loss:0.3069274127483368\n",
      "Epoch 18[89/625] Time:0.708, Train Loss:0.33009958267211914\n",
      "Epoch 18[90/625] Time:0.708, Train Loss:0.267683744430542\n",
      "Epoch 18[91/625] Time:0.705, Train Loss:0.28548067808151245\n",
      "Epoch 18[92/625] Time:0.706, Train Loss:0.2591444253921509\n",
      "Epoch 18[93/625] Time:0.706, Train Loss:0.33524149656295776\n",
      "Epoch 18[94/625] Time:0.706, Train Loss:0.36121076345443726\n",
      "Epoch 18[95/625] Time:0.704, Train Loss:0.23077994585037231\n",
      "Epoch 18[96/625] Time:0.706, Train Loss:0.2376471310853958\n",
      "Epoch 18[97/625] Time:0.705, Train Loss:0.3024137020111084\n",
      "Epoch 18[98/625] Time:0.715, Train Loss:0.36003807187080383\n",
      "Epoch 18[99/625] Time:0.705, Train Loss:0.351993590593338\n",
      "Epoch 18[100/625] Time:0.724, Train Loss:0.24214677512645721\n",
      "Epoch 18[101/625] Time:0.691, Train Loss:0.26114362478256226\n",
      "Epoch 18[102/625] Time:0.705, Train Loss:0.2287987768650055\n",
      "Epoch 18[103/625] Time:0.705, Train Loss:0.31742149591445923\n",
      "Epoch 18[104/625] Time:0.705, Train Loss:0.29392924904823303\n",
      "Epoch 18[105/625] Time:0.704, Train Loss:0.2955394387245178\n",
      "Epoch 18[106/625] Time:0.703, Train Loss:0.3336554169654846\n",
      "Epoch 18[107/625] Time:0.704, Train Loss:0.20015960931777954\n",
      "Epoch 18[108/625] Time:0.703, Train Loss:0.25757351517677307\n",
      "Epoch 18[109/625] Time:0.703, Train Loss:0.35173124074935913\n",
      "Epoch 18[110/625] Time:0.706, Train Loss:0.2512511909008026\n",
      "Epoch 18[111/625] Time:0.704, Train Loss:0.3270379900932312\n",
      "Epoch 18[112/625] Time:0.704, Train Loss:0.3004243075847626\n",
      "Epoch 18[113/625] Time:0.706, Train Loss:0.2645515203475952\n",
      "Epoch 18[114/625] Time:0.703, Train Loss:0.278933048248291\n",
      "Epoch 18[115/625] Time:0.705, Train Loss:0.22537493705749512\n",
      "Epoch 18[116/625] Time:0.703, Train Loss:0.26436614990234375\n",
      "Epoch 18[117/625] Time:0.7, Train Loss:0.2037142664194107\n",
      "Epoch 18[118/625] Time:0.703, Train Loss:0.3172434866428375\n",
      "Epoch 18[119/625] Time:0.704, Train Loss:0.3854842782020569\n",
      "Epoch 18[120/625] Time:0.703, Train Loss:0.23666344583034515\n",
      "Epoch 18[121/625] Time:0.703, Train Loss:0.2072206735610962\n",
      "Epoch 18[122/625] Time:0.747, Train Loss:0.4114557206630707\n",
      "Epoch 18[123/625] Time:0.703, Train Loss:0.30803826451301575\n",
      "Epoch 18[124/625] Time:0.703, Train Loss:0.2924666404724121\n",
      "Epoch 18[125/625] Time:0.703, Train Loss:0.2513168454170227\n",
      "Epoch 18[126/625] Time:0.702, Train Loss:0.24774836003780365\n",
      "Epoch 18[127/625] Time:0.703, Train Loss:0.3013000786304474\n",
      "Epoch 18[128/625] Time:0.703, Train Loss:0.3107738792896271\n",
      "Epoch 18[129/625] Time:0.703, Train Loss:0.2628607749938965\n",
      "Epoch 18[130/625] Time:0.703, Train Loss:0.2809494137763977\n",
      "Epoch 18[131/625] Time:0.704, Train Loss:0.2517743706703186\n",
      "Epoch 18[132/625] Time:0.703, Train Loss:0.26154547929763794\n",
      "Epoch 18[133/625] Time:0.702, Train Loss:0.2701500952243805\n",
      "Epoch 18[134/625] Time:0.705, Train Loss:0.20204441249370575\n",
      "Epoch 18[135/625] Time:0.704, Train Loss:0.2096385359764099\n",
      "Epoch 18[136/625] Time:0.703, Train Loss:0.25948992371559143\n",
      "Epoch 18[137/625] Time:0.703, Train Loss:0.22736550867557526\n",
      "Epoch 18[138/625] Time:0.703, Train Loss:0.34778979420661926\n",
      "Epoch 18[139/625] Time:0.703, Train Loss:0.20543517172336578\n",
      "Epoch 18[140/625] Time:0.702, Train Loss:0.29265400767326355\n",
      "Epoch 18[141/625] Time:0.703, Train Loss:0.32059749960899353\n",
      "Epoch 18[142/625] Time:0.703, Train Loss:0.252096027135849\n",
      "Epoch 18[143/625] Time:0.702, Train Loss:0.2709619998931885\n",
      "Epoch 18[144/625] Time:0.704, Train Loss:0.3250972628593445\n",
      "Epoch 18[145/625] Time:0.704, Train Loss:0.2269054800271988\n",
      "Epoch 18[146/625] Time:0.703, Train Loss:0.2529197037220001\n",
      "Epoch 18[147/625] Time:0.71, Train Loss:0.2519169747829437\n",
      "Epoch 18[148/625] Time:0.704, Train Loss:0.21222470700740814\n",
      "Epoch 18[149/625] Time:0.704, Train Loss:0.2536728084087372\n",
      "Epoch 18[150/625] Time:0.703, Train Loss:0.2778695225715637\n",
      "Epoch 18[151/625] Time:0.703, Train Loss:0.3010076880455017\n",
      "Epoch 18[152/625] Time:0.703, Train Loss:0.18392786383628845\n",
      "Epoch 18[153/625] Time:0.702, Train Loss:0.2887123227119446\n",
      "Epoch 18[154/625] Time:0.702, Train Loss:0.22650636732578278\n",
      "Epoch 18[155/625] Time:0.703, Train Loss:0.22024618089199066\n",
      "Epoch 18[156/625] Time:0.702, Train Loss:0.25760412216186523\n",
      "Epoch 18[157/625] Time:0.703, Train Loss:0.30965280532836914\n",
      "Epoch 18[158/625] Time:0.701, Train Loss:0.20816223323345184\n",
      "Epoch 18[159/625] Time:0.703, Train Loss:0.17640995979309082\n",
      "Epoch 18[160/625] Time:0.703, Train Loss:0.17109690606594086\n",
      "Epoch 18[161/625] Time:0.705, Train Loss:0.26549285650253296\n",
      "Epoch 18[162/625] Time:0.704, Train Loss:0.25598427653312683\n",
      "Epoch 18[163/625] Time:0.705, Train Loss:0.31211233139038086\n",
      "Epoch 18[164/625] Time:0.704, Train Loss:0.3328833281993866\n",
      "Epoch 18[165/625] Time:0.704, Train Loss:0.24148234724998474\n",
      "Epoch 18[166/625] Time:0.705, Train Loss:0.2788654565811157\n",
      "Epoch 18[167/625] Time:0.704, Train Loss:0.2285141795873642\n",
      "Epoch 18[168/625] Time:0.703, Train Loss:0.2196718156337738\n",
      "Epoch 18[169/625] Time:0.703, Train Loss:0.26011890172958374\n",
      "Epoch 18[170/625] Time:0.704, Train Loss:0.2914707660675049\n",
      "Epoch 18[171/625] Time:0.704, Train Loss:0.30311739444732666\n",
      "Epoch 18[172/625] Time:0.705, Train Loss:0.3049463927745819\n",
      "Epoch 18[173/625] Time:0.703, Train Loss:0.23313386738300323\n",
      "Epoch 18[174/625] Time:0.704, Train Loss:0.2142060250043869\n",
      "Epoch 18[175/625] Time:0.705, Train Loss:0.24398590624332428\n",
      "Epoch 18[176/625] Time:0.706, Train Loss:0.18489016592502594\n",
      "Epoch 18[177/625] Time:0.705, Train Loss:0.16417798399925232\n",
      "Epoch 18[178/625] Time:0.704, Train Loss:0.19127359986305237\n",
      "Epoch 18[179/625] Time:0.723, Train Loss:0.299941748380661\n",
      "Epoch 18[180/625] Time:0.704, Train Loss:0.2754933536052704\n",
      "Epoch 18[181/625] Time:0.703, Train Loss:0.27117520570755005\n",
      "Epoch 18[182/625] Time:0.704, Train Loss:0.3112780451774597\n",
      "Epoch 18[183/625] Time:0.706, Train Loss:0.3114675283432007\n",
      "Epoch 18[184/625] Time:0.706, Train Loss:0.2361038476228714\n",
      "Epoch 18[185/625] Time:0.707, Train Loss:0.26093435287475586\n",
      "Epoch 18[186/625] Time:0.705, Train Loss:0.34196704626083374\n",
      "Epoch 18[187/625] Time:0.703, Train Loss:0.27275219559669495\n",
      "Epoch 18[188/625] Time:0.703, Train Loss:0.29706722497940063\n",
      "Epoch 18[189/625] Time:0.704, Train Loss:0.2644803524017334\n",
      "Epoch 18[190/625] Time:0.704, Train Loss:0.29361942410469055\n",
      "Epoch 18[191/625] Time:0.702, Train Loss:0.37250611186027527\n",
      "Epoch 18[192/625] Time:0.704, Train Loss:0.25599637627601624\n",
      "Epoch 18[193/625] Time:0.703, Train Loss:0.3064250946044922\n",
      "Epoch 18[194/625] Time:0.71, Train Loss:0.2965250015258789\n",
      "Epoch 18[195/625] Time:0.703, Train Loss:0.2725500762462616\n",
      "Epoch 18[196/625] Time:0.7, Train Loss:0.26351413130760193\n",
      "Epoch 18[197/625] Time:0.701, Train Loss:0.327087938785553\n",
      "Epoch 18[198/625] Time:0.703, Train Loss:0.2784914970397949\n",
      "Epoch 18[199/625] Time:0.703, Train Loss:0.29662805795669556\n",
      "Epoch 18[200/625] Time:0.703, Train Loss:0.2297687828540802\n",
      "Epoch 18[201/625] Time:0.725, Train Loss:0.33143627643585205\n",
      "Epoch 18[202/625] Time:0.695, Train Loss:0.25053972005844116\n",
      "Epoch 18[203/625] Time:0.701, Train Loss:0.30351147055625916\n",
      "Epoch 18[204/625] Time:0.702, Train Loss:0.31619057059288025\n",
      "Epoch 18[205/625] Time:0.702, Train Loss:0.2630458474159241\n",
      "Epoch 18[206/625] Time:0.7, Train Loss:0.3495308458805084\n",
      "Epoch 18[207/625] Time:0.701, Train Loss:0.2760838270187378\n",
      "Epoch 18[208/625] Time:0.702, Train Loss:0.24352839589118958\n",
      "Epoch 18[209/625] Time:0.704, Train Loss:0.267039030790329\n",
      "Epoch 18[210/625] Time:0.705, Train Loss:0.27702298760414124\n",
      "Epoch 18[211/625] Time:0.704, Train Loss:0.23675553500652313\n",
      "Epoch 18[212/625] Time:0.702, Train Loss:0.2229240983724594\n",
      "Epoch 18[213/625] Time:0.703, Train Loss:0.29471203684806824\n",
      "Epoch 18[214/625] Time:0.704, Train Loss:0.25867798924446106\n",
      "Epoch 18[215/625] Time:0.703, Train Loss:0.2648354470729828\n",
      "Epoch 18[216/625] Time:0.703, Train Loss:0.2763523459434509\n",
      "Epoch 18[217/625] Time:0.703, Train Loss:0.24069565534591675\n",
      "Epoch 18[218/625] Time:0.711, Train Loss:0.21502435207366943\n",
      "Epoch 18[219/625] Time:0.704, Train Loss:0.27512070536613464\n",
      "Epoch 18[220/625] Time:0.703, Train Loss:0.3049164414405823\n",
      "Epoch 18[221/625] Time:0.703, Train Loss:0.25797542929649353\n",
      "Epoch 18[222/625] Time:0.704, Train Loss:0.269083708524704\n",
      "Epoch 18[223/625] Time:0.704, Train Loss:0.33646804094314575\n",
      "Epoch 18[224/625] Time:0.707, Train Loss:0.260953426361084\n",
      "Epoch 18[225/625] Time:0.705, Train Loss:0.2260141223669052\n",
      "Epoch 18[226/625] Time:0.706, Train Loss:0.22873224318027496\n",
      "Epoch 18[227/625] Time:0.703, Train Loss:0.2880553901195526\n",
      "Epoch 18[228/625] Time:0.702, Train Loss:0.29470857977867126\n",
      "Epoch 18[229/625] Time:0.703, Train Loss:0.24196837842464447\n",
      "Epoch 18[230/625] Time:0.705, Train Loss:0.29859593510627747\n",
      "Epoch 18[231/625] Time:0.704, Train Loss:0.23378776013851166\n",
      "Epoch 18[232/625] Time:0.705, Train Loss:0.2620874047279358\n",
      "Epoch 18[233/625] Time:0.705, Train Loss:0.2729286253452301\n",
      "Epoch 18[234/625] Time:0.704, Train Loss:0.2443276345729828\n",
      "Epoch 18[235/625] Time:0.702, Train Loss:0.31335562467575073\n",
      "Epoch 18[236/625] Time:0.708, Train Loss:0.23723755776882172\n",
      "Epoch 18[237/625] Time:0.703, Train Loss:0.2453526109457016\n",
      "Epoch 18[238/625] Time:0.701, Train Loss:0.2991188168525696\n",
      "Epoch 18[239/625] Time:0.702, Train Loss:0.25252601504325867\n",
      "Epoch 18[240/625] Time:0.704, Train Loss:0.29991424083709717\n",
      "Epoch 18[241/625] Time:0.703, Train Loss:0.20755477249622345\n",
      "Epoch 18[242/625] Time:0.704, Train Loss:0.2678056061267853\n",
      "Epoch 18[243/625] Time:0.735, Train Loss:0.22590826451778412\n",
      "Epoch 18[244/625] Time:0.691, Train Loss:0.30264410376548767\n",
      "Epoch 18[245/625] Time:0.692, Train Loss:0.2795214056968689\n",
      "Epoch 18[246/625] Time:0.692, Train Loss:0.24072030186653137\n",
      "Epoch 18[247/625] Time:0.692, Train Loss:0.25327977538108826\n",
      "Epoch 18[248/625] Time:0.692, Train Loss:0.2241031974554062\n",
      "Epoch 18[249/625] Time:0.694, Train Loss:0.22993692755699158\n",
      "Epoch 18[250/625] Time:0.693, Train Loss:0.2549089193344116\n",
      "Epoch 18[251/625] Time:0.692, Train Loss:0.25981879234313965\n",
      "Epoch 18[252/625] Time:0.692, Train Loss:0.2563316226005554\n",
      "Epoch 18[253/625] Time:0.736, Train Loss:0.2507670521736145\n",
      "Epoch 18[254/625] Time:0.704, Train Loss:0.21072494983673096\n",
      "Epoch 18[255/625] Time:0.707, Train Loss:0.18870516121387482\n",
      "Epoch 18[256/625] Time:0.704, Train Loss:0.28270965814590454\n",
      "Epoch 18[257/625] Time:0.721, Train Loss:0.2464912086725235\n",
      "Epoch 18[258/625] Time:0.703, Train Loss:0.2688295841217041\n",
      "Epoch 18[259/625] Time:0.703, Train Loss:0.17759642004966736\n",
      "Epoch 18[260/625] Time:0.703, Train Loss:0.19502277672290802\n",
      "Epoch 18[261/625] Time:0.703, Train Loss:0.2509281635284424\n",
      "Epoch 18[262/625] Time:0.704, Train Loss:0.2275630384683609\n",
      "Epoch 18[263/625] Time:0.703, Train Loss:0.26167118549346924\n",
      "Epoch 18[264/625] Time:0.707, Train Loss:0.30670973658561707\n",
      "Epoch 18[265/625] Time:0.703, Train Loss:0.17339272797107697\n",
      "Epoch 18[266/625] Time:0.703, Train Loss:0.23099841177463531\n",
      "Epoch 18[267/625] Time:0.706, Train Loss:0.20486052334308624\n",
      "Epoch 18[268/625] Time:0.722, Train Loss:0.19230639934539795\n",
      "Epoch 18[269/625] Time:0.704, Train Loss:0.22125473618507385\n",
      "Epoch 18[270/625] Time:0.703, Train Loss:0.23182876408100128\n",
      "Epoch 18[271/625] Time:0.704, Train Loss:0.2438901662826538\n",
      "Epoch 18[272/625] Time:0.704, Train Loss:0.29246336221694946\n",
      "Epoch 18[273/625] Time:0.71, Train Loss:0.3675128221511841\n",
      "Epoch 18[274/625] Time:0.711, Train Loss:0.34325897693634033\n",
      "Epoch 18[275/625] Time:0.703, Train Loss:0.22462286055088043\n",
      "Epoch 18[276/625] Time:0.702, Train Loss:0.25940150022506714\n",
      "Epoch 18[277/625] Time:0.693, Train Loss:0.3027603328227997\n",
      "Epoch 18[278/625] Time:0.693, Train Loss:0.2172107994556427\n",
      "Epoch 18[279/625] Time:0.692, Train Loss:0.19956602156162262\n",
      "Epoch 18[280/625] Time:0.703, Train Loss:0.33306142687797546\n",
      "Epoch 18[281/625] Time:0.703, Train Loss:0.30611589550971985\n",
      "Epoch 18[282/625] Time:0.705, Train Loss:0.3722001910209656\n",
      "Epoch 18[283/625] Time:0.705, Train Loss:0.18029890954494476\n",
      "Epoch 18[284/625] Time:0.704, Train Loss:0.39475691318511963\n",
      "Epoch 18[285/625] Time:0.703, Train Loss:0.28034162521362305\n",
      "Epoch 18[286/625] Time:0.703, Train Loss:0.3194889426231384\n",
      "Epoch 18[287/625] Time:0.704, Train Loss:0.2931287884712219\n",
      "Epoch 18[288/625] Time:0.704, Train Loss:0.17345008254051208\n",
      "Epoch 18[289/625] Time:0.704, Train Loss:0.2258414477109909\n",
      "Epoch 18[290/625] Time:0.703, Train Loss:0.2070440649986267\n",
      "Epoch 18[291/625] Time:0.703, Train Loss:0.22324028611183167\n",
      "Epoch 18[292/625] Time:0.702, Train Loss:0.24322059750556946\n",
      "Epoch 18[293/625] Time:0.703, Train Loss:0.18913009762763977\n",
      "Epoch 18[294/625] Time:0.703, Train Loss:0.24594752490520477\n",
      "Epoch 18[295/625] Time:0.702, Train Loss:0.24826277792453766\n",
      "Epoch 18[296/625] Time:0.703, Train Loss:0.3295007348060608\n",
      "Epoch 18[297/625] Time:0.703, Train Loss:0.23647740483283997\n",
      "Epoch 18[298/625] Time:0.702, Train Loss:0.2266566902399063\n",
      "Epoch 18[299/625] Time:0.703, Train Loss:0.3403564989566803\n",
      "Epoch 18[300/625] Time:0.703, Train Loss:0.22071893513202667\n",
      "Epoch 18[301/625] Time:0.704, Train Loss:0.3648686408996582\n",
      "Epoch 18[302/625] Time:0.703, Train Loss:0.27536246180534363\n",
      "Epoch 18[303/625] Time:0.704, Train Loss:0.3078881800174713\n",
      "Epoch 18[304/625] Time:0.702, Train Loss:0.3084206283092499\n",
      "Epoch 18[305/625] Time:0.704, Train Loss:0.22708851099014282\n",
      "Epoch 18[306/625] Time:0.704, Train Loss:0.22258500754833221\n",
      "Epoch 18[307/625] Time:0.703, Train Loss:0.22703492641448975\n",
      "Epoch 18[308/625] Time:0.702, Train Loss:0.26059314608573914\n",
      "Epoch 18[309/625] Time:0.702, Train Loss:0.2698979079723358\n",
      "Epoch 18[310/625] Time:0.705, Train Loss:0.3181098699569702\n",
      "Epoch 18[311/625] Time:0.713, Train Loss:0.27841633558273315\n",
      "Epoch 18[312/625] Time:0.724, Train Loss:0.2683548033237457\n",
      "Epoch 18[313/625] Time:0.727, Train Loss:0.3049454092979431\n",
      "Epoch 18[314/625] Time:0.712, Train Loss:0.23009897768497467\n",
      "Epoch 18[315/625] Time:0.711, Train Loss:0.35640019178390503\n",
      "Epoch 18[316/625] Time:0.721, Train Loss:0.3258720934391022\n",
      "Epoch 18[317/625] Time:0.707, Train Loss:0.22020269930362701\n",
      "Epoch 18[318/625] Time:0.707, Train Loss:0.2757757306098938\n",
      "Epoch 18[319/625] Time:0.73, Train Loss:0.24286481738090515\n",
      "Epoch 18[320/625] Time:0.699, Train Loss:0.27440589666366577\n",
      "Epoch 18[321/625] Time:0.705, Train Loss:0.334655225276947\n",
      "Epoch 18[322/625] Time:0.713, Train Loss:0.2528412342071533\n",
      "Epoch 18[323/625] Time:0.706, Train Loss:0.3061921000480652\n",
      "Epoch 18[324/625] Time:0.703, Train Loss:0.32164058089256287\n",
      "Epoch 18[325/625] Time:0.693, Train Loss:0.23038464784622192\n",
      "Epoch 18[326/625] Time:0.705, Train Loss:0.35048389434814453\n",
      "Epoch 18[327/625] Time:0.706, Train Loss:0.23888041079044342\n",
      "Epoch 18[328/625] Time:0.705, Train Loss:0.27740246057510376\n",
      "Epoch 18[329/625] Time:0.704, Train Loss:0.34419384598731995\n",
      "Epoch 18[330/625] Time:0.704, Train Loss:0.2513625919818878\n",
      "Epoch 18[331/625] Time:0.705, Train Loss:0.3400968611240387\n",
      "Epoch 18[332/625] Time:0.705, Train Loss:0.22995813190937042\n",
      "Epoch 18[333/625] Time:0.705, Train Loss:0.22886249423027039\n",
      "Epoch 18[334/625] Time:0.707, Train Loss:0.3457719087600708\n",
      "Epoch 18[335/625] Time:0.705, Train Loss:0.2942701578140259\n",
      "Epoch 18[336/625] Time:0.704, Train Loss:0.3730659782886505\n",
      "Epoch 18[337/625] Time:0.705, Train Loss:0.2803558111190796\n",
      "Epoch 18[338/625] Time:0.705, Train Loss:0.24687409400939941\n",
      "Epoch 18[339/625] Time:0.704, Train Loss:0.2790859341621399\n",
      "Epoch 18[340/625] Time:0.706, Train Loss:0.23058022558689117\n",
      "Epoch 18[341/625] Time:0.706, Train Loss:0.2168874889612198\n",
      "Epoch 18[342/625] Time:0.703, Train Loss:0.26764950156211853\n",
      "Epoch 18[343/625] Time:0.703, Train Loss:0.30316221714019775\n",
      "Epoch 18[344/625] Time:0.704, Train Loss:0.2685650587081909\n",
      "Epoch 18[345/625] Time:0.705, Train Loss:0.2653524875640869\n",
      "Epoch 18[346/625] Time:0.704, Train Loss:0.2506682574748993\n",
      "Epoch 18[347/625] Time:0.703, Train Loss:0.210068479180336\n",
      "Epoch 18[348/625] Time:0.704, Train Loss:0.29594913125038147\n",
      "Epoch 18[349/625] Time:0.704, Train Loss:0.22583870589733124\n",
      "Epoch 18[350/625] Time:0.704, Train Loss:0.21402530372142792\n",
      "Epoch 18[351/625] Time:0.704, Train Loss:0.29007935523986816\n",
      "Epoch 18[352/625] Time:0.705, Train Loss:0.2157561480998993\n",
      "Epoch 18[353/625] Time:0.704, Train Loss:0.25792041420936584\n",
      "Epoch 18[354/625] Time:0.705, Train Loss:0.19984610378742218\n",
      "Epoch 18[355/625] Time:0.703, Train Loss:0.2545951008796692\n",
      "Epoch 18[356/625] Time:0.702, Train Loss:0.18221327662467957\n",
      "Epoch 18[357/625] Time:0.704, Train Loss:0.20744095742702484\n",
      "Epoch 18[358/625] Time:0.703, Train Loss:0.2455035150051117\n",
      "Epoch 18[359/625] Time:0.704, Train Loss:0.29658275842666626\n",
      "Epoch 18[360/625] Time:0.703, Train Loss:0.2684149146080017\n",
      "Epoch 18[361/625] Time:0.704, Train Loss:0.22121155261993408\n",
      "Epoch 18[362/625] Time:0.693, Train Loss:0.21953007578849792\n",
      "Epoch 18[363/625] Time:0.704, Train Loss:0.187196284532547\n",
      "Epoch 18[364/625] Time:0.704, Train Loss:0.1910165250301361\n",
      "Epoch 18[365/625] Time:0.704, Train Loss:0.24616806209087372\n",
      "Epoch 18[366/625] Time:0.704, Train Loss:0.27441859245300293\n",
      "Epoch 18[367/625] Time:0.704, Train Loss:0.24459584057331085\n",
      "Epoch 18[368/625] Time:0.705, Train Loss:0.3680647015571594\n",
      "Epoch 18[369/625] Time:0.705, Train Loss:0.2086126208305359\n",
      "Epoch 18[370/625] Time:0.706, Train Loss:0.1843116730451584\n",
      "Epoch 18[371/625] Time:0.704, Train Loss:0.2517910599708557\n",
      "Epoch 18[372/625] Time:0.705, Train Loss:0.36859920620918274\n",
      "Epoch 18[373/625] Time:0.703, Train Loss:0.19754473865032196\n",
      "Epoch 18[374/625] Time:0.727, Train Loss:0.2684517204761505\n",
      "Epoch 18[375/625] Time:0.706, Train Loss:0.3076326847076416\n",
      "Epoch 18[376/625] Time:0.74, Train Loss:0.22629904747009277\n",
      "Epoch 18[377/625] Time:0.702, Train Loss:0.29456576704978943\n",
      "Epoch 18[378/625] Time:0.703, Train Loss:0.23133517801761627\n",
      "Epoch 18[379/625] Time:0.703, Train Loss:0.19005374610424042\n",
      "Epoch 18[380/625] Time:0.704, Train Loss:0.3692268133163452\n",
      "Epoch 18[381/625] Time:0.704, Train Loss:0.31661736965179443\n",
      "Epoch 18[382/625] Time:0.702, Train Loss:0.23992714285850525\n",
      "Epoch 18[383/625] Time:0.704, Train Loss:0.3635410666465759\n",
      "Epoch 18[384/625] Time:0.702, Train Loss:0.3561653792858124\n",
      "Epoch 18[385/625] Time:0.703, Train Loss:0.2955482304096222\n",
      "Epoch 18[386/625] Time:0.705, Train Loss:0.353009968996048\n",
      "Epoch 18[387/625] Time:0.705, Train Loss:0.20331312716007233\n",
      "Epoch 18[388/625] Time:0.703, Train Loss:0.3502446413040161\n",
      "Epoch 18[389/625] Time:0.703, Train Loss:0.29220667481422424\n",
      "Epoch 18[390/625] Time:0.703, Train Loss:0.26515844464302063\n",
      "Epoch 18[391/625] Time:0.703, Train Loss:0.3044638931751251\n",
      "Epoch 18[392/625] Time:0.703, Train Loss:0.2801518440246582\n",
      "Epoch 18[393/625] Time:0.704, Train Loss:0.3269248902797699\n",
      "Epoch 18[394/625] Time:0.703, Train Loss:0.20922230184078217\n",
      "Epoch 18[395/625] Time:0.702, Train Loss:0.29155221581459045\n",
      "Epoch 18[396/625] Time:0.703, Train Loss:0.3969780206680298\n",
      "Epoch 18[397/625] Time:0.703, Train Loss:0.30267786979675293\n",
      "Epoch 18[398/625] Time:0.703, Train Loss:0.27823755145072937\n",
      "Epoch 18[399/625] Time:0.702, Train Loss:0.327374666929245\n",
      "Epoch 18[400/625] Time:0.748, Train Loss:0.22985325753688812\n",
      "Epoch 18[401/625] Time:0.693, Train Loss:0.2577929198741913\n",
      "Epoch 18[402/625] Time:0.702, Train Loss:0.21264928579330444\n",
      "Epoch 18[403/625] Time:0.703, Train Loss:0.2646169066429138\n",
      "Epoch 18[404/625] Time:0.702, Train Loss:0.2001037895679474\n",
      "Epoch 18[405/625] Time:0.705, Train Loss:0.2615801990032196\n",
      "Epoch 18[406/625] Time:0.704, Train Loss:0.3364473283290863\n",
      "Epoch 18[407/625] Time:0.705, Train Loss:0.2597495913505554\n",
      "Epoch 18[408/625] Time:0.707, Train Loss:0.3658204674720764\n",
      "Epoch 18[409/625] Time:0.703, Train Loss:0.3322604298591614\n",
      "Epoch 18[410/625] Time:0.703, Train Loss:0.3387056589126587\n",
      "Epoch 18[411/625] Time:0.705, Train Loss:0.28105777502059937\n",
      "Epoch 18[412/625] Time:0.704, Train Loss:0.2440403252840042\n",
      "Epoch 18[413/625] Time:0.706, Train Loss:0.2732762396335602\n",
      "Epoch 18[414/625] Time:0.705, Train Loss:0.2697899341583252\n",
      "Epoch 18[415/625] Time:0.704, Train Loss:0.2877461612224579\n",
      "Epoch 18[416/625] Time:0.704, Train Loss:0.22812804579734802\n",
      "Epoch 18[417/625] Time:0.703, Train Loss:0.28352752327919006\n",
      "Epoch 18[418/625] Time:0.703, Train Loss:0.2620718479156494\n",
      "Epoch 18[419/625] Time:0.703, Train Loss:0.3107147812843323\n",
      "Epoch 18[420/625] Time:0.703, Train Loss:0.20352092385292053\n",
      "Epoch 18[421/625] Time:0.704, Train Loss:0.25397107005119324\n",
      "Epoch 18[422/625] Time:0.701, Train Loss:0.2232559323310852\n",
      "Epoch 18[423/625] Time:0.737, Train Loss:0.2814556956291199\n",
      "Epoch 18[424/625] Time:0.693, Train Loss:0.27131912112236023\n",
      "Epoch 18[425/625] Time:0.693, Train Loss:0.3353082537651062\n",
      "Epoch 18[426/625] Time:0.694, Train Loss:0.27840444445610046\n",
      "Epoch 18[427/625] Time:0.693, Train Loss:0.2964479625225067\n",
      "Epoch 18[428/625] Time:0.693, Train Loss:0.3733559846878052\n",
      "Epoch 18[429/625] Time:0.694, Train Loss:0.21613582968711853\n",
      "Epoch 18[430/625] Time:0.693, Train Loss:0.2645755708217621\n",
      "Epoch 18[431/625] Time:0.694, Train Loss:0.42468884587287903\n",
      "Epoch 18[432/625] Time:0.694, Train Loss:0.23029716312885284\n",
      "Epoch 18[433/625] Time:0.694, Train Loss:0.20999285578727722\n",
      "Epoch 18[434/625] Time:0.694, Train Loss:0.2838461399078369\n",
      "Epoch 18[435/625] Time:0.693, Train Loss:0.22008304297924042\n",
      "Epoch 18[436/625] Time:0.694, Train Loss:0.2337045669555664\n",
      "Epoch 18[437/625] Time:0.693, Train Loss:0.2565922737121582\n",
      "Epoch 18[438/625] Time:0.694, Train Loss:0.2820644974708557\n",
      "Epoch 18[439/625] Time:0.694, Train Loss:0.3101252317428589\n",
      "Epoch 18[440/625] Time:0.704, Train Loss:0.2719184458255768\n",
      "Epoch 18[441/625] Time:0.702, Train Loss:0.27463293075561523\n",
      "Epoch 18[442/625] Time:0.704, Train Loss:0.24281449615955353\n",
      "Epoch 18[443/625] Time:0.702, Train Loss:0.27455440163612366\n",
      "Epoch 18[444/625] Time:0.702, Train Loss:0.3044101595878601\n",
      "Epoch 18[445/625] Time:0.702, Train Loss:0.19140490889549255\n",
      "Epoch 18[446/625] Time:0.704, Train Loss:0.36334365606307983\n",
      "Epoch 18[447/625] Time:0.703, Train Loss:0.2605014741420746\n",
      "Epoch 18[448/625] Time:0.745, Train Loss:0.23812374472618103\n",
      "Epoch 18[449/625] Time:0.694, Train Loss:0.3277977406978607\n",
      "Epoch 18[450/625] Time:0.694, Train Loss:0.28660115599632263\n",
      "Epoch 18[451/625] Time:0.693, Train Loss:0.23056450486183167\n",
      "Epoch 18[452/625] Time:0.697, Train Loss:0.24372760951519012\n",
      "Epoch 18[453/625] Time:0.692, Train Loss:0.20029817521572113\n",
      "Epoch 18[454/625] Time:0.692, Train Loss:0.1965026706457138\n",
      "Epoch 18[455/625] Time:0.697, Train Loss:0.24558474123477936\n",
      "Epoch 18[456/625] Time:0.697, Train Loss:0.22877512872219086\n",
      "Epoch 18[457/625] Time:0.692, Train Loss:0.2687476575374603\n",
      "Epoch 18[458/625] Time:0.693, Train Loss:0.2616867125034332\n",
      "Epoch 18[459/625] Time:0.692, Train Loss:0.2744925916194916\n",
      "Epoch 18[460/625] Time:0.693, Train Loss:0.22337403893470764\n",
      "Epoch 18[461/625] Time:0.693, Train Loss:0.2657688856124878\n",
      "Epoch 18[462/625] Time:0.693, Train Loss:0.3222433030605316\n",
      "Epoch 18[463/625] Time:0.695, Train Loss:0.2528110146522522\n",
      "Epoch 18[464/625] Time:0.703, Train Loss:0.17347164452075958\n",
      "Epoch 18[465/625] Time:0.705, Train Loss:0.32825616002082825\n",
      "Epoch 18[466/625] Time:0.698, Train Loss:0.2642386555671692\n",
      "Epoch 18[467/625] Time:0.704, Train Loss:0.41632577776908875\n",
      "Epoch 18[468/625] Time:0.704, Train Loss:0.22270631790161133\n",
      "Epoch 18[469/625] Time:0.706, Train Loss:0.24316994845867157\n",
      "Epoch 18[470/625] Time:0.704, Train Loss:0.290785551071167\n",
      "Epoch 18[471/625] Time:0.703, Train Loss:0.269276887178421\n",
      "Epoch 18[472/625] Time:0.703, Train Loss:0.2503345012664795\n",
      "Epoch 18[473/625] Time:0.702, Train Loss:0.2128448337316513\n",
      "Epoch 18[474/625] Time:0.703, Train Loss:0.3063538670539856\n",
      "Epoch 18[475/625] Time:0.703, Train Loss:0.3745519518852234\n",
      "Epoch 18[476/625] Time:0.704, Train Loss:0.3334296941757202\n",
      "Epoch 18[477/625] Time:0.704, Train Loss:0.21940568089485168\n",
      "Epoch 18[478/625] Time:0.703, Train Loss:0.2851848006248474\n",
      "Epoch 18[479/625] Time:0.704, Train Loss:0.28377658128738403\n",
      "Epoch 18[480/625] Time:0.704, Train Loss:0.22369396686553955\n",
      "Epoch 18[481/625] Time:0.694, Train Loss:0.28761526942253113\n",
      "Epoch 18[482/625] Time:0.703, Train Loss:0.2984350323677063\n",
      "Epoch 18[483/625] Time:0.705, Train Loss:0.2592313885688782\n",
      "Epoch 18[484/625] Time:0.702, Train Loss:0.270881325006485\n",
      "Epoch 18[485/625] Time:0.703, Train Loss:0.20529323816299438\n",
      "Epoch 18[486/625] Time:0.704, Train Loss:0.27766913175582886\n",
      "Epoch 18[487/625] Time:0.709, Train Loss:0.3092702627182007\n",
      "Epoch 18[488/625] Time:0.702, Train Loss:0.3186611533164978\n",
      "Epoch 18[489/625] Time:0.702, Train Loss:0.22212257981300354\n",
      "Epoch 18[490/625] Time:0.703, Train Loss:0.23874326050281525\n",
      "Epoch 18[491/625] Time:0.703, Train Loss:0.3006662428379059\n",
      "Epoch 18[492/625] Time:0.702, Train Loss:0.31040316820144653\n",
      "Epoch 18[493/625] Time:0.703, Train Loss:0.30423659086227417\n",
      "Epoch 18[494/625] Time:0.702, Train Loss:0.25083401799201965\n",
      "Epoch 18[495/625] Time:0.706, Train Loss:0.262023001909256\n",
      "Epoch 18[496/625] Time:0.702, Train Loss:0.2717088758945465\n",
      "Epoch 18[497/625] Time:0.703, Train Loss:0.30862948298454285\n",
      "Epoch 18[498/625] Time:0.703, Train Loss:0.26437485218048096\n",
      "Epoch 18[499/625] Time:0.704, Train Loss:0.28838956356048584\n",
      "Epoch 18[500/625] Time:0.704, Train Loss:0.29325950145721436\n",
      "Epoch 18[501/625] Time:0.704, Train Loss:0.25659433007240295\n",
      "Epoch 18[502/625] Time:0.703, Train Loss:0.18888424336910248\n",
      "Epoch 18[503/625] Time:0.705, Train Loss:0.24948152899742126\n",
      "Epoch 18[504/625] Time:0.705, Train Loss:0.2931031882762909\n",
      "Epoch 18[505/625] Time:0.705, Train Loss:0.23752018809318542\n",
      "Epoch 18[506/625] Time:0.702, Train Loss:0.2464418262243271\n",
      "Epoch 18[507/625] Time:0.704, Train Loss:0.24012470245361328\n",
      "Epoch 18[508/625] Time:0.703, Train Loss:0.18697451055049896\n",
      "Epoch 18[509/625] Time:0.702, Train Loss:0.24451091885566711\n",
      "Epoch 18[510/625] Time:0.704, Train Loss:0.24285615980625153\n",
      "Epoch 18[511/625] Time:0.707, Train Loss:0.2551635205745697\n",
      "Epoch 18[512/625] Time:0.702, Train Loss:0.20634987950325012\n",
      "Epoch 18[513/625] Time:0.704, Train Loss:0.27147889137268066\n",
      "Epoch 18[514/625] Time:0.691, Train Loss:0.2537963092327118\n",
      "Epoch 18[515/625] Time:0.692, Train Loss:0.23496903479099274\n",
      "Epoch 18[516/625] Time:0.692, Train Loss:0.28080323338508606\n",
      "Epoch 18[517/625] Time:0.721, Train Loss:0.3233505189418793\n",
      "Epoch 18[518/625] Time:0.71, Train Loss:0.23650245368480682\n",
      "Epoch 18[519/625] Time:0.691, Train Loss:0.26403337717056274\n",
      "Epoch 18[520/625] Time:0.703, Train Loss:0.2732875943183899\n",
      "Epoch 18[521/625] Time:0.705, Train Loss:0.2813100814819336\n",
      "Epoch 18[522/625] Time:0.705, Train Loss:0.26021668314933777\n",
      "Epoch 18[523/625] Time:0.705, Train Loss:0.2252979874610901\n",
      "Epoch 18[524/625] Time:0.704, Train Loss:0.37052562832832336\n",
      "Epoch 18[525/625] Time:0.704, Train Loss:0.28962182998657227\n",
      "Epoch 18[526/625] Time:0.703, Train Loss:0.2445531189441681\n",
      "Epoch 18[527/625] Time:0.705, Train Loss:0.2977657914161682\n",
      "Epoch 18[528/625] Time:0.709, Train Loss:0.27338913083076477\n",
      "Epoch 18[529/625] Time:0.704, Train Loss:0.34139424562454224\n",
      "Epoch 18[530/625] Time:0.704, Train Loss:0.21915854513645172\n",
      "Epoch 18[531/625] Time:0.704, Train Loss:0.28388550877571106\n",
      "Epoch 18[532/625] Time:0.733, Train Loss:0.25441840291023254\n",
      "Epoch 18[533/625] Time:0.691, Train Loss:0.2832317054271698\n",
      "Epoch 18[534/625] Time:0.707, Train Loss:0.31419894099235535\n",
      "Epoch 18[535/625] Time:0.722, Train Loss:0.2163631021976471\n",
      "Epoch 18[536/625] Time:0.693, Train Loss:0.18317784368991852\n",
      "Epoch 18[537/625] Time:0.694, Train Loss:0.2590125799179077\n",
      "Epoch 18[538/625] Time:0.694, Train Loss:0.252897709608078\n",
      "Epoch 18[539/625] Time:0.694, Train Loss:0.2975715398788452\n",
      "Epoch 18[540/625] Time:0.693, Train Loss:0.2643548250198364\n",
      "Epoch 18[541/625] Time:0.693, Train Loss:0.2947477400302887\n",
      "Epoch 18[542/625] Time:0.695, Train Loss:0.27959325909614563\n",
      "Epoch 18[543/625] Time:0.703, Train Loss:0.24927279353141785\n",
      "Epoch 18[544/625] Time:0.702, Train Loss:0.2794267237186432\n",
      "Epoch 18[545/625] Time:0.702, Train Loss:0.3679959774017334\n",
      "Epoch 18[546/625] Time:0.706, Train Loss:0.2059992551803589\n",
      "Epoch 18[547/625] Time:0.705, Train Loss:0.2237091362476349\n",
      "Epoch 18[548/625] Time:0.705, Train Loss:0.2808791399002075\n",
      "Epoch 18[549/625] Time:0.702, Train Loss:0.270786851644516\n",
      "Epoch 18[550/625] Time:0.706, Train Loss:0.27310672402381897\n",
      "Epoch 18[551/625] Time:0.702, Train Loss:0.19807520508766174\n",
      "Epoch 18[552/625] Time:0.703, Train Loss:0.1999160200357437\n",
      "Epoch 18[553/625] Time:0.703, Train Loss:0.3530620038509369\n",
      "Epoch 18[554/625] Time:0.704, Train Loss:0.24514985084533691\n",
      "Epoch 18[555/625] Time:0.706, Train Loss:0.25255414843559265\n",
      "Epoch 18[556/625] Time:0.704, Train Loss:0.22833788394927979\n",
      "Epoch 18[557/625] Time:0.702, Train Loss:0.2999824285507202\n",
      "Epoch 18[558/625] Time:0.703, Train Loss:0.35373038053512573\n",
      "Epoch 18[559/625] Time:0.703, Train Loss:0.36880892515182495\n",
      "Epoch 18[560/625] Time:0.703, Train Loss:0.30719277262687683\n",
      "Epoch 18[561/625] Time:0.704, Train Loss:0.2449163794517517\n",
      "Epoch 18[562/625] Time:0.702, Train Loss:0.255906879901886\n",
      "Epoch 18[563/625] Time:0.702, Train Loss:0.265411913394928\n",
      "Epoch 18[564/625] Time:0.703, Train Loss:0.25336238741874695\n",
      "Epoch 18[565/625] Time:0.702, Train Loss:0.2530207633972168\n",
      "Epoch 18[566/625] Time:0.708, Train Loss:0.23512646555900574\n",
      "Epoch 18[567/625] Time:0.704, Train Loss:0.3533421456813812\n",
      "Epoch 18[568/625] Time:0.702, Train Loss:0.3440344035625458\n",
      "Epoch 18[569/625] Time:0.702, Train Loss:0.2909848690032959\n",
      "Epoch 18[570/625] Time:0.705, Train Loss:0.3054177761077881\n",
      "Epoch 18[571/625] Time:0.703, Train Loss:0.2614048719406128\n",
      "Epoch 18[572/625] Time:0.747, Train Loss:0.2516689896583557\n",
      "Epoch 18[573/625] Time:0.692, Train Loss:0.3003929853439331\n",
      "Epoch 18[574/625] Time:0.696, Train Loss:0.33193638920783997\n",
      "Epoch 18[575/625] Time:0.703, Train Loss:0.29618290066719055\n",
      "Epoch 18[576/625] Time:0.703, Train Loss:0.24595865607261658\n",
      "Epoch 18[577/625] Time:0.703, Train Loss:0.2869916260242462\n",
      "Epoch 18[578/625] Time:0.703, Train Loss:0.32616761326789856\n",
      "Epoch 18[579/625] Time:0.703, Train Loss:0.3193529546260834\n",
      "Epoch 18[580/625] Time:0.703, Train Loss:0.3403913080692291\n",
      "Epoch 18[581/625] Time:0.703, Train Loss:0.2590363025665283\n",
      "Epoch 18[582/625] Time:0.703, Train Loss:0.21530446410179138\n",
      "Epoch 18[583/625] Time:0.703, Train Loss:0.26894405484199524\n",
      "Epoch 18[584/625] Time:0.703, Train Loss:0.2405426800251007\n",
      "Epoch 18[585/625] Time:0.703, Train Loss:0.2666141986846924\n",
      "Epoch 18[586/625] Time:0.702, Train Loss:0.2485964596271515\n",
      "Epoch 18[587/625] Time:0.704, Train Loss:0.34782183170318604\n",
      "Epoch 18[588/625] Time:0.703, Train Loss:0.2692483961582184\n",
      "Epoch 18[589/625] Time:0.704, Train Loss:0.2990027964115143\n",
      "Epoch 18[590/625] Time:0.703, Train Loss:0.19261571764945984\n",
      "Epoch 18[591/625] Time:0.712, Train Loss:0.23905420303344727\n",
      "Epoch 18[592/625] Time:0.694, Train Loss:0.22918862104415894\n",
      "Epoch 18[593/625] Time:0.693, Train Loss:0.2885825037956238\n",
      "Epoch 18[594/625] Time:0.693, Train Loss:0.24613498151302338\n",
      "Epoch 18[595/625] Time:0.694, Train Loss:0.3344433009624481\n",
      "Epoch 18[596/625] Time:0.73, Train Loss:0.2949894666671753\n",
      "Epoch 18[597/625] Time:0.703, Train Loss:0.22871418297290802\n",
      "Epoch 18[598/625] Time:0.744, Train Loss:0.2439940869808197\n",
      "Epoch 18[599/625] Time:0.701, Train Loss:0.2828303873538971\n",
      "Epoch 18[600/625] Time:0.706, Train Loss:0.29184332489967346\n",
      "Epoch 18[601/625] Time:0.704, Train Loss:0.26144111156463623\n",
      "Epoch 18[602/625] Time:0.703, Train Loss:0.2284044325351715\n",
      "Epoch 18[603/625] Time:0.702, Train Loss:0.2878122329711914\n",
      "Epoch 18[604/625] Time:0.704, Train Loss:0.3663863241672516\n",
      "Epoch 18[605/625] Time:0.703, Train Loss:0.23936328291893005\n",
      "Epoch 18[606/625] Time:0.703, Train Loss:0.27513423562049866\n",
      "Epoch 18[607/625] Time:0.702, Train Loss:0.24834531545639038\n",
      "Epoch 18[608/625] Time:0.702, Train Loss:0.22608762979507446\n",
      "Epoch 18[609/625] Time:0.702, Train Loss:0.3203115463256836\n",
      "Epoch 18[610/625] Time:0.703, Train Loss:0.22976356744766235\n",
      "Epoch 18[611/625] Time:0.703, Train Loss:0.3167274594306946\n",
      "Epoch 18[612/625] Time:0.731, Train Loss:0.1933320313692093\n",
      "Epoch 18[613/625] Time:0.692, Train Loss:0.24780480563640594\n",
      "Epoch 18[614/625] Time:0.693, Train Loss:0.27857550978660583\n",
      "Epoch 18[615/625] Time:0.703, Train Loss:0.28105854988098145\n",
      "Epoch 18[616/625] Time:0.704, Train Loss:0.27064794301986694\n",
      "Epoch 18[617/625] Time:0.702, Train Loss:0.24741210043430328\n",
      "Epoch 18[618/625] Time:0.705, Train Loss:0.2743570804595947\n",
      "Epoch 18[619/625] Time:0.703, Train Loss:0.26180657744407654\n",
      "Epoch 18[620/625] Time:0.704, Train Loss:0.22162379324436188\n",
      "Epoch 18[621/625] Time:0.704, Train Loss:0.2509930729866028\n",
      "Epoch 18[622/625] Time:0.703, Train Loss:0.25399911403656006\n",
      "Epoch 18[623/625] Time:0.703, Train Loss:0.21031934022903442\n",
      "Epoch 18[624/625] Time:0.703, Train Loss:0.18488021194934845\n",
      "Epoch 18[0/78] Val Loss:0.1755460798740387\n",
      "Epoch 18[1/78] Val Loss:0.133313849568367\n",
      "Epoch 18[2/78] Val Loss:0.16744084656238556\n",
      "Epoch 18[3/78] Val Loss:0.17470213770866394\n",
      "Epoch 18[4/78] Val Loss:0.23829947412014008\n",
      "Epoch 18[5/78] Val Loss:0.16986845433712006\n",
      "Epoch 18[6/78] Val Loss:0.201656311750412\n",
      "Epoch 18[7/78] Val Loss:0.25862354040145874\n",
      "Epoch 18[8/78] Val Loss:0.13236132264137268\n",
      "Epoch 18[9/78] Val Loss:0.15333659946918488\n",
      "Epoch 18[10/78] Val Loss:0.10202743858098984\n",
      "Epoch 18[11/78] Val Loss:0.14202743768692017\n",
      "Epoch 18[12/78] Val Loss:0.1189567968249321\n",
      "Epoch 18[13/78] Val Loss:0.09042676538228989\n",
      "Epoch 18[14/78] Val Loss:0.17337891459465027\n",
      "Epoch 18[15/78] Val Loss:0.1475726068019867\n",
      "Epoch 18[16/78] Val Loss:0.18094557523727417\n",
      "Epoch 18[17/78] Val Loss:0.13827279210090637\n",
      "Epoch 18[18/78] Val Loss:0.2276715636253357\n",
      "Epoch 18[19/78] Val Loss:0.2751063108444214\n",
      "Epoch 18[20/78] Val Loss:0.1921200007200241\n",
      "Epoch 18[21/78] Val Loss:0.49795663356781006\n",
      "Epoch 18[22/78] Val Loss:0.7187783718109131\n",
      "Epoch 18[23/78] Val Loss:0.47526517510414124\n",
      "Epoch 18[24/78] Val Loss:0.31072646379470825\n",
      "Epoch 18[25/78] Val Loss:0.39625462889671326\n",
      "Epoch 18[26/78] Val Loss:0.35001546144485474\n",
      "Epoch 18[27/78] Val Loss:0.33101150393486023\n",
      "Epoch 18[28/78] Val Loss:0.31738242506980896\n",
      "Epoch 18[29/78] Val Loss:0.4354586601257324\n",
      "Epoch 18[30/78] Val Loss:1.4874516725540161\n",
      "Epoch 18[31/78] Val Loss:1.21639883518219\n",
      "Epoch 18[32/78] Val Loss:0.9481499791145325\n",
      "Epoch 18[33/78] Val Loss:0.35442981123924255\n",
      "Epoch 18[34/78] Val Loss:0.27730098366737366\n",
      "Epoch 18[35/78] Val Loss:0.27581822872161865\n",
      "Epoch 18[36/78] Val Loss:0.2735392451286316\n",
      "Epoch 18[37/78] Val Loss:0.3862680196762085\n",
      "Epoch 18[38/78] Val Loss:0.19078655540943146\n",
      "Epoch 18[39/78] Val Loss:0.18983973562717438\n",
      "Epoch 18[40/78] Val Loss:0.18924415111541748\n",
      "Epoch 18[41/78] Val Loss:0.2151506096124649\n",
      "Epoch 18[42/78] Val Loss:0.21261875331401825\n",
      "Epoch 18[43/78] Val Loss:0.113041952252388\n",
      "Epoch 18[44/78] Val Loss:0.08013498783111572\n",
      "Epoch 18[45/78] Val Loss:0.06440633535385132\n",
      "Epoch 18[46/78] Val Loss:0.08591853082180023\n",
      "Epoch 18[47/78] Val Loss:0.09728550910949707\n",
      "Epoch 18[48/78] Val Loss:0.13606742024421692\n",
      "Epoch 18[49/78] Val Loss:0.10698069632053375\n",
      "Epoch 18[50/78] Val Loss:0.06136372685432434\n",
      "Epoch 18[51/78] Val Loss:0.08358226716518402\n",
      "Epoch 18[52/78] Val Loss:0.10643172264099121\n",
      "Epoch 18[53/78] Val Loss:0.09453596919775009\n",
      "Epoch 18[54/78] Val Loss:0.0725162923336029\n",
      "Epoch 18[55/78] Val Loss:0.10514800995588303\n",
      "Epoch 18[56/78] Val Loss:0.56265789270401\n",
      "Epoch 18[57/78] Val Loss:0.5100435614585876\n",
      "Epoch 18[58/78] Val Loss:0.4918489456176758\n",
      "Epoch 18[59/78] Val Loss:0.4038555920124054\n",
      "Epoch 18[60/78] Val Loss:0.3767506182193756\n",
      "Epoch 18[61/78] Val Loss:0.408067524433136\n",
      "Epoch 18[62/78] Val Loss:0.4608181118965149\n",
      "Epoch 18[63/78] Val Loss:0.23553919792175293\n",
      "Epoch 18[64/78] Val Loss:0.14213423430919647\n",
      "Epoch 18[65/78] Val Loss:0.14716802537441254\n",
      "Epoch 18[66/78] Val Loss:0.1550108790397644\n",
      "Epoch 18[67/78] Val Loss:0.15172044932842255\n",
      "Epoch 18[68/78] Val Loss:0.34601274132728577\n",
      "Epoch 18[69/78] Val Loss:0.31832799315452576\n",
      "Epoch 18[70/78] Val Loss:0.3321700394153595\n",
      "Epoch 18[71/78] Val Loss:0.2585560083389282\n",
      "Epoch 18[72/78] Val Loss:0.1916961371898651\n",
      "Epoch 18[73/78] Val Loss:0.18772996962070465\n",
      "Epoch 18[74/78] Val Loss:0.3919733762741089\n",
      "Epoch 18[75/78] Val Loss:0.5098297595977783\n",
      "Epoch 18[76/78] Val Loss:0.5090537667274475\n",
      "Epoch 18[77/78] Val Loss:0.5279750823974609\n",
      "Epoch 18[78/78] Val Loss:0.6254918575286865\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.92     15691\n",
      "           1       0.69      0.84      0.76      4309\n",
      "\n",
      "    accuracy                           0.89     20000\n",
      "   macro avg       0.82      0.87      0.84     20000\n",
      "weighted avg       0.90      0.89      0.89     20000\n",
      "\n",
      "Epoch 18: Train Loss 0.2677319704771042, Val Loss 0.29609417829376, Train Time 803.1874125003815, Val Time 37.269805908203125\n",
      "Epoch 19[0/625] Time:0.678, Train Loss:0.26528483629226685\n",
      "Epoch 19[1/625] Time:0.693, Train Loss:0.33342617750167847\n",
      "Epoch 19[2/625] Time:0.693, Train Loss:0.32620614767074585\n",
      "Epoch 19[3/625] Time:0.694, Train Loss:0.1986805945634842\n",
      "Epoch 19[4/625] Time:0.694, Train Loss:0.2977539002895355\n",
      "Epoch 19[5/625] Time:0.715, Train Loss:0.34954842925071716\n",
      "Epoch 19[6/625] Time:0.702, Train Loss:0.2648518681526184\n",
      "Epoch 19[7/625] Time:0.703, Train Loss:0.1739172488451004\n",
      "Epoch 19[8/625] Time:0.707, Train Loss:0.2777969539165497\n",
      "Epoch 19[9/625] Time:0.706, Train Loss:0.36223214864730835\n",
      "Epoch 19[10/625] Time:0.724, Train Loss:0.2404996156692505\n",
      "Epoch 19[11/625] Time:0.693, Train Loss:0.2532176673412323\n",
      "Epoch 19[12/625] Time:0.694, Train Loss:0.25190016627311707\n",
      "Epoch 19[13/625] Time:0.695, Train Loss:0.2576279640197754\n",
      "Epoch 19[14/625] Time:0.694, Train Loss:0.2509772479534149\n",
      "Epoch 19[15/625] Time:0.693, Train Loss:0.3597543239593506\n",
      "Epoch 19[16/625] Time:0.695, Train Loss:0.2787834107875824\n",
      "Epoch 19[17/625] Time:0.695, Train Loss:0.29718002676963806\n",
      "Epoch 19[18/625] Time:0.703, Train Loss:0.20077718794345856\n",
      "Epoch 19[19/625] Time:0.703, Train Loss:0.3279590904712677\n",
      "Epoch 19[20/625] Time:0.703, Train Loss:0.22373932600021362\n",
      "Epoch 19[21/625] Time:0.703, Train Loss:0.25792035460472107\n",
      "Epoch 19[22/625] Time:0.704, Train Loss:0.19081230461597443\n",
      "Epoch 19[23/625] Time:0.704, Train Loss:0.27894139289855957\n",
      "Epoch 19[24/625] Time:0.703, Train Loss:0.1928788721561432\n",
      "Epoch 19[25/625] Time:0.703, Train Loss:0.37880733609199524\n",
      "Epoch 19[26/625] Time:0.702, Train Loss:0.26530328392982483\n",
      "Epoch 19[27/625] Time:0.703, Train Loss:0.3592599034309387\n",
      "Epoch 19[28/625] Time:0.703, Train Loss:0.27040356397628784\n",
      "Epoch 19[29/625] Time:0.703, Train Loss:0.2311498522758484\n",
      "Epoch 19[30/625] Time:0.712, Train Loss:0.29513412714004517\n",
      "Epoch 19[31/625] Time:0.704, Train Loss:0.22838836908340454\n",
      "Epoch 19[32/625] Time:0.703, Train Loss:0.20660577714443207\n",
      "Epoch 19[33/625] Time:0.704, Train Loss:0.25894567370414734\n",
      "Epoch 19[34/625] Time:0.703, Train Loss:0.2330765277147293\n",
      "Epoch 19[35/625] Time:0.704, Train Loss:0.39652180671691895\n",
      "Epoch 19[36/625] Time:0.702, Train Loss:0.18905994296073914\n",
      "Epoch 19[37/625] Time:0.703, Train Loss:0.26462313532829285\n",
      "Epoch 19[38/625] Time:0.706, Train Loss:0.28903114795684814\n",
      "Epoch 19[39/625] Time:0.703, Train Loss:0.1963755488395691\n",
      "Epoch 19[40/625] Time:0.704, Train Loss:0.2434951215982437\n",
      "Epoch 19[41/625] Time:0.704, Train Loss:0.21746471524238586\n",
      "Epoch 19[42/625] Time:0.703, Train Loss:0.32600539922714233\n",
      "Epoch 19[43/625] Time:0.693, Train Loss:0.2990960478782654\n",
      "Epoch 19[44/625] Time:0.695, Train Loss:0.26719480752944946\n",
      "Epoch 19[45/625] Time:0.695, Train Loss:0.32434260845184326\n",
      "Epoch 19[46/625] Time:0.696, Train Loss:0.2961147129535675\n",
      "Epoch 19[47/625] Time:0.705, Train Loss:0.24809613823890686\n",
      "Epoch 19[48/625] Time:0.702, Train Loss:0.2649213671684265\n",
      "Epoch 19[49/625] Time:0.703, Train Loss:0.260142058134079\n",
      "Epoch 19[50/625] Time:0.706, Train Loss:0.2768493592739105\n",
      "Epoch 19[51/625] Time:0.704, Train Loss:0.19625025987625122\n",
      "Epoch 19[52/625] Time:0.708, Train Loss:0.21959254145622253\n",
      "Epoch 19[53/625] Time:0.705, Train Loss:0.18709680438041687\n",
      "Epoch 19[54/625] Time:0.704, Train Loss:0.2694779634475708\n",
      "Epoch 19[55/625] Time:0.704, Train Loss:0.24294625222682953\n",
      "Epoch 19[56/625] Time:0.704, Train Loss:0.21053104102611542\n",
      "Epoch 19[57/625] Time:0.704, Train Loss:0.2775243818759918\n",
      "Epoch 19[58/625] Time:0.703, Train Loss:0.3009364604949951\n",
      "Epoch 19[59/625] Time:0.703, Train Loss:0.35364317893981934\n",
      "Epoch 19[60/625] Time:0.703, Train Loss:0.280173659324646\n",
      "Epoch 19[61/625] Time:0.704, Train Loss:0.19643773138523102\n",
      "Epoch 19[62/625] Time:0.705, Train Loss:0.2551315724849701\n",
      "Epoch 19[63/625] Time:0.703, Train Loss:0.2226347029209137\n",
      "Epoch 19[64/625] Time:0.703, Train Loss:0.2939598858356476\n",
      "Epoch 19[65/625] Time:0.705, Train Loss:0.2300311177968979\n",
      "Epoch 19[66/625] Time:0.706, Train Loss:0.23976050317287445\n",
      "Epoch 19[67/625] Time:0.705, Train Loss:0.22255778312683105\n",
      "Epoch 19[68/625] Time:0.703, Train Loss:0.2651574909687042\n",
      "Epoch 19[69/625] Time:0.704, Train Loss:0.21407712996006012\n",
      "Epoch 19[70/625] Time:0.704, Train Loss:0.19395723938941956\n",
      "Epoch 19[71/625] Time:0.703, Train Loss:0.22974072396755219\n",
      "Epoch 19[72/625] Time:0.704, Train Loss:0.20765310525894165\n",
      "Epoch 19[73/625] Time:0.705, Train Loss:0.33948659896850586\n",
      "Epoch 19[74/625] Time:0.703, Train Loss:0.2361571043729782\n",
      "Epoch 19[75/625] Time:0.703, Train Loss:0.23524592816829681\n",
      "Epoch 19[76/625] Time:0.703, Train Loss:0.3218382000923157\n",
      "Epoch 19[77/625] Time:0.703, Train Loss:0.2672711908817291\n",
      "Epoch 19[78/625] Time:0.706, Train Loss:0.2903328835964203\n",
      "Epoch 19[79/625] Time:0.707, Train Loss:0.27800145745277405\n",
      "Epoch 19[80/625] Time:0.703, Train Loss:0.2520311176776886\n",
      "Epoch 19[81/625] Time:0.705, Train Loss:0.28937748074531555\n",
      "Epoch 19[82/625] Time:0.706, Train Loss:0.26695042848587036\n",
      "Epoch 19[83/625] Time:0.705, Train Loss:0.33083680272102356\n",
      "Epoch 19[84/625] Time:0.704, Train Loss:0.2269820123910904\n",
      "Epoch 19[85/625] Time:0.705, Train Loss:0.2794328033924103\n",
      "Epoch 19[86/625] Time:0.704, Train Loss:0.23577284812927246\n",
      "Epoch 19[87/625] Time:0.704, Train Loss:0.31270793080329895\n",
      "Epoch 19[88/625] Time:0.738, Train Loss:0.26358360052108765\n",
      "Epoch 19[89/625] Time:0.702, Train Loss:0.3366166651248932\n",
      "Epoch 19[90/625] Time:0.702, Train Loss:0.2660629451274872\n",
      "Epoch 19[91/625] Time:0.701, Train Loss:0.26659631729125977\n",
      "Epoch 19[92/625] Time:0.692, Train Loss:0.230219304561615\n",
      "Epoch 19[93/625] Time:0.692, Train Loss:0.2875889241695404\n",
      "Epoch 19[94/625] Time:0.693, Train Loss:0.25287455320358276\n",
      "Epoch 19[95/625] Time:0.693, Train Loss:0.2505483627319336\n",
      "Epoch 19[96/625] Time:0.727, Train Loss:0.2661173939704895\n",
      "Epoch 19[97/625] Time:0.702, Train Loss:0.23116768896579742\n",
      "Epoch 19[98/625] Time:0.703, Train Loss:0.22497832775115967\n",
      "Epoch 19[99/625] Time:0.704, Train Loss:0.25315800309181213\n",
      "Epoch 19[100/625] Time:0.693, Train Loss:0.23413953185081482\n",
      "Epoch 19[101/625] Time:0.704, Train Loss:0.21506576240062714\n",
      "Epoch 19[102/625] Time:0.704, Train Loss:0.2248164564371109\n",
      "Epoch 19[103/625] Time:0.704, Train Loss:0.29277509450912476\n",
      "Epoch 19[104/625] Time:0.703, Train Loss:0.2702617049217224\n",
      "Epoch 19[105/625] Time:0.702, Train Loss:0.2594173848628998\n",
      "Epoch 19[106/625] Time:0.702, Train Loss:0.23742328584194183\n",
      "Epoch 19[107/625] Time:0.703, Train Loss:0.2654843330383301\n",
      "Epoch 19[108/625] Time:0.704, Train Loss:0.3099552392959595\n",
      "Epoch 19[109/625] Time:0.711, Train Loss:0.25589942932128906\n",
      "Epoch 19[110/625] Time:0.703, Train Loss:0.3148227334022522\n",
      "Epoch 19[111/625] Time:0.703, Train Loss:0.39102309942245483\n",
      "Epoch 19[112/625] Time:0.704, Train Loss:0.2514904737472534\n",
      "Epoch 19[113/625] Time:0.704, Train Loss:0.2463645488023758\n",
      "Epoch 19[114/625] Time:0.704, Train Loss:0.31476879119873047\n",
      "Epoch 19[115/625] Time:0.708, Train Loss:0.3003779947757721\n",
      "Epoch 19[116/625] Time:0.704, Train Loss:0.21572375297546387\n",
      "Epoch 19[117/625] Time:0.705, Train Loss:0.2614808678627014\n",
      "Epoch 19[118/625] Time:0.707, Train Loss:0.3024033308029175\n",
      "Epoch 19[119/625] Time:0.706, Train Loss:0.2081950306892395\n",
      "Epoch 19[120/625] Time:0.704, Train Loss:0.25989341735839844\n",
      "Epoch 19[121/625] Time:0.703, Train Loss:0.2715214788913727\n",
      "Epoch 19[122/625] Time:0.704, Train Loss:0.2586277425289154\n",
      "Epoch 19[123/625] Time:0.703, Train Loss:0.27329307794570923\n",
      "Epoch 19[124/625] Time:0.704, Train Loss:0.3602595627307892\n",
      "Epoch 19[125/625] Time:0.704, Train Loss:0.2770097851753235\n",
      "Epoch 19[126/625] Time:0.702, Train Loss:0.31249481439590454\n",
      "Epoch 19[127/625] Time:0.703, Train Loss:0.23862114548683167\n",
      "Epoch 19[128/625] Time:0.704, Train Loss:0.2439125031232834\n",
      "Epoch 19[129/625] Time:0.703, Train Loss:0.292386919260025\n",
      "Epoch 19[130/625] Time:0.703, Train Loss:0.26785632967948914\n",
      "Epoch 19[131/625] Time:0.702, Train Loss:0.2593197822570801\n",
      "Epoch 19[132/625] Time:0.703, Train Loss:0.257684588432312\n",
      "Epoch 19[133/625] Time:0.709, Train Loss:0.21064503490924835\n",
      "Epoch 19[134/625] Time:0.702, Train Loss:0.27725937962532043\n",
      "Epoch 19[135/625] Time:0.702, Train Loss:0.22213828563690186\n",
      "Epoch 19[136/625] Time:0.702, Train Loss:0.2321898490190506\n",
      "Epoch 19[137/625] Time:0.735, Train Loss:0.21634618937969208\n",
      "Epoch 19[138/625] Time:0.693, Train Loss:0.30159902572631836\n",
      "Epoch 19[139/625] Time:0.694, Train Loss:0.222489133477211\n",
      "Epoch 19[140/625] Time:0.696, Train Loss:0.20467078685760498\n",
      "Epoch 19[141/625] Time:0.704, Train Loss:0.2283061146736145\n",
      "Epoch 19[142/625] Time:0.734, Train Loss:0.24542880058288574\n",
      "Epoch 19[143/625] Time:0.726, Train Loss:0.236586794257164\n",
      "Epoch 19[144/625] Time:0.694, Train Loss:0.2512364387512207\n",
      "Epoch 19[145/625] Time:0.703, Train Loss:0.3210721015930176\n",
      "Epoch 19[146/625] Time:0.703, Train Loss:0.267580509185791\n",
      "Epoch 19[147/625] Time:0.703, Train Loss:0.23017875850200653\n",
      "Epoch 19[148/625] Time:0.702, Train Loss:0.271881639957428\n",
      "Epoch 19[149/625] Time:0.704, Train Loss:0.20457662642002106\n",
      "Epoch 19[150/625] Time:0.703, Train Loss:0.23974332213401794\n",
      "Epoch 19[151/625] Time:0.702, Train Loss:0.1482962816953659\n",
      "Epoch 19[152/625] Time:0.702, Train Loss:0.37047889828681946\n",
      "Epoch 19[153/625] Time:0.703, Train Loss:0.22251835465431213\n",
      "Epoch 19[154/625] Time:0.703, Train Loss:0.21940703690052032\n",
      "Epoch 19[155/625] Time:0.706, Train Loss:0.21838800609111786\n",
      "Epoch 19[156/625] Time:0.706, Train Loss:0.25251883268356323\n",
      "Epoch 19[157/625] Time:0.704, Train Loss:0.25023263692855835\n",
      "Epoch 19[158/625] Time:0.702, Train Loss:0.24135737121105194\n",
      "Epoch 19[159/625] Time:0.702, Train Loss:0.16425678133964539\n",
      "Epoch 19[160/625] Time:0.704, Train Loss:0.3487743139266968\n",
      "Epoch 19[161/625] Time:0.703, Train Loss:0.27615591883659363\n",
      "Epoch 19[162/625] Time:0.704, Train Loss:0.28019505739212036\n",
      "Epoch 19[163/625] Time:0.731, Train Loss:0.3237025737762451\n",
      "Epoch 19[164/625] Time:0.702, Train Loss:0.27055132389068604\n",
      "Epoch 19[165/625] Time:0.703, Train Loss:0.22880558669567108\n",
      "Epoch 19[166/625] Time:0.703, Train Loss:0.25112879276275635\n",
      "Epoch 19[167/625] Time:0.703, Train Loss:0.22297798097133636\n",
      "Epoch 19[168/625] Time:0.704, Train Loss:0.23895902931690216\n",
      "Epoch 19[169/625] Time:0.704, Train Loss:0.23157696425914764\n",
      "Epoch 19[170/625] Time:0.705, Train Loss:0.2406376600265503\n",
      "Epoch 19[171/625] Time:0.703, Train Loss:0.27282819151878357\n",
      "Epoch 19[172/625] Time:0.705, Train Loss:0.22868743538856506\n",
      "Epoch 19[173/625] Time:0.703, Train Loss:0.2836560904979706\n",
      "Epoch 19[174/625] Time:0.706, Train Loss:0.2879106402397156\n",
      "Epoch 19[175/625] Time:0.704, Train Loss:0.20551010966300964\n",
      "Epoch 19[176/625] Time:0.706, Train Loss:0.25717106461524963\n",
      "Epoch 19[177/625] Time:0.705, Train Loss:0.2868170142173767\n",
      "Epoch 19[178/625] Time:0.704, Train Loss:0.21687744557857513\n",
      "Epoch 19[179/625] Time:0.708, Train Loss:0.25433415174484253\n",
      "Epoch 19[180/625] Time:0.731, Train Loss:0.34468957781791687\n",
      "Epoch 19[181/625] Time:0.734, Train Loss:0.3058479428291321\n",
      "Epoch 19[182/625] Time:0.703, Train Loss:0.2633366882801056\n",
      "Epoch 19[183/625] Time:0.703, Train Loss:0.2357403188943863\n",
      "Epoch 19[184/625] Time:0.703, Train Loss:0.22990961372852325\n",
      "Epoch 19[185/625] Time:0.703, Train Loss:0.26116666197776794\n",
      "Epoch 19[186/625] Time:0.703, Train Loss:0.2653732895851135\n",
      "Epoch 19[187/625] Time:0.703, Train Loss:0.20512250065803528\n",
      "Epoch 19[188/625] Time:0.703, Train Loss:0.23335842788219452\n",
      "Epoch 19[189/625] Time:0.704, Train Loss:0.24025100469589233\n",
      "Epoch 19[190/625] Time:0.703, Train Loss:0.21991091966629028\n",
      "Epoch 19[191/625] Time:0.705, Train Loss:0.3235176205635071\n",
      "Epoch 19[192/625] Time:0.704, Train Loss:0.2818939983844757\n",
      "Epoch 19[193/625] Time:0.705, Train Loss:0.2464446723461151\n",
      "Epoch 19[194/625] Time:0.704, Train Loss:0.3161223530769348\n",
      "Epoch 19[195/625] Time:0.704, Train Loss:0.3614102900028229\n",
      "Epoch 19[196/625] Time:0.707, Train Loss:0.2052953988313675\n",
      "Epoch 19[197/625] Time:0.703, Train Loss:0.18981987237930298\n",
      "Epoch 19[198/625] Time:0.703, Train Loss:0.19274412095546722\n",
      "Epoch 19[199/625] Time:0.704, Train Loss:0.2708073854446411\n",
      "Epoch 19[200/625] Time:0.704, Train Loss:0.31153276562690735\n",
      "Epoch 19[201/625] Time:0.704, Train Loss:0.21957676112651825\n",
      "Epoch 19[202/625] Time:0.705, Train Loss:0.2583227753639221\n",
      "Epoch 19[203/625] Time:0.703, Train Loss:0.27313554286956787\n",
      "Epoch 19[204/625] Time:0.706, Train Loss:0.27331382036209106\n",
      "Epoch 19[205/625] Time:0.705, Train Loss:0.22027146816253662\n",
      "Epoch 19[206/625] Time:0.707, Train Loss:0.2385224550962448\n",
      "Epoch 19[207/625] Time:0.706, Train Loss:0.3191622495651245\n",
      "Epoch 19[208/625] Time:0.705, Train Loss:0.2973807454109192\n",
      "Epoch 19[209/625] Time:0.704, Train Loss:0.31965798139572144\n",
      "Epoch 19[210/625] Time:0.705, Train Loss:0.2273477017879486\n",
      "Epoch 19[211/625] Time:0.705, Train Loss:0.23643513023853302\n",
      "Epoch 19[212/625] Time:0.707, Train Loss:0.4007474184036255\n",
      "Epoch 19[213/625] Time:0.703, Train Loss:0.2537842094898224\n",
      "Epoch 19[214/625] Time:0.702, Train Loss:0.33957669138908386\n",
      "Epoch 19[215/625] Time:0.702, Train Loss:0.30805811285972595\n",
      "Epoch 19[216/625] Time:0.703, Train Loss:0.27440205216407776\n",
      "Epoch 19[217/625] Time:0.702, Train Loss:0.2868719696998596\n",
      "Epoch 19[218/625] Time:0.702, Train Loss:0.24615781009197235\n",
      "Epoch 19[219/625] Time:0.703, Train Loss:0.2657987177371979\n",
      "Epoch 19[220/625] Time:0.704, Train Loss:0.24243205785751343\n",
      "Epoch 19[221/625] Time:0.71, Train Loss:0.3038240075111389\n",
      "Epoch 19[222/625] Time:0.703, Train Loss:0.2923073470592499\n",
      "Epoch 19[223/625] Time:0.713, Train Loss:0.22150789201259613\n",
      "Epoch 19[224/625] Time:0.704, Train Loss:0.317736953496933\n",
      "Epoch 19[225/625] Time:0.703, Train Loss:0.2767930030822754\n",
      "Epoch 19[226/625] Time:0.704, Train Loss:0.25559842586517334\n",
      "Epoch 19[227/625] Time:0.706, Train Loss:0.3233374059200287\n",
      "Epoch 19[228/625] Time:0.703, Train Loss:0.22199033200740814\n",
      "Epoch 19[229/625] Time:0.703, Train Loss:0.2594096064567566\n",
      "Epoch 19[230/625] Time:0.705, Train Loss:0.33742669224739075\n",
      "Epoch 19[231/625] Time:0.702, Train Loss:0.35702580213546753\n",
      "Epoch 19[232/625] Time:0.704, Train Loss:0.2729940414428711\n",
      "Epoch 19[233/625] Time:0.709, Train Loss:0.30739539861679077\n",
      "Epoch 19[234/625] Time:0.703, Train Loss:0.34574609994888306\n",
      "Epoch 19[235/625] Time:0.703, Train Loss:0.22619511187076569\n",
      "Epoch 19[236/625] Time:0.703, Train Loss:0.3826053738594055\n",
      "Epoch 19[237/625] Time:0.695, Train Loss:0.2614307105541229\n",
      "Epoch 19[238/625] Time:0.702, Train Loss:0.19876433908939362\n",
      "Epoch 19[239/625] Time:0.703, Train Loss:0.21683038771152496\n",
      "Epoch 19[240/625] Time:0.703, Train Loss:0.26743584871292114\n",
      "Epoch 19[241/625] Time:0.702, Train Loss:0.22632162272930145\n",
      "Epoch 19[242/625] Time:0.7, Train Loss:0.30385005474090576\n",
      "Epoch 19[243/625] Time:0.741, Train Loss:0.2739698588848114\n",
      "Epoch 19[244/625] Time:0.698, Train Loss:0.2413017749786377\n",
      "Epoch 19[245/625] Time:0.699, Train Loss:0.22853313386440277\n",
      "Epoch 19[246/625] Time:0.701, Train Loss:0.22773432731628418\n",
      "Epoch 19[247/625] Time:0.719, Train Loss:0.40124115347862244\n",
      "Epoch 19[248/625] Time:0.7, Train Loss:0.33479228615760803\n",
      "Epoch 19[249/625] Time:0.703, Train Loss:0.2815414369106293\n",
      "Epoch 19[250/625] Time:0.703, Train Loss:0.2726876735687256\n",
      "Epoch 19[251/625] Time:0.692, Train Loss:0.22753793001174927\n",
      "Epoch 19[252/625] Time:0.698, Train Loss:0.3910614550113678\n",
      "Epoch 19[253/625] Time:0.716, Train Loss:0.21475817263126373\n",
      "Epoch 19[254/625] Time:0.702, Train Loss:0.2612946033477783\n",
      "Epoch 19[255/625] Time:0.703, Train Loss:0.2642418444156647\n",
      "Epoch 19[256/625] Time:0.703, Train Loss:0.2862307131290436\n",
      "Epoch 19[257/625] Time:0.703, Train Loss:0.3422916829586029\n",
      "Epoch 19[258/625] Time:0.703, Train Loss:0.2647900879383087\n",
      "Epoch 19[259/625] Time:0.702, Train Loss:0.35264840722084045\n",
      "Epoch 19[260/625] Time:0.701, Train Loss:0.21671324968338013\n",
      "Epoch 19[261/625] Time:0.693, Train Loss:0.19586175680160522\n",
      "Epoch 19[262/625] Time:0.706, Train Loss:0.29555636644363403\n",
      "Epoch 19[263/625] Time:0.695, Train Loss:0.2358454465866089\n",
      "Epoch 19[264/625] Time:0.702, Train Loss:0.24447673559188843\n",
      "Epoch 19[265/625] Time:0.703, Train Loss:0.2718867063522339\n",
      "Epoch 19[266/625] Time:0.705, Train Loss:0.21397410333156586\n",
      "Epoch 19[267/625] Time:0.703, Train Loss:0.21486730873584747\n",
      "Epoch 19[268/625] Time:0.705, Train Loss:0.35324493050575256\n",
      "Epoch 19[269/625] Time:0.703, Train Loss:0.25485336780548096\n",
      "Epoch 19[270/625] Time:0.705, Train Loss:0.24304494261741638\n",
      "Epoch 19[271/625] Time:0.704, Train Loss:0.24160455167293549\n",
      "Epoch 19[272/625] Time:0.705, Train Loss:0.25077638030052185\n",
      "Epoch 19[273/625] Time:0.704, Train Loss:0.2900555729866028\n",
      "Epoch 19[274/625] Time:0.705, Train Loss:0.19087937474250793\n",
      "Epoch 19[275/625] Time:0.704, Train Loss:0.2134632021188736\n",
      "Epoch 19[276/625] Time:0.703, Train Loss:0.2230503261089325\n",
      "Epoch 19[277/625] Time:0.704, Train Loss:0.28377997875213623\n",
      "Epoch 19[278/625] Time:0.705, Train Loss:0.3116380572319031\n",
      "Epoch 19[279/625] Time:0.705, Train Loss:0.26184648275375366\n",
      "Epoch 19[280/625] Time:0.705, Train Loss:0.3123238980770111\n",
      "Epoch 19[281/625] Time:0.704, Train Loss:0.29409849643707275\n",
      "Epoch 19[282/625] Time:0.711, Train Loss:0.26216354966163635\n",
      "Epoch 19[283/625] Time:0.704, Train Loss:0.24297067523002625\n",
      "Epoch 19[284/625] Time:0.705, Train Loss:0.27551740407943726\n",
      "Epoch 19[285/625] Time:0.704, Train Loss:0.2759283483028412\n",
      "Epoch 19[286/625] Time:0.706, Train Loss:0.2817326486110687\n",
      "Epoch 19[287/625] Time:0.706, Train Loss:0.26918092370033264\n",
      "Epoch 19[288/625] Time:0.706, Train Loss:0.23840002715587616\n",
      "Epoch 19[289/625] Time:0.706, Train Loss:0.21299628913402557\n",
      "Epoch 19[290/625] Time:0.704, Train Loss:0.24807509779930115\n",
      "Epoch 19[291/625] Time:0.741, Train Loss:0.25267258286476135\n",
      "Epoch 19[292/625] Time:0.704, Train Loss:0.3179369568824768\n",
      "Epoch 19[293/625] Time:0.704, Train Loss:0.25605612993240356\n",
      "Epoch 19[294/625] Time:0.704, Train Loss:0.25845763087272644\n",
      "Epoch 19[295/625] Time:0.705, Train Loss:0.23372691869735718\n",
      "Epoch 19[296/625] Time:0.705, Train Loss:0.2880741357803345\n",
      "Epoch 19[297/625] Time:0.705, Train Loss:0.23010073602199554\n",
      "Epoch 19[298/625] Time:0.706, Train Loss:0.19649845361709595\n",
      "Epoch 19[299/625] Time:0.705, Train Loss:0.20417675375938416\n",
      "Epoch 19[300/625] Time:0.707, Train Loss:0.276832640171051\n",
      "Epoch 19[301/625] Time:0.704, Train Loss:0.2363298088312149\n",
      "Epoch 19[302/625] Time:0.705, Train Loss:0.31449389457702637\n",
      "Epoch 19[303/625] Time:0.704, Train Loss:0.26213538646698\n",
      "Epoch 19[304/625] Time:0.704, Train Loss:0.3302938938140869\n",
      "Epoch 19[305/625] Time:0.704, Train Loss:0.28345027565956116\n",
      "Epoch 19[306/625] Time:0.705, Train Loss:0.2728869318962097\n",
      "Epoch 19[307/625] Time:0.704, Train Loss:0.34059804677963257\n",
      "Epoch 19[308/625] Time:0.704, Train Loss:0.26400068402290344\n",
      "Epoch 19[309/625] Time:0.702, Train Loss:0.26483553647994995\n",
      "Epoch 19[310/625] Time:0.703, Train Loss:0.29330646991729736\n",
      "Epoch 19[311/625] Time:0.703, Train Loss:0.3444593548774719\n",
      "Epoch 19[312/625] Time:0.703, Train Loss:0.31311407685279846\n",
      "Epoch 19[313/625] Time:0.703, Train Loss:0.3340979814529419\n",
      "Epoch 19[314/625] Time:0.706, Train Loss:0.20342493057250977\n",
      "Epoch 19[315/625] Time:0.704, Train Loss:0.27656033635139465\n",
      "Epoch 19[316/625] Time:0.704, Train Loss:0.28715816140174866\n",
      "Epoch 19[317/625] Time:0.704, Train Loss:0.2795913517475128\n",
      "Epoch 19[318/625] Time:0.707, Train Loss:0.24567386507987976\n",
      "Epoch 19[319/625] Time:0.706, Train Loss:0.2506072521209717\n",
      "Epoch 19[320/625] Time:0.705, Train Loss:0.2569296956062317\n",
      "Epoch 19[321/625] Time:0.705, Train Loss:0.29488667845726013\n",
      "Epoch 19[322/625] Time:0.705, Train Loss:0.19402442872524261\n",
      "Epoch 19[323/625] Time:0.704, Train Loss:0.2956436574459076\n",
      "Epoch 19[324/625] Time:0.704, Train Loss:0.32202762365341187\n",
      "Epoch 19[325/625] Time:0.704, Train Loss:0.24144650995731354\n",
      "Epoch 19[326/625] Time:0.704, Train Loss:0.24066394567489624\n",
      "Epoch 19[327/625] Time:0.704, Train Loss:0.3056753873825073\n",
      "Epoch 19[328/625] Time:0.704, Train Loss:0.3019218444824219\n",
      "Epoch 19[329/625] Time:0.702, Train Loss:0.29581189155578613\n",
      "Epoch 19[330/625] Time:0.695, Train Loss:0.30609580874443054\n",
      "Epoch 19[331/625] Time:0.702, Train Loss:0.26221248507499695\n",
      "Epoch 19[332/625] Time:0.702, Train Loss:0.23868931829929352\n",
      "Epoch 19[333/625] Time:0.704, Train Loss:0.2993159592151642\n",
      "Epoch 19[334/625] Time:0.703, Train Loss:0.21482932567596436\n",
      "Epoch 19[335/625] Time:0.704, Train Loss:0.24546390771865845\n",
      "Epoch 19[336/625] Time:0.704, Train Loss:0.3061959445476532\n",
      "Epoch 19[337/625] Time:0.704, Train Loss:0.266779363155365\n",
      "Epoch 19[338/625] Time:0.706, Train Loss:0.2357850819826126\n",
      "Epoch 19[339/625] Time:0.704, Train Loss:0.23919758200645447\n",
      "Epoch 19[340/625] Time:0.707, Train Loss:0.3080061376094818\n",
      "Epoch 19[341/625] Time:0.704, Train Loss:0.23488040268421173\n",
      "Epoch 19[342/625] Time:0.704, Train Loss:0.3127044141292572\n",
      "Epoch 19[343/625] Time:0.704, Train Loss:0.14487165212631226\n",
      "Epoch 19[344/625] Time:0.704, Train Loss:0.2425931990146637\n",
      "Epoch 19[345/625] Time:0.704, Train Loss:0.25552883744239807\n",
      "Epoch 19[346/625] Time:0.705, Train Loss:0.2889498472213745\n",
      "Epoch 19[347/625] Time:0.704, Train Loss:0.22665035724639893\n",
      "Epoch 19[348/625] Time:0.704, Train Loss:0.2891346514225006\n",
      "Epoch 19[349/625] Time:0.703, Train Loss:0.25284436345100403\n",
      "Epoch 19[350/625] Time:0.704, Train Loss:0.19485975801944733\n",
      "Epoch 19[351/625] Time:0.703, Train Loss:0.18758033215999603\n",
      "Epoch 19[352/625] Time:0.704, Train Loss:0.24006310105323792\n",
      "Epoch 19[353/625] Time:0.705, Train Loss:0.2217121124267578\n",
      "Epoch 19[354/625] Time:0.704, Train Loss:0.2626475989818573\n",
      "Epoch 19[355/625] Time:0.704, Train Loss:0.2754232883453369\n",
      "Epoch 19[356/625] Time:0.703, Train Loss:0.19462572038173676\n",
      "Epoch 19[357/625] Time:0.703, Train Loss:0.24469219148159027\n",
      "Epoch 19[358/625] Time:0.703, Train Loss:0.23906372487545013\n",
      "Epoch 19[359/625] Time:0.703, Train Loss:0.22399914264678955\n",
      "Epoch 19[360/625] Time:0.704, Train Loss:0.21687522530555725\n",
      "Epoch 19[361/625] Time:0.704, Train Loss:0.2188880443572998\n",
      "Epoch 19[362/625] Time:0.705, Train Loss:0.2400864064693451\n",
      "Epoch 19[363/625] Time:0.705, Train Loss:0.32603007555007935\n",
      "Epoch 19[364/625] Time:0.704, Train Loss:0.2406550496816635\n",
      "Epoch 19[365/625] Time:0.704, Train Loss:0.2490319162607193\n",
      "Epoch 19[366/625] Time:0.704, Train Loss:0.24665486812591553\n",
      "Epoch 19[367/625] Time:0.704, Train Loss:0.2540284991264343\n",
      "Epoch 19[368/625] Time:0.704, Train Loss:0.2465384304523468\n",
      "Epoch 19[369/625] Time:0.705, Train Loss:0.28868797421455383\n",
      "Epoch 19[370/625] Time:0.704, Train Loss:0.2509513795375824\n",
      "Epoch 19[371/625] Time:0.704, Train Loss:0.20497792959213257\n",
      "Epoch 19[372/625] Time:0.704, Train Loss:0.2456636279821396\n",
      "Epoch 19[373/625] Time:0.704, Train Loss:0.17505589127540588\n",
      "Epoch 19[374/625] Time:0.704, Train Loss:0.2638380825519562\n",
      "Epoch 19[375/625] Time:0.704, Train Loss:0.24093012511730194\n",
      "Epoch 19[376/625] Time:0.704, Train Loss:0.3312529921531677\n",
      "Epoch 19[377/625] Time:0.704, Train Loss:0.2629226744174957\n",
      "Epoch 19[378/625] Time:0.705, Train Loss:0.2433738112449646\n",
      "Epoch 19[379/625] Time:0.703, Train Loss:0.23400981724262238\n",
      "Epoch 19[380/625] Time:0.703, Train Loss:0.31711751222610474\n",
      "Epoch 19[381/625] Time:0.692, Train Loss:0.19783949851989746\n",
      "Epoch 19[382/625] Time:0.703, Train Loss:0.23735123872756958\n",
      "Epoch 19[383/625] Time:0.703, Train Loss:0.3625968396663666\n",
      "Epoch 19[384/625] Time:0.705, Train Loss:0.18667848408222198\n",
      "Epoch 19[385/625] Time:0.703, Train Loss:0.2650204300880432\n",
      "Epoch 19[386/625] Time:0.703, Train Loss:0.25945475697517395\n",
      "Epoch 19[387/625] Time:0.702, Train Loss:0.3266340494155884\n",
      "Epoch 19[388/625] Time:0.703, Train Loss:0.237509086728096\n",
      "Epoch 19[389/625] Time:0.703, Train Loss:0.23756109178066254\n",
      "Epoch 19[390/625] Time:0.703, Train Loss:0.2970621585845947\n",
      "Epoch 19[391/625] Time:0.703, Train Loss:0.23525796830654144\n",
      "Epoch 19[392/625] Time:0.702, Train Loss:0.33744683861732483\n",
      "Epoch 19[393/625] Time:0.705, Train Loss:0.27486875653266907\n",
      "Epoch 19[394/625] Time:0.736, Train Loss:0.31233081221580505\n",
      "Epoch 19[395/625] Time:0.693, Train Loss:0.36461585760116577\n",
      "Epoch 19[396/625] Time:0.692, Train Loss:0.25617215037345886\n",
      "Epoch 19[397/625] Time:0.703, Train Loss:0.3247504234313965\n",
      "Epoch 19[398/625] Time:0.704, Train Loss:0.2893081605434418\n",
      "Epoch 19[399/625] Time:0.731, Train Loss:0.2608642578125\n",
      "Epoch 19[400/625] Time:0.695, Train Loss:0.31707000732421875\n",
      "Epoch 19[401/625] Time:0.699, Train Loss:0.2759110927581787\n",
      "Epoch 19[402/625] Time:0.696, Train Loss:0.2501494884490967\n",
      "Epoch 19[403/625] Time:0.695, Train Loss:0.2700543999671936\n",
      "Epoch 19[404/625] Time:0.695, Train Loss:0.23707744479179382\n",
      "Epoch 19[405/625] Time:0.693, Train Loss:0.2033759355545044\n",
      "Epoch 19[406/625] Time:0.693, Train Loss:0.19658821821212769\n",
      "Epoch 19[407/625] Time:0.702, Train Loss:0.2538793385028839\n",
      "Epoch 19[408/625] Time:0.703, Train Loss:0.29029351472854614\n",
      "Epoch 19[409/625] Time:0.721, Train Loss:0.2876533567905426\n",
      "Epoch 19[410/625] Time:0.7, Train Loss:0.32919302582740784\n",
      "Epoch 19[411/625] Time:0.703, Train Loss:0.25575244426727295\n",
      "Epoch 19[412/625] Time:0.703, Train Loss:0.29630547761917114\n",
      "Epoch 19[413/625] Time:0.705, Train Loss:0.25430551171302795\n",
      "Epoch 19[414/625] Time:0.716, Train Loss:0.2791483700275421\n",
      "Epoch 19[415/625] Time:0.692, Train Loss:0.2895170748233795\n",
      "Epoch 19[416/625] Time:0.696, Train Loss:0.31793245673179626\n",
      "Epoch 19[417/625] Time:0.692, Train Loss:0.2197752743959427\n",
      "Epoch 19[418/625] Time:0.693, Train Loss:0.21349290013313293\n",
      "Epoch 19[419/625] Time:0.692, Train Loss:0.23287035524845123\n",
      "Epoch 19[420/625] Time:0.695, Train Loss:0.24449774622917175\n",
      "Epoch 19[421/625] Time:0.7, Train Loss:0.2704063951969147\n",
      "Epoch 19[422/625] Time:0.703, Train Loss:0.2421661615371704\n",
      "Epoch 19[423/625] Time:0.704, Train Loss:0.21707645058631897\n",
      "Epoch 19[424/625] Time:0.703, Train Loss:0.26185786724090576\n",
      "Epoch 19[425/625] Time:0.703, Train Loss:0.258148729801178\n",
      "Epoch 19[426/625] Time:0.704, Train Loss:0.25701406598091125\n",
      "Epoch 19[427/625] Time:0.702, Train Loss:0.2390356957912445\n",
      "Epoch 19[428/625] Time:0.703, Train Loss:0.31979644298553467\n",
      "Epoch 19[429/625] Time:0.702, Train Loss:0.2634803354740143\n",
      "Epoch 19[430/625] Time:0.703, Train Loss:0.24725621938705444\n",
      "Epoch 19[431/625] Time:0.704, Train Loss:0.3010709285736084\n",
      "Epoch 19[432/625] Time:0.706, Train Loss:0.2780563235282898\n",
      "Epoch 19[433/625] Time:0.703, Train Loss:0.2680776119232178\n",
      "Epoch 19[434/625] Time:0.704, Train Loss:0.20237644016742706\n",
      "Epoch 19[435/625] Time:0.707, Train Loss:0.2670353949069977\n",
      "Epoch 19[436/625] Time:0.705, Train Loss:0.3279675245285034\n",
      "Epoch 19[437/625] Time:0.704, Train Loss:0.23786166310310364\n",
      "Epoch 19[438/625] Time:0.704, Train Loss:0.23001728951931\n",
      "Epoch 19[439/625] Time:0.704, Train Loss:0.18168233335018158\n",
      "Epoch 19[440/625] Time:0.708, Train Loss:0.24835419654846191\n",
      "Epoch 19[441/625] Time:0.702, Train Loss:0.25836268067359924\n",
      "Epoch 19[442/625] Time:0.703, Train Loss:0.2705616056919098\n",
      "Epoch 19[443/625] Time:0.702, Train Loss:0.20005065202713013\n",
      "Epoch 19[444/625] Time:0.702, Train Loss:0.264242023229599\n",
      "Epoch 19[445/625] Time:0.702, Train Loss:0.2091294676065445\n",
      "Epoch 19[446/625] Time:0.703, Train Loss:0.1776583343744278\n",
      "Epoch 19[447/625] Time:0.702, Train Loss:0.27484044432640076\n",
      "Epoch 19[448/625] Time:0.708, Train Loss:0.2714770436286926\n",
      "Epoch 19[449/625] Time:0.703, Train Loss:0.23970207571983337\n",
      "Epoch 19[450/625] Time:0.702, Train Loss:0.20658762753009796\n",
      "Epoch 19[451/625] Time:0.702, Train Loss:0.3677549362182617\n",
      "Epoch 19[452/625] Time:0.703, Train Loss:0.28650522232055664\n",
      "Epoch 19[453/625] Time:0.704, Train Loss:0.24256347119808197\n",
      "Epoch 19[454/625] Time:0.703, Train Loss:0.24259480834007263\n",
      "Epoch 19[455/625] Time:0.702, Train Loss:0.25172027945518494\n",
      "Epoch 19[456/625] Time:0.712, Train Loss:0.23294909298419952\n",
      "Epoch 19[457/625] Time:0.706, Train Loss:0.24907106161117554\n",
      "Epoch 19[458/625] Time:0.702, Train Loss:0.2572799324989319\n",
      "Epoch 19[459/625] Time:0.702, Train Loss:0.3520875871181488\n",
      "Epoch 19[460/625] Time:0.743, Train Loss:0.3336290717124939\n",
      "Epoch 19[461/625] Time:0.693, Train Loss:0.26670488715171814\n",
      "Epoch 19[462/625] Time:0.695, Train Loss:0.2744678854942322\n",
      "Epoch 19[463/625] Time:0.703, Train Loss:0.3008659780025482\n",
      "Epoch 19[464/625] Time:0.703, Train Loss:0.28756359219551086\n",
      "Epoch 19[465/625] Time:0.704, Train Loss:0.24636472761631012\n",
      "Epoch 19[466/625] Time:0.703, Train Loss:0.22058020532131195\n",
      "Epoch 19[467/625] Time:0.704, Train Loss:0.31566670536994934\n",
      "Epoch 19[468/625] Time:0.704, Train Loss:0.2506778836250305\n",
      "Epoch 19[469/625] Time:0.706, Train Loss:0.27640584111213684\n",
      "Epoch 19[470/625] Time:0.705, Train Loss:0.24691171944141388\n",
      "Epoch 19[471/625] Time:0.704, Train Loss:0.22324684262275696\n",
      "Epoch 19[472/625] Time:0.702, Train Loss:0.3315330743789673\n",
      "Epoch 19[473/625] Time:0.704, Train Loss:0.19493718445301056\n",
      "Epoch 19[474/625] Time:0.702, Train Loss:0.2381506711244583\n",
      "Epoch 19[475/625] Time:0.702, Train Loss:0.24069282412528992\n",
      "Epoch 19[476/625] Time:0.702, Train Loss:0.3005553185939789\n",
      "Epoch 19[477/625] Time:0.73, Train Loss:0.19302399456501007\n",
      "Epoch 19[478/625] Time:0.694, Train Loss:0.29637929797172546\n",
      "Epoch 19[479/625] Time:0.703, Train Loss:0.1711830347776413\n",
      "Epoch 19[480/625] Time:0.703, Train Loss:0.2341151237487793\n",
      "Epoch 19[481/625] Time:0.702, Train Loss:0.30053645372390747\n",
      "Epoch 19[482/625] Time:0.702, Train Loss:0.24591541290283203\n",
      "Epoch 19[483/625] Time:0.708, Train Loss:0.22717517614364624\n",
      "Epoch 19[484/625] Time:0.702, Train Loss:0.2660394608974457\n",
      "Epoch 19[485/625] Time:0.703, Train Loss:0.27644777297973633\n",
      "Epoch 19[486/625] Time:0.703, Train Loss:0.24639756977558136\n",
      "Epoch 19[487/625] Time:0.702, Train Loss:0.2532464861869812\n",
      "Epoch 19[488/625] Time:0.703, Train Loss:0.25465813279151917\n",
      "Epoch 19[489/625] Time:0.702, Train Loss:0.2626737356185913\n",
      "Epoch 19[490/625] Time:0.702, Train Loss:0.30589690804481506\n",
      "Epoch 19[491/625] Time:0.702, Train Loss:0.31855452060699463\n",
      "Epoch 19[492/625] Time:0.702, Train Loss:0.19485871493816376\n",
      "Epoch 19[493/625] Time:0.702, Train Loss:0.2086559683084488\n",
      "Epoch 19[494/625] Time:0.704, Train Loss:0.2726712226867676\n",
      "Epoch 19[495/625] Time:0.736, Train Loss:0.1789492517709732\n",
      "Epoch 19[496/625] Time:0.692, Train Loss:0.20925205945968628\n",
      "Epoch 19[497/625] Time:0.695, Train Loss:0.27706748247146606\n",
      "Epoch 19[498/625] Time:0.694, Train Loss:0.23617304861545563\n",
      "Epoch 19[499/625] Time:0.697, Train Loss:0.2831466495990753\n",
      "Epoch 19[500/625] Time:0.694, Train Loss:0.22461864352226257\n",
      "Epoch 19[501/625] Time:0.695, Train Loss:0.2511190176010132\n",
      "Epoch 19[502/625] Time:0.695, Train Loss:0.24762286245822906\n",
      "Epoch 19[503/625] Time:0.696, Train Loss:0.25555795431137085\n",
      "Epoch 19[504/625] Time:0.694, Train Loss:0.2830197811126709\n",
      "Epoch 19[505/625] Time:0.703, Train Loss:0.27061671018600464\n",
      "Epoch 19[506/625] Time:0.704, Train Loss:0.24269182980060577\n",
      "Epoch 19[507/625] Time:0.703, Train Loss:0.2866365611553192\n",
      "Epoch 19[508/625] Time:0.694, Train Loss:0.30175331234931946\n",
      "Epoch 19[509/625] Time:0.703, Train Loss:0.1983826458454132\n",
      "Epoch 19[510/625] Time:0.702, Train Loss:0.24051088094711304\n",
      "Epoch 19[511/625] Time:0.703, Train Loss:0.3041153848171234\n",
      "Epoch 19[512/625] Time:0.704, Train Loss:0.3106469511985779\n",
      "Epoch 19[513/625] Time:0.702, Train Loss:0.2848559319972992\n",
      "Epoch 19[514/625] Time:0.703, Train Loss:0.2611328363418579\n",
      "Epoch 19[515/625] Time:0.703, Train Loss:0.197343111038208\n",
      "Epoch 19[516/625] Time:0.705, Train Loss:0.3217589259147644\n",
      "Epoch 19[517/625] Time:0.702, Train Loss:0.2513674795627594\n",
      "Epoch 19[518/625] Time:0.705, Train Loss:0.27932843565940857\n",
      "Epoch 19[519/625] Time:0.703, Train Loss:0.25466203689575195\n",
      "Epoch 19[520/625] Time:0.706, Train Loss:0.2843552529811859\n",
      "Epoch 19[521/625] Time:0.703, Train Loss:0.25419124960899353\n",
      "Epoch 19[522/625] Time:0.703, Train Loss:0.19693347811698914\n",
      "Epoch 19[523/625] Time:0.703, Train Loss:0.4105982482433319\n",
      "Epoch 19[524/625] Time:0.703, Train Loss:0.31919053196907043\n",
      "Epoch 19[525/625] Time:0.702, Train Loss:0.2250426858663559\n",
      "Epoch 19[526/625] Time:0.702, Train Loss:0.2501579523086548\n",
      "Epoch 19[527/625] Time:0.721, Train Loss:0.41858556866645813\n",
      "Epoch 19[528/625] Time:0.693, Train Loss:0.23843015730381012\n",
      "Epoch 19[529/625] Time:0.693, Train Loss:0.23209694027900696\n",
      "Epoch 19[530/625] Time:0.693, Train Loss:0.3201676607131958\n",
      "Epoch 19[531/625] Time:0.693, Train Loss:0.28772151470184326\n",
      "Epoch 19[532/625] Time:0.691, Train Loss:0.26145681738853455\n",
      "Epoch 19[533/625] Time:0.691, Train Loss:0.2682989835739136\n",
      "Epoch 19[534/625] Time:0.69, Train Loss:0.1504608690738678\n",
      "Epoch 19[535/625] Time:0.717, Train Loss:0.29099562764167786\n",
      "Epoch 19[536/625] Time:0.704, Train Loss:0.23345953226089478\n",
      "Epoch 19[537/625] Time:0.703, Train Loss:0.3340536952018738\n",
      "Epoch 19[538/625] Time:0.704, Train Loss:0.3174039125442505\n",
      "Epoch 19[539/625] Time:0.706, Train Loss:0.237018883228302\n",
      "Epoch 19[540/625] Time:0.706, Train Loss:0.271001935005188\n",
      "Epoch 19[541/625] Time:0.706, Train Loss:0.19427916407585144\n",
      "Epoch 19[542/625] Time:0.703, Train Loss:0.22472581267356873\n",
      "Epoch 19[543/625] Time:0.705, Train Loss:0.21208631992340088\n",
      "Epoch 19[544/625] Time:0.703, Train Loss:0.2853185534477234\n",
      "Epoch 19[545/625] Time:0.703, Train Loss:0.2298879325389862\n",
      "Epoch 19[546/625] Time:0.704, Train Loss:0.2552596628665924\n",
      "Epoch 19[547/625] Time:0.704, Train Loss:0.2288757562637329\n",
      "Epoch 19[548/625] Time:0.705, Train Loss:0.22818394005298615\n",
      "Epoch 19[549/625] Time:0.706, Train Loss:0.30173182487487793\n",
      "Epoch 19[550/625] Time:0.705, Train Loss:0.31743523478507996\n",
      "Epoch 19[551/625] Time:0.704, Train Loss:0.33968794345855713\n",
      "Epoch 19[552/625] Time:0.705, Train Loss:0.15817606449127197\n",
      "Epoch 19[553/625] Time:0.713, Train Loss:0.22795958817005157\n",
      "Epoch 19[554/625] Time:0.692, Train Loss:0.22742317616939545\n",
      "Epoch 19[555/625] Time:0.692, Train Loss:0.28890615701675415\n",
      "Epoch 19[556/625] Time:0.694, Train Loss:0.21023975312709808\n",
      "Epoch 19[557/625] Time:0.691, Train Loss:0.27404823899269104\n",
      "Epoch 19[558/625] Time:0.692, Train Loss:0.25677573680877686\n",
      "Epoch 19[559/625] Time:0.704, Train Loss:0.20264172554016113\n",
      "Epoch 19[560/625] Time:0.705, Train Loss:0.24134568870067596\n",
      "Epoch 19[561/625] Time:0.706, Train Loss:0.24217474460601807\n",
      "Epoch 19[562/625] Time:0.706, Train Loss:0.25933101773262024\n",
      "Epoch 19[563/625] Time:0.705, Train Loss:0.3584655523300171\n",
      "Epoch 19[564/625] Time:0.734, Train Loss:0.2980717718601227\n",
      "Epoch 19[565/625] Time:0.703, Train Loss:0.3219844102859497\n",
      "Epoch 19[566/625] Time:0.762, Train Loss:0.24710752069950104\n",
      "Epoch 19[567/625] Time:0.701, Train Loss:0.26416319608688354\n",
      "Epoch 19[568/625] Time:0.703, Train Loss:0.24668380618095398\n",
      "Epoch 19[569/625] Time:0.702, Train Loss:0.27964866161346436\n",
      "Epoch 19[570/625] Time:0.704, Train Loss:0.2706718146800995\n",
      "Epoch 19[571/625] Time:0.703, Train Loss:0.24933920800685883\n",
      "Epoch 19[572/625] Time:0.705, Train Loss:0.2816232740879059\n",
      "Epoch 19[573/625] Time:0.706, Train Loss:0.2673974335193634\n",
      "Epoch 19[574/625] Time:0.704, Train Loss:0.23937663435935974\n",
      "Epoch 19[575/625] Time:0.703, Train Loss:0.21668237447738647\n",
      "Epoch 19[576/625] Time:0.702, Train Loss:0.2452554851770401\n",
      "Epoch 19[577/625] Time:0.702, Train Loss:0.21156103909015656\n",
      "Epoch 19[578/625] Time:0.705, Train Loss:0.295067697763443\n",
      "Epoch 19[579/625] Time:0.704, Train Loss:0.20477516949176788\n",
      "Epoch 19[580/625] Time:0.704, Train Loss:0.20867089927196503\n",
      "Epoch 19[581/625] Time:0.703, Train Loss:0.23051980137825012\n",
      "Epoch 19[582/625] Time:0.702, Train Loss:0.2673293650150299\n",
      "Epoch 19[583/625] Time:0.703, Train Loss:0.234505757689476\n",
      "Epoch 19[584/625] Time:0.703, Train Loss:0.26477479934692383\n",
      "Epoch 19[585/625] Time:0.71, Train Loss:0.2152533382177353\n",
      "Epoch 19[586/625] Time:0.692, Train Loss:0.25573188066482544\n",
      "Epoch 19[587/625] Time:0.705, Train Loss:0.24652189016342163\n",
      "Epoch 19[588/625] Time:0.703, Train Loss:0.23647865653038025\n",
      "Epoch 19[589/625] Time:0.75, Train Loss:0.23693078756332397\n",
      "Epoch 19[590/625] Time:0.708, Train Loss:0.25232234597206116\n",
      "Epoch 19[591/625] Time:0.704, Train Loss:0.24537540972232819\n",
      "Epoch 19[592/625] Time:0.704, Train Loss:0.34993186593055725\n",
      "Epoch 19[593/625] Time:0.703, Train Loss:0.4202752411365509\n",
      "Epoch 19[594/625] Time:0.705, Train Loss:0.2078786939382553\n",
      "Epoch 19[595/625] Time:0.724, Train Loss:0.25212275981903076\n",
      "Epoch 19[596/625] Time:0.695, Train Loss:0.23652280867099762\n",
      "Epoch 19[597/625] Time:0.693, Train Loss:0.2508929669857025\n",
      "Epoch 19[598/625] Time:0.696, Train Loss:0.2752422094345093\n",
      "Epoch 19[599/625] Time:0.702, Train Loss:0.2367335855960846\n",
      "Epoch 19[600/625] Time:0.747, Train Loss:0.2382597178220749\n",
      "Epoch 19[601/625] Time:0.692, Train Loss:0.2798209488391876\n",
      "Epoch 19[602/625] Time:0.693, Train Loss:0.22857992351055145\n",
      "Epoch 19[603/625] Time:0.745, Train Loss:0.21200191974639893\n",
      "Epoch 19[604/625] Time:0.694, Train Loss:0.2175176590681076\n",
      "Epoch 19[605/625] Time:0.702, Train Loss:0.21734824776649475\n",
      "Epoch 19[606/625] Time:0.705, Train Loss:0.19672690331935883\n",
      "Epoch 19[607/625] Time:0.706, Train Loss:0.34183335304260254\n",
      "Epoch 19[608/625] Time:0.746, Train Loss:0.29288849234580994\n",
      "Epoch 19[609/625] Time:0.692, Train Loss:0.21287772059440613\n",
      "Epoch 19[610/625] Time:0.703, Train Loss:0.27169105410575867\n",
      "Epoch 19[611/625] Time:0.705, Train Loss:0.2580451965332031\n",
      "Epoch 19[612/625] Time:0.709, Train Loss:0.24029532074928284\n",
      "Epoch 19[613/625] Time:0.705, Train Loss:0.2840951383113861\n",
      "Epoch 19[614/625] Time:0.701, Train Loss:0.21434162557125092\n",
      "Epoch 19[615/625] Time:0.704, Train Loss:0.25270193815231323\n",
      "Epoch 19[616/625] Time:0.705, Train Loss:0.21425674855709076\n",
      "Epoch 19[617/625] Time:0.703, Train Loss:0.3594963550567627\n",
      "Epoch 19[618/625] Time:0.704, Train Loss:0.24017415940761566\n",
      "Epoch 19[619/625] Time:0.705, Train Loss:0.2797083556652069\n",
      "Epoch 19[620/625] Time:0.704, Train Loss:0.23402412235736847\n",
      "Epoch 19[621/625] Time:0.709, Train Loss:0.34107643365859985\n",
      "Epoch 19[622/625] Time:0.704, Train Loss:0.2483959197998047\n",
      "Epoch 19[623/625] Time:0.703, Train Loss:0.23521852493286133\n",
      "Epoch 19[624/625] Time:0.704, Train Loss:0.3100226819515228\n",
      "Epoch 19[0/78] Val Loss:0.21993733942508698\n",
      "Epoch 19[1/78] Val Loss:0.19576771557331085\n",
      "Epoch 19[2/78] Val Loss:0.21033692359924316\n",
      "Epoch 19[3/78] Val Loss:0.24139511585235596\n",
      "Epoch 19[4/78] Val Loss:0.3299645483493805\n",
      "Epoch 19[5/78] Val Loss:0.28089892864227295\n",
      "Epoch 19[6/78] Val Loss:0.24081379175186157\n",
      "Epoch 19[7/78] Val Loss:0.3581791818141937\n",
      "Epoch 19[8/78] Val Loss:0.1830264925956726\n",
      "Epoch 19[9/78] Val Loss:0.13101710379123688\n",
      "Epoch 19[10/78] Val Loss:0.099638432264328\n",
      "Epoch 19[11/78] Val Loss:0.13367454707622528\n",
      "Epoch 19[12/78] Val Loss:0.09940759837627411\n",
      "Epoch 19[13/78] Val Loss:0.08587341010570526\n",
      "Epoch 19[14/78] Val Loss:0.263683557510376\n",
      "Epoch 19[15/78] Val Loss:0.2145058810710907\n",
      "Epoch 19[16/78] Val Loss:0.1975947767496109\n",
      "Epoch 19[17/78] Val Loss:0.1869976669549942\n",
      "Epoch 19[18/78] Val Loss:0.25826412439346313\n",
      "Epoch 19[19/78] Val Loss:0.3695814907550812\n",
      "Epoch 19[20/78] Val Loss:0.24477505683898926\n",
      "Epoch 19[21/78] Val Loss:0.515897810459137\n",
      "Epoch 19[22/78] Val Loss:0.7424480319023132\n",
      "Epoch 19[23/78] Val Loss:0.42722922563552856\n",
      "Epoch 19[24/78] Val Loss:0.32894477248191833\n",
      "Epoch 19[25/78] Val Loss:0.414795845746994\n",
      "Epoch 19[26/78] Val Loss:0.3407036364078522\n",
      "Epoch 19[27/78] Val Loss:0.32895103096961975\n",
      "Epoch 19[28/78] Val Loss:0.2847820222377777\n",
      "Epoch 19[29/78] Val Loss:0.45163750648498535\n",
      "Epoch 19[30/78] Val Loss:1.681058406829834\n",
      "Epoch 19[31/78] Val Loss:1.5569658279418945\n",
      "Epoch 19[32/78] Val Loss:1.1723753213882446\n",
      "Epoch 19[33/78] Val Loss:0.4674605131149292\n",
      "Epoch 19[34/78] Val Loss:0.3656483590602875\n",
      "Epoch 19[35/78] Val Loss:0.32025080919265747\n",
      "Epoch 19[36/78] Val Loss:0.32062098383903503\n",
      "Epoch 19[37/78] Val Loss:0.4106970429420471\n",
      "Epoch 19[38/78] Val Loss:0.20618636906147003\n",
      "Epoch 19[39/78] Val Loss:0.19345732033252716\n",
      "Epoch 19[40/78] Val Loss:0.19468499720096588\n",
      "Epoch 19[41/78] Val Loss:0.2160709947347641\n",
      "Epoch 19[42/78] Val Loss:0.20477043092250824\n",
      "Epoch 19[43/78] Val Loss:0.11255909502506256\n",
      "Epoch 19[44/78] Val Loss:0.08955385535955429\n",
      "Epoch 19[45/78] Val Loss:0.08502672612667084\n",
      "Epoch 19[46/78] Val Loss:0.08466798812150955\n",
      "Epoch 19[47/78] Val Loss:0.08839891850948334\n",
      "Epoch 19[48/78] Val Loss:0.12201276421546936\n",
      "Epoch 19[49/78] Val Loss:0.0767342746257782\n",
      "Epoch 19[50/78] Val Loss:0.08351698517799377\n",
      "Epoch 19[51/78] Val Loss:0.06572599709033966\n",
      "Epoch 19[52/78] Val Loss:0.09837426245212555\n",
      "Epoch 19[53/78] Val Loss:0.0948697105050087\n",
      "Epoch 19[54/78] Val Loss:0.06875152140855789\n",
      "Epoch 19[55/78] Val Loss:0.12718431651592255\n",
      "Epoch 19[56/78] Val Loss:0.6425072550773621\n",
      "Epoch 19[57/78] Val Loss:0.5705081820487976\n",
      "Epoch 19[58/78] Val Loss:0.5464657545089722\n",
      "Epoch 19[59/78] Val Loss:0.3636947572231293\n",
      "Epoch 19[60/78] Val Loss:0.29195696115493774\n",
      "Epoch 19[61/78] Val Loss:0.3758528530597687\n",
      "Epoch 19[62/78] Val Loss:0.4059639275074005\n",
      "Epoch 19[63/78] Val Loss:0.23628444969654083\n",
      "Epoch 19[64/78] Val Loss:0.1732526272535324\n",
      "Epoch 19[65/78] Val Loss:0.16650088131427765\n",
      "Epoch 19[66/78] Val Loss:0.2133616954088211\n",
      "Epoch 19[67/78] Val Loss:0.19662632048130035\n",
      "Epoch 19[68/78] Val Loss:0.6281613707542419\n",
      "Epoch 19[69/78] Val Loss:0.6487613916397095\n",
      "Epoch 19[70/78] Val Loss:0.6491445302963257\n",
      "Epoch 19[71/78] Val Loss:0.5214720964431763\n",
      "Epoch 19[72/78] Val Loss:0.2514648139476776\n",
      "Epoch 19[73/78] Val Loss:0.22242045402526855\n",
      "Epoch 19[74/78] Val Loss:0.48539865016937256\n",
      "Epoch 19[75/78] Val Loss:0.6872205138206482\n",
      "Epoch 19[76/78] Val Loss:0.6479952931404114\n",
      "Epoch 19[77/78] Val Loss:0.6809766292572021\n",
      "Epoch 19[78/78] Val Loss:0.7257641553878784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.87      0.91     15691\n",
      "           1       0.64      0.87      0.74      4309\n",
      "\n",
      "    accuracy                           0.87     20000\n",
      "   macro avg       0.80      0.87      0.83     20000\n",
      "weighted avg       0.89      0.87      0.87     20000\n",
      "\n",
      "Epoch 19: Train Loss 0.26230342943668367, Val Loss 0.3489756784760035, Train Time 793.1448407173157, Val Time 38.09227442741394\n",
      "Saving best model at epoch 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (features): Sequential(\n",
       "    (init_block): ResInitBlock(\n",
       "      (conv): ConvBlock(\n",
       "        (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "      (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (stage1): Sequential(\n",
       "      (unit1): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (identity_conv): ConvBlock(\n",
       "          (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit2): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit3): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (stage2): Sequential(\n",
       "      (unit1): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (identity_conv): ConvBlock(\n",
       "          (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit2): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit3): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit4): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (stage3): Sequential(\n",
       "      (unit1): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (identity_conv): ConvBlock(\n",
       "          (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit2): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit3): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit4): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit5): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit6): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (stage4): Sequential(\n",
       "      (unit1): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (identity_conv): ConvBlock(\n",
       "          (conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit2): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit3): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (final_pool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  )\n",
       "  (output): Linear(in_features=2048, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathpretrain.train_model import train_model\n",
    "train_model(inputs_dir=\"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/cnn_model_input_1223/\",\n",
    "                learning_rate=1e-3,\n",
    "                n_epochs=20,\n",
    "                batch_size=256,\n",
    "                model_save_loc='/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/cnn_model_1223.pkl',\n",
    "                verbose=2,\n",
    "                class_balance=True,\n",
    "                predict=False,\n",
    "                pickle_dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e0dcb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "!sleep 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fe9b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathpretrain.predict import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "747fac65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': <pathpretrain.datasets.PickleDataset object at 0x2ac4284cd3a0>, 'val': <pathpretrain.datasets.PickleDataset object at 0x2ab9dccb10a0>, 'test': <pathpretrain.datasets.PickleDataset object at 0x2ab9dd0c4cd0>}\n",
      "ResNet(\n",
      "  (features): Sequential(\n",
      "    (init_block): ResInitBlock(\n",
      "      (conv): ConvBlock(\n",
      "        (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (stage1): Sequential(\n",
      "      (unit1): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (identity_conv): ConvBlock(\n",
      "          (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit2): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit3): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (stage2): Sequential(\n",
      "      (unit1): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (identity_conv): ConvBlock(\n",
      "          (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit2): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit3): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit4): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (stage3): Sequential(\n",
      "      (unit1): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (identity_conv): ConvBlock(\n",
      "          (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit2): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit3): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit4): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit5): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit6): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (stage4): Sequential(\n",
      "      (unit1): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (identity_conv): ConvBlock(\n",
      "          (conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit2): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit3): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (final_pool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  )\n",
      "  (output): Linear(in_features=2048, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:37,  8.25it/s]                                                                 \n"
     ]
    }
   ],
   "source": [
    "predict(inputs_dir='/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/cnn_model_input_1223/',\n",
    "                batch_size=64,\n",
    "                model_save_loc=\"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/cnn_model_1223.pkl\",\n",
    "                predict_set='test',\n",
    "                pickle_dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78fa265c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9509396065119922"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import roc_auc_score\n",
    "predictions=torch.load(\"/dartfs-hpc/rc/home/3/f006n33/predictions.pkl\")\n",
    "predictions['pred']=softmax(predictions['pred'])\n",
    "roc_auc_score(predictions['true'],predictions['pred'][:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17602649",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pred': array([[1.5933154e-05, 3.9055425e-07],\n",
      "       [2.5007705e-05, 2.4779158e-07],\n",
      "       [3.6103197e-06, 1.7127517e-06],\n",
      "       ...,\n",
      "       [9.8847986e-06, 6.2929161e-07],\n",
      "       [8.9098858e-06, 6.9818577e-07],\n",
      "       [8.8561387e-07, 6.9204730e-06]], dtype=float32), 'true': array([0, 0, 0, ..., 0, 0, 1])}\n"
     ]
    }
   ],
   "source": [
    "print (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d6dfe58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "# fpr and tpr of all thresohlds\n",
    "true = predictions['true']\n",
    "preds = predictions['pred'][:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(true, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59825228",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ipykernel_86851/2083274585.py:10: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('AUC-ROC')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bb90d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ipykernel_86851/2122436968.py:1: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "777f9f46",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pathpretrain 0.1\n",
      "Uninstalling pathpretrain-0.1:\n",
      "  Successfully uninstalled pathpretrain-0.1\n",
      "Collecting git+https://github.com/jlevy44/PathPretrain\n",
      "  Cloning https://github.com/jlevy44/PathPretrain to /scratch/pip-req-build-b9lj_yhd\n",
      "  Running command git clone --quiet https://github.com/jlevy44/PathPretrain /scratch/pip-req-build-b9lj_yhd\n",
      "  Resolved https://github.com/jlevy44/PathPretrain to commit 6b1172c055c45fb01d80ad02ed6d334247c24e1b\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tifffile==2021.11.2 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from pathpretrain==0.1) (2021.11.2)\n",
      "Requirement already satisfied: scikit-image==0.18.3 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from pathpretrain==0.1) (0.18.3)\n",
      "Requirement already satisfied: networkx>=2.0 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from scikit-image==0.18.3->pathpretrain==0.1) (2.8.5)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from scikit-image==0.18.3->pathpretrain==0.1) (3.5.2)\n",
      "Requirement already satisfied: scipy>=1.0.1 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from scikit-image==0.18.3->pathpretrain==0.1) (1.9.0)\n",
      "Requirement already satisfied: numpy>=1.16.5 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from scikit-image==0.18.3->pathpretrain==0.1) (1.23.1)\n",
      "Requirement already satisfied: imageio>=2.3.0 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from scikit-image==0.18.3->pathpretrain==0.1) (2.20.0)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from scikit-image==0.18.3->pathpretrain==0.1) (9.2.0)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from scikit-image==0.18.3->pathpretrain==0.1) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.18.3->pathpretrain==0.1) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.18.3->pathpretrain==0.1) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.18.3->pathpretrain==0.1) (4.34.4)\n",
      "Requirement already satisfied: cycler>=0.10 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.18.3->pathpretrain==0.1) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.18.3->pathpretrain==0.1) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.18.3->pathpretrain==0.1) (1.4.4)\n",
      "Requirement already satisfied: six>=1.5 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib!=3.0.0,>=2.0.0->scikit-image==0.18.3->pathpretrain==0.1) (1.16.0)\n",
      "Building wheels for collected packages: pathpretrain\n",
      "  Building wheel for pathpretrain (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathpretrain: filename=pathpretrain-0.1-py3-none-any.whl size=23572 sha256=d2ff3bbeb61701a35c06694e40ae34db795074ec3ccb5f5ae9c515d64885be18\n",
      "  Stored in directory: /scratch/pip-ephem-wheel-cache-xtr84xbr/wheels/95/0c/6f/7498ced330de24503d0b19e7997fa3290b64e695b46d04ed61\n",
      "Successfully built pathpretrain\n",
      "Installing collected packages: pathpretrain\n",
      "Successfully installed pathpretrain-0.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y pathpretrain\n",
    "!pip install git+https://github.com/jlevy44/PathPretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a59c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathpretrain.embed import generate_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97c314fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1291it [01:28, 14.63it/s]                                                          \n"
     ]
    }
   ],
   "source": [
    "generate_embeddings(patch_info_file=\"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/scc_tumor_data/prelim_patch_info_v2/109_A1c_ASAP_tumor_map.pkl\",\n",
    "                   image_file=\"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/scc_tumor_data/prelim_patch_info/109_A1c_ASAP_tumor_map.npy\",\n",
    "                   model_save_loc=\"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/cnn_model.pkl\",\n",
    "                   architecture='resnet50',\n",
    "                   num_classes=2,\n",
    "                   image_stack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e3b9a68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "embeddings=torch.load(\"/dartfs-hpc/rc/home/3/f006n33/cnn_embeddings/112_b_ASAP_tumor_map.pkl\")\n",
    "import umap\n",
    "z=umap.UMAP(random_state=42, n_components=2).fit_transform(embeddings['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3ac150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5debc85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2ad980a3e6a0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib, matplotlib.pyplot as plt\n",
    "plt.scatter(z[:,0],z[:,1],c=embeddings['patch_info']['scc'],s=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8df5ce02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2ad9817cc0a0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(embeddings['patch_info']['x'],embeddings['patch_info']['y'],c=z[:,1], s=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "103f122c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1291it [01:30, 14.30it/s]                                                       \n",
      "515it [00:31, 16.53it/s]                                                        \n",
      "532it [00:34, 15.37it/s]                                                        \n",
      "576it [00:37, 15.55it/s]                                                        \n",
      "100%|█████████████████████████████████████████| 558/558 [00:32<00:00, 17.02it/s]\n",
      "195it [00:11, 17.34it/s]                                                        \n",
      "214it [00:12, 17.29it/s]                                                        \n",
      "500it [00:31, 15.77it/s]                                                        \n",
      "654it [00:42, 15.36it/s]                                                        \n",
      "457it [00:26, 17.12it/s]                                                        \n",
      "462it [00:26, 17.17it/s]                                                        \n",
      "486it [00:27, 17.64it/s]                                                        \n",
      "668it [00:42, 15.62it/s]                                                        \n",
      "648it [00:39, 16.42it/s]                                                        \n",
      "598it [00:38, 15.60it/s]                                                        \n",
      "232it [00:17, 13.12it/s]                                                        \n",
      "416it [00:27, 15.17it/s]                                                        \n",
      "450it [00:29, 15.35it/s]                                                        \n",
      "421it [00:28, 14.78it/s]                                                        \n",
      "386it [00:22, 17.42it/s]                                                        \n",
      " 95%|██████████████████████████████████████▉  | 772/813 [00:44<00:02, 17.55it/s]\n",
      "100%|█████████████████████████████████████████| 510/510 [00:33<00:00, 15.24it/s]\n",
      "1204it [01:19, 15.09it/s]                                                       \n",
      "294it [00:18, 15.77it/s]                                                        \n",
      "212it [00:12, 16.55it/s]                                                        \n",
      "195it [00:13, 14.33it/s]                                                        \n",
      "214it [00:13, 16.00it/s]                                                        \n",
      "533it [00:33, 16.11it/s]                                                        \n",
      "699it [00:46, 15.05it/s]                                                        \n",
      "726it [00:42, 17.20it/s]                                                        \n",
      "100%|█████████████████████████████████████████| 745/745 [00:43<00:00, 17.29it/s]\n",
      "100%|█████████████████████████████████████████| 312/312 [00:19<00:00, 16.01it/s]\n",
      "324it [00:21, 14.97it/s]                                                        \n",
      "455it [00:29, 15.69it/s]                                                        \n",
      "463it [00:32, 14.42it/s]                                                        \n",
      "343it [00:21, 16.10it/s]                                                        \n",
      "353it [00:22, 15.64it/s]                                                        \n",
      "501it [00:31, 16.12it/s]                                                        \n",
      "607it [00:35, 17.15it/s]                                                        \n",
      "100%|█████████████████████████████████████████| 578/578 [00:36<00:00, 15.88it/s]\n",
      "527it [00:34, 15.08it/s]                                                        \n",
      "100%|█████████████████████████████████████████| 451/451 [00:29<00:00, 15.26it/s]\n",
      "322it [00:19, 16.49it/s]                                                        \n",
      "100%|█████████████████████████████████████████| 664/664 [00:41<00:00, 16.17it/s]\n",
      "647it [00:39, 16.25it/s]                                                        \n",
      "532it [00:32, 16.21it/s]                                                        \n",
      "463it [00:29, 15.84it/s]                                                        \n",
      "456it [00:26, 16.89it/s]                                                        \n",
      "100%|█████████████████████████████████████████| 558/558 [00:33<00:00, 16.84it/s]\n",
      "423it [00:24, 17.10it/s]                                                        \n",
      "491it [00:30, 16.04it/s]                                                        \n",
      "569it [00:38, 14.92it/s]                                                        \n",
      "414it [00:26, 15.55it/s]                                                        \n",
      "685it [00:43, 15.86it/s]                                                        \n",
      "803it [00:52, 15.25it/s]                                                        \n",
      "470it [00:26, 17.69it/s]                                                        \n",
      "264it [00:16, 15.79it/s]                                                        \n",
      "337it [00:19, 17.00it/s]                                                        \n",
      "500it [00:29, 16.87it/s]                                                        \n",
      "579it [00:35, 16.54it/s]                                                        \n",
      "524it [00:31, 16.87it/s]                                                        \n",
      "676it [00:47, 14.34it/s]                                                        \n",
      "613it [00:37, 16.30it/s]                                                        \n",
      "664it [00:39, 16.79it/s]                                                        \n",
      "588it [00:38, 15.31it/s]                                                        \n",
      "650it [00:42, 15.21it/s]                                                        \n",
      "644it [00:38, 16.88it/s]                                                        \n",
      "790it [00:51, 15.20it/s]                                                        \n",
      "734it [00:43, 16.71it/s]                                                        \n",
      "696it [00:43, 16.13it/s]                                                        \n",
      "707it [00:43, 16.23it/s]                                                        \n",
      "688it [00:45, 15.09it/s]                                                        \n",
      "611it [00:41, 14.84it/s]                                                        \n",
      "572it [00:35, 15.98it/s]                                                        \n",
      "433it [00:26, 16.41it/s]                                                        \n",
      "521it [00:32, 16.01it/s]                                                        \n",
      "532it [00:34, 15.63it/s]                                                        \n",
      "350it [00:21, 16.56it/s]                                                        \n",
      "907it [00:53, 16.85it/s]                                                        \n",
      "996it [01:07, 14.76it/s]                                                        \n",
      "1067it [01:09, 15.38it/s]                                                       \n",
      "455it [00:30, 15.09it/s]                                                        \n",
      "407it [00:24, 16.96it/s]                                                        \n",
      "447it [00:26, 16.56it/s]                                                        \n",
      "324it [00:18, 17.35it/s]                                                        \n",
      "278it [00:20, 13.61it/s]                                                        \n",
      "301it [00:19, 15.41it/s]                                                        \n",
      "100%|█████████████████████████████████████████| 578/578 [00:34<00:00, 16.65it/s]\n",
      "527it [00:30, 17.11it/s]                                                        \n",
      "100%|█████████████████████████████████████████| 451/451 [00:30<00:00, 14.84it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathpretrain.embed import generate_embeddings\n",
    "for ID in df:\n",
    "    generate_embeddings(patch_info_file=f\"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/scc_tumor_data/prelim_patch_info_v2/{ID}.pkl\",\n",
    "                        image_file=f\"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/scc_tumor_data/prelim_patch_info/{ID}.npy\",\n",
    "                        model_save_loc='cnn_model.pkl',\n",
    "                        architecture=\"resnet50\",\n",
    "                        num_classes=3,\n",
    "                        image_stack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a96e129",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hiss",
   "language": "python",
   "name": "hiss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
