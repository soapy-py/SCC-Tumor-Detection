{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "339da00e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 18:08:01.658173: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import fire\n",
    "from visdom import Visdom\n",
    "import pickle\n",
    "import sys, os\n",
    "import umap, numba\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import os,glob, pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "import copy\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GCNConv, GATConv, DeepGraphInfomax, SAGEConv\n",
    "from torch_geometric.nn import DenseGraphConv\n",
    "from torch_geometric.utils import to_dense_batch, to_dense_adj, dense_to_sparse\n",
    "from torch_geometric.nn import GINEConv\n",
    "from torch_geometric.utils import dropout_adj\n",
    "from torch_geometric.nn import APPNP\n",
    "from torch_cluster import knn_graph\n",
    "from torch_geometric.data import Data \n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "from torch_geometric.data import InMemoryDataset,DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32ee50a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-15\n",
    "\n",
    "class GCNNet(torch.nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim, hidden_topology=[32,64,128,128], p=0.5, p2=0.1, drop_each=True):\n",
    "        super(GCNNet, self).__init__()\n",
    "        self.out_dim=out_dim\n",
    "        self.convs = nn.ModuleList([GATConv(inp_dim, hidden_topology[0])]+[GATConv(hidden_topology[i],hidden_topology[i+1]) for i in range(len(hidden_topology[:-1]))])\n",
    "        self.drop_edge = lambda edge_index: dropout_adj(edge_index,p=p2)[0]\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.fc = nn.Linear(hidden_topology[-1], out_dim)\n",
    "        self.drop_each=drop_each\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        for conv in self.convs:\n",
    "            if self.drop_each and self.training: edge_index=self.drop_edge(edge_index)\n",
    "            x = F.relu(conv(x, edge_index, edge_attr))\n",
    "        if self.training:\n",
    "            x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "class GCNFeatures(torch.nn.Module):\n",
    "    def __init__(self, gcn, bayes=False):\n",
    "        super(GCNFeatures, self).__init__()\n",
    "        self.gcn=gcn\n",
    "        self.drop_each=bayes\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        for conv in self.gcn.convs:\n",
    "            if self.drop_each: edge_index=self.gcn.drop_edge(edge_index)\n",
    "            x = F.relu(conv(x, edge_index, edge_attr))\n",
    "        if self.drop_each:\n",
    "            x = self.gcn.dropout(x)\n",
    "        y = F.softmax(self.gcn.fc(x))\n",
    "        return x,y\n",
    "\n",
    "def fit_model(graph_data='',\n",
    "                use_weights=False,\n",
    "                use_model=None,\n",
    "                n_batches_backward=1,\n",
    "                f1_metric='weighted',\n",
    "                n_epochs=1500,\n",
    "                out_dir=\"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/gnn/\",\n",
    "                lr=1e-2,\n",
    "                eta_min=1e-4,\n",
    "                T_max=20,\n",
    "                wd=0,\n",
    "                hidden_topology=[32,64,128,128],\n",
    "                p=0.5,\n",
    "                p2=0.3,\n",
    "                burnin=400,\n",
    "                warmup=100,\n",
    "                gpu_id=0,\n",
    "                batch_size=1\n",
    "                ):\n",
    "    print(gpu_id); torch.cuda.set_device(gpu_id)\n",
    "    datasets=pickle.load(open(graph_data,'rb'))\n",
    "    train_dataset = [dataset for key in datasets for dataset in datasets[key]['train']]\n",
    "    val_dataset = [dataset for key in datasets for dataset in datasets[key]['val']]\n",
    "    print(len(train_dataset), 'training graphs,', len(val_dataset), 'validation graphs')\n",
    "    \n",
    "    print(\"training graph sizes:\", end='')\n",
    "    for x in train_dataset:\n",
    "        print(x.x.shape[0], end=\", \")\n",
    "    print(\"\\nValidation graph sizes:\", end=\" \")\n",
    "    for x in val_dataset:\n",
    "        print(x.x.shape[0], end=\", \")\n",
    "    \n",
    "    y_train=np.hstack(graph.y.numpy() for graph in train_dataset)\n",
    "    y_true=np.hstack(graph.y_true.numpy() for graph in train_dataset)\n",
    "    if use_weights: \n",
    "        weights=compute_class_weight('balanced',classes=np.unique(y_train),y=y_train)\n",
    "    else: \n",
    "        weights=None \n",
    "       \n",
    "    # load model\n",
    "    model=GCNNet(train_dataset[0].x.shape[1],len(np.unique(y_train)),hidden_topology=hidden_topology,p=p,p2=p2)\n",
    "    model=model.cuda()\n",
    "\n",
    "    # load optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr,weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=eta_min, last_epoch=-1)\n",
    "    criterion=nn.CrossEntropyLoss(weight=torch.tensor(weights).float() if use_weights else None)\n",
    "    criterion=criterion.cuda()\n",
    "\n",
    "    # initialize val saving\n",
    "    save_mod=False\n",
    "    past_performance=[0]\n",
    "\n",
    "    # dataloaders\n",
    "    dataloaders={}\n",
    "\n",
    "    dataloaders['train']=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "    dataloaders['val']=DataLoader(val_dataset,shuffle=True)\n",
    "    dataloaders['warmup']=DataLoader(train_dataset,shuffle=False)\n",
    "    train_loader=dataloaders['warmup']\n",
    "\n",
    "    n_total_batches=0\n",
    "    train_val_f1=[]\n",
    "    for epoch in range(n_epochs):\n",
    "        Y,Y_Pred=[],[]\n",
    "        for i,data in enumerate(train_loader):\n",
    "            n_total_batches+=1\n",
    "            model.train(True)\n",
    "            x=data.x.cuda()\n",
    "            edge_index=data.edge_index.cuda()\n",
    "            y=data.y.cuda()\n",
    "            y_out=model(x,edge_index)\n",
    "            loss = criterion(y_out, y) / n_batches_backward\n",
    "            loss.backward()\n",
    "            if n_total_batches%n_batches_backward==0 or (i==len(train_loader.dataset)-1):\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            Y_Pred.append(F.softmax(y_out).argmax(1).detach().cpu().numpy().flatten())\n",
    "            Y.append(y.detach().cpu().numpy().flatten())\n",
    "            del x, edge_index, loss, y_out\n",
    "            if epoch <=warmup:\n",
    "                break \n",
    "        if epoch == warmup:\n",
    "            train_loader=dataloaders['train']\n",
    "        if epoch>=burnin:\n",
    "            save_mod=True\n",
    "        train_f1=f1_score(np.hstack(Y),np.hstack(Y_Pred),average=f1_metric)\n",
    "        scheduler.step()\n",
    "        Y,Y_Pred=[],[]\n",
    "        for i,data in enumerate(dataloaders['val']):\n",
    "            model.train(False)\n",
    "            x=data.x.cuda()\n",
    "            edge_index=data.edge_index.cuda()\n",
    "            y=data.y.cuda()\n",
    "            y_out=model(x,edge_index)\n",
    "            loss = criterion(y_out, y) \n",
    "            y_prob=F.softmax(y_out).detach().cpu().numpy()\n",
    "            y_pred=y_prob.argmax(1).flatten()\n",
    "            y_true=y.detach().cpu().numpy().flatten()\n",
    "            Y_Pred.append(y_pred)\n",
    "            Y.append(y_true)\n",
    "            #if vis_every and epoch%vis_every==0 and not i:\n",
    "               # vis.scatter(data.pos.numpy(),opts=dict(markercolor=(y_pred*255).astype(int),webgl=False,markerborderwidth=0,markersize=5),win=\"pred\")\n",
    "               # vis.scatter(data.pos.numpy(),opts=dict(markercolor=y_true*255,webgl=False,markerborderwidth=0,markersize=5),win=\"true\")\n",
    "            del x, edge_index, loss, y_out\n",
    "        val_f1=f1_score(np.hstack(Y),np.hstack(Y_Pred),average=f1_metric)\n",
    "        if save_mod and val_f1>=max(past_performance):\n",
    "            best_model_dict=copy.deepcopy(model.state_dict())\n",
    "            past_performance.append(val_f1)\n",
    "        print(epoch,train_f1,val_f1,flush=True)\n",
    "        train_val_f1.append((train_f1,val_f1))\n",
    "\n",
    "    model.load_state_dict(best_model_dict)\n",
    "    torch.save(model.state_dict(),os.path.join(out_dir,f\"model.pth\"))\n",
    "    torch.save(train_val_f1,os.path.join(out_dir,f\"f1.log.pth\"))\n",
    "    return Y, Y_Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8659e37f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "234 training graphs, 90 validation graphs\n",
      "training graph sizes:"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mfit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/graph_dataset/graph_dataset.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                      \u001b[49m\u001b[43muse_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/gnn/gnn_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mgpu_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mburnin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36mfit_model\u001b[0;34m(graph_data, use_weights, use_model, n_batches_backward, f1_metric, n_epochs, out_dir, lr, eta_min, T_max, wd, hidden_topology, p, p2, burnin, warmup, gpu_id, batch_size)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining graph sizes:\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m train_dataset:\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m], end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mValidation graph sizes:\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m val_dataset:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "y, y_pred = fit_model(\"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/graph_dataset/graph_dataset.pkl\", \n",
    "                      use_weights=True, \n",
    "                      lr=1e-3, \n",
    "                      out_dir='/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/gnn/gnn_model', \n",
    "                      n_epochs=1000, \n",
    "                      gpu_id=0, \n",
    "                      burnin=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93cbc1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2375fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_pickle(\"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/graph_dataset/graph_dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3c1975a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>109_A1c_ASAP_tumor_map</th>\n",
       "      <th>10_A1a_ASAP_tumor_map</th>\n",
       "      <th>10_A1b_ASAP_tumor_map</th>\n",
       "      <th>10_A2b_ASAP_tumor_map</th>\n",
       "      <th>110_A2b_ASAP_tumor_map</th>\n",
       "      <th>112_a_ASAP_tumor_map</th>\n",
       "      <th>112_b_ASAP_tumor_map</th>\n",
       "      <th>123_A1a_ASAP_tumor_map</th>\n",
       "      <th>12_A1c_ASAP_tumor_map</th>\n",
       "      <th>14_A1b_ASAP_tumor_map</th>\n",
       "      <th>...</th>\n",
       "      <th>370_A1b_ASAP_tumor_map</th>\n",
       "      <th>370_A2a_ASAP_tumor_map</th>\n",
       "      <th>370_A2b_ASAP_tumor_map</th>\n",
       "      <th>37_A2d_ASAP_tumor_map</th>\n",
       "      <th>61_A1a_ASAP_tumor_map</th>\n",
       "      <th>61_B1a_ASAP_tumor_map</th>\n",
       "      <th>70_A2b_ASAP_tumor_map</th>\n",
       "      <th>7_A1c_ASAP_tumor_map</th>\n",
       "      <th>7_A1d_ASAP_tumor_map</th>\n",
       "      <th>7_A1e_ASAP_tumor_map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>[[(x, [20589, 2048]), (edge_index, [2, 102219]...</td>\n",
       "      <td>[[(x, [5563, 2048]), (edge_index, [2, 27309]),...</td>\n",
       "      <td>[[(x, [5719, 2048]), (edge_index, [2, 28113]),...</td>\n",
       "      <td>[[(x, [6098, 2048]), (edge_index, [2, 29956]),...</td>\n",
       "      <td>[[(x, [4220, 2048]), (edge_index, [2, 20574]),...</td>\n",
       "      <td>[[(x, [817, 2048]), (edge_index, [2, 3897]), (...</td>\n",
       "      <td>[[(x, [902, 2048]), (edge_index, [2, 4298]), (...</td>\n",
       "      <td>[[(x, [2626, 2048]), (edge_index, [2, 12492]),...</td>\n",
       "      <td>[[(x, [7126, 2048]), (edge_index, [2, 35066]),...</td>\n",
       "      <td>[[(x, [5176, 2048]), (edge_index, [2, 25114]),...</td>\n",
       "      <td>...</td>\n",
       "      <td>[[(x, [3522, 2048]), (edge_index, [2, 17230]),...</td>\n",
       "      <td>[[(x, [3051, 2048]), (edge_index, [2, 14891]),...</td>\n",
       "      <td>[[(x, [3236, 2048]), (edge_index, [2, 15876]),...</td>\n",
       "      <td>[[(x, [7697, 2048]), (edge_index, [2, 36761]),...</td>\n",
       "      <td>[[(x, [1708, 2048]), (edge_index, [2, 8198]), ...</td>\n",
       "      <td>[[(x, [2145, 2048]), (edge_index, [2, 10425]),...</td>\n",
       "      <td>[[(x, [1686, 2048]), (edge_index, [2, 8148]), ...</td>\n",
       "      <td>[[(x, [6353, 2048]), (edge_index, [2, 31203]),...</td>\n",
       "      <td>[[(x, [5494, 2048]), (edge_index, [2, 26986]),...</td>\n",
       "      <td>[[(x, [4710, 2048]), (edge_index, [2, 23116]),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>[[(x, [20706, 2048]), (edge_index, [2, 102704]...</td>\n",
       "      <td>[[(x, [5804, 2048]), (edge_index, [2, 28532]),...</td>\n",
       "      <td>[[(x, [5642, 2048]), (edge_index, [2, 27702]),...</td>\n",
       "      <td>[[(x, [6085, 2048]), (edge_index, [2, 29929]),...</td>\n",
       "      <td>[[(x, [4472, 2048]), (edge_index, [2, 21880]),...</td>\n",
       "      <td>[[(x, [935, 2048]), (edge_index, [2, 4489]), (...</td>\n",
       "      <td>[[(x, [850, 2048]), (edge_index, [2, 4038]), (...</td>\n",
       "      <td>[[(x, [3280, 2048]), (edge_index, [2, 15970]),...</td>\n",
       "      <td>[[(x, [6814, 2048]), (edge_index, [2, 33326]),...</td>\n",
       "      <td>[[(x, [4577, 2048]), (edge_index, [2, 22143]),...</td>\n",
       "      <td>...</td>\n",
       "      <td>[[(x, [3362, 2048]), (edge_index, [2, 16446]),...</td>\n",
       "      <td>[[(x, [3383, 2048]), (edge_index, [2, 16557]),...</td>\n",
       "      <td>[[(x, [3095, 2048]), (edge_index, [2, 15181]),...</td>\n",
       "      <td>[[(x, [7718, 2048]), (edge_index, [2, 36598]),...</td>\n",
       "      <td>[[(x, [2201, 2048]), (edge_index, [2, 10721]),...</td>\n",
       "      <td>[[(x, [2088, 2048]), (edge_index, [2, 10144]),...</td>\n",
       "      <td>[[(x, [1672, 2048]), (edge_index, [2, 8158]), ...</td>\n",
       "      <td>[[(x, [5921, 2048]), (edge_index, [2, 29101]),...</td>\n",
       "      <td>[[(x, [5516, 2048]), (edge_index, [2, 27058]),...</td>\n",
       "      <td>[[(x, [4639, 2048]), (edge_index, [2, 22765]),...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  109_A1c_ASAP_tumor_map  \\\n",
       "train  [[(x, [20589, 2048]), (edge_index, [2, 102219]...   \n",
       "val    [[(x, [20706, 2048]), (edge_index, [2, 102704]...   \n",
       "\n",
       "                                   10_A1a_ASAP_tumor_map  \\\n",
       "train  [[(x, [5563, 2048]), (edge_index, [2, 27309]),...   \n",
       "val    [[(x, [5804, 2048]), (edge_index, [2, 28532]),...   \n",
       "\n",
       "                                   10_A1b_ASAP_tumor_map  \\\n",
       "train  [[(x, [5719, 2048]), (edge_index, [2, 28113]),...   \n",
       "val    [[(x, [5642, 2048]), (edge_index, [2, 27702]),...   \n",
       "\n",
       "                                   10_A2b_ASAP_tumor_map  \\\n",
       "train  [[(x, [6098, 2048]), (edge_index, [2, 29956]),...   \n",
       "val    [[(x, [6085, 2048]), (edge_index, [2, 29929]),...   \n",
       "\n",
       "                                  110_A2b_ASAP_tumor_map  \\\n",
       "train  [[(x, [4220, 2048]), (edge_index, [2, 20574]),...   \n",
       "val    [[(x, [4472, 2048]), (edge_index, [2, 21880]),...   \n",
       "\n",
       "                                    112_a_ASAP_tumor_map  \\\n",
       "train  [[(x, [817, 2048]), (edge_index, [2, 3897]), (...   \n",
       "val    [[(x, [935, 2048]), (edge_index, [2, 4489]), (...   \n",
       "\n",
       "                                    112_b_ASAP_tumor_map  \\\n",
       "train  [[(x, [902, 2048]), (edge_index, [2, 4298]), (...   \n",
       "val    [[(x, [850, 2048]), (edge_index, [2, 4038]), (...   \n",
       "\n",
       "                                  123_A1a_ASAP_tumor_map  \\\n",
       "train  [[(x, [2626, 2048]), (edge_index, [2, 12492]),...   \n",
       "val    [[(x, [3280, 2048]), (edge_index, [2, 15970]),...   \n",
       "\n",
       "                                   12_A1c_ASAP_tumor_map  \\\n",
       "train  [[(x, [7126, 2048]), (edge_index, [2, 35066]),...   \n",
       "val    [[(x, [6814, 2048]), (edge_index, [2, 33326]),...   \n",
       "\n",
       "                                   14_A1b_ASAP_tumor_map  ...  \\\n",
       "train  [[(x, [5176, 2048]), (edge_index, [2, 25114]),...  ...   \n",
       "val    [[(x, [4577, 2048]), (edge_index, [2, 22143]),...  ...   \n",
       "\n",
       "                                  370_A1b_ASAP_tumor_map  \\\n",
       "train  [[(x, [3522, 2048]), (edge_index, [2, 17230]),...   \n",
       "val    [[(x, [3362, 2048]), (edge_index, [2, 16446]),...   \n",
       "\n",
       "                                  370_A2a_ASAP_tumor_map  \\\n",
       "train  [[(x, [3051, 2048]), (edge_index, [2, 14891]),...   \n",
       "val    [[(x, [3383, 2048]), (edge_index, [2, 16557]),...   \n",
       "\n",
       "                                  370_A2b_ASAP_tumor_map  \\\n",
       "train  [[(x, [3236, 2048]), (edge_index, [2, 15876]),...   \n",
       "val    [[(x, [3095, 2048]), (edge_index, [2, 15181]),...   \n",
       "\n",
       "                                   37_A2d_ASAP_tumor_map  \\\n",
       "train  [[(x, [7697, 2048]), (edge_index, [2, 36761]),...   \n",
       "val    [[(x, [7718, 2048]), (edge_index, [2, 36598]),...   \n",
       "\n",
       "                                   61_A1a_ASAP_tumor_map  \\\n",
       "train  [[(x, [1708, 2048]), (edge_index, [2, 8198]), ...   \n",
       "val    [[(x, [2201, 2048]), (edge_index, [2, 10721]),...   \n",
       "\n",
       "                                   61_B1a_ASAP_tumor_map  \\\n",
       "train  [[(x, [2145, 2048]), (edge_index, [2, 10425]),...   \n",
       "val    [[(x, [2088, 2048]), (edge_index, [2, 10144]),...   \n",
       "\n",
       "                                   70_A2b_ASAP_tumor_map  \\\n",
       "train  [[(x, [1686, 2048]), (edge_index, [2, 8148]), ...   \n",
       "val    [[(x, [1672, 2048]), (edge_index, [2, 8158]), ...   \n",
       "\n",
       "                                    7_A1c_ASAP_tumor_map  \\\n",
       "train  [[(x, [6353, 2048]), (edge_index, [2, 31203]),...   \n",
       "val    [[(x, [5921, 2048]), (edge_index, [2, 29101]),...   \n",
       "\n",
       "                                    7_A1d_ASAP_tumor_map  \\\n",
       "train  [[(x, [5494, 2048]), (edge_index, [2, 26986]),...   \n",
       "val    [[(x, [5516, 2048]), (edge_index, [2, 27058]),...   \n",
       "\n",
       "                                    7_A1e_ASAP_tumor_map  \n",
       "train  [[(x, [4710, 2048]), (edge_index, [2, 23116]),...  \n",
       "val    [[(x, [4639, 2048]), (edge_index, [2, 22765]),...  \n",
       "\n",
       "[2 rows x 95 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51606b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "data=torch.load(\"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/cnn_embeddings/109_A1c_ASAP_tumor_map.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hiss",
   "language": "python",
   "name": "hiss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
