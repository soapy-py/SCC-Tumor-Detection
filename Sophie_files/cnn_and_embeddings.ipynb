{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc486bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "326985d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:05<00:00, 18.80it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import glob, os, pickle\n",
    "dfs=[]\n",
    "path= \"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/scc_tumor_data/prelim_patch_info_v2/\"\n",
    "\n",
    "for file in tqdm.tqdm(sorted(glob.glob(path + '/*.pkl'))):\n",
    "    basename=os.path.basename(file).replace(\".pkl\",\"\")\n",
    "    df=pd.read_pickle(file)\n",
    "    df['patch_index']=np.arange(df.shape[0])\n",
    "    if 'scc' not in df.columns: df['scc']=0\n",
    "    df['scc']=(df['scc']==1).astype(int)  \n",
    "    df['ID']=basename\n",
    "    dfs.append(df)\n",
    "df=pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39cac951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>patch_size</th>\n",
       "      <th>annotation</th>\n",
       "      <th>y_true</th>\n",
       "      <th>inflamm</th>\n",
       "      <th>scc</th>\n",
       "      <th>patch_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>109_A1c_ASAP_tumor_map</td>\n",
       "      <td>1024</td>\n",
       "      <td>16640</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109_A1c_ASAP_tumor_map</td>\n",
       "      <td>1280</td>\n",
       "      <td>15872</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109_A1c_ASAP_tumor_map</td>\n",
       "      <td>1280</td>\n",
       "      <td>16128</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>109_A1c_ASAP_tumor_map</td>\n",
       "      <td>1280</td>\n",
       "      <td>16384</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>109_A1c_ASAP_tumor_map</td>\n",
       "      <td>1280</td>\n",
       "      <td>16640</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14427</th>\n",
       "      <td>7_A1e_ASAP_tumor_map</td>\n",
       "      <td>76288</td>\n",
       "      <td>14336</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14428</th>\n",
       "      <td>7_A1e_ASAP_tumor_map</td>\n",
       "      <td>76288</td>\n",
       "      <td>14592</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14429</th>\n",
       "      <td>7_A1e_ASAP_tumor_map</td>\n",
       "      <td>76288</td>\n",
       "      <td>14848</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14430</th>\n",
       "      <td>7_A1e_ASAP_tumor_map</td>\n",
       "      <td>76288</td>\n",
       "      <td>15104</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14431</th>\n",
       "      <td>7_A1e_ASAP_tumor_map</td>\n",
       "      <td>76288</td>\n",
       "      <td>15360</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1608083 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           ID      x      y  patch_size annotation  y_true  \\\n",
       "0      109_A1c_ASAP_tumor_map   1024  16640         256          0       0   \n",
       "1      109_A1c_ASAP_tumor_map   1280  15872         256          0       0   \n",
       "2      109_A1c_ASAP_tumor_map   1280  16128         256          0       0   \n",
       "3      109_A1c_ASAP_tumor_map   1280  16384         256          0       0   \n",
       "4      109_A1c_ASAP_tumor_map   1280  16640         256          0       0   \n",
       "...                       ...    ...    ...         ...        ...     ...   \n",
       "14427    7_A1e_ASAP_tumor_map  76288  14336         256          0       0   \n",
       "14428    7_A1e_ASAP_tumor_map  76288  14592         256          0       0   \n",
       "14429    7_A1e_ASAP_tumor_map  76288  14848         256          0       0   \n",
       "14430    7_A1e_ASAP_tumor_map  76288  15104         256          0       0   \n",
       "14431    7_A1e_ASAP_tumor_map  76288  15360         256          0       0   \n",
       "\n",
       "       inflamm  scc  patch_index  \n",
       "0            0    0            0  \n",
       "1            0    0            1  \n",
       "2            0    0            2  \n",
       "3            0    0            3  \n",
       "4            0    0            4  \n",
       "...        ...  ...          ...  \n",
       "14427        0    0        14427  \n",
       "14428        0    0        14428  \n",
       "14429        0    0        14429  \n",
       "14430        0    0        14430  \n",
       "14431        0    0        14431  \n",
       "\n",
       "[1608083 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see what the new df looks like \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9644ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30fe5679",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TUMOR_THRESHOLD=0.2\n",
    "IDs=(df.groupby([\"ID\"])['scc'].mean()>TUMOR_THRESHOLD).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e289a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "109_A1c_ASAP_tumor_map    0\n",
       "10_A1a_ASAP_tumor_map     0\n",
       "10_A1b_ASAP_tumor_map     0\n",
       "10_A2b_ASAP_tumor_map     0\n",
       "110_A2b_ASAP_tumor_map    0\n",
       "                         ..\n",
       "61_B1a_ASAP_tumor_map     0\n",
       "70_A2b_ASAP_tumor_map     0\n",
       "7_A1c_ASAP_tumor_map      1\n",
       "7_A1d_ASAP_tumor_map      1\n",
       "7_A1e_ASAP_tumor_map      1\n",
       "Name: scc, Length: 95, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#figure out what IDs is doing here \n",
    "IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e4fc47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ID,test_ID=train_test_split(IDs,stratify=IDs.values, random_state=42)\n",
    "train_ID,val_ID=train_test_split(train_ID,stratify=train_ID.values, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2306c92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 24 18\n"
     ]
    }
   ],
   "source": [
    "#figure out train/test split \n",
    "print(len(train_ID), len(test_ID), len(val_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "687f1f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_test_slide_ids=dict(train=train_ID.index,val=val_ID.index,test=test_ID.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b9df3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Index(['352_A1d_ASAP_tumor_map', '341_a_ASAP_tumor_map',\n",
       "        '7_A1e_ASAP_tumor_map', '281_A1d_ASAP_tumor_map',\n",
       "        '343_a_ASAP_tumor_map', '363_A2b_ASAP_tumor_map',\n",
       "        '70_A2b_ASAP_tumor_map', '354_A3a_ASAP_tumor_map',\n",
       "        '342_b_ASAP_tumor_map', '61_A1a_ASAP_tumor_map',\n",
       "        '370_A1b_ASAP_tumor_map', '361_a_ASAP_tumor_map',\n",
       "        '7_A1d_ASAP_tumor_map', '343_c_ASAP_tumor_map',\n",
       "        '365_A2b_ASAP_tumor_map', '7_A1c_ASAP_tumor_map',\n",
       "        '350_A1e_ASAP_tumor_map', '354_A3c_ASAP_tumor_map',\n",
       "        '368_A1d_ASAP_tumor_map', '365_A1b_ASAP_tumor_map',\n",
       "        '270_A1e_ASAP_tumor_map', '355_A1d_ASAP_tumor_map',\n",
       "        '364_A1b_ASAP_tumor_map', '350_A1d_ASAP_tumor_map',\n",
       "        '37_A2d_ASAP_tumor_map', '346_b_ASAP_tumor_map',\n",
       "        '123_A1a_ASAP_tumor_map', '281_A1f_ASAP_tumor_map',\n",
       "        '61_B1a_ASAP_tumor_map', '354_A3b_ASAP_tumor_map',\n",
       "        '362_A1c_ASAP_tumor_map', '364_A2b_ASAP_tumor_map',\n",
       "        '344_a_ASAP_tumor_map', '363_A3b_ASAP_tumor_map',\n",
       "        '169_A2b_ASAP_tumor_map', '369_A2b_ASAP_tumor_map',\n",
       "        '270_A1d_ASAP_tumor_map', '345_a_ASAP_tumor_map',\n",
       "        '12_A1c_ASAP_tumor_map', '14_A2b_ASAP_tumor_map',\n",
       "        '342_a_ASAP_tumor_map', '364_A4b_ASAP_tumor_map',\n",
       "        '112_a_ASAP_tumor_map', '368_A1c_ASAP_tumor_map',\n",
       "        '370_A2b_ASAP_tumor_map', '353_A2b_ASAP_tumor_map',\n",
       "        '352_A1e_ASAP_tumor_map', '354_A1b_ASAP_tumor_map',\n",
       "        '10_A1b_ASAP_tumor_map', '358_A1b_ASAP_tumor_map',\n",
       "        '343_d_ASAP_tumor_map', '358_A1a_ASAP_tumor_map',\n",
       "        '354_A1d_ASAP_tumor_map'],\n",
       "       dtype='object', name='ID'),\n",
       " 'val': Index(['344_b_ASAP_tumor_map', '109_A1c_ASAP_tumor_map',\n",
       "        '354_D1b_ASAP_tumor_map', '367_A2b_ASAP_tumor_map',\n",
       "        '112_b_ASAP_tumor_map', '363_A1b_ASAP_tumor_map',\n",
       "        '352_A1i_ASAP_tumor_map', '327_A1a_ASAP_tumor_map',\n",
       "        '352_A1g_ASAP_tumor_map', '361_b_ASAP_tumor_map',\n",
       "        '270_A2b_ASAP_tumor_map', '352_A1h_ASAP_tumor_map',\n",
       "        '110_A2b_ASAP_tumor_map', '370_A2a_ASAP_tumor_map',\n",
       "        '281_A2eX_ASAP_tumor_map', '10_A2b_ASAP_tumor_map',\n",
       "        '366_A1a_ASAP_tumor_map', '10_A1a_ASAP_tumor_map'],\n",
       "       dtype='object', name='ID'),\n",
       " 'test': Index(['346_a_ASAP_tumor_map', '362_A1b_ASAP_tumor_map',\n",
       "        '270_A2f_ASAP_tumor_map', '311_A2c_ASAP_tumor_map',\n",
       "        '368_A1b_ASAP_tumor_map', '366_A1b_ASAP_tumor_map',\n",
       "        '362_A1a_ASAP_tumor_map', '327_A1d_ASAP_tumor_map',\n",
       "        '343_b_ASAP_tumor_map', '356_A1b_ASAP_tumor_map',\n",
       "        '363_A1c_ASAP_tumor_map', '351_A2b_ASAP_tumor_map',\n",
       "        '354_A1c_ASAP_tumor_map', '350_A1c_ASAP_tumor_map',\n",
       "        '369_A1b_ASAP_tumor_map', '369_A1c_ASAP_tumor_map',\n",
       "        '366_A1c_ASAP_tumor_map', '350_A1a_ASAP_tumor_map',\n",
       "        '327_B1c_ASAP_tumor_map', '341_b_ASAP_tumor_map',\n",
       "        '345_b_ASAP_tumor_map', '14_A1b_ASAP_tumor_map',\n",
       "        '270_A1b_ASAP_tumor_map', '350_A1b_ASAP_tumor_map'],\n",
       "       dtype='object', name='ID')}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#figure out what this dict does\n",
    "train_val_test_slide_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e0a6c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "patch_dict=dict(train=df.loc[df['ID'].isin(train_val_test_slide_ids['train'])].sample(n=80000),\n",
    "               val=df.loc[df['ID'].isin(train_val_test_slide_ids['val'])].sample(n=10000),\n",
    "               test=df.loc[df['ID'].isin(train_val_test_slide_ids['test'])].sample(n=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3769d9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_test_slide_ids={k:patch_dict[k]['ID'].unique() for k in patch_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92c3ba80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': array(['368_A1d_ASAP_tumor_map', '350_A1e_ASAP_tumor_map',\n",
       "        '364_A1b_ASAP_tumor_map', '354_A1d_ASAP_tumor_map',\n",
       "        '12_A1c_ASAP_tumor_map', '354_A3b_ASAP_tumor_map',\n",
       "        '365_A2b_ASAP_tumor_map', '7_A1e_ASAP_tumor_map',\n",
       "        '341_a_ASAP_tumor_map', '10_A1b_ASAP_tumor_map',\n",
       "        '123_A1a_ASAP_tumor_map', '353_A2b_ASAP_tumor_map',\n",
       "        '7_A1d_ASAP_tumor_map', '363_A3b_ASAP_tumor_map',\n",
       "        '365_A1b_ASAP_tumor_map', '112_a_ASAP_tumor_map',\n",
       "        '270_A1d_ASAP_tumor_map', '358_A1b_ASAP_tumor_map',\n",
       "        '344_a_ASAP_tumor_map', '369_A2b_ASAP_tumor_map',\n",
       "        '346_b_ASAP_tumor_map', '368_A1c_ASAP_tumor_map',\n",
       "        '281_A1f_ASAP_tumor_map', '270_A1e_ASAP_tumor_map',\n",
       "        '343_c_ASAP_tumor_map', '352_A1d_ASAP_tumor_map',\n",
       "        '355_A1d_ASAP_tumor_map', '354_A1b_ASAP_tumor_map',\n",
       "        '342_a_ASAP_tumor_map', '61_B1a_ASAP_tumor_map',\n",
       "        '352_A1e_ASAP_tumor_map', '7_A1c_ASAP_tumor_map',\n",
       "        '354_A3c_ASAP_tumor_map', '350_A1d_ASAP_tumor_map',\n",
       "        '361_a_ASAP_tumor_map', '370_A1b_ASAP_tumor_map',\n",
       "        '37_A2d_ASAP_tumor_map', '364_A4b_ASAP_tumor_map',\n",
       "        '364_A2b_ASAP_tumor_map', '363_A2b_ASAP_tumor_map',\n",
       "        '345_a_ASAP_tumor_map', '362_A1c_ASAP_tumor_map',\n",
       "        '343_d_ASAP_tumor_map', '343_a_ASAP_tumor_map',\n",
       "        '14_A2b_ASAP_tumor_map', '61_A1a_ASAP_tumor_map',\n",
       "        '70_A2b_ASAP_tumor_map', '169_A2b_ASAP_tumor_map',\n",
       "        '370_A2b_ASAP_tumor_map', '342_b_ASAP_tumor_map',\n",
       "        '281_A1d_ASAP_tumor_map', '358_A1a_ASAP_tumor_map',\n",
       "        '354_A3a_ASAP_tumor_map'], dtype=object),\n",
       " 'val': array(['366_A1a_ASAP_tumor_map', '10_A2b_ASAP_tumor_map',\n",
       "        '109_A1c_ASAP_tumor_map', '281_A2eX_ASAP_tumor_map',\n",
       "        '352_A1i_ASAP_tumor_map', '352_A1g_ASAP_tumor_map',\n",
       "        '344_b_ASAP_tumor_map', '370_A2a_ASAP_tumor_map',\n",
       "        '270_A2b_ASAP_tumor_map', '361_b_ASAP_tumor_map',\n",
       "        '363_A1b_ASAP_tumor_map', '110_A2b_ASAP_tumor_map',\n",
       "        '367_A2b_ASAP_tumor_map', '10_A1a_ASAP_tumor_map',\n",
       "        '352_A1h_ASAP_tumor_map', '354_D1b_ASAP_tumor_map',\n",
       "        '327_A1a_ASAP_tumor_map', '112_b_ASAP_tumor_map'], dtype=object),\n",
       " 'test': array(['327_B1c_ASAP_tumor_map', '270_A2f_ASAP_tumor_map',\n",
       "        '369_A1b_ASAP_tumor_map', '350_A1a_ASAP_tumor_map',\n",
       "        '343_b_ASAP_tumor_map', '366_A1c_ASAP_tumor_map',\n",
       "        '311_A2c_ASAP_tumor_map', '346_a_ASAP_tumor_map',\n",
       "        '368_A1b_ASAP_tumor_map', '363_A1c_ASAP_tumor_map',\n",
       "        '350_A1c_ASAP_tumor_map', '354_A1c_ASAP_tumor_map',\n",
       "        '362_A1a_ASAP_tumor_map', '362_A1b_ASAP_tumor_map',\n",
       "        '356_A1b_ASAP_tumor_map', '366_A1b_ASAP_tumor_map',\n",
       "        '351_A2b_ASAP_tumor_map', '270_A1b_ASAP_tumor_map',\n",
       "        '341_b_ASAP_tumor_map', '345_b_ASAP_tumor_map',\n",
       "        '14_A1b_ASAP_tumor_map', '350_A1b_ASAP_tumor_map',\n",
       "        '327_A1d_ASAP_tumor_map', '369_A1c_ASAP_tumor_map'], dtype=object)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_test_slide_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9df0968",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Set']='train'\n",
    "for k in train_val_test_slide_ids:\n",
    "    df.loc[df[\"ID\"].isin(train_val_test_slide_ids[k]),\"Set\"]=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3740d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.to_pickle(\"Master_Dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d62370e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 12 14:28:47 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:18:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    42W / 300W |      0MiB / 32768MiB |      0%   E. Process |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  Off  | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    39W / 300W |      0MiB / 32768MiB |      0%   E. Process |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  Off  | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    55W / 300W |   1240MiB / 32768MiB |     11%   E. Process |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    2   N/A  N/A    213624      C   ...conda3/envs/LP/bin/python     1237MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75311dfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_val_test:   0%|                                     | 0/3 [00:00<?, ?it/s]\n",
      "case:   0%|                                              | 0/53 [00:00<?, ?it/s]\u001b[A\n",
      "case:   2%|▋                                     | 1/53 [00:02<01:54,  2.21s/it]\u001b[A\n",
      "case:   4%|█▍                                    | 2/53 [00:02<01:06,  1.30s/it]\u001b[A\n",
      "case:   6%|██▏                                   | 3/53 [00:04<01:21,  1.63s/it]\u001b[A\n",
      "case:   8%|██▊                                   | 4/53 [00:07<01:39,  2.03s/it]\u001b[A\n",
      "case:   9%|███▌                                  | 5/53 [00:09<01:34,  1.96s/it]\u001b[A\n",
      "case:  11%|████▎                                 | 6/53 [00:11<01:31,  1.95s/it]\u001b[A\n",
      "case:  13%|█████                                 | 7/53 [00:13<01:39,  2.16s/it]\u001b[A\n",
      "case:  15%|█████▋                                | 8/53 [00:16<01:43,  2.29s/it]\u001b[A\n",
      "case:  17%|██████▍                               | 9/53 [00:18<01:32,  2.09s/it]\u001b[A\n",
      "case:  19%|██████▉                              | 10/53 [00:19<01:26,  2.01s/it]\u001b[A\n",
      "case:  21%|███████▋                             | 11/53 [00:21<01:14,  1.76s/it]\u001b[A\n",
      "case:  23%|████████▍                            | 12/53 [00:21<00:59,  1.44s/it]\u001b[A\n",
      "case:  25%|█████████                            | 13/53 [00:22<00:48,  1.22s/it]\u001b[A\n",
      "case:  26%|█████████▊                           | 14/53 [00:24<00:58,  1.50s/it]\u001b[A\n",
      "case:  28%|██████████▍                          | 15/53 [00:27<01:12,  1.91s/it]\u001b[A\n",
      "case:  30%|███████████▏                         | 16/53 [00:30<01:22,  2.23s/it]\u001b[A\n",
      "case:  32%|███████████▊                         | 17/53 [00:31<01:09,  1.93s/it]\u001b[A\n",
      "case:  34%|████████████▌                        | 18/53 [00:33<01:05,  1.88s/it]\u001b[A\n",
      "case:  36%|█████████████▎                       | 19/53 [00:34<00:58,  1.72s/it]\u001b[A\n",
      "case:  38%|█████████████▉                       | 20/53 [00:37<01:02,  1.90s/it]\u001b[A\n",
      "case:  40%|██████████████▋                      | 21/53 [00:38<00:59,  1.85s/it]\u001b[A\n",
      "case:  42%|███████████████▎                     | 22/53 [00:41<01:03,  2.05s/it]\u001b[A\n",
      "case:  43%|████████████████                     | 23/53 [00:43<01:04,  2.16s/it]\u001b[A\n",
      "case:  45%|████████████████▊                    | 24/53 [00:46<01:02,  2.17s/it]\u001b[A\n",
      "case:  47%|█████████████████▍                   | 25/53 [00:47<00:56,  2.01s/it]\u001b[A\n",
      "case:  49%|██████████████████▏                  | 26/53 [00:50<00:57,  2.13s/it]\u001b[A\n",
      "case:  51%|██████████████████▊                  | 27/53 [00:51<00:51,  1.99s/it]\u001b[A\n",
      "case:  53%|███████████████████▌                 | 28/53 [00:54<00:54,  2.19s/it]\u001b[A\n",
      "case:  55%|████████████████████▏                | 29/53 [00:57<00:59,  2.47s/it]\u001b[A\n",
      "case:  57%|████████████████████▉                | 30/53 [00:58<00:47,  2.04s/it]\u001b[A\n",
      "case:  58%|█████████████████████▋               | 31/53 [01:00<00:44,  2.02s/it]\u001b[A\n",
      "case:  60%|██████████████████████▎              | 32/53 [01:02<00:43,  2.09s/it]\u001b[A\n",
      "case:  62%|███████████████████████              | 33/53 [01:04<00:41,  2.07s/it]\u001b[A\n",
      "case:  64%|███████████████████████▋             | 34/53 [01:07<00:40,  2.14s/it]\u001b[A\n",
      "case:  66%|████████████████████████▍            | 35/53 [01:10<00:43,  2.41s/it]\u001b[A\n",
      "case:  68%|█████████████████████████▏           | 36/53 [01:13<00:43,  2.54s/it]\u001b[A\n",
      "case:  70%|█████████████████████████▊           | 37/53 [01:15<00:41,  2.60s/it]\u001b[A\n",
      "case:  72%|██████████████████████████▌          | 38/53 [01:18<00:39,  2.64s/it]\u001b[A\n",
      "case:  74%|███████████████████████████▏         | 39/53 [01:21<00:37,  2.70s/it]\u001b[A\n",
      "case:  75%|███████████████████████████▉         | 40/53 [01:23<00:33,  2.60s/it]\u001b[A\n",
      "case:  77%|████████████████████████████▌        | 41/53 [01:26<00:30,  2.53s/it]\u001b[A\n",
      "case:  79%|█████████████████████████████▎       | 42/53 [01:30<00:34,  3.16s/it]\u001b[A\n",
      "case:  81%|██████████████████████████████       | 43/53 [01:35<00:37,  3.71s/it]\u001b[A\n",
      "case:  83%|██████████████████████████████▋      | 44/53 [01:37<00:28,  3.15s/it]\u001b[A\n",
      "case:  85%|███████████████████████████████▍     | 45/53 [01:38<00:20,  2.61s/it]\u001b[A\n",
      "case:  87%|████████████████████████████████     | 46/53 [01:40<00:15,  2.21s/it]\u001b[A\n",
      "case:  89%|████████████████████████████████▊    | 47/53 [01:42<00:12,  2.15s/it]\u001b[A\n",
      "case:  91%|█████████████████████████████████▌   | 48/53 [01:43<00:09,  1.91s/it]\u001b[A\n",
      "case:  92%|██████████████████████████████████▏  | 49/53 [01:44<00:06,  1.71s/it]\u001b[A\n",
      "case:  94%|██████████████████████████████████▉  | 50/53 [01:46<00:04,  1.62s/it]\u001b[A\n",
      "case:  96%|███████████████████████████████████▌ | 51/53 [01:48<00:03,  1.94s/it]\u001b[A\n",
      "case:  98%|████████████████████████████████████▎| 52/53 [01:51<00:02,  2.07s/it]\u001b[A\n",
      "case: 100%|█████████████████████████████████████| 53/53 [01:53<00:00,  2.13s/it]\u001b[A\n",
      "train_val_test:  33%|█████████▎                  | 1/3 [03:45<07:31, 225.71s/it]\n",
      "case:   0%|                                              | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "case:   6%|██                                    | 1/18 [00:05<01:41,  5.98s/it]\u001b[A\n",
      "case:  11%|████▏                                 | 2/18 [00:08<00:58,  3.66s/it]\u001b[A\n",
      "case:  17%|██████▎                               | 3/18 [00:10<00:45,  3.06s/it]\u001b[A\n",
      "case:  22%|████████▍                             | 4/18 [00:12<00:38,  2.76s/it]\u001b[A\n",
      "case:  28%|██████████▌                           | 5/18 [00:13<00:26,  2.06s/it]\u001b[A\n",
      "case:  33%|████████████▋                         | 6/18 [00:16<00:26,  2.23s/it]\u001b[A\n",
      "case:  39%|██████████████▊                       | 7/18 [00:17<00:22,  2.06s/it]\u001b[A\n",
      "case:  44%|████████████████▉                     | 8/18 [00:20<00:24,  2.43s/it]\u001b[A\n",
      "case:  50%|███████████████████                   | 9/18 [00:22<00:18,  2.07s/it]\u001b[A\n",
      "case:  56%|████████████████████▌                | 10/18 [00:24<00:16,  2.09s/it]\u001b[A\n",
      "case:  61%|██████████████████████▌              | 11/18 [00:26<00:14,  2.02s/it]\u001b[A\n",
      "case:  67%|████████████████████████▋            | 12/18 [00:28<00:11,  1.96s/it]\u001b[A\n",
      "case:  72%|██████████████████████████▋          | 13/18 [00:29<00:09,  1.93s/it]\u001b[A\n",
      "case:  78%|████████████████████████████▊        | 14/18 [00:32<00:08,  2.16s/it]\u001b[A\n",
      "case:  83%|██████████████████████████████▊      | 15/18 [00:35<00:06,  2.28s/it]\u001b[A\n",
      "case:  89%|████████████████████████████████▉    | 16/18 [00:36<00:04,  2.11s/it]\u001b[A\n",
      "case:  94%|██████████████████████████████████▉  | 17/18 [00:38<00:01,  1.86s/it]\u001b[A\n",
      "case: 100%|█████████████████████████████████████| 18/18 [00:39<00:00,  2.18s/it]\u001b[A\n",
      "train_val_test:  67%|██████████████████▋         | 2/3 [04:41<02:05, 125.94s/it]\n",
      "case:   0%|                                              | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "case:   4%|█▌                                    | 1/24 [00:01<00:40,  1.77s/it]\u001b[A\n",
      "case:   8%|███▏                                  | 2/24 [00:04<00:49,  2.27s/it]\u001b[A\n",
      "case:  12%|████▊                                 | 3/24 [00:05<00:34,  1.62s/it]\u001b[A\n",
      "case:  17%|██████▎                               | 4/24 [00:06<00:31,  1.57s/it]\u001b[A\n",
      "case:  21%|███████▉                              | 5/24 [00:08<00:32,  1.71s/it]\u001b[A\n",
      "case:  25%|█████████▌                            | 6/24 [00:13<00:48,  2.71s/it]\u001b[A\n",
      "case:  29%|███████████                           | 7/24 [00:14<00:35,  2.08s/it]\u001b[A\n",
      "case:  33%|████████████▋                         | 8/24 [00:18<00:43,  2.71s/it]\u001b[A\n",
      "case:  38%|██████████████▎                       | 9/24 [00:20<00:36,  2.44s/it]\u001b[A\n",
      "case:  42%|███████████████▍                     | 10/24 [00:21<00:29,  2.08s/it]\u001b[A\n",
      "case:  46%|████████████████▉                    | 11/24 [00:23<00:26,  2.04s/it]\u001b[A\n",
      "case:  50%|██████████████████▌                  | 12/24 [00:25<00:25,  2.14s/it]\u001b[A\n",
      "case:  54%|████████████████████                 | 13/24 [00:27<00:23,  2.16s/it]\u001b[A\n",
      "case:  58%|█████████████████████▌               | 14/24 [00:29<00:18,  1.87s/it]\u001b[A\n",
      "case:  62%|███████████████████████▏             | 15/24 [00:30<00:16,  1.87s/it]\u001b[A\n",
      "case:  67%|████████████████████████▋            | 16/24 [00:32<00:13,  1.68s/it]\u001b[A\n",
      "case:  71%|██████████████████████████▏          | 17/24 [00:34<00:13,  1.91s/it]\u001b[A\n",
      "case:  75%|███████████████████████████▊         | 18/24 [00:37<00:12,  2.12s/it]\u001b[A\n",
      "case:  79%|█████████████████████████████▎       | 19/24 [00:39<00:11,  2.22s/it]\u001b[A\n",
      "case:  83%|██████████████████████████████▊      | 20/24 [00:41<00:08,  2.16s/it]\u001b[A\n",
      "case:  88%|████████████████████████████████▍    | 21/24 [00:43<00:06,  2.13s/it]\u001b[A\n",
      "case:  92%|█████████████████████████████████▉   | 22/24 [00:47<00:05,  2.53s/it]\u001b[A\n",
      "case:  96%|███████████████████████████████████▍ | 23/24 [00:48<00:02,  2.29s/it]\u001b[A\n",
      "case: 100%|█████████████████████████████████████| 24/24 [00:50<00:00,  2.10s/it]\u001b[A\n",
      "train_val_test: 100%|████████████████████████████| 3/3 [05:43<00:00, 114.64s/it]\n"
     ]
    }
   ],
   "source": [
    "for k in tqdm.tqdm(list(patch_dict.keys()),desc=\"train_val_test\"):\n",
    "    if not os.path.exists(f'cnn_model_input/{k}_data.pkl'):\n",
    "        X,y=[],[]\n",
    "        for name, dff in tqdm.tqdm(patch_dict[k].groupby('ID'),total=patch_dict[k]['ID'].nunique(),desc=\"case\"):\n",
    "            arr=np.load(f\"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/scc_tumor_data/prelim_patch_info/{name}.npy\")\n",
    "            X.append(arr[dff['patch_index'].values])\n",
    "            y.append(dff['scc'].values.flatten().astype(int))\n",
    "            del arr\n",
    "        X=np.concatenate(X,0)\n",
    "        y=np.hstack(y)\n",
    "        with open(f'/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/scc_tumor_data/prelim_patch_info/{k}_data.pkl','wb') as f:\n",
    "            pickle.dump(dict(X=X,y=y,patch_info=patch_dict[k]),f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec41b27c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-12 13:21:52.333085: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "lrr=ReduceLROnPlateau (monitor='val_acc', factor=.01, patience=3, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bea194c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': <pathpretrain.datasets.PickleDataset object at 0x2aee5be6ce20>, 'val': <pathpretrain.datasets.PickleDataset object at 0x2aee5be8eca0>}\n",
      "ResNet(\n",
      "  (features): Sequential(\n",
      "    (init_block): ResInitBlock(\n",
      "      (conv): ConvBlock(\n",
      "        (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (stage1): Sequential(\n",
      "      (unit1): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (identity_conv): ConvBlock(\n",
      "          (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit2): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit3): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (stage2): Sequential(\n",
      "      (unit1): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (identity_conv): ConvBlock(\n",
      "          (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit2): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit3): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit4): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (stage3): Sequential(\n",
      "      (unit1): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (identity_conv): ConvBlock(\n",
      "          (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit2): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit3): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit4): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit5): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit6): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (stage4): Sequential(\n",
      "      (unit1): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (identity_conv): ConvBlock(\n",
      "          (conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit2): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit3): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (final_pool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  )\n",
      "  (output): Linear(in_features=2048, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dartfs-hpc/rc/home/3/f006n33/anaconda3/envs/hiss/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1], y=[0 0 1 ... 1 1 1] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [0.61175175 2.73710141]\n",
      "Epoch 0[0/312] Time:0.708, Train Loss:0.7548606991767883\n",
      "Epoch 0[1/312] Time:8.604, Train Loss:3.517637252807617\n",
      "Epoch 0[2/312] Time:8.679, Train Loss:0.9312488436698914\n",
      "Epoch 0[3/312] Time:0.709, Train Loss:1.2491579055786133\n",
      "Epoch 0[4/312] Time:8.566, Train Loss:0.5711482763290405\n",
      "Epoch 0[5/312] Time:8.681, Train Loss:0.6116561889648438\n",
      "Epoch 0[6/312] Time:0.705, Train Loss:0.7049227356910706\n",
      "Epoch 0[7/312] Time:8.541, Train Loss:0.6395843625068665\n",
      "Epoch 0[8/312] Time:8.64, Train Loss:0.5654151439666748\n",
      "Epoch 0[9/312] Time:0.71, Train Loss:0.6616304516792297\n",
      "Epoch 0[10/312] Time:8.465, Train Loss:0.659267783164978\n",
      "Epoch 0[11/312] Time:8.647, Train Loss:0.7416934370994568\n",
      "Epoch 0[12/312] Time:0.706, Train Loss:0.6575286388397217\n",
      "Epoch 0[13/312] Time:8.446, Train Loss:0.599494457244873\n",
      "Epoch 0[14/312] Time:8.537, Train Loss:0.6214976906776428\n",
      "Epoch 0[15/312] Time:0.709, Train Loss:0.6190921068191528\n",
      "Epoch 0[16/312] Time:8.375, Train Loss:0.5331701636314392\n",
      "Epoch 0[17/312] Time:8.606, Train Loss:0.540090799331665\n",
      "Epoch 0[18/312] Time:0.702, Train Loss:0.5540368556976318\n",
      "Epoch 0[19/312] Time:8.483, Train Loss:0.5461347103118896\n",
      "Epoch 0[20/312] Time:8.666, Train Loss:0.45998015999794006\n",
      "Epoch 0[21/312] Time:0.711, Train Loss:0.5102076530456543\n",
      "Epoch 0[22/312] Time:8.378, Train Loss:0.663171648979187\n",
      "Epoch 0[23/312] Time:8.608, Train Loss:0.6905218958854675\n",
      "Epoch 0[24/312] Time:0.708, Train Loss:0.5297186374664307\n",
      "Epoch 0[25/312] Time:8.502, Train Loss:0.5599246621131897\n",
      "Epoch 0[26/312] Time:8.56, Train Loss:0.5990715622901917\n",
      "Epoch 0[27/312] Time:0.708, Train Loss:0.6640897989273071\n",
      "Epoch 0[28/312] Time:8.461, Train Loss:0.48471251130104065\n",
      "Epoch 0[29/312] Time:8.667, Train Loss:0.5226156115531921\n",
      "Epoch 0[30/312] Time:0.707, Train Loss:0.5607171654701233\n",
      "Epoch 0[31/312] Time:8.515, Train Loss:0.6888023018836975\n",
      "Epoch 0[32/312] Time:8.676, Train Loss:0.6026046872138977\n",
      "Epoch 0[33/312] Time:0.709, Train Loss:0.47016042470932007\n",
      "Epoch 0[34/312] Time:8.165, Train Loss:0.5502460598945618\n",
      "Epoch 0[35/312] Time:8.579, Train Loss:0.47687196731567383\n",
      "Epoch 0[36/312] Time:0.712, Train Loss:0.5958154797554016\n",
      "Epoch 0[37/312] Time:8.502, Train Loss:0.5626758933067322\n",
      "Epoch 0[38/312] Time:8.604, Train Loss:0.5142263174057007\n",
      "Epoch 0[39/312] Time:0.71, Train Loss:0.4316505193710327\n",
      "Epoch 0[40/312] Time:8.528, Train Loss:0.4942312240600586\n",
      "Epoch 0[41/312] Time:8.69, Train Loss:0.46295201778411865\n",
      "Epoch 0[42/312] Time:0.708, Train Loss:0.4586871266365051\n",
      "Epoch 0[43/312] Time:8.443, Train Loss:0.49397242069244385\n",
      "Epoch 0[44/312] Time:8.638, Train Loss:0.504000723361969\n",
      "Epoch 0[45/312] Time:0.707, Train Loss:0.44575977325439453\n",
      "Epoch 0[46/312] Time:8.442, Train Loss:0.4294939339160919\n",
      "Epoch 0[47/312] Time:8.589, Train Loss:0.6169885396957397\n",
      "Epoch 0[48/312] Time:0.708, Train Loss:0.4251406192779541\n",
      "Epoch 0[49/312] Time:8.292, Train Loss:0.4088332951068878\n",
      "Epoch 0[50/312] Time:8.735, Train Loss:0.4634363055229187\n",
      "Epoch 0[51/312] Time:0.708, Train Loss:0.4638032019138336\n",
      "Epoch 0[52/312] Time:8.522, Train Loss:0.4908459186553955\n",
      "Epoch 0[53/312] Time:8.677, Train Loss:0.6378647685050964\n",
      "Epoch 0[54/312] Time:0.706, Train Loss:0.5776494741439819\n",
      "Epoch 0[55/312] Time:8.486, Train Loss:0.4942677617073059\n",
      "Epoch 0[56/312] Time:8.641, Train Loss:0.42523014545440674\n",
      "Epoch 0[57/312] Time:0.707, Train Loss:0.45509985089302063\n",
      "Epoch 0[58/312] Time:8.493, Train Loss:0.47002771496772766\n",
      "Epoch 0[59/312] Time:8.602, Train Loss:0.44302839040756226\n",
      "Epoch 0[60/312] Time:0.709, Train Loss:0.5443280935287476\n",
      "Epoch 0[61/312] Time:8.508, Train Loss:0.48281365633010864\n",
      "Epoch 0[62/312] Time:8.64, Train Loss:0.46923089027404785\n",
      "Epoch 0[63/312] Time:0.721, Train Loss:0.43784862756729126\n",
      "Epoch 0[64/312] Time:8.363, Train Loss:0.38225752115249634\n",
      "Epoch 0[65/312] Time:8.645, Train Loss:0.49438726902008057\n",
      "Epoch 0[66/312] Time:0.705, Train Loss:0.4424828886985779\n",
      "Epoch 0[67/312] Time:8.507, Train Loss:0.5037312507629395\n",
      "Epoch 0[68/312] Time:8.592, Train Loss:0.560015082359314\n",
      "Epoch 0[69/312] Time:0.709, Train Loss:0.43719106912612915\n",
      "Epoch 0[70/312] Time:8.586, Train Loss:0.44482624530792236\n",
      "Epoch 0[71/312] Time:8.653, Train Loss:0.47726669907569885\n",
      "Epoch 0[72/312] Time:0.725, Train Loss:0.4764503240585327\n",
      "Epoch 0[73/312] Time:8.517, Train Loss:0.4645078778266907\n",
      "Epoch 0[74/312] Time:8.596, Train Loss:0.5106003880500793\n",
      "Epoch 0[75/312] Time:0.707, Train Loss:0.5055052042007446\n",
      "Epoch 0[76/312] Time:8.507, Train Loss:0.46246328949928284\n",
      "Epoch 0[77/312] Time:8.642, Train Loss:0.47267499566078186\n",
      "Epoch 0[78/312] Time:0.707, Train Loss:0.4056116044521332\n",
      "Epoch 0[79/312] Time:8.312, Train Loss:0.432851105928421\n",
      "Epoch 0[80/312] Time:8.629, Train Loss:0.398343563079834\n",
      "Epoch 0[81/312] Time:0.706, Train Loss:0.454792857170105\n",
      "Epoch 0[82/312] Time:8.512, Train Loss:0.40477314591407776\n",
      "Epoch 0[83/312] Time:8.672, Train Loss:0.43744921684265137\n",
      "Epoch 0[84/312] Time:0.709, Train Loss:0.47676628828048706\n",
      "Epoch 0[85/312] Time:8.473, Train Loss:0.4814276099205017\n",
      "Epoch 0[86/312] Time:8.701, Train Loss:0.5069051384925842\n",
      "Epoch 0[87/312] Time:0.708, Train Loss:0.4014114737510681\n",
      "Epoch 0[88/312] Time:8.569, Train Loss:0.41681593656539917\n",
      "Epoch 0[89/312] Time:8.673, Train Loss:0.43500375747680664\n",
      "Epoch 0[90/312] Time:0.71, Train Loss:0.41586101055145264\n",
      "Epoch 0[91/312] Time:8.533, Train Loss:0.4348563253879547\n",
      "Epoch 0[92/312] Time:8.71, Train Loss:0.45798131823539734\n",
      "Epoch 0[93/312] Time:0.707, Train Loss:0.5058961510658264\n",
      "Epoch 0[94/312] Time:8.345, Train Loss:0.46801304817199707\n",
      "Epoch 0[95/312] Time:8.686, Train Loss:0.7412421703338623\n",
      "Epoch 0[96/312] Time:0.707, Train Loss:0.42148271203041077\n",
      "Epoch 0[97/312] Time:8.567, Train Loss:0.45432043075561523\n",
      "Epoch 0[98/312] Time:8.634, Train Loss:0.4450788199901581\n",
      "Epoch 0[99/312] Time:0.708, Train Loss:0.4079035818576813\n",
      "Epoch 0[100/312] Time:8.622, Train Loss:0.4458409249782562\n",
      "Epoch 0[101/312] Time:8.692, Train Loss:0.4025494158267975\n",
      "Epoch 0[102/312] Time:0.706, Train Loss:0.38730406761169434\n",
      "Epoch 0[103/312] Time:8.592, Train Loss:0.4108422100543976\n",
      "Epoch 0[104/312] Time:8.667, Train Loss:0.5110239386558533\n",
      "Epoch 0[105/312] Time:0.707, Train Loss:0.5565452575683594\n",
      "Epoch 0[106/312] Time:8.502, Train Loss:0.349820613861084\n",
      "Epoch 0[107/312] Time:8.652, Train Loss:0.4970761239528656\n",
      "Epoch 0[108/312] Time:0.732, Train Loss:0.4374675750732422\n",
      "Epoch 0[109/312] Time:8.277, Train Loss:0.3768654763698578\n",
      "Epoch 0[110/312] Time:8.514, Train Loss:0.3623577654361725\n",
      "Epoch 0[111/312] Time:0.709, Train Loss:0.44411736726760864\n",
      "Epoch 0[112/312] Time:8.449, Train Loss:0.4463557302951813\n",
      "Epoch 0[113/312] Time:8.681, Train Loss:0.5230923295021057\n",
      "Epoch 0[114/312] Time:0.706, Train Loss:0.39417925477027893\n",
      "Epoch 0[115/312] Time:8.579, Train Loss:0.43984711170196533\n",
      "Epoch 0[116/312] Time:8.611, Train Loss:0.3894481658935547\n",
      "Epoch 0[117/312] Time:0.707, Train Loss:0.3872085213661194\n",
      "Epoch 0[118/312] Time:8.593, Train Loss:0.42735642194747925\n",
      "Epoch 0[119/312] Time:8.637, Train Loss:0.4540090560913086\n",
      "Epoch 0[120/312] Time:0.709, Train Loss:0.44649311900138855\n",
      "Epoch 0[121/312] Time:8.513, Train Loss:0.40442192554473877\n",
      "Epoch 0[122/312] Time:8.443, Train Loss:0.46528205275535583\n",
      "Epoch 0[123/312] Time:0.709, Train Loss:0.45858073234558105\n",
      "Epoch 0[124/312] Time:8.437, Train Loss:0.432980477809906\n",
      "Epoch 0[125/312] Time:8.635, Train Loss:0.48304659128189087\n",
      "Epoch 0[126/312] Time:0.707, Train Loss:0.39832603931427\n",
      "Epoch 0[127/312] Time:8.547, Train Loss:0.38349685072898865\n",
      "Epoch 0[128/312] Time:8.685, Train Loss:0.3524879515171051\n",
      "Epoch 0[129/312] Time:0.705, Train Loss:0.4385366141796112\n",
      "Epoch 0[130/312] Time:8.562, Train Loss:0.4012947976589203\n",
      "Epoch 0[131/312] Time:8.646, Train Loss:0.36027806997299194\n",
      "Epoch 0[132/312] Time:0.708, Train Loss:0.3885060250759125\n",
      "Epoch 0[133/312] Time:8.492, Train Loss:0.5858860015869141\n",
      "Epoch 0[134/312] Time:8.627, Train Loss:0.45193636417388916\n",
      "Epoch 0[135/312] Time:0.708, Train Loss:0.42121636867523193\n",
      "Epoch 0[136/312] Time:8.321, Train Loss:0.40808454155921936\n",
      "Epoch 0[137/312] Time:8.559, Train Loss:0.4466698169708252\n",
      "Epoch 0[138/312] Time:0.708, Train Loss:0.36180776357650757\n",
      "Epoch 0[139/312] Time:8.48, Train Loss:0.3088945746421814\n",
      "Epoch 0[140/312] Time:8.647, Train Loss:0.3375307619571686\n",
      "Epoch 0[141/312] Time:0.723, Train Loss:0.407286137342453\n",
      "Epoch 0[142/312] Time:8.604, Train Loss:0.4649350047111511\n",
      "Epoch 0[143/312] Time:8.666, Train Loss:0.5406147241592407\n",
      "Epoch 0[144/312] Time:0.707, Train Loss:0.4542483985424042\n",
      "Epoch 0[145/312] Time:8.54, Train Loss:0.43411654233932495\n",
      "Epoch 0[146/312] Time:8.535, Train Loss:0.5046111941337585\n",
      "Epoch 0[147/312] Time:0.706, Train Loss:0.37363502383232117\n",
      "Epoch 0[148/312] Time:8.446, Train Loss:0.4323902130126953\n",
      "Epoch 0[149/312] Time:8.513, Train Loss:0.47308075428009033\n",
      "Epoch 0[150/312] Time:0.703, Train Loss:0.3766802251338959\n",
      "Epoch 0[151/312] Time:8.426, Train Loss:0.36275503039360046\n",
      "Epoch 0[152/312] Time:8.601, Train Loss:0.5476256608963013\n",
      "Epoch 0[153/312] Time:0.708, Train Loss:0.41463837027549744\n",
      "Epoch 0[154/312] Time:8.442, Train Loss:0.5416370630264282\n",
      "Epoch 0[155/312] Time:8.668, Train Loss:0.38588839769363403\n",
      "Epoch 0[156/312] Time:0.71, Train Loss:0.5225696563720703\n",
      "Epoch 0[157/312] Time:8.452, Train Loss:0.3218204975128174\n",
      "Epoch 0[158/312] Time:8.641, Train Loss:0.4933205544948578\n",
      "Epoch 0[159/312] Time:0.708, Train Loss:0.43785300850868225\n",
      "Epoch 0[160/312] Time:8.372, Train Loss:0.49450400471687317\n",
      "Epoch 0[161/312] Time:8.533, Train Loss:0.4182327687740326\n",
      "Epoch 0[162/312] Time:0.698, Train Loss:0.4765210449695587\n",
      "Epoch 0[163/312] Time:8.37, Train Loss:0.4389277994632721\n",
      "Epoch 0[164/312] Time:8.652, Train Loss:0.4408068060874939\n",
      "Epoch 0[165/312] Time:0.719, Train Loss:0.4428662657737732\n",
      "Epoch 0[166/312] Time:8.371, Train Loss:0.4565199315547943\n",
      "Epoch 0[167/312] Time:8.637, Train Loss:0.4838065207004547\n",
      "Epoch 0[168/312] Time:0.727, Train Loss:0.4058651924133301\n",
      "Epoch 0[169/312] Time:8.377, Train Loss:0.40238338708877563\n",
      "Epoch 0[170/312] Time:8.683, Train Loss:0.39565253257751465\n",
      "Epoch 0[171/312] Time:0.706, Train Loss:0.4109851121902466\n",
      "Epoch 0[172/312] Time:8.455, Train Loss:0.4377976953983307\n",
      "Epoch 0[173/312] Time:8.645, Train Loss:0.3901430368423462\n",
      "Epoch 0[174/312] Time:0.707, Train Loss:0.4482530951499939\n",
      "Epoch 0[175/312] Time:8.502, Train Loss:0.4431161880493164\n",
      "Epoch 0[176/312] Time:8.559, Train Loss:0.3602607250213623\n",
      "Epoch 0[177/312] Time:0.703, Train Loss:0.4439361095428467\n",
      "Epoch 0[178/312] Time:8.369, Train Loss:0.3855285048484802\n",
      "Epoch 0[179/312] Time:8.662, Train Loss:0.45701470971107483\n",
      "Epoch 0[180/312] Time:0.708, Train Loss:0.5083099007606506\n",
      "Epoch 0[181/312] Time:8.551, Train Loss:0.42236244678497314\n",
      "Epoch 0[182/312] Time:8.646, Train Loss:0.37540677189826965\n",
      "Epoch 0[183/312] Time:0.705, Train Loss:0.41367387771606445\n",
      "Epoch 0[184/312] Time:8.6, Train Loss:0.41302964091300964\n",
      "Epoch 0[185/312] Time:8.552, Train Loss:0.42257988452911377\n",
      "Epoch 0[186/312] Time:0.707, Train Loss:0.5529045462608337\n",
      "Epoch 0[187/312] Time:8.604, Train Loss:0.3770883083343506\n",
      "Epoch 0[188/312] Time:8.714, Train Loss:0.4040939211845398\n",
      "Epoch 0[189/312] Time:0.706, Train Loss:0.40361639857292175\n",
      "Epoch 0[190/312] Time:8.46, Train Loss:0.7788890600204468\n",
      "Epoch 0[191/312] Time:8.627, Train Loss:0.4778791666030884\n",
      "Epoch 0[192/312] Time:0.705, Train Loss:0.45032691955566406\n",
      "Epoch 0[193/312] Time:8.585, Train Loss:0.46163687109947205\n",
      "Epoch 0[194/312] Time:8.7, Train Loss:0.43658554553985596\n",
      "Epoch 0[195/312] Time:0.705, Train Loss:0.4084939956665039\n",
      "Epoch 0[196/312] Time:8.605, Train Loss:0.3679385185241699\n",
      "Epoch 0[197/312] Time:8.67, Train Loss:0.35997751355171204\n",
      "Epoch 0[198/312] Time:0.704, Train Loss:0.4045865535736084\n",
      "Epoch 0[199/312] Time:8.544, Train Loss:0.4572525918483734\n",
      "Epoch 0[200/312] Time:8.711, Train Loss:0.44050419330596924\n",
      "Epoch 0[201/312] Time:0.706, Train Loss:0.3758503496646881\n",
      "Epoch 0[202/312] Time:8.546, Train Loss:0.45671403408050537\n",
      "Epoch 0[203/312] Time:8.626, Train Loss:0.47228074073791504\n",
      "Epoch 0[204/312] Time:0.705, Train Loss:0.49164754152297974\n",
      "Epoch 0[205/312] Time:8.562, Train Loss:0.340628057718277\n",
      "Epoch 0[206/312] Time:8.647, Train Loss:0.43634936213493347\n",
      "Epoch 0[207/312] Time:0.704, Train Loss:0.42718881368637085\n",
      "Epoch 0[208/312] Time:8.514, Train Loss:0.4289742708206177\n",
      "Epoch 0[209/312] Time:8.634, Train Loss:0.7002813816070557\n",
      "Epoch 0[210/312] Time:0.705, Train Loss:0.46188607811927795\n",
      "Epoch 0[211/312] Time:8.435, Train Loss:0.3768951892852783\n",
      "Epoch 0[212/312] Time:8.674, Train Loss:0.4539082944393158\n",
      "Epoch 0[213/312] Time:0.707, Train Loss:0.45383399724960327\n",
      "Epoch 0[214/312] Time:8.622, Train Loss:0.47183480858802795\n",
      "Epoch 0[215/312] Time:8.68, Train Loss:0.4336027204990387\n",
      "Epoch 0[216/312] Time:0.703, Train Loss:0.5477463006973267\n",
      "Epoch 0[217/312] Time:8.575, Train Loss:0.4655560851097107\n",
      "Epoch 0[218/312] Time:8.641, Train Loss:0.4447742700576782\n",
      "Epoch 0[219/312] Time:0.707, Train Loss:0.44261741638183594\n",
      "Epoch 0[220/312] Time:8.323, Train Loss:0.40409159660339355\n",
      "Epoch 0[221/312] Time:8.627, Train Loss:0.5761749744415283\n",
      "Epoch 0[222/312] Time:0.703, Train Loss:0.42671653628349304\n",
      "Epoch 0[223/312] Time:8.436, Train Loss:0.45077845454216003\n",
      "Epoch 0[224/312] Time:8.709, Train Loss:0.600216269493103\n",
      "Epoch 0[225/312] Time:0.708, Train Loss:0.498720645904541\n",
      "Epoch 0[226/312] Time:8.537, Train Loss:0.4442458152770996\n",
      "Epoch 0[227/312] Time:8.66, Train Loss:0.5193530917167664\n",
      "Epoch 0[228/312] Time:0.706, Train Loss:0.4928589165210724\n",
      "Epoch 0[229/312] Time:8.602, Train Loss:0.4430984854698181\n",
      "Epoch 0[230/312] Time:8.682, Train Loss:0.5377042293548584\n",
      "Epoch 0[231/312] Time:0.708, Train Loss:0.45029133558273315\n",
      "Epoch 0[232/312] Time:8.529, Train Loss:0.4638729989528656\n",
      "Epoch 0[233/312] Time:8.676, Train Loss:0.4055258333683014\n",
      "Epoch 0[234/312] Time:0.707, Train Loss:0.4788041114807129\n",
      "Epoch 0[235/312] Time:8.561, Train Loss:0.3765154778957367\n",
      "Epoch 0[236/312] Time:8.517, Train Loss:0.46240901947021484\n",
      "Epoch 0[237/312] Time:0.706, Train Loss:0.4079074263572693\n",
      "Epoch 0[238/312] Time:8.551, Train Loss:0.4837176501750946\n",
      "Epoch 0[239/312] Time:8.648, Train Loss:0.45813480019569397\n",
      "Epoch 0[240/312] Time:0.709, Train Loss:0.3187408745288849\n",
      "Epoch 0[241/312] Time:8.58, Train Loss:0.42651695013046265\n",
      "Epoch 0[242/312] Time:8.646, Train Loss:0.4307270646095276\n",
      "Epoch 0[243/312] Time:0.718, Train Loss:0.3696269989013672\n",
      "Epoch 0[244/312] Time:8.512, Train Loss:0.45274773240089417\n",
      "Epoch 0[245/312] Time:8.767, Train Loss:0.40808236598968506\n",
      "Epoch 0[246/312] Time:0.7, Train Loss:0.43492811918258667\n",
      "Epoch 0[247/312] Time:8.541, Train Loss:0.4179351031780243\n",
      "Epoch 0[248/312] Time:8.625, Train Loss:0.626832127571106\n",
      "Epoch 0[249/312] Time:0.706, Train Loss:0.3779965937137604\n",
      "Epoch 0[250/312] Time:8.526, Train Loss:0.47025808691978455\n",
      "Epoch 0[251/312] Time:8.641, Train Loss:0.5098862051963806\n",
      "Epoch 0[252/312] Time:0.707, Train Loss:0.36435073614120483\n",
      "Epoch 0[253/312] Time:8.542, Train Loss:0.4962916374206543\n",
      "Epoch 0[254/312] Time:8.626, Train Loss:0.469147264957428\n",
      "Epoch 0[255/312] Time:0.708, Train Loss:0.4994341731071472\n",
      "Epoch 0[256/312] Time:8.544, Train Loss:0.4615275263786316\n",
      "Epoch 0[257/312] Time:8.676, Train Loss:0.4341224730014801\n",
      "Epoch 0[258/312] Time:0.717, Train Loss:0.3900162875652313\n",
      "Epoch 0[259/312] Time:8.498, Train Loss:0.4303092360496521\n",
      "Epoch 0[260/312] Time:8.67, Train Loss:0.41082876920700073\n",
      "Epoch 0[261/312] Time:0.709, Train Loss:0.35618796944618225\n",
      "Epoch 0[262/312] Time:8.606, Train Loss:0.34607693552970886\n",
      "Epoch 0[263/312] Time:8.66, Train Loss:0.4007752537727356\n",
      "Epoch 0[264/312] Time:0.706, Train Loss:0.390994668006897\n",
      "Epoch 0[265/312] Time:8.349, Train Loss:0.4328710734844208\n",
      "Epoch 0[266/312] Time:8.538, Train Loss:0.45842987298965454\n",
      "Epoch 0[267/312] Time:0.705, Train Loss:0.35688868165016174\n",
      "Epoch 0[268/312] Time:8.384, Train Loss:0.45126739144325256\n",
      "Epoch 0[269/312] Time:8.638, Train Loss:0.38937875628471375\n",
      "Epoch 0[270/312] Time:0.707, Train Loss:0.3941679894924164\n",
      "Epoch 0[271/312] Time:8.365, Train Loss:0.48870787024497986\n",
      "Epoch 0[272/312] Time:8.674, Train Loss:0.556384801864624\n",
      "Epoch 0[273/312] Time:0.705, Train Loss:0.37078943848609924\n",
      "Epoch 0[274/312] Time:8.462, Train Loss:0.37225842475891113\n",
      "Epoch 0[275/312] Time:8.59, Train Loss:0.4638197124004364\n",
      "Epoch 0[276/312] Time:0.707, Train Loss:0.453046590089798\n",
      "Epoch 0[277/312] Time:8.397, Train Loss:0.37465691566467285\n",
      "Epoch 0[278/312] Time:8.633, Train Loss:0.35413745045661926\n",
      "Epoch 0[279/312] Time:0.708, Train Loss:0.43506261706352234\n",
      "Epoch 0[280/312] Time:8.413, Train Loss:0.4117465317249298\n",
      "Epoch 0[281/312] Time:8.633, Train Loss:0.40909498929977417\n",
      "Epoch 0[282/312] Time:0.704, Train Loss:0.44675225019454956\n",
      "Epoch 0[283/312] Time:8.481, Train Loss:0.4065128266811371\n",
      "Epoch 0[284/312] Time:8.595, Train Loss:0.33885177969932556\n",
      "Epoch 0[285/312] Time:0.706, Train Loss:0.429054856300354\n",
      "Epoch 0[286/312] Time:8.432, Train Loss:0.4995366930961609\n",
      "Epoch 0[287/312] Time:8.627, Train Loss:0.4513028562068939\n",
      "Epoch 0[288/312] Time:0.721, Train Loss:0.3696160912513733\n",
      "Epoch 0[289/312] Time:8.606, Train Loss:0.4287973940372467\n",
      "Epoch 0[290/312] Time:8.604, Train Loss:0.40877652168273926\n",
      "Epoch 0[291/312] Time:0.709, Train Loss:0.3428279161453247\n",
      "Epoch 0[292/312] Time:8.542, Train Loss:0.2879018187522888\n",
      "Epoch 0[293/312] Time:8.719, Train Loss:0.3551327884197235\n",
      "Epoch 0[294/312] Time:0.706, Train Loss:0.4114282429218292\n",
      "Epoch 0[295/312] Time:8.521, Train Loss:0.5546753406524658\n",
      "Epoch 0[296/312] Time:8.705, Train Loss:0.43194580078125\n",
      "Epoch 0[297/312] Time:0.705, Train Loss:0.41687601804733276\n",
      "Epoch 0[298/312] Time:8.584, Train Loss:0.35715073347091675\n",
      "Epoch 0[299/312] Time:8.677, Train Loss:0.36683040857315063\n",
      "Epoch 0[300/312] Time:0.708, Train Loss:0.332351952791214\n",
      "Epoch 0[301/312] Time:8.565, Train Loss:0.40107107162475586\n",
      "Epoch 0[302/312] Time:8.643, Train Loss:0.3901818096637726\n",
      "Epoch 0[303/312] Time:0.705, Train Loss:0.43967631459236145\n",
      "Epoch 0[304/312] Time:8.503, Train Loss:0.48741450905799866\n",
      "Epoch 0[305/312] Time:8.646, Train Loss:0.4358404874801636\n",
      "Epoch 0[306/312] Time:0.708, Train Loss:0.37926754355430603\n",
      "Epoch 0[307/312] Time:8.573, Train Loss:0.4342934191226959\n",
      "Epoch 0[308/312] Time:8.673, Train Loss:0.40135085582733154\n",
      "Epoch 0[309/312] Time:0.705, Train Loss:0.3263700604438782\n",
      "Epoch 0[310/312] Time:8.532, Train Loss:0.4273323714733124\n",
      "Epoch 0[311/312] Time:8.675, Train Loss:0.4003748893737793\n",
      "Epoch 0[0/39] Val Loss:0.5967384576797485\n",
      "Epoch 0[1/39] Val Loss:0.5705508589744568\n",
      "Epoch 0[2/39] Val Loss:0.5608647465705872\n",
      "Epoch 0[3/39] Val Loss:0.6013049483299255\n",
      "Epoch 0[4/39] Val Loss:0.6128535270690918\n",
      "Epoch 0[5/39] Val Loss:0.5749223828315735\n",
      "Epoch 0[6/39] Val Loss:0.6544855237007141\n",
      "Epoch 0[7/39] Val Loss:0.6286689639091492\n",
      "Epoch 0[8/39] Val Loss:0.5816518068313599\n",
      "Epoch 0[9/39] Val Loss:0.5349038243293762\n",
      "Epoch 0[10/39] Val Loss:0.2763262987136841\n",
      "Epoch 0[11/39] Val Loss:0.25317761301994324\n",
      "Epoch 0[12/39] Val Loss:0.4975670874118805\n",
      "Epoch 0[13/39] Val Loss:0.32881829142570496\n",
      "Epoch 0[14/39] Val Loss:0.22324837744235992\n",
      "Epoch 0[15/39] Val Loss:0.3974449634552002\n",
      "Epoch 0[16/39] Val Loss:0.7448128461837769\n",
      "Epoch 0[17/39] Val Loss:0.630612313747406\n",
      "Epoch 0[18/39] Val Loss:0.588553249835968\n",
      "Epoch 0[19/39] Val Loss:0.6615864634513855\n",
      "Epoch 0[20/39] Val Loss:0.9684252142906189\n",
      "Epoch 0[21/39] Val Loss:0.9209101796150208\n",
      "Epoch 0[22/39] Val Loss:0.46021759510040283\n",
      "Epoch 0[23/39] Val Loss:0.4576527178287506\n",
      "Epoch 0[24/39] Val Loss:0.5101472735404968\n",
      "Epoch 0[25/39] Val Loss:0.49653586745262146\n",
      "Epoch 0[26/39] Val Loss:0.5255970358848572\n",
      "Epoch 0[27/39] Val Loss:0.3822498917579651\n",
      "Epoch 0[28/39] Val Loss:0.13155844807624817\n",
      "Epoch 0[29/39] Val Loss:0.3396719992160797\n",
      "Epoch 0[30/39] Val Loss:0.6719872951507568\n",
      "Epoch 0[31/39] Val Loss:0.7199923992156982\n",
      "Epoch 0[32/39] Val Loss:0.50658118724823\n",
      "Epoch 0[33/39] Val Loss:0.37670859694480896\n",
      "Epoch 0[34/39] Val Loss:0.3877831995487213\n",
      "Epoch 0[35/39] Val Loss:0.285513311624527\n",
      "Epoch 0[36/39] Val Loss:0.42278406023979187\n",
      "Epoch 0[37/39] Val Loss:0.543551504611969\n",
      "Epoch 0[38/39] Val Loss:0.22533196210861206\n",
      "Epoch 0[39/39] Val Loss:0.1551748365163803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86      8608\n",
      "           1       0.28      0.44      0.35      1392\n",
      "\n",
      "    accuracy                           0.77     10000\n",
      "   macro avg       0.59      0.63      0.60     10000\n",
      "weighted avg       0.81      0.77      0.79     10000\n",
      "\n",
      "Epoch 0: Train Loss 0.473016082858428, Val Loss 0.5130119774586115, Train Time 2080.3446748256683, Val Time 21.586907625198364\n",
      "New best model at epoch 0\n",
      "Epoch 1[0/312] Time:8.453, Train Loss:0.4351713955402374\n",
      "Epoch 1[1/312] Time:0.706, Train Loss:0.31798020005226135\n",
      "Epoch 1[2/312] Time:8.189, Train Loss:0.4407416582107544\n",
      "Epoch 1[3/312] Time:8.635, Train Loss:0.40448233485221863\n",
      "Epoch 1[4/312] Time:0.71, Train Loss:0.5340979099273682\n",
      "Epoch 1[5/312] Time:8.502, Train Loss:0.41907596588134766\n",
      "Epoch 1[6/312] Time:8.661, Train Loss:0.3868626058101654\n",
      "Epoch 1[7/312] Time:0.719, Train Loss:0.31968361139297485\n",
      "Epoch 1[8/312] Time:8.426, Train Loss:0.38444021344184875\n",
      "Epoch 1[9/312] Time:8.648, Train Loss:0.40501800179481506\n",
      "Epoch 1[10/312] Time:0.708, Train Loss:0.4381884038448334\n",
      "Epoch 1[11/312] Time:8.54, Train Loss:0.3302692472934723\n",
      "Epoch 1[12/312] Time:8.748, Train Loss:0.506033182144165\n",
      "Epoch 1[13/312] Time:0.706, Train Loss:0.5361157059669495\n",
      "Epoch 1[14/312] Time:8.557, Train Loss:0.40910473465919495\n",
      "Epoch 1[15/312] Time:8.649, Train Loss:0.3710654377937317\n",
      "Epoch 1[16/312] Time:0.705, Train Loss:0.36491718888282776\n",
      "Epoch 1[17/312] Time:8.366, Train Loss:0.3157403767108917\n",
      "Epoch 1[18/312] Time:8.642, Train Loss:0.47792282700538635\n",
      "Epoch 1[19/312] Time:0.707, Train Loss:0.48143643140792847\n",
      "Epoch 1[20/312] Time:8.52, Train Loss:0.4304010272026062\n",
      "Epoch 1[21/312] Time:8.677, Train Loss:0.40583306550979614\n",
      "Epoch 1[22/312] Time:0.707, Train Loss:0.37291935086250305\n",
      "Epoch 1[23/312] Time:8.514, Train Loss:0.40100112557411194\n",
      "Epoch 1[24/312] Time:8.594, Train Loss:0.34701016545295715\n",
      "Epoch 1[25/312] Time:0.705, Train Loss:0.29482898116111755\n",
      "Epoch 1[26/312] Time:8.517, Train Loss:0.3679136037826538\n",
      "Epoch 1[27/312] Time:8.655, Train Loss:0.34376662969589233\n",
      "Epoch 1[28/312] Time:0.708, Train Loss:0.42145681381225586\n",
      "Epoch 1[29/312] Time:8.483, Train Loss:0.40896689891815186\n",
      "Epoch 1[30/312] Time:8.504, Train Loss:0.36344268918037415\n",
      "Epoch 1[31/312] Time:0.718, Train Loss:0.4008093774318695\n",
      "Epoch 1[32/312] Time:8.413, Train Loss:0.445908784866333\n",
      "Epoch 1[33/312] Time:8.577, Train Loss:0.4553229510784149\n",
      "Epoch 1[34/312] Time:0.708, Train Loss:0.46480369567871094\n",
      "Epoch 1[35/312] Time:8.526, Train Loss:0.36522412300109863\n",
      "Epoch 1[36/312] Time:8.659, Train Loss:0.39160025119781494\n",
      "Epoch 1[37/312] Time:0.708, Train Loss:0.4067589342594147\n",
      "Epoch 1[38/312] Time:8.59, Train Loss:0.4429348409175873\n",
      "Epoch 1[39/312] Time:8.634, Train Loss:0.42107298970222473\n",
      "Epoch 1[40/312] Time:0.705, Train Loss:0.531897783279419\n",
      "Epoch 1[41/312] Time:8.468, Train Loss:0.4030865430831909\n",
      "Epoch 1[42/312] Time:8.677, Train Loss:0.4323424696922302\n",
      "Epoch 1[43/312] Time:0.706, Train Loss:0.4554251432418823\n",
      "Epoch 1[44/312] Time:8.336, Train Loss:0.3850674629211426\n",
      "Epoch 1[45/312] Time:8.587, Train Loss:0.34248924255371094\n",
      "Epoch 1[46/312] Time:0.708, Train Loss:0.4289824664592743\n",
      "Epoch 1[47/312] Time:8.425, Train Loss:0.37669387459754944\n",
      "Epoch 1[48/312] Time:8.647, Train Loss:0.406122624874115\n",
      "Epoch 1[49/312] Time:0.706, Train Loss:0.3956621289253235\n",
      "Epoch 1[50/312] Time:8.546, Train Loss:0.3722563683986664\n",
      "Epoch 1[51/312] Time:8.625, Train Loss:0.3521377742290497\n",
      "Epoch 1[52/312] Time:0.713, Train Loss:0.34765365719795227\n",
      "Epoch 1[53/312] Time:8.535, Train Loss:0.3198898732662201\n",
      "Epoch 1[54/312] Time:8.635, Train Loss:0.460583359003067\n",
      "Epoch 1[55/312] Time:0.704, Train Loss:0.3925189673900604\n",
      "Epoch 1[56/312] Time:8.423, Train Loss:0.4198410212993622\n",
      "Epoch 1[57/312] Time:8.649, Train Loss:0.4060727655887604\n",
      "Epoch 1[58/312] Time:0.712, Train Loss:0.3545648753643036\n",
      "Epoch 1[59/312] Time:8.367, Train Loss:0.42567670345306396\n",
      "Epoch 1[60/312] Time:8.597, Train Loss:0.3603764772415161\n",
      "Epoch 1[61/312] Time:0.714, Train Loss:0.33164629340171814\n",
      "Epoch 1[62/312] Time:8.33, Train Loss:0.42995184659957886\n",
      "Epoch 1[63/312] Time:8.69, Train Loss:0.41846445202827454\n",
      "Epoch 1[64/312] Time:0.706, Train Loss:0.3998536765575409\n",
      "Epoch 1[65/312] Time:8.475, Train Loss:0.3491850793361664\n",
      "Epoch 1[66/312] Time:8.631, Train Loss:0.44967225193977356\n",
      "Epoch 1[67/312] Time:0.71, Train Loss:0.35779303312301636\n",
      "Epoch 1[68/312] Time:8.512, Train Loss:0.4246610999107361\n",
      "Epoch 1[69/312] Time:8.591, Train Loss:0.4351750612258911\n",
      "Epoch 1[70/312] Time:0.709, Train Loss:0.4988672733306885\n",
      "Epoch 1[71/312] Time:8.381, Train Loss:0.3114980161190033\n",
      "Epoch 1[72/312] Time:8.68, Train Loss:0.4547629952430725\n",
      "Epoch 1[73/312] Time:0.714, Train Loss:0.395677775144577\n",
      "Epoch 1[74/312] Time:8.542, Train Loss:0.3610209822654724\n",
      "Epoch 1[75/312] Time:8.503, Train Loss:0.39576756954193115\n",
      "Epoch 1[76/312] Time:0.708, Train Loss:0.30944326519966125\n",
      "Epoch 1[77/312] Time:8.325, Train Loss:0.37076202034950256\n",
      "Epoch 1[78/312] Time:8.568, Train Loss:0.3888784945011139\n",
      "Epoch 1[79/312] Time:0.707, Train Loss:0.4578370451927185\n",
      "Epoch 1[80/312] Time:8.473, Train Loss:0.33065417408943176\n",
      "Epoch 1[81/312] Time:8.625, Train Loss:0.3643988072872162\n",
      "Epoch 1[82/312] Time:0.71, Train Loss:0.40089666843414307\n",
      "Epoch 1[83/312] Time:8.376, Train Loss:0.32140883803367615\n",
      "Epoch 1[84/312] Time:8.619, Train Loss:0.4311179518699646\n",
      "Epoch 1[85/312] Time:0.706, Train Loss:0.3291115462779999\n",
      "Epoch 1[86/312] Time:8.441, Train Loss:0.4188997745513916\n",
      "Epoch 1[87/312] Time:8.668, Train Loss:0.3469800651073456\n",
      "Epoch 1[88/312] Time:0.704, Train Loss:0.38013002276420593\n",
      "Epoch 1[89/312] Time:8.508, Train Loss:0.3860524296760559\n",
      "Epoch 1[90/312] Time:8.6, Train Loss:0.39703112840652466\n",
      "Epoch 1[91/312] Time:0.719, Train Loss:0.29195380210876465\n",
      "Epoch 1[92/312] Time:8.308, Train Loss:0.3724313974380493\n",
      "Epoch 1[93/312] Time:8.551, Train Loss:0.4740496575832367\n",
      "Epoch 1[94/312] Time:0.704, Train Loss:0.36849939823150635\n",
      "Epoch 1[95/312] Time:8.422, Train Loss:0.3457912504673004\n",
      "Epoch 1[96/312] Time:8.622, Train Loss:0.35558539628982544\n",
      "Epoch 1[97/312] Time:0.707, Train Loss:0.45738598704338074\n",
      "Epoch 1[98/312] Time:8.551, Train Loss:0.4301202893257141\n",
      "Epoch 1[99/312] Time:8.65, Train Loss:0.34412646293640137\n",
      "Epoch 1[100/312] Time:0.707, Train Loss:0.49360159039497375\n",
      "Epoch 1[101/312] Time:8.407, Train Loss:0.41510674357414246\n",
      "Epoch 1[102/312] Time:8.761, Train Loss:0.3979089856147766\n",
      "Epoch 1[103/312] Time:0.701, Train Loss:0.4235548675060272\n",
      "Epoch 1[104/312] Time:8.562, Train Loss:0.43923690915107727\n",
      "Epoch 1[105/312] Time:8.666, Train Loss:0.42003709077835083\n",
      "Epoch 1[106/312] Time:0.706, Train Loss:0.3486799895763397\n",
      "Epoch 1[107/312] Time:8.458, Train Loss:0.34424883127212524\n",
      "Epoch 1[108/312] Time:8.616, Train Loss:0.41823986172676086\n",
      "Epoch 1[109/312] Time:0.705, Train Loss:0.39345768094062805\n",
      "Epoch 1[110/312] Time:8.521, Train Loss:0.421676903963089\n",
      "Epoch 1[111/312] Time:8.512, Train Loss:0.3808010518550873\n",
      "Epoch 1[112/312] Time:0.705, Train Loss:0.4046642780303955\n",
      "Epoch 1[113/312] Time:8.42, Train Loss:0.4435429573059082\n",
      "Epoch 1[114/312] Time:8.509, Train Loss:0.3711181879043579\n",
      "Epoch 1[115/312] Time:0.7, Train Loss:0.36348095536231995\n",
      "Epoch 1[116/312] Time:8.532, Train Loss:0.4228244125843048\n",
      "Epoch 1[117/312] Time:8.679, Train Loss:0.36972394585609436\n",
      "Epoch 1[118/312] Time:0.706, Train Loss:0.4877498149871826\n",
      "Epoch 1[119/312] Time:8.496, Train Loss:0.438444048166275\n",
      "Epoch 1[120/312] Time:8.682, Train Loss:0.40447351336479187\n",
      "Epoch 1[121/312] Time:0.709, Train Loss:0.41144198179244995\n",
      "Epoch 1[122/312] Time:8.482, Train Loss:0.34703776240348816\n",
      "Epoch 1[123/312] Time:8.607, Train Loss:0.4149117171764374\n",
      "Epoch 1[124/312] Time:0.708, Train Loss:0.4614226222038269\n",
      "Epoch 1[125/312] Time:8.351, Train Loss:0.43781521916389465\n",
      "Epoch 1[126/312] Time:8.57, Train Loss:0.34955093264579773\n",
      "Epoch 1[127/312] Time:0.71, Train Loss:0.39700382947921753\n",
      "Epoch 1[128/312] Time:8.503, Train Loss:0.4010518789291382\n",
      "Epoch 1[129/312] Time:8.602, Train Loss:0.3496944308280945\n",
      "Epoch 1[130/312] Time:0.705, Train Loss:0.4004001021385193\n",
      "Epoch 1[131/312] Time:8.545, Train Loss:0.33728933334350586\n",
      "Epoch 1[132/312] Time:8.634, Train Loss:0.42332878708839417\n",
      "Epoch 1[133/312] Time:0.706, Train Loss:0.43428096175193787\n",
      "Epoch 1[134/312] Time:8.55, Train Loss:0.4810931980609894\n",
      "Epoch 1[135/312] Time:8.611, Train Loss:0.4071961045265198\n",
      "Epoch 1[136/312] Time:0.708, Train Loss:0.361708402633667\n",
      "Epoch 1[137/312] Time:8.572, Train Loss:0.3772699534893036\n",
      "Epoch 1[138/312] Time:8.647, Train Loss:0.3826908767223358\n",
      "Epoch 1[139/312] Time:0.708, Train Loss:0.3717823028564453\n",
      "Epoch 1[140/312] Time:8.347, Train Loss:0.2997257709503174\n",
      "Epoch 1[141/312] Time:8.6, Train Loss:0.31894391775131226\n",
      "Epoch 1[142/312] Time:0.704, Train Loss:0.4001603126525879\n",
      "Epoch 1[143/312] Time:8.461, Train Loss:0.397873193025589\n",
      "Epoch 1[144/312] Time:8.628, Train Loss:0.39691609144210815\n",
      "Epoch 1[145/312] Time:0.701, Train Loss:0.4689197838306427\n",
      "Epoch 1[146/312] Time:8.464, Train Loss:0.31902557611465454\n",
      "Epoch 1[147/312] Time:8.638, Train Loss:0.5223587155342102\n",
      "Epoch 1[148/312] Time:0.707, Train Loss:0.3233086168766022\n",
      "Epoch 1[149/312] Time:8.48, Train Loss:0.5156909823417664\n",
      "Epoch 1[150/312] Time:8.607, Train Loss:0.370369553565979\n",
      "Epoch 1[151/312] Time:0.705, Train Loss:0.4706549644470215\n",
      "Epoch 1[152/312] Time:8.458, Train Loss:0.36882296204566956\n",
      "Epoch 1[153/312] Time:8.602, Train Loss:0.4406326115131378\n",
      "Epoch 1[154/312] Time:0.706, Train Loss:0.39251643419265747\n",
      "Epoch 1[155/312] Time:8.439, Train Loss:0.34607070684432983\n",
      "Epoch 1[156/312] Time:8.546, Train Loss:0.32954227924346924\n",
      "Epoch 1[157/312] Time:0.711, Train Loss:0.416744589805603\n",
      "Epoch 1[158/312] Time:8.393, Train Loss:0.3888830542564392\n",
      "Epoch 1[159/312] Time:8.608, Train Loss:0.4075436294078827\n",
      "Epoch 1[160/312] Time:0.708, Train Loss:0.40396854281425476\n",
      "Epoch 1[161/312] Time:8.439, Train Loss:0.33644798398017883\n",
      "Epoch 1[162/312] Time:8.664, Train Loss:0.34159737825393677\n",
      "Epoch 1[163/312] Time:0.708, Train Loss:0.3491906523704529\n",
      "Epoch 1[164/312] Time:8.396, Train Loss:0.3786373734474182\n",
      "Epoch 1[165/312] Time:8.644, Train Loss:0.4010426700115204\n",
      "Epoch 1[166/312] Time:0.708, Train Loss:0.3829745650291443\n",
      "Epoch 1[167/312] Time:8.438, Train Loss:0.37941259145736694\n",
      "Epoch 1[168/312] Time:8.558, Train Loss:0.38838663697242737\n",
      "Epoch 1[169/312] Time:0.712, Train Loss:0.35185566544532776\n",
      "Epoch 1[170/312] Time:8.476, Train Loss:0.3334014415740967\n",
      "Epoch 1[171/312] Time:8.586, Train Loss:0.40523049235343933\n",
      "Epoch 1[172/312] Time:0.711, Train Loss:0.37921085953712463\n",
      "Epoch 1[173/312] Time:8.429, Train Loss:0.39935678243637085\n",
      "Epoch 1[174/312] Time:8.633, Train Loss:0.311685174703598\n",
      "Epoch 1[175/312] Time:0.717, Train Loss:0.35984233021736145\n",
      "Epoch 1[176/312] Time:8.505, Train Loss:0.44165709614753723\n",
      "Epoch 1[177/312] Time:8.646, Train Loss:0.3111577033996582\n",
      "Epoch 1[178/312] Time:0.709, Train Loss:0.3602299094200134\n",
      "Epoch 1[179/312] Time:8.44, Train Loss:0.36650213599205017\n",
      "Epoch 1[180/312] Time:8.636, Train Loss:0.37325605750083923\n",
      "Epoch 1[181/312] Time:0.709, Train Loss:0.37834885716438293\n",
      "Epoch 1[182/312] Time:8.481, Train Loss:0.36009496450424194\n",
      "Epoch 1[183/312] Time:8.671, Train Loss:0.4200823903083801\n",
      "Epoch 1[184/312] Time:0.708, Train Loss:0.41232264041900635\n",
      "Epoch 1[185/312] Time:8.476, Train Loss:0.3395242393016815\n",
      "Epoch 1[186/312] Time:8.621, Train Loss:0.419141560792923\n",
      "Epoch 1[187/312] Time:0.708, Train Loss:0.3489832580089569\n",
      "Epoch 1[188/312] Time:8.372, Train Loss:0.34729185700416565\n",
      "Epoch 1[189/312] Time:8.626, Train Loss:0.33890703320503235\n",
      "Epoch 1[190/312] Time:0.708, Train Loss:0.3810943365097046\n",
      "Epoch 1[191/312] Time:8.372, Train Loss:0.4499576985836029\n",
      "Epoch 1[192/312] Time:8.626, Train Loss:0.3428993225097656\n",
      "Epoch 1[193/312] Time:0.708, Train Loss:0.3674966096878052\n",
      "Epoch 1[194/312] Time:8.508, Train Loss:0.42870742082595825\n",
      "Epoch 1[195/312] Time:8.618, Train Loss:0.2991071939468384\n",
      "Epoch 1[196/312] Time:0.709, Train Loss:0.42344412207603455\n",
      "Epoch 1[197/312] Time:8.475, Train Loss:0.35886532068252563\n",
      "Epoch 1[198/312] Time:8.637, Train Loss:0.3075713813304901\n",
      "Epoch 1[199/312] Time:0.707, Train Loss:0.312913179397583\n",
      "Epoch 1[200/312] Time:8.239, Train Loss:0.38540202379226685\n",
      "Epoch 1[201/312] Time:8.62, Train Loss:0.4881484806537628\n",
      "Epoch 1[202/312] Time:0.706, Train Loss:0.4339871406555176\n",
      "Epoch 1[203/312] Time:8.429, Train Loss:0.31118255853652954\n",
      "Epoch 1[204/312] Time:8.595, Train Loss:0.3714354634284973\n",
      "Epoch 1[205/312] Time:0.711, Train Loss:0.3792693614959717\n",
      "Epoch 1[206/312] Time:8.512, Train Loss:0.3996860384941101\n",
      "Epoch 1[207/312] Time:8.664, Train Loss:0.3844601809978485\n",
      "Epoch 1[208/312] Time:0.707, Train Loss:0.3269139230251312\n",
      "Epoch 1[209/312] Time:8.459, Train Loss:0.3342929184436798\n",
      "Epoch 1[210/312] Time:8.631, Train Loss:0.39691516757011414\n",
      "Epoch 1[211/312] Time:0.706, Train Loss:0.70447838306427\n",
      "Epoch 1[212/312] Time:8.506, Train Loss:0.4234112799167633\n",
      "Epoch 1[213/312] Time:8.494, Train Loss:0.3054105341434479\n",
      "Epoch 1[214/312] Time:0.724, Train Loss:0.39973586797714233\n",
      "Epoch 1[215/312] Time:8.503, Train Loss:0.42398348450660706\n",
      "Epoch 1[216/312] Time:8.595, Train Loss:0.42265570163726807\n",
      "Epoch 1[217/312] Time:0.706, Train Loss:0.3499278128147125\n",
      "Epoch 1[218/312] Time:8.472, Train Loss:0.36535799503326416\n",
      "Epoch 1[219/312] Time:8.637, Train Loss:0.3182256817817688\n",
      "Epoch 1[220/312] Time:0.707, Train Loss:0.32623955607414246\n",
      "Epoch 1[221/312] Time:8.496, Train Loss:0.4571877419948578\n",
      "Epoch 1[222/312] Time:8.661, Train Loss:0.3851678967475891\n",
      "Epoch 1[223/312] Time:0.704, Train Loss:0.41270843148231506\n",
      "Epoch 1[224/312] Time:8.53, Train Loss:0.48038598895072937\n",
      "Epoch 1[225/312] Time:8.669, Train Loss:0.37611114978790283\n",
      "Epoch 1[226/312] Time:0.706, Train Loss:0.45735543966293335\n",
      "Epoch 1[227/312] Time:8.46, Train Loss:0.3556511402130127\n",
      "Epoch 1[228/312] Time:8.614, Train Loss:0.4140225052833557\n",
      "Epoch 1[229/312] Time:0.709, Train Loss:0.45572131872177124\n",
      "Epoch 1[230/312] Time:8.36, Train Loss:0.40686699748039246\n",
      "Epoch 1[231/312] Time:8.589, Train Loss:0.33918657898902893\n",
      "Epoch 1[232/312] Time:0.706, Train Loss:0.33112940192222595\n",
      "Epoch 1[233/312] Time:8.473, Train Loss:0.4792358875274658\n",
      "Epoch 1[234/312] Time:8.635, Train Loss:0.39345067739486694\n",
      "Epoch 1[235/312] Time:0.7, Train Loss:0.43009743094444275\n",
      "Epoch 1[236/312] Time:8.496, Train Loss:0.3787953853607178\n",
      "Epoch 1[237/312] Time:8.64, Train Loss:0.4393734633922577\n",
      "Epoch 1[238/312] Time:0.693, Train Loss:0.4859189987182617\n",
      "Epoch 1[239/312] Time:8.65, Train Loss:0.2916051149368286\n",
      "Epoch 1[240/312] Time:8.62, Train Loss:0.3861618638038635\n",
      "Epoch 1[241/312] Time:0.707, Train Loss:0.4055781364440918\n",
      "Epoch 1[242/312] Time:8.542, Train Loss:0.3895127773284912\n",
      "Epoch 1[243/312] Time:8.651, Train Loss:0.3824789822101593\n",
      "Epoch 1[244/312] Time:0.706, Train Loss:0.38732030987739563\n",
      "Epoch 1[245/312] Time:8.429, Train Loss:0.322128564119339\n",
      "Epoch 1[246/312] Time:8.639, Train Loss:0.34968101978302\n",
      "Epoch 1[247/312] Time:0.707, Train Loss:0.4830554127693176\n",
      "Epoch 1[248/312] Time:8.473, Train Loss:0.4256923496723175\n",
      "Epoch 1[249/312] Time:8.54, Train Loss:0.4462185800075531\n",
      "Epoch 1[250/312] Time:0.706, Train Loss:0.3103145658969879\n",
      "Epoch 1[251/312] Time:8.398, Train Loss:0.37803083658218384\n",
      "Epoch 1[252/312] Time:8.637, Train Loss:0.4169393479824066\n",
      "Epoch 1[253/312] Time:0.706, Train Loss:0.3387436866760254\n",
      "Epoch 1[254/312] Time:8.487, Train Loss:0.3936121165752411\n",
      "Epoch 1[255/312] Time:8.62, Train Loss:0.40497660636901855\n",
      "Epoch 1[256/312] Time:0.706, Train Loss:0.4251234233379364\n",
      "Epoch 1[257/312] Time:8.489, Train Loss:0.4068099558353424\n",
      "Epoch 1[258/312] Time:8.675, Train Loss:0.2933442294597626\n",
      "Epoch 1[259/312] Time:0.709, Train Loss:0.4284215569496155\n",
      "Epoch 1[260/312] Time:8.584, Train Loss:0.3384222388267517\n",
      "Epoch 1[261/312] Time:8.662, Train Loss:0.39849939942359924\n",
      "Epoch 1[262/312] Time:0.706, Train Loss:0.42495808005332947\n",
      "Epoch 1[263/312] Time:8.506, Train Loss:0.345950722694397\n",
      "Epoch 1[264/312] Time:8.572, Train Loss:0.26670679450035095\n",
      "Epoch 1[265/312] Time:0.699, Train Loss:0.3483705222606659\n",
      "Epoch 1[266/312] Time:8.37, Train Loss:0.4581669569015503\n",
      "Epoch 1[267/312] Time:8.558, Train Loss:0.4294820725917816\n",
      "Epoch 1[268/312] Time:0.705, Train Loss:0.3315642774105072\n",
      "Epoch 1[269/312] Time:8.466, Train Loss:0.47037267684936523\n",
      "Epoch 1[270/312] Time:8.609, Train Loss:0.43936440348625183\n",
      "Epoch 1[271/312] Time:0.707, Train Loss:0.4579239785671234\n",
      "Epoch 1[272/312] Time:8.446, Train Loss:0.39350080490112305\n",
      "Epoch 1[273/312] Time:8.598, Train Loss:0.4202602207660675\n",
      "Epoch 1[274/312] Time:0.714, Train Loss:0.3783295452594757\n",
      "Epoch 1[275/312] Time:8.556, Train Loss:0.32741984724998474\n",
      "Epoch 1[276/312] Time:8.619, Train Loss:0.3499300479888916\n",
      "Epoch 1[277/312] Time:0.707, Train Loss:0.44021138548851013\n",
      "Epoch 1[278/312] Time:8.524, Train Loss:0.4348513185977936\n",
      "Epoch 1[279/312] Time:8.665, Train Loss:0.3541674017906189\n",
      "Epoch 1[280/312] Time:0.707, Train Loss:0.3049846887588501\n",
      "Epoch 1[281/312] Time:8.181, Train Loss:0.36878228187561035\n",
      "Epoch 1[282/312] Time:8.54, Train Loss:0.43262460827827454\n",
      "Epoch 1[283/312] Time:0.707, Train Loss:0.376894474029541\n",
      "Epoch 1[284/312] Time:8.425, Train Loss:0.3827323615550995\n",
      "Epoch 1[285/312] Time:8.568, Train Loss:0.42107146978378296\n",
      "Epoch 1[286/312] Time:0.707, Train Loss:0.38422349095344543\n",
      "Epoch 1[287/312] Time:8.513, Train Loss:0.3971799314022064\n",
      "Epoch 1[288/312] Time:8.655, Train Loss:0.36433762311935425\n",
      "Epoch 1[289/312] Time:0.722, Train Loss:0.29310810565948486\n",
      "Epoch 1[290/312] Time:8.476, Train Loss:0.3772653341293335\n",
      "Epoch 1[291/312] Time:8.677, Train Loss:0.34554770588874817\n",
      "Epoch 1[292/312] Time:0.707, Train Loss:0.3453512489795685\n",
      "Epoch 1[293/312] Time:8.38, Train Loss:0.4140177369117737\n",
      "Epoch 1[294/312] Time:8.571, Train Loss:0.37929439544677734\n",
      "Epoch 1[295/312] Time:0.698, Train Loss:0.3562755286693573\n",
      "Epoch 1[296/312] Time:8.338, Train Loss:0.4286026954650879\n",
      "Epoch 1[297/312] Time:8.611, Train Loss:0.36116692423820496\n",
      "Epoch 1[298/312] Time:0.705, Train Loss:0.40850022435188293\n",
      "Epoch 1[299/312] Time:8.442, Train Loss:0.3324282467365265\n",
      "Epoch 1[300/312] Time:8.667, Train Loss:0.39180412888526917\n",
      "Epoch 1[301/312] Time:0.709, Train Loss:0.361875057220459\n",
      "Epoch 1[302/312] Time:8.485, Train Loss:0.32552826404571533\n",
      "Epoch 1[303/312] Time:8.699, Train Loss:0.3647971451282501\n",
      "Epoch 1[304/312] Time:0.705, Train Loss:0.3520934283733368\n",
      "Epoch 1[305/312] Time:8.52, Train Loss:0.3884899616241455\n",
      "Epoch 1[306/312] Time:8.642, Train Loss:0.3452524244785309\n",
      "Epoch 1[307/312] Time:0.709, Train Loss:0.34555694460868835\n",
      "Epoch 1[308/312] Time:8.52, Train Loss:0.4565233886241913\n",
      "Epoch 1[309/312] Time:8.598, Train Loss:0.3494586944580078\n",
      "Epoch 1[310/312] Time:0.706, Train Loss:0.34303903579711914\n",
      "Epoch 1[311/312] Time:8.467, Train Loss:0.3586393892765045\n",
      "Epoch 1[0/39] Val Loss:0.9781836867332458\n",
      "Epoch 1[1/39] Val Loss:0.9138050079345703\n",
      "Epoch 1[2/39] Val Loss:0.8792030811309814\n",
      "Epoch 1[3/39] Val Loss:0.9731887578964233\n",
      "Epoch 1[4/39] Val Loss:1.0410234928131104\n",
      "Epoch 1[5/39] Val Loss:0.6163359880447388\n",
      "Epoch 1[6/39] Val Loss:0.4537881910800934\n",
      "Epoch 1[7/39] Val Loss:0.46127191185951233\n",
      "Epoch 1[8/39] Val Loss:0.5110387802124023\n",
      "Epoch 1[9/39] Val Loss:0.4927956163883209\n",
      "Epoch 1[10/39] Val Loss:0.18628068268299103\n",
      "Epoch 1[11/39] Val Loss:0.1778416931629181\n",
      "Epoch 1[12/39] Val Loss:0.5225017666816711\n",
      "Epoch 1[13/39] Val Loss:0.2870664894580841\n",
      "Epoch 1[14/39] Val Loss:0.14095909893512726\n",
      "Epoch 1[15/39] Val Loss:0.3370213508605957\n",
      "Epoch 1[16/39] Val Loss:0.6448163390159607\n",
      "Epoch 1[17/39] Val Loss:0.8626635670661926\n",
      "Epoch 1[18/39] Val Loss:1.0402030944824219\n",
      "Epoch 1[19/39] Val Loss:1.1179325580596924\n",
      "Epoch 1[20/39] Val Loss:0.9350151419639587\n",
      "Epoch 1[21/39] Val Loss:0.6552503705024719\n",
      "Epoch 1[22/39] Val Loss:0.3712368905544281\n",
      "Epoch 1[23/39] Val Loss:0.38227853178977966\n",
      "Epoch 1[24/39] Val Loss:0.4139046370983124\n",
      "Epoch 1[25/39] Val Loss:0.4540265202522278\n",
      "Epoch 1[26/39] Val Loss:0.4628664553165436\n",
      "Epoch 1[27/39] Val Loss:0.37497344613075256\n",
      "Epoch 1[28/39] Val Loss:0.13425959646701813\n",
      "Epoch 1[29/39] Val Loss:0.3108918368816376\n",
      "Epoch 1[30/39] Val Loss:0.5851173996925354\n",
      "Epoch 1[31/39] Val Loss:0.5558164119720459\n",
      "Epoch 1[32/39] Val Loss:0.3207802176475525\n",
      "Epoch 1[33/39] Val Loss:0.22642643749713898\n",
      "Epoch 1[34/39] Val Loss:0.2499367594718933\n",
      "Epoch 1[35/39] Val Loss:0.13584747910499573\n",
      "Epoch 1[36/39] Val Loss:0.31657859683036804\n",
      "Epoch 1[37/39] Val Loss:0.5184866786003113\n",
      "Epoch 1[38/39] Val Loss:0.2174806147813797\n",
      "Epoch 1[39/39] Val Loss:0.16923806071281433\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.77      0.85      8608\n",
      "           1       0.34      0.72      0.46      1392\n",
      "\n",
      "    accuracy                           0.77     10000\n",
      "   macro avg       0.64      0.75      0.66     10000\n",
      "weighted avg       0.86      0.77      0.80     10000\n",
      "\n",
      "Epoch 1: Train Loss 0.3909756020666697, Val Loss 0.5238034163530056, Train Time 2080.2395124435425, Val Time 24.738582611083984\n",
      "Epoch 2[0/312] Time:4.056, Train Loss:0.5516367554664612\n",
      "Epoch 2[1/312] Time:8.687, Train Loss:0.345915287733078\n",
      "Epoch 2[2/312] Time:0.721, Train Loss:0.44964301586151123\n",
      "Epoch 2[3/312] Time:8.484, Train Loss:0.4153140187263489\n",
      "Epoch 2[4/312] Time:8.578, Train Loss:0.32948875427246094\n",
      "Epoch 2[5/312] Time:0.709, Train Loss:0.30734094977378845\n",
      "Epoch 2[6/312] Time:8.575, Train Loss:0.41202011704444885\n",
      "Epoch 2[7/312] Time:8.573, Train Loss:0.36180195212364197\n",
      "Epoch 2[8/312] Time:0.706, Train Loss:0.4223627746105194\n",
      "Epoch 2[9/312] Time:8.438, Train Loss:0.44003209471702576\n",
      "Epoch 2[10/312] Time:8.623, Train Loss:0.37851187586784363\n",
      "Epoch 2[11/312] Time:0.707, Train Loss:0.3425105810165405\n",
      "Epoch 2[12/312] Time:8.485, Train Loss:0.39792460203170776\n",
      "Epoch 2[13/312] Time:8.611, Train Loss:0.404143750667572\n",
      "Epoch 2[14/312] Time:0.707, Train Loss:0.31815484166145325\n",
      "Epoch 2[15/312] Time:8.259, Train Loss:0.413587749004364\n",
      "Epoch 2[16/312] Time:8.579, Train Loss:0.3500962257385254\n",
      "Epoch 2[17/312] Time:0.707, Train Loss:0.477114200592041\n",
      "Epoch 2[18/312] Time:8.479, Train Loss:0.35897281765937805\n",
      "Epoch 2[19/312] Time:8.629, Train Loss:0.2916935086250305\n",
      "Epoch 2[20/312] Time:0.711, Train Loss:0.39132359623908997\n",
      "Epoch 2[21/312] Time:8.473, Train Loss:0.41325685381889343\n",
      "Epoch 2[22/312] Time:8.658, Train Loss:0.3124273717403412\n",
      "Epoch 2[23/312] Time:0.708, Train Loss:0.4295367896556854\n",
      "Epoch 2[24/312] Time:8.442, Train Loss:0.3429024815559387\n",
      "Epoch 2[25/312] Time:8.734, Train Loss:0.48872309923171997\n",
      "Epoch 2[26/312] Time:0.706, Train Loss:0.37869611382484436\n",
      "Epoch 2[27/312] Time:8.485, Train Loss:0.4237259328365326\n",
      "Epoch 2[28/312] Time:8.619, Train Loss:0.39215704798698425\n",
      "Epoch 2[29/312] Time:0.707, Train Loss:0.4959997832775116\n",
      "Epoch 2[30/312] Time:8.513, Train Loss:0.4698530435562134\n",
      "Epoch 2[31/312] Time:8.616, Train Loss:0.3965938091278076\n",
      "Epoch 2[32/312] Time:0.706, Train Loss:0.3825002908706665\n",
      "Epoch 2[33/312] Time:8.437, Train Loss:0.3422605097293854\n",
      "Epoch 2[34/312] Time:8.608, Train Loss:0.38400140404701233\n",
      "Epoch 2[35/312] Time:0.706, Train Loss:0.3559957444667816\n",
      "Epoch 2[36/312] Time:8.428, Train Loss:0.3441755473613739\n",
      "Epoch 2[37/312] Time:8.565, Train Loss:0.4241745173931122\n",
      "Epoch 2[38/312] Time:0.708, Train Loss:0.41059616208076477\n",
      "Epoch 2[39/312] Time:8.612, Train Loss:0.3709688186645508\n",
      "Epoch 2[40/312] Time:8.67, Train Loss:0.3703364133834839\n",
      "Epoch 2[41/312] Time:0.706, Train Loss:0.2788154184818268\n",
      "Epoch 2[42/312] Time:8.461, Train Loss:0.41471067070961\n",
      "Epoch 2[43/312] Time:8.68, Train Loss:0.3352726995944977\n",
      "Epoch 2[44/312] Time:0.708, Train Loss:0.3732009530067444\n",
      "Epoch 2[45/312] Time:8.476, Train Loss:0.410717636346817\n",
      "Epoch 2[46/312] Time:8.66, Train Loss:0.3483532667160034\n",
      "Epoch 2[47/312] Time:0.708, Train Loss:0.3134821951389313\n",
      "Epoch 2[48/312] Time:8.352, Train Loss:0.32987624406814575\n",
      "Epoch 2[49/312] Time:8.621, Train Loss:0.2992658317089081\n",
      "Epoch 2[50/312] Time:0.706, Train Loss:0.38297832012176514\n",
      "Epoch 2[51/312] Time:8.469, Train Loss:0.36303213238716125\n",
      "Epoch 2[52/312] Time:8.682, Train Loss:0.3024064302444458\n",
      "Epoch 2[53/312] Time:0.707, Train Loss:0.3545468747615814\n",
      "Epoch 2[54/312] Time:8.522, Train Loss:0.37655046582221985\n",
      "Epoch 2[55/312] Time:8.624, Train Loss:0.2948123812675476\n",
      "Epoch 2[56/312] Time:0.706, Train Loss:0.3726637661457062\n",
      "Epoch 2[57/312] Time:8.54, Train Loss:0.42155882716178894\n",
      "Epoch 2[58/312] Time:8.657, Train Loss:0.30056312680244446\n",
      "Epoch 2[59/312] Time:0.706, Train Loss:0.3761143088340759\n",
      "Epoch 2[60/312] Time:8.416, Train Loss:0.34842345118522644\n",
      "Epoch 2[61/312] Time:8.58, Train Loss:0.43669989705085754\n",
      "Epoch 2[62/312] Time:0.72, Train Loss:0.25545257329940796\n",
      "Epoch 2[63/312] Time:8.359, Train Loss:0.4039430320262909\n",
      "Epoch 2[64/312] Time:8.624, Train Loss:0.3252800405025482\n",
      "Epoch 2[65/312] Time:0.707, Train Loss:0.3658374547958374\n",
      "Epoch 2[66/312] Time:8.434, Train Loss:0.35443052649497986\n",
      "Epoch 2[67/312] Time:8.652, Train Loss:0.35934749245643616\n",
      "Epoch 2[68/312] Time:0.707, Train Loss:0.2848075032234192\n",
      "Epoch 2[69/312] Time:8.482, Train Loss:0.39053165912628174\n",
      "Epoch 2[70/312] Time:8.65, Train Loss:0.3366446793079376\n",
      "Epoch 2[71/312] Time:0.704, Train Loss:0.4761367738246918\n",
      "Epoch 2[72/312] Time:8.567, Train Loss:0.3433702886104584\n",
      "Epoch 2[73/312] Time:8.643, Train Loss:0.3571137487888336\n",
      "Epoch 2[74/312] Time:0.707, Train Loss:0.3282930552959442\n",
      "Epoch 2[75/312] Time:8.467, Train Loss:0.4661579132080078\n",
      "Epoch 2[76/312] Time:8.578, Train Loss:0.4099493622779846\n",
      "Epoch 2[77/312] Time:0.708, Train Loss:0.3806879222393036\n",
      "Epoch 2[78/312] Time:8.434, Train Loss:0.3478657305240631\n",
      "Epoch 2[79/312] Time:8.629, Train Loss:0.3176564574241638\n",
      "Epoch 2[80/312] Time:0.706, Train Loss:0.42642468214035034\n",
      "Epoch 2[81/312] Time:8.461, Train Loss:0.44844716787338257\n",
      "Epoch 2[82/312] Time:8.612, Train Loss:0.36856794357299805\n",
      "Epoch 2[83/312] Time:0.707, Train Loss:0.329694002866745\n",
      "Epoch 2[84/312] Time:8.429, Train Loss:0.2901177406311035\n",
      "Epoch 2[85/312] Time:8.632, Train Loss:0.4261239469051361\n",
      "Epoch 2[86/312] Time:0.736, Train Loss:0.4448148012161255\n",
      "Epoch 2[87/312] Time:8.577, Train Loss:0.3510943353176117\n",
      "Epoch 2[88/312] Time:8.596, Train Loss:0.34131938219070435\n",
      "Epoch 2[89/312] Time:0.711, Train Loss:0.34977227449417114\n",
      "Epoch 2[90/312] Time:8.543, Train Loss:0.31102538108825684\n",
      "Epoch 2[91/312] Time:8.642, Train Loss:0.3036634922027588\n",
      "Epoch 2[92/312] Time:0.718, Train Loss:0.3470757305622101\n",
      "Epoch 2[93/312] Time:8.276, Train Loss:0.34429067373275757\n",
      "Epoch 2[94/312] Time:8.569, Train Loss:0.41694700717926025\n",
      "Epoch 2[95/312] Time:0.707, Train Loss:0.3517184555530548\n",
      "Epoch 2[96/312] Time:8.465, Train Loss:0.3065586984157562\n",
      "Epoch 2[97/312] Time:8.62, Train Loss:0.29787391424179077\n",
      "Epoch 2[98/312] Time:0.708, Train Loss:0.3580824136734009\n",
      "Epoch 2[99/312] Time:8.469, Train Loss:0.4236505329608917\n",
      "Epoch 2[100/312] Time:8.63, Train Loss:0.4085894823074341\n",
      "Epoch 2[101/312] Time:0.708, Train Loss:0.33301660418510437\n",
      "Epoch 2[102/312] Time:8.513, Train Loss:0.3040241599082947\n",
      "Epoch 2[103/312] Time:8.632, Train Loss:0.42317986488342285\n",
      "Epoch 2[104/312] Time:0.707, Train Loss:0.28883904218673706\n",
      "Epoch 2[105/312] Time:8.464, Train Loss:0.4527346193790436\n",
      "Epoch 2[106/312] Time:8.647, Train Loss:0.3306061029434204\n",
      "Epoch 2[107/312] Time:0.708, Train Loss:0.3076724708080292\n",
      "Epoch 2[108/312] Time:8.399, Train Loss:0.27794212102890015\n",
      "Epoch 2[109/312] Time:8.624, Train Loss:0.34967634081840515\n",
      "Epoch 2[110/312] Time:0.708, Train Loss:0.2855210602283478\n",
      "Epoch 2[111/312] Time:8.499, Train Loss:0.35222816467285156\n",
      "Epoch 2[112/312] Time:8.604, Train Loss:0.4366249144077301\n",
      "Epoch 2[113/312] Time:0.707, Train Loss:0.39403387904167175\n",
      "Epoch 2[114/312] Time:8.45, Train Loss:0.4241277575492859\n",
      "Epoch 2[115/312] Time:8.59, Train Loss:0.352617472410202\n",
      "Epoch 2[116/312] Time:0.708, Train Loss:0.41234681010246277\n",
      "Epoch 2[117/312] Time:8.504, Train Loss:0.37242984771728516\n",
      "Epoch 2[118/312] Time:8.692, Train Loss:0.38980624079704285\n",
      "Epoch 2[119/312] Time:0.708, Train Loss:0.391253262758255\n",
      "Epoch 2[120/312] Time:8.476, Train Loss:0.43195995688438416\n",
      "Epoch 2[121/312] Time:8.634, Train Loss:0.4228229224681854\n",
      "Epoch 2[122/312] Time:0.707, Train Loss:0.4432164132595062\n",
      "Epoch 2[123/312] Time:8.494, Train Loss:0.41942140460014343\n",
      "Epoch 2[124/312] Time:8.653, Train Loss:0.36387160420417786\n",
      "Epoch 2[125/312] Time:0.706, Train Loss:0.46973374485969543\n",
      "Epoch 2[126/312] Time:8.379, Train Loss:0.3945251405239105\n",
      "Epoch 2[127/312] Time:8.618, Train Loss:0.484057754278183\n",
      "Epoch 2[128/312] Time:0.707, Train Loss:0.3320852220058441\n",
      "Epoch 2[129/312] Time:8.497, Train Loss:0.3785780072212219\n",
      "Epoch 2[130/312] Time:8.596, Train Loss:0.3806467652320862\n",
      "Epoch 2[131/312] Time:0.705, Train Loss:0.34434470534324646\n",
      "Epoch 2[132/312] Time:8.545, Train Loss:0.4769054055213928\n",
      "Epoch 2[133/312] Time:8.663, Train Loss:0.33348795771598816\n",
      "Epoch 2[134/312] Time:0.706, Train Loss:0.3707379400730133\n",
      "Epoch 2[135/312] Time:8.52, Train Loss:0.3989170491695404\n",
      "Epoch 2[136/312] Time:8.647, Train Loss:0.3578146994113922\n",
      "Epoch 2[137/312] Time:0.707, Train Loss:0.3242656886577606\n",
      "Epoch 2[138/312] Time:8.516, Train Loss:0.4088541865348816\n",
      "Epoch 2[139/312] Time:8.656, Train Loss:0.4645645022392273\n",
      "Epoch 2[140/312] Time:0.706, Train Loss:0.35539326071739197\n",
      "Epoch 2[141/312] Time:8.505, Train Loss:0.2847065329551697\n",
      "Epoch 2[142/312] Time:8.63, Train Loss:0.40943294763565063\n",
      "Epoch 2[143/312] Time:0.708, Train Loss:0.30690300464630127\n",
      "Epoch 2[144/312] Time:8.399, Train Loss:0.3890301585197449\n",
      "Epoch 2[145/312] Time:8.606, Train Loss:0.4038037061691284\n",
      "Epoch 2[146/312] Time:0.707, Train Loss:0.37875136733055115\n",
      "Epoch 2[147/312] Time:8.564, Train Loss:0.36608365178108215\n",
      "Epoch 2[148/312] Time:8.591, Train Loss:0.4244489371776581\n",
      "Epoch 2[149/312] Time:0.709, Train Loss:0.3217695951461792\n",
      "Epoch 2[150/312] Time:8.538, Train Loss:0.347973108291626\n",
      "Epoch 2[151/312] Time:8.641, Train Loss:0.39742323756217957\n",
      "Epoch 2[152/312] Time:0.708, Train Loss:0.2994481325149536\n",
      "Epoch 2[153/312] Time:8.461, Train Loss:0.4042842388153076\n",
      "Epoch 2[154/312] Time:8.641, Train Loss:0.2897946834564209\n",
      "Epoch 2[155/312] Time:0.708, Train Loss:0.4196895956993103\n",
      "Epoch 2[156/312] Time:8.535, Train Loss:0.37109455466270447\n",
      "Epoch 2[157/312] Time:8.598, Train Loss:0.43904924392700195\n",
      "Epoch 2[158/312] Time:0.708, Train Loss:0.3250596523284912\n",
      "Epoch 2[159/312] Time:8.451, Train Loss:0.3935486078262329\n",
      "Epoch 2[160/312] Time:8.58, Train Loss:0.3416273891925812\n",
      "Epoch 2[161/312] Time:0.709, Train Loss:0.3846699595451355\n",
      "Epoch 2[162/312] Time:8.6, Train Loss:0.3668990731239319\n",
      "Epoch 2[163/312] Time:8.637, Train Loss:0.32042914628982544\n",
      "Epoch 2[164/312] Time:0.708, Train Loss:0.3359123468399048\n",
      "Epoch 2[165/312] Time:8.436, Train Loss:0.33325645327568054\n",
      "Epoch 2[166/312] Time:8.685, Train Loss:0.32494378089904785\n",
      "Epoch 2[167/312] Time:0.707, Train Loss:0.30513957142829895\n",
      "Epoch 2[168/312] Time:8.553, Train Loss:0.34862932562828064\n",
      "Epoch 2[169/312] Time:8.704, Train Loss:0.35876908898353577\n",
      "Epoch 2[170/312] Time:0.708, Train Loss:0.4104248285293579\n",
      "Epoch 2[171/312] Time:8.498, Train Loss:0.36413222551345825\n",
      "Epoch 2[172/312] Time:8.547, Train Loss:0.37502673268318176\n",
      "Epoch 2[173/312] Time:0.706, Train Loss:0.3426786959171295\n",
      "Epoch 2[174/312] Time:8.492, Train Loss:0.3624565601348877\n",
      "Epoch 2[175/312] Time:8.537, Train Loss:0.36633145809173584\n",
      "Epoch 2[176/312] Time:0.707, Train Loss:0.39845478534698486\n",
      "Epoch 2[177/312] Time:8.596, Train Loss:0.34930184483528137\n",
      "Epoch 2[178/312] Time:8.619, Train Loss:0.2787591218948364\n",
      "Epoch 2[179/312] Time:0.709, Train Loss:0.41477108001708984\n",
      "Epoch 2[180/312] Time:8.535, Train Loss:0.3757585287094116\n",
      "Epoch 2[181/312] Time:8.636, Train Loss:0.3826148211956024\n",
      "Epoch 2[182/312] Time:0.711, Train Loss:0.388685941696167\n",
      "Epoch 2[183/312] Time:8.492, Train Loss:0.34433865547180176\n",
      "Epoch 2[184/312] Time:8.594, Train Loss:0.35942864418029785\n",
      "Epoch 2[185/312] Time:0.719, Train Loss:0.4125528335571289\n",
      "Epoch 2[186/312] Time:8.388, Train Loss:0.35872483253479004\n",
      "Epoch 2[187/312] Time:8.559, Train Loss:0.3594030737876892\n",
      "Epoch 2[188/312] Time:0.699, Train Loss:0.4443875253200531\n",
      "Epoch 2[189/312] Time:8.473, Train Loss:0.3648756742477417\n",
      "Epoch 2[190/312] Time:8.657, Train Loss:0.38387686014175415\n",
      "Epoch 2[191/312] Time:0.705, Train Loss:0.37283313274383545\n",
      "Epoch 2[192/312] Time:8.554, Train Loss:0.2839825749397278\n",
      "Epoch 2[193/312] Time:8.644, Train Loss:0.32175251841545105\n",
      "Epoch 2[194/312] Time:0.706, Train Loss:0.4257003664970398\n",
      "Epoch 2[195/312] Time:8.51, Train Loss:0.3701876997947693\n",
      "Epoch 2[196/312] Time:8.65, Train Loss:0.30174487829208374\n",
      "Epoch 2[197/312] Time:0.694, Train Loss:0.4414927661418915\n",
      "Epoch 2[198/312] Time:8.493, Train Loss:0.3219708800315857\n",
      "Epoch 2[199/312] Time:8.531, Train Loss:0.32424578070640564\n",
      "Epoch 2[200/312] Time:0.706, Train Loss:0.4221344590187073\n",
      "Epoch 2[201/312] Time:8.483, Train Loss:0.2981016933917999\n",
      "Epoch 2[202/312] Time:8.707, Train Loss:0.3293061852455139\n",
      "Epoch 2[203/312] Time:0.708, Train Loss:0.32791754603385925\n",
      "Epoch 2[204/312] Time:8.484, Train Loss:0.34981870651245117\n",
      "Epoch 2[205/312] Time:8.668, Train Loss:0.4340285062789917\n",
      "Epoch 2[206/312] Time:0.707, Train Loss:0.35678791999816895\n",
      "Epoch 2[207/312] Time:8.507, Train Loss:0.3106851577758789\n",
      "Epoch 2[208/312] Time:8.633, Train Loss:0.29172536730766296\n",
      "Epoch 2[209/312] Time:0.706, Train Loss:0.45499756932258606\n",
      "Epoch 2[210/312] Time:8.514, Train Loss:0.42047786712646484\n",
      "Epoch 2[211/312] Time:8.521, Train Loss:0.36968380212783813\n",
      "Epoch 2[212/312] Time:0.707, Train Loss:0.3496766686439514\n",
      "Epoch 2[213/312] Time:8.497, Train Loss:0.5074567198753357\n",
      "Epoch 2[214/312] Time:8.581, Train Loss:0.38794276118278503\n",
      "Epoch 2[215/312] Time:0.708, Train Loss:0.29979318380355835\n",
      "Epoch 2[216/312] Time:8.492, Train Loss:0.3115192949771881\n",
      "Epoch 2[217/312] Time:8.661, Train Loss:0.44824621081352234\n",
      "Epoch 2[218/312] Time:0.705, Train Loss:0.35409197211265564\n",
      "Epoch 2[219/312] Time:8.491, Train Loss:0.35852667689323425\n",
      "Epoch 2[220/312] Time:8.619, Train Loss:0.3821442723274231\n",
      "Epoch 2[221/312] Time:0.707, Train Loss:0.30638670921325684\n",
      "Epoch 2[222/312] Time:8.564, Train Loss:0.42211657762527466\n",
      "Epoch 2[223/312] Time:8.632, Train Loss:0.35153740644454956\n",
      "Epoch 2[224/312] Time:0.714, Train Loss:0.3624311089515686\n",
      "Epoch 2[225/312] Time:8.441, Train Loss:0.3706440031528473\n",
      "Epoch 2[226/312] Time:8.595, Train Loss:0.3860692083835602\n",
      "Epoch 2[227/312] Time:0.705, Train Loss:0.36391255259513855\n",
      "Epoch 2[228/312] Time:8.468, Train Loss:0.3711603283882141\n",
      "Epoch 2[229/312] Time:8.674, Train Loss:0.35603848099708557\n",
      "Epoch 2[230/312] Time:0.707, Train Loss:0.3255690336227417\n",
      "Epoch 2[231/312] Time:8.543, Train Loss:0.37537992000579834\n",
      "Epoch 2[232/312] Time:8.639, Train Loss:0.3801293969154358\n",
      "Epoch 2[233/312] Time:0.712, Train Loss:0.3323676586151123\n",
      "Epoch 2[234/312] Time:8.484, Train Loss:0.3862587809562683\n",
      "Epoch 2[235/312] Time:8.631, Train Loss:0.3411880135536194\n",
      "Epoch 2[236/312] Time:0.708, Train Loss:0.48362669348716736\n",
      "Epoch 2[237/312] Time:8.548, Train Loss:0.2586401402950287\n",
      "Epoch 2[238/312] Time:8.528, Train Loss:0.3447636663913727\n",
      "Epoch 2[239/312] Time:0.706, Train Loss:0.2438252717256546\n",
      "Epoch 2[240/312] Time:8.418, Train Loss:0.3910353481769562\n",
      "Epoch 2[241/312] Time:8.603, Train Loss:0.30144619941711426\n",
      "Epoch 2[242/312] Time:0.706, Train Loss:0.4432164132595062\n",
      "Epoch 2[243/312] Time:8.543, Train Loss:0.2852252721786499\n",
      "Epoch 2[244/312] Time:8.65, Train Loss:0.36789339780807495\n",
      "Epoch 2[245/312] Time:0.708, Train Loss:0.2950282394886017\n",
      "Epoch 2[246/312] Time:8.475, Train Loss:0.3476332128047943\n",
      "Epoch 2[247/312] Time:8.658, Train Loss:0.33782151341438293\n",
      "Epoch 2[248/312] Time:0.716, Train Loss:0.46228402853012085\n",
      "Epoch 2[249/312] Time:8.484, Train Loss:0.43359947204589844\n",
      "Epoch 2[250/312] Time:8.642, Train Loss:0.428038626909256\n",
      "Epoch 2[251/312] Time:0.707, Train Loss:0.36896267533302307\n",
      "Epoch 2[252/312] Time:8.491, Train Loss:0.3239256739616394\n",
      "Epoch 2[253/312] Time:8.606, Train Loss:0.2713022232055664\n",
      "Epoch 2[254/312] Time:0.705, Train Loss:0.33750292658805847\n",
      "Epoch 2[255/312] Time:8.425, Train Loss:0.38405993580818176\n",
      "Epoch 2[256/312] Time:8.629, Train Loss:0.2866869568824768\n",
      "Epoch 2[257/312] Time:0.698, Train Loss:0.42081403732299805\n",
      "Epoch 2[258/312] Time:8.441, Train Loss:0.35629764199256897\n",
      "Epoch 2[259/312] Time:8.643, Train Loss:0.30553489923477173\n",
      "Epoch 2[260/312] Time:0.708, Train Loss:0.329812228679657\n",
      "Epoch 2[261/312] Time:8.43, Train Loss:0.33840858936309814\n",
      "Epoch 2[262/312] Time:8.654, Train Loss:0.3056889474391937\n",
      "Epoch 2[263/312] Time:0.708, Train Loss:0.4377192556858063\n",
      "Epoch 2[264/312] Time:8.482, Train Loss:0.3455633223056793\n",
      "Epoch 2[265/312] Time:8.596, Train Loss:0.33983179926872253\n",
      "Epoch 2[266/312] Time:0.707, Train Loss:0.3795170783996582\n",
      "Epoch 2[267/312] Time:8.374, Train Loss:0.4012600779533386\n",
      "Epoch 2[268/312] Time:8.572, Train Loss:0.376522958278656\n",
      "Epoch 2[269/312] Time:0.707, Train Loss:0.34366512298583984\n",
      "Epoch 2[270/312] Time:8.545, Train Loss:0.43590882420539856\n",
      "Epoch 2[271/312] Time:8.58, Train Loss:0.3443576693534851\n",
      "Epoch 2[272/312] Time:0.708, Train Loss:0.37865951657295227\n",
      "Epoch 2[273/312] Time:8.534, Train Loss:0.2696589529514313\n",
      "Epoch 2[274/312] Time:8.62, Train Loss:0.3125007450580597\n",
      "Epoch 2[275/312] Time:0.705, Train Loss:0.3429294526576996\n",
      "Epoch 2[276/312] Time:8.469, Train Loss:0.33166173100471497\n",
      "Epoch 2[277/312] Time:8.712, Train Loss:0.33628717064857483\n",
      "Epoch 2[278/312] Time:0.707, Train Loss:0.3592228591442108\n",
      "Epoch 2[279/312] Time:8.496, Train Loss:0.381257027387619\n",
      "Epoch 2[280/312] Time:8.664, Train Loss:0.4586046040058136\n",
      "Epoch 2[281/312] Time:0.707, Train Loss:0.2935164272785187\n",
      "Epoch 2[282/312] Time:8.302, Train Loss:0.37761276960372925\n",
      "Epoch 2[283/312] Time:8.642, Train Loss:0.30713963508605957\n",
      "Epoch 2[284/312] Time:0.717, Train Loss:0.3838389217853546\n",
      "Epoch 2[285/312] Time:8.434, Train Loss:0.32405030727386475\n",
      "Epoch 2[286/312] Time:8.648, Train Loss:0.3064737319946289\n",
      "Epoch 2[287/312] Time:0.707, Train Loss:0.37209928035736084\n",
      "Epoch 2[288/312] Time:8.508, Train Loss:0.3891827166080475\n",
      "Epoch 2[289/312] Time:8.633, Train Loss:0.32953721284866333\n",
      "Epoch 2[290/312] Time:0.708, Train Loss:0.4503203332424164\n",
      "Epoch 2[291/312] Time:8.523, Train Loss:0.2864336669445038\n",
      "Epoch 2[292/312] Time:8.663, Train Loss:0.34004202485084534\n",
      "Epoch 2[293/312] Time:0.706, Train Loss:0.3496580123901367\n",
      "Epoch 2[294/312] Time:8.531, Train Loss:0.27977484464645386\n",
      "Epoch 2[295/312] Time:8.615, Train Loss:0.42217719554901123\n",
      "Epoch 2[296/312] Time:0.708, Train Loss:0.30940115451812744\n",
      "Epoch 2[297/312] Time:8.455, Train Loss:0.3614606261253357\n",
      "Epoch 2[298/312] Time:8.564, Train Loss:0.37084126472473145\n",
      "Epoch 2[299/312] Time:0.705, Train Loss:0.3543858230113983\n",
      "Epoch 2[300/312] Time:8.487, Train Loss:0.3508736789226532\n",
      "Epoch 2[301/312] Time:8.625, Train Loss:0.33842653036117554\n",
      "Epoch 2[302/312] Time:0.708, Train Loss:0.47030869126319885\n",
      "Epoch 2[303/312] Time:8.472, Train Loss:0.33751600980758667\n",
      "Epoch 2[304/312] Time:8.621, Train Loss:0.3493068814277649\n",
      "Epoch 3[3/312] Time:8.181, Train Loss:0.38063403964042664\n",
      "Epoch 3[4/312] Time:8.508, Train Loss:0.4274612069129944\n",
      "Epoch 3[5/312] Time:8.447, Train Loss:0.3597501516342163\n",
      "Epoch 3[6/312] Time:8.588, Train Loss:0.3420126736164093\n",
      "Epoch 3[7/312] Time:0.707, Train Loss:0.3374250531196594\n",
      "Epoch 3[8/312] Time:8.495, Train Loss:0.3517599105834961\n",
      "Epoch 3[9/312] Time:8.637, Train Loss:0.41552096605300903\n",
      "Epoch 3[10/312] Time:0.706, Train Loss:0.350944459438324\n",
      "Epoch 3[11/312] Time:8.413, Train Loss:0.3808176517486572\n",
      "Epoch 3[12/312] Time:8.644, Train Loss:0.337695449590683\n",
      "Epoch 3[13/312] Time:0.706, Train Loss:0.34413740038871765\n",
      "Epoch 3[14/312] Time:8.342, Train Loss:0.3317580819129944\n",
      "Epoch 3[15/312] Time:8.594, Train Loss:0.3806632161140442\n",
      "Epoch 3[16/312] Time:0.704, Train Loss:0.39102450013160706\n",
      "Epoch 3[17/312] Time:8.386, Train Loss:0.2931465208530426\n",
      "Epoch 3[18/312] Time:8.654, Train Loss:0.3522006869316101\n",
      "Epoch 3[19/312] Time:0.706, Train Loss:0.32906875014305115\n",
      "Epoch 3[20/312] Time:8.492, Train Loss:0.29080867767333984\n",
      "Epoch 3[21/312] Time:8.593, Train Loss:0.35564398765563965\n",
      "Epoch 3[22/312] Time:0.706, Train Loss:0.30318683385849\n",
      "Epoch 3[23/312] Time:8.352, Train Loss:0.243223175406456\n",
      "Epoch 3[24/312] Time:8.664, Train Loss:0.34174028038978577\n",
      "Epoch 3[25/312] Time:0.706, Train Loss:0.4166012406349182\n",
      "Epoch 3[26/312] Time:8.476, Train Loss:0.5260366201400757\n",
      "Epoch 3[27/312] Time:8.601, Train Loss:0.33393630385398865\n",
      "Epoch 3[28/312] Time:0.71, Train Loss:0.37413790822029114\n",
      "Epoch 3[29/312] Time:8.478, Train Loss:0.348952978849411\n",
      "Epoch 3[30/312] Time:8.63, Train Loss:0.3579525351524353\n",
      "Epoch 3[31/312] Time:0.705, Train Loss:0.374764621257782\n",
      "Epoch 3[32/312] Time:8.503, Train Loss:0.31591638922691345\n",
      "Epoch 3[33/312] Time:8.629, Train Loss:0.3464067578315735\n",
      "Epoch 3[34/312] Time:0.707, Train Loss:0.36436736583709717\n",
      "Epoch 3[35/312] Time:8.577, Train Loss:0.32902151346206665\n",
      "Epoch 3[36/312] Time:8.62, Train Loss:0.3795800507068634\n",
      "Epoch 3[37/312] Time:0.707, Train Loss:0.3749895989894867\n",
      "Epoch 3[38/312] Time:8.432, Train Loss:0.3953377902507782\n",
      "Epoch 3[39/312] Time:8.593, Train Loss:0.4335487484931946\n",
      "Epoch 3[40/312] Time:0.707, Train Loss:0.37080076336860657\n",
      "Epoch 3[41/312] Time:8.479, Train Loss:0.3581772446632385\n",
      "Epoch 3[42/312] Time:8.591, Train Loss:0.2740083932876587\n",
      "Epoch 3[43/312] Time:0.707, Train Loss:0.33896350860595703\n",
      "Epoch 3[44/312] Time:8.283, Train Loss:0.37600770592689514\n",
      "Epoch 3[45/312] Time:8.645, Train Loss:0.33913519978523254\n",
      "Epoch 3[46/312] Time:0.704, Train Loss:0.28430965542793274\n",
      "Epoch 3[47/312] Time:8.502, Train Loss:0.2706678509712219\n",
      "Epoch 3[48/312] Time:8.564, Train Loss:0.39257746934890747\n",
      "Epoch 3[49/312] Time:0.707, Train Loss:0.34929516911506653\n",
      "Epoch 3[50/312] Time:8.488, Train Loss:0.3502542972564697\n",
      "Epoch 3[51/312] Time:8.655, Train Loss:0.43218469619750977\n",
      "Epoch 3[52/312] Time:0.707, Train Loss:0.4010339677333832\n",
      "Epoch 3[53/312] Time:8.523, Train Loss:0.3362407088279724\n",
      "Epoch 3[54/312] Time:8.648, Train Loss:0.36985114216804504\n",
      "Epoch 3[55/312] Time:0.709, Train Loss:0.2834711968898773\n",
      "Epoch 3[56/312] Time:8.43, Train Loss:0.4466172456741333\n",
      "Epoch 3[57/312] Time:8.616, Train Loss:0.37364622950553894\n",
      "Epoch 3[58/312] Time:0.705, Train Loss:0.3576330840587616\n",
      "Epoch 3[59/312] Time:8.392, Train Loss:0.35376980900764465\n",
      "Epoch 3[60/312] Time:8.688, Train Loss:0.38524478673934937\n",
      "Epoch 3[61/312] Time:0.706, Train Loss:0.32695716619491577\n",
      "Epoch 3[62/312] Time:8.43, Train Loss:0.35985878109931946\n",
      "Epoch 3[63/312] Time:8.641, Train Loss:0.36449766159057617\n",
      "Epoch 3[64/312] Time:0.708, Train Loss:0.3361571133136749\n",
      "Epoch 3[65/312] Time:8.479, Train Loss:0.2774927616119385\n",
      "Epoch 3[66/312] Time:8.708, Train Loss:0.3279486894607544\n",
      "Epoch 3[67/312] Time:0.707, Train Loss:0.3345508873462677\n",
      "Epoch 3[68/312] Time:8.477, Train Loss:0.42771899700164795\n",
      "Epoch 3[69/312] Time:8.606, Train Loss:0.37254834175109863\n",
      "Epoch 3[70/312] Time:0.708, Train Loss:0.3478333055973053\n",
      "Epoch 3[71/312] Time:8.516, Train Loss:0.27147161960601807\n",
      "Epoch 3[72/312] Time:8.58, Train Loss:0.33334970474243164\n",
      "Epoch 3[73/312] Time:0.706, Train Loss:0.36917608976364136\n",
      "Epoch 3[74/312] Time:8.474, Train Loss:0.3542943596839905\n",
      "Epoch 3[75/312] Time:8.627, Train Loss:0.3904505968093872\n",
      "Epoch 3[76/312] Time:0.707, Train Loss:0.4283730983734131\n",
      "Epoch 3[77/312] Time:8.466, Train Loss:0.33261120319366455\n",
      "Epoch 3[78/312] Time:8.671, Train Loss:0.30520206689834595\n",
      "Epoch 3[79/312] Time:0.706, Train Loss:0.3304780125617981\n",
      "Epoch 3[80/312] Time:8.488, Train Loss:0.39913690090179443\n",
      "Epoch 3[81/312] Time:8.647, Train Loss:0.350600004196167\n",
      "Epoch 3[82/312] Time:0.706, Train Loss:0.3501236140727997\n",
      "Epoch 3[83/312] Time:8.486, Train Loss:0.3296719491481781\n",
      "Epoch 3[84/312] Time:8.572, Train Loss:0.2852005064487457\n",
      "Epoch 3[85/312] Time:0.708, Train Loss:0.3400678038597107\n",
      "Epoch 3[86/312] Time:8.502, Train Loss:0.4008861482143402\n",
      "Epoch 3[87/312] Time:8.579, Train Loss:0.3006155490875244\n",
      "Epoch 3[88/312] Time:0.707, Train Loss:0.3560503125190735\n",
      "Epoch 3[89/312] Time:8.514, Train Loss:0.34261974692344666\n",
      "Epoch 3[90/312] Time:8.585, Train Loss:0.3607251048088074\n",
      "Epoch 3[91/312] Time:0.707, Train Loss:0.34767523407936096\n",
      "Epoch 3[92/312] Time:8.455, Train Loss:0.3748636841773987\n",
      "Epoch 3[93/312] Time:8.644, Train Loss:0.3660888075828552\n",
      "Epoch 3[94/312] Time:0.705, Train Loss:0.3757833242416382\n",
      "Epoch 3[95/312] Time:8.395, Train Loss:0.4091832637786865\n",
      "Epoch 3[96/312] Time:8.666, Train Loss:0.3297281265258789\n",
      "Epoch 3[97/312] Time:0.705, Train Loss:0.3449068069458008\n",
      "Epoch 3[98/312] Time:8.482, Train Loss:0.3284884989261627\n",
      "Epoch 3[99/312] Time:8.604, Train Loss:0.399788498878479\n",
      "Epoch 3[100/312] Time:0.713, Train Loss:0.2530567944049835\n",
      "Epoch 3[101/312] Time:8.43, Train Loss:0.36294567584991455\n",
      "Epoch 3[102/312] Time:8.654, Train Loss:0.3994123339653015\n",
      "Epoch 3[103/312] Time:0.705, Train Loss:0.3913268744945526\n",
      "Epoch 3[104/312] Time:8.389, Train Loss:0.3651678264141083\n",
      "Epoch 3[105/312] Time:8.627, Train Loss:0.29542845487594604\n",
      "Epoch 3[106/312] Time:0.705, Train Loss:0.36947813630104065\n",
      "Epoch 3[107/312] Time:8.502, Train Loss:0.3497450053691864\n",
      "Epoch 3[108/312] Time:8.573, Train Loss:0.3757663369178772\n",
      "Epoch 3[109/312] Time:0.708, Train Loss:0.34014222025871277\n",
      "Epoch 3[110/312] Time:8.402, Train Loss:0.27710142731666565\n",
      "Epoch 3[111/312] Time:8.587, Train Loss:0.27747249603271484\n",
      "Epoch 3[112/312] Time:0.707, Train Loss:0.3478904664516449\n",
      "Epoch 3[113/312] Time:8.45, Train Loss:0.3634328544139862\n",
      "Epoch 3[114/312] Time:8.636, Train Loss:0.3539619743824005\n",
      "Epoch 3[115/312] Time:0.718, Train Loss:0.34556856751441956\n",
      "Epoch 3[116/312] Time:8.42, Train Loss:0.351319283246994\n",
      "Epoch 3[117/312] Time:8.582, Train Loss:0.34270331263542175\n",
      "Epoch 3[118/312] Time:0.705, Train Loss:0.3770740330219269\n",
      "Epoch 3[119/312] Time:8.512, Train Loss:0.448538601398468\n",
      "Epoch 3[120/312] Time:8.668, Train Loss:0.3249453604221344\n",
      "Epoch 3[121/312] Time:0.705, Train Loss:0.27473577857017517\n",
      "Epoch 3[122/312] Time:8.429, Train Loss:0.3391681909561157\n",
      "Epoch 3[123/312] Time:8.74, Train Loss:0.3507375717163086\n",
      "Epoch 3[124/312] Time:0.706, Train Loss:0.3985828459262848\n",
      "Epoch 3[125/312] Time:8.568, Train Loss:0.3876134157180786\n",
      "Epoch 3[126/312] Time:8.663, Train Loss:0.34149304032325745\n",
      "Epoch 3[127/312] Time:0.704, Train Loss:0.4664192199707031\n",
      "Epoch 3[128/312] Time:8.545, Train Loss:0.37223467230796814\n",
      "Epoch 3[129/312] Time:8.66, Train Loss:0.36741286516189575\n",
      "Epoch 3[130/312] Time:0.704, Train Loss:0.3011570870876312\n",
      "Epoch 3[131/312] Time:8.433, Train Loss:0.35360389947891235\n",
      "Epoch 3[132/312] Time:8.673, Train Loss:0.29776740074157715\n",
      "Epoch 3[133/312] Time:0.706, Train Loss:0.26091310381889343\n",
      "Epoch 3[134/312] Time:8.493, Train Loss:0.3258732855319977\n",
      "Epoch 3[135/312] Time:8.639, Train Loss:0.33397603034973145\n",
      "Epoch 3[136/312] Time:0.707, Train Loss:0.3336833715438843\n",
      "Epoch 3[137/312] Time:8.415, Train Loss:0.2803899645805359\n",
      "Epoch 3[138/312] Time:8.669, Train Loss:0.33041128516197205\n",
      "Epoch 3[139/312] Time:0.723, Train Loss:0.40994805097579956\n",
      "Epoch 3[140/312] Time:8.329, Train Loss:0.3949509561061859\n",
      "Epoch 3[141/312] Time:8.608, Train Loss:0.3780515491962433\n",
      "Epoch 3[142/312] Time:0.707, Train Loss:0.3185242712497711\n",
      "Epoch 3[143/312] Time:8.462, Train Loss:0.28923705220222473\n",
      "Epoch 3[144/312] Time:8.62, Train Loss:0.39960286021232605\n",
      "Epoch 3[145/312] Time:0.706, Train Loss:0.3657357394695282\n",
      "Epoch 3[146/312] Time:8.46, Train Loss:0.3157089650630951\n",
      "Epoch 3[147/312] Time:8.62, Train Loss:0.3117155134677887\n",
      "Epoch 3[148/312] Time:0.71, Train Loss:0.40494996309280396\n",
      "Epoch 3[149/312] Time:8.53, Train Loss:0.31789636611938477\n",
      "Epoch 3[150/312] Time:8.684, Train Loss:0.3197779357433319\n",
      "Epoch 3[151/312] Time:0.705, Train Loss:0.28517359495162964\n",
      "Epoch 3[152/312] Time:8.417, Train Loss:0.3826221525669098\n",
      "Epoch 3[153/312] Time:8.668, Train Loss:0.3042446970939636\n",
      "Epoch 3[154/312] Time:0.707, Train Loss:0.3991542160511017\n",
      "Epoch 3[155/312] Time:8.408, Train Loss:0.411231130361557\n",
      "Epoch 3[156/312] Time:8.673, Train Loss:0.4018505811691284\n",
      "Epoch 3[157/312] Time:0.706, Train Loss:0.3219083845615387\n",
      "Epoch 3[158/312] Time:8.388, Train Loss:0.3643614947795868\n",
      "Epoch 3[159/312] Time:8.649, Train Loss:0.31319135427474976\n",
      "Epoch 3[160/312] Time:0.709, Train Loss:0.33398327231407166\n",
      "Epoch 3[161/312] Time:8.425, Train Loss:0.3597619831562042\n",
      "Epoch 3[162/312] Time:8.677, Train Loss:0.27474868297576904\n",
      "Epoch 3[163/312] Time:0.706, Train Loss:0.32832127809524536\n",
      "Epoch 3[164/312] Time:8.453, Train Loss:0.2943468391895294\n",
      "Epoch 3[165/312] Time:8.592, Train Loss:0.284548819065094\n",
      "Epoch 3[166/312] Time:0.709, Train Loss:0.373893141746521\n",
      "Epoch 3[167/312] Time:8.433, Train Loss:0.2873287498950958\n",
      "Epoch 3[168/312] Time:8.653, Train Loss:0.30292388796806335\n",
      "Epoch 3[169/312] Time:0.705, Train Loss:0.2957307696342468\n",
      "Epoch 3[170/312] Time:8.544, Train Loss:0.4326745867729187\n",
      "Epoch 3[171/312] Time:8.664, Train Loss:0.3319735825061798\n",
      "Epoch 3[172/312] Time:0.707, Train Loss:0.35273054242134094\n",
      "Epoch 3[173/312] Time:8.547, Train Loss:0.29848334193229675\n",
      "Epoch 3[174/312] Time:8.636, Train Loss:0.2553636431694031\n",
      "Epoch 3[175/312] Time:0.706, Train Loss:0.2701742351055145\n",
      "Epoch 3[176/312] Time:8.553, Train Loss:0.4145549535751343\n",
      "Epoch 3[177/312] Time:8.575, Train Loss:0.39704909920692444\n",
      "Epoch 3[178/312] Time:0.709, Train Loss:0.3631756603717804\n",
      "Epoch 3[179/312] Time:8.569, Train Loss:0.3125748634338379\n",
      "Epoch 3[180/312] Time:8.656, Train Loss:0.32869234681129456\n",
      "Epoch 3[181/312] Time:0.707, Train Loss:0.39768722653388977\n",
      "Epoch 3[182/312] Time:8.374, Train Loss:0.44747981429100037\n",
      "Epoch 3[183/312] Time:8.634, Train Loss:0.33632510900497437\n",
      "Epoch 3[184/312] Time:0.711, Train Loss:0.29340627789497375\n",
      "Epoch 3[185/312] Time:8.404, Train Loss:0.3508487343788147\n",
      "Epoch 3[186/312] Time:8.674, Train Loss:0.3274346888065338\n",
      "Epoch 3[187/312] Time:0.706, Train Loss:0.4107670783996582\n",
      "Epoch 3[188/312] Time:8.507, Train Loss:0.30256447196006775\n",
      "Epoch 3[189/312] Time:8.634, Train Loss:0.4112662076950073\n",
      "Epoch 3[190/312] Time:0.706, Train Loss:0.3404519557952881\n",
      "Epoch 3[191/312] Time:8.496, Train Loss:0.41428282856941223\n",
      "Epoch 3[192/312] Time:8.653, Train Loss:0.34674781560897827\n",
      "Epoch 3[193/312] Time:0.705, Train Loss:0.3681546747684479\n",
      "Epoch 3[194/312] Time:8.537, Train Loss:0.3984776735305786\n",
      "Epoch 3[195/312] Time:8.635, Train Loss:0.3524015545845032\n",
      "Epoch 3[196/312] Time:0.705, Train Loss:0.3386259973049164\n",
      "Epoch 3[197/312] Time:8.449, Train Loss:0.3066900670528412\n",
      "Epoch 3[198/312] Time:8.63, Train Loss:0.2965250015258789\n",
      "Epoch 3[199/312] Time:0.705, Train Loss:0.37451788783073425\n",
      "Epoch 3[200/312] Time:8.516, Train Loss:0.3794768750667572\n",
      "Epoch 3[201/312] Time:8.693, Train Loss:0.3580554127693176\n",
      "Epoch 3[202/312] Time:0.705, Train Loss:0.2781968116760254\n",
      "Epoch 3[203/312] Time:8.472, Train Loss:0.3226943910121918\n",
      "Epoch 3[204/312] Time:8.68, Train Loss:0.23481914401054382\n",
      "Epoch 3[205/312] Time:0.707, Train Loss:0.3725050985813141\n",
      "Epoch 3[206/312] Time:8.533, Train Loss:0.37295079231262207\n",
      "Epoch 3[207/312] Time:8.628, Train Loss:0.3973870277404785\n",
      "Epoch 3[208/312] Time:0.706, Train Loss:0.3577411472797394\n",
      "Epoch 3[209/312] Time:8.489, Train Loss:0.46749401092529297\n",
      "Epoch 3[210/312] Time:8.645, Train Loss:0.30206868052482605\n",
      "Epoch 3[211/312] Time:0.716, Train Loss:0.485664427280426\n",
      "Epoch 3[212/312] Time:8.512, Train Loss:0.3532003164291382\n",
      "Epoch 3[213/312] Time:8.695, Train Loss:0.32156872749328613\n",
      "Epoch 3[214/312] Time:0.705, Train Loss:0.3084530234336853\n",
      "Epoch 3[215/312] Time:8.501, Train Loss:0.37506502866744995\n",
      "Epoch 3[216/312] Time:8.408, Train Loss:0.3227969706058502\n",
      "Epoch 3[217/312] Time:0.705, Train Loss:0.38258132338523865\n",
      "Epoch 3[218/312] Time:8.474, Train Loss:0.22452913224697113\n",
      "Epoch 3[219/312] Time:8.658, Train Loss:0.28477367758750916\n",
      "Epoch 3[220/312] Time:0.71, Train Loss:0.29050493240356445\n",
      "Epoch 3[221/312] Time:8.523, Train Loss:0.4152081608772278\n",
      "Epoch 3[222/312] Time:8.602, Train Loss:0.28113576769828796\n",
      "Epoch 3[223/312] Time:0.712, Train Loss:0.308061420917511\n",
      "Epoch 3[224/312] Time:8.514, Train Loss:0.3239826560020447\n",
      "Epoch 3[225/312] Time:8.651, Train Loss:0.34095561504364014\n",
      "Epoch 3[226/312] Time:0.709, Train Loss:0.3389408588409424\n",
      "Epoch 3[227/312] Time:8.516, Train Loss:0.37046563625335693\n",
      "Epoch 3[228/312] Time:8.62, Train Loss:0.3600861728191376\n",
      "Epoch 3[229/312] Time:0.705, Train Loss:0.41473931074142456\n",
      "Epoch 3[230/312] Time:8.513, Train Loss:0.3534105718135834\n",
      "Epoch 3[231/312] Time:8.524, Train Loss:0.3687751293182373\n",
      "Epoch 3[232/312] Time:0.706, Train Loss:0.32513245940208435\n",
      "Epoch 3[233/312] Time:8.375, Train Loss:0.4229665696620941\n",
      "Epoch 3[234/312] Time:8.681, Train Loss:0.3616953492164612\n",
      "Epoch 3[235/312] Time:0.705, Train Loss:0.3203791677951813\n",
      "Epoch 3[236/312] Time:8.536, Train Loss:0.37219974398612976\n",
      "Epoch 3[237/312] Time:8.665, Train Loss:0.29024776816368103\n",
      "Epoch 3[238/312] Time:0.705, Train Loss:0.2792970836162567\n",
      "Epoch 3[239/312] Time:8.467, Train Loss:0.2784554362297058\n",
      "Epoch 3[240/312] Time:8.639, Train Loss:0.43048912286758423\n",
      "Epoch 3[241/312] Time:0.707, Train Loss:0.37828052043914795\n",
      "Epoch 3[242/312] Time:8.68, Train Loss:0.34518787264823914\n",
      "Epoch 3[243/312] Time:8.645, Train Loss:0.3668084740638733\n",
      "Epoch 3[244/312] Time:0.706, Train Loss:0.3210741877555847\n",
      "Epoch 3[245/312] Time:8.5, Train Loss:0.3040120303630829\n",
      "Epoch 3[246/312] Time:8.675, Train Loss:0.2920578718185425\n",
      "Epoch 3[247/312] Time:0.705, Train Loss:0.3520834445953369\n",
      "Epoch 3[248/312] Time:8.528, Train Loss:0.3408215343952179\n",
      "Epoch 3[249/312] Time:8.64, Train Loss:0.317802369594574\n",
      "Epoch 3[250/312] Time:0.706, Train Loss:0.33545824885368347\n",
      "Epoch 3[251/312] Time:8.576, Train Loss:0.34955742955207825\n",
      "Epoch 3[252/312] Time:8.629, Train Loss:0.30099037289619446\n",
      "Epoch 3[253/312] Time:0.709, Train Loss:0.3879672586917877\n",
      "Epoch 3[254/312] Time:8.533, Train Loss:0.31967565417289734\n",
      "Epoch 3[255/312] Time:8.675, Train Loss:0.45884788036346436\n",
      "Epoch 3[256/312] Time:0.706, Train Loss:0.3997211158275604\n",
      "Epoch 3[257/312] Time:8.472, Train Loss:0.3823578357696533\n",
      "Epoch 3[258/312] Time:8.633, Train Loss:0.31177425384521484\n",
      "Epoch 3[259/312] Time:0.706, Train Loss:0.26349014043807983\n",
      "Epoch 3[260/312] Time:8.57, Train Loss:0.3199286460876465\n",
      "Epoch 3[261/312] Time:8.647, Train Loss:0.35308054089546204\n",
      "Epoch 3[262/312] Time:0.707, Train Loss:0.26589059829711914\n",
      "Epoch 3[263/312] Time:8.561, Train Loss:0.33085325360298157\n",
      "Epoch 3[264/312] Time:8.629, Train Loss:0.3363363742828369\n",
      "Epoch 3[265/312] Time:0.706, Train Loss:0.3211278021335602\n",
      "Epoch 3[266/312] Time:8.571, Train Loss:0.276027113199234\n",
      "Epoch 3[267/312] Time:8.584, Train Loss:0.40213093161582947\n",
      "Epoch 3[268/312] Time:0.707, Train Loss:0.3746873438358307\n",
      "Epoch 3[269/312] Time:8.556, Train Loss:0.2961168587207794\n",
      "Epoch 3[270/312] Time:8.626, Train Loss:0.3401985168457031\n",
      "Epoch 3[271/312] Time:0.708, Train Loss:0.3445172607898712\n",
      "Epoch 3[272/312] Time:8.513, Train Loss:0.32993897795677185\n",
      "Epoch 3[273/312] Time:8.643, Train Loss:0.28949204087257385\n",
      "Epoch 3[274/312] Time:0.707, Train Loss:0.3842846751213074\n",
      "Epoch 3[275/312] Time:8.524, Train Loss:0.44068047404289246\n",
      "Epoch 3[276/312] Time:8.611, Train Loss:0.27102285623550415\n",
      "Epoch 3[277/312] Time:0.705, Train Loss:0.38549819588661194\n",
      "Epoch 3[278/312] Time:8.567, Train Loss:0.3786271810531616\n",
      "Epoch 3[279/312] Time:8.619, Train Loss:0.33418509364128113\n",
      "Epoch 3[280/312] Time:0.707, Train Loss:0.2997274398803711\n",
      "Epoch 3[281/312] Time:8.613, Train Loss:0.3664120137691498\n",
      "Epoch 3[282/312] Time:8.608, Train Loss:0.308976411819458\n",
      "Epoch 3[283/312] Time:0.709, Train Loss:0.23167811334133148\n",
      "Epoch 3[284/312] Time:8.602, Train Loss:0.3233136534690857\n",
      "Epoch 3[285/312] Time:8.678, Train Loss:0.35141199827194214\n",
      "Epoch 3[286/312] Time:0.706, Train Loss:0.321786105632782\n",
      "Epoch 3[287/312] Time:8.458, Train Loss:0.4278811514377594\n",
      "Epoch 3[288/312] Time:8.577, Train Loss:0.33319371938705444\n",
      "Epoch 3[289/312] Time:0.708, Train Loss:0.39143621921539307\n",
      "Epoch 3[290/312] Time:8.612, Train Loss:0.3291271924972534\n",
      "Epoch 3[291/312] Time:8.62, Train Loss:0.33093833923339844\n",
      "Epoch 3[292/312] Time:0.703, Train Loss:0.3285304605960846\n",
      "Epoch 3[293/312] Time:8.52, Train Loss:0.3340010344982147\n",
      "Epoch 3[294/312] Time:8.693, Train Loss:0.3574131727218628\n",
      "Epoch 3[295/312] Time:0.706, Train Loss:0.3078957498073578\n",
      "Epoch 3[296/312] Time:8.565, Train Loss:0.34337177872657776\n",
      "Epoch 3[297/312] Time:8.705, Train Loss:0.2740233242511749\n",
      "Epoch 3[298/312] Time:0.708, Train Loss:0.37664520740509033\n",
      "Epoch 3[299/312] Time:8.515, Train Loss:0.3167138397693634\n",
      "Epoch 3[300/312] Time:8.672, Train Loss:0.3784870207309723\n",
      "Epoch 3[301/312] Time:0.705, Train Loss:0.3916552662849426\n",
      "Epoch 3[302/312] Time:8.548, Train Loss:0.3579804599285126\n",
      "Epoch 3[303/312] Time:8.686, Train Loss:0.36267220973968506\n",
      "Epoch 3[304/312] Time:0.705, Train Loss:0.39720454812049866\n",
      "Epoch 3[305/312] Time:8.569, Train Loss:0.325827956199646\n",
      "Epoch 3[306/312] Time:8.598, Train Loss:0.33057039976119995\n",
      "Epoch 3[307/312] Time:0.708, Train Loss:0.36588895320892334\n",
      "Epoch 3[308/312] Time:8.57, Train Loss:0.3438279926776886\n",
      "Epoch 3[309/312] Time:8.632, Train Loss:0.285493940114975\n",
      "Epoch 3[310/312] Time:0.706, Train Loss:0.2911352813243866\n",
      "Epoch 3[311/312] Time:8.592, Train Loss:0.4044281244277954\n",
      "Epoch 3[0/39] Val Loss:0.10711119323968887\n",
      "Epoch 3[1/39] Val Loss:0.06693652272224426\n",
      "Epoch 3[2/39] Val Loss:0.08880005776882172\n",
      "Epoch 3[3/39] Val Loss:0.08851136267185211\n",
      "Epoch 3[4/39] Val Loss:0.06887822598218918\n",
      "Epoch 3[5/39] Val Loss:0.056013599038124084\n",
      "Epoch 3[6/39] Val Loss:0.08930553495883942\n",
      "Epoch 3[7/39] Val Loss:0.10489214211702347\n",
      "Epoch 3[8/39] Val Loss:0.15593881905078888\n",
      "Epoch 3[9/39] Val Loss:0.1778053641319275\n",
      "Epoch 3[10/39] Val Loss:0.04297800734639168\n",
      "Epoch 3[11/39] Val Loss:0.03411390259861946\n",
      "Epoch 3[12/39] Val Loss:0.5264772176742554\n",
      "Epoch 3[13/39] Val Loss:0.3298787772655487\n",
      "Epoch 3[14/39] Val Loss:0.055592358112335205\n",
      "Epoch 3[15/39] Val Loss:0.0803583636879921\n",
      "Epoch 3[16/39] Val Loss:0.10191626846790314\n",
      "Epoch 3[17/39] Val Loss:0.12112066149711609\n",
      "Epoch 3[18/39] Val Loss:0.22721433639526367\n",
      "Epoch 3[19/39] Val Loss:0.23604385554790497\n",
      "Epoch 3[20/39] Val Loss:1.2718267440795898\n",
      "Epoch 3[21/39] Val Loss:1.7902767658233643\n",
      "Epoch 3[22/39] Val Loss:0.7721625566482544\n",
      "Epoch 3[23/39] Val Loss:0.9812567830085754\n",
      "Epoch 3[24/39] Val Loss:0.7286115288734436\n",
      "Epoch 3[25/39] Val Loss:0.787003755569458\n",
      "Epoch 3[26/39] Val Loss:1.0683419704437256\n",
      "Epoch 3[27/39] Val Loss:0.7495798468589783\n",
      "Epoch 3[28/39] Val Loss:0.033632244914770126\n",
      "Epoch 3[29/39] Val Loss:0.691119909286499\n",
      "Epoch 3[30/39] Val Loss:1.5139168500900269\n",
      "Epoch 3[31/39] Val Loss:1.5010334253311157\n",
      "Epoch 3[32/39] Val Loss:0.4413577616214752\n",
      "Epoch 3[33/39] Val Loss:0.06061780825257301\n",
      "Epoch 3[34/39] Val Loss:0.08377941697835922\n",
      "Epoch 3[35/39] Val Loss:0.03193242847919464\n",
      "Epoch 3[36/39] Val Loss:0.10469929128885269\n",
      "Epoch 3[37/39] Val Loss:0.25256142020225525\n",
      "Epoch 3[38/39] Val Loss:0.09693936258554459\n",
      "Epoch 3[39/39] Val Loss:0.008667013607919216\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93      8608\n",
      "           1       0.64      0.23      0.34      1392\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.76      0.60      0.63     10000\n",
      "weighted avg       0.85      0.87      0.85     10000\n",
      "\n",
      "Epoch 3: Train Loss 0.34748107524445426, Val Loss 0.4033129090825335, Train Time 2088.833381175995, Val Time 21.19317865371704\n",
      "Epoch 4[0/312] Time:7.675, Train Loss:0.3169640004634857\n",
      "Epoch 4[1/312] Time:8.753, Train Loss:0.29943251609802246\n",
      "Epoch 4[2/312] Time:0.707, Train Loss:0.3820369243621826\n",
      "Epoch 4[3/312] Time:8.498, Train Loss:0.2921135723590851\n",
      "Epoch 4[4/312] Time:8.701, Train Loss:0.34599462151527405\n",
      "Epoch 4[5/312] Time:0.706, Train Loss:0.37566062808036804\n",
      "Epoch 4[6/312] Time:8.48, Train Loss:0.4184865355491638\n",
      "Epoch 4[7/312] Time:8.632, Train Loss:0.30356237292289734\n",
      "Epoch 4[8/312] Time:0.705, Train Loss:0.32470008730888367\n",
      "Epoch 4[9/312] Time:8.47, Train Loss:0.3373650908470154\n",
      "Epoch 4[10/312] Time:8.642, Train Loss:0.30484166741371155\n",
      "Epoch 4[11/312] Time:0.707, Train Loss:0.3621159791946411\n",
      "Epoch 4[12/312] Time:8.527, Train Loss:0.28848353028297424\n",
      "Epoch 4[13/312] Time:8.569, Train Loss:0.2820483446121216\n",
      "Epoch 4[14/312] Time:0.706, Train Loss:0.43461737036705017\n",
      "Epoch 4[15/312] Time:8.525, Train Loss:0.31330201029777527\n",
      "Epoch 4[16/312] Time:8.659, Train Loss:0.34568241238594055\n",
      "Epoch 4[17/312] Time:0.705, Train Loss:0.31475865840911865\n",
      "Epoch 4[18/312] Time:8.632, Train Loss:0.39155176281929016\n",
      "Epoch 4[19/312] Time:8.672, Train Loss:0.45234015583992004\n",
      "Epoch 4[20/312] Time:0.706, Train Loss:0.3157179355621338\n",
      "Epoch 4[21/312] Time:8.593, Train Loss:0.2997783124446869\n",
      "Epoch 4[22/312] Time:8.724, Train Loss:0.37317797541618347\n",
      "Epoch 4[23/312] Time:0.705, Train Loss:0.34878477454185486\n",
      "Epoch 4[24/312] Time:8.579, Train Loss:0.30657994747161865\n",
      "Epoch 4[25/312] Time:8.671, Train Loss:0.3782894015312195\n",
      "Epoch 4[26/312] Time:0.707, Train Loss:0.29167357087135315\n",
      "Epoch 4[27/312] Time:8.563, Train Loss:0.30031466484069824\n",
      "Epoch 4[28/312] Time:8.693, Train Loss:0.28789278864860535\n",
      "Epoch 4[29/312] Time:0.715, Train Loss:0.3698391616344452\n",
      "Epoch 4[30/312] Time:8.53, Train Loss:0.30575060844421387\n",
      "Epoch 4[31/312] Time:8.62, Train Loss:0.39975377917289734\n",
      "Epoch 4[32/312] Time:0.707, Train Loss:0.31244999170303345\n",
      "Epoch 4[33/312] Time:8.545, Train Loss:0.3726985454559326\n",
      "Epoch 4[34/312] Time:8.662, Train Loss:0.26234662532806396\n",
      "Epoch 4[35/312] Time:0.705, Train Loss:0.3369900584220886\n",
      "Epoch 4[36/312] Time:8.491, Train Loss:0.2715354561805725\n",
      "Epoch 4[37/312] Time:8.768, Train Loss:0.3103564381599426\n",
      "Epoch 4[38/312] Time:0.701, Train Loss:0.28934741020202637\n",
      "Epoch 4[39/312] Time:8.524, Train Loss:0.3519584834575653\n",
      "Epoch 4[40/312] Time:8.687, Train Loss:0.347073495388031\n",
      "Epoch 4[41/312] Time:0.704, Train Loss:0.2649664282798767\n",
      "Epoch 4[42/312] Time:8.563, Train Loss:0.3047867715358734\n",
      "Epoch 4[43/312] Time:8.687, Train Loss:0.3562014400959015\n",
      "Epoch 4[44/312] Time:0.705, Train Loss:0.35382354259490967\n",
      "Epoch 4[45/312] Time:8.573, Train Loss:0.3839188814163208\n",
      "Epoch 4[46/312] Time:8.669, Train Loss:0.2579827308654785\n",
      "Epoch 4[47/312] Time:0.708, Train Loss:0.2815108299255371\n",
      "Epoch 4[48/312] Time:8.644, Train Loss:0.3265075981616974\n",
      "Epoch 4[49/312] Time:8.701, Train Loss:0.3834031820297241\n",
      "Epoch 4[50/312] Time:0.705, Train Loss:0.41014760732650757\n",
      "Epoch 4[51/312] Time:8.531, Train Loss:0.3153875172138214\n",
      "Epoch 4[52/312] Time:8.635, Train Loss:0.28957265615463257\n",
      "Epoch 4[53/312] Time:0.71, Train Loss:0.3201528787612915\n",
      "Epoch 4[54/312] Time:8.638, Train Loss:0.31735315918922424\n",
      "Epoch 4[55/312] Time:8.626, Train Loss:0.39382755756378174\n",
      "Epoch 4[56/312] Time:0.707, Train Loss:0.3885849416255951\n",
      "Epoch 4[57/312] Time:8.533, Train Loss:0.35540446639060974\n",
      "Epoch 4[58/312] Time:8.679, Train Loss:0.36179208755493164\n",
      "Epoch 4[59/312] Time:0.706, Train Loss:0.4075551927089691\n",
      "Epoch 4[60/312] Time:8.53, Train Loss:0.302640825510025\n",
      "Epoch 4[61/312] Time:8.665, Train Loss:0.3824700713157654\n",
      "Epoch 4[62/312] Time:0.705, Train Loss:0.34065884351730347\n",
      "Epoch 4[63/312] Time:8.631, Train Loss:0.35399129986763\n",
      "Epoch 4[64/312] Time:8.655, Train Loss:0.3697855770587921\n",
      "Epoch 4[65/312] Time:0.704, Train Loss:0.3510031998157501\n",
      "Epoch 4[66/312] Time:8.536, Train Loss:0.37358883023262024\n",
      "Epoch 4[67/312] Time:8.726, Train Loss:0.3155263066291809\n",
      "Epoch 4[68/312] Time:0.708, Train Loss:0.38642337918281555\n",
      "Epoch 4[69/312] Time:8.594, Train Loss:0.3159776031970978\n",
      "Epoch 4[70/312] Time:8.704, Train Loss:0.32265156507492065\n",
      "Epoch 4[71/312] Time:0.704, Train Loss:0.3656284213066101\n",
      "Epoch 4[72/312] Time:8.564, Train Loss:0.36210229992866516\n",
      "Epoch 4[73/312] Time:8.73, Train Loss:0.3738030195236206\n",
      "Epoch 4[74/312] Time:0.706, Train Loss:0.250398188829422\n",
      "Epoch 4[75/312] Time:8.564, Train Loss:0.404571533203125\n",
      "Epoch 4[76/312] Time:8.685, Train Loss:0.3683607578277588\n",
      "Epoch 4[77/312] Time:0.706, Train Loss:0.27998754382133484\n",
      "Epoch 4[78/312] Time:8.515, Train Loss:0.36763468384742737\n",
      "Epoch 4[79/312] Time:8.662, Train Loss:0.28586527705192566\n",
      "Epoch 4[80/312] Time:0.705, Train Loss:0.42201924324035645\n",
      "Epoch 4[81/312] Time:8.532, Train Loss:0.30799606442451477\n",
      "Epoch 4[82/312] Time:8.69, Train Loss:0.27741459012031555\n",
      "Epoch 4[83/312] Time:0.703, Train Loss:0.2883480191230774\n",
      "Epoch 4[84/312] Time:8.559, Train Loss:0.37207579612731934\n",
      "Epoch 4[85/312] Time:8.648, Train Loss:0.2962016463279724\n",
      "Epoch 4[86/312] Time:0.707, Train Loss:0.3883436918258667\n",
      "Epoch 4[87/312] Time:8.539, Train Loss:0.3094560503959656\n",
      "Epoch 4[88/312] Time:8.645, Train Loss:0.2956370711326599\n",
      "Epoch 4[89/312] Time:0.702, Train Loss:0.4055541157722473\n",
      "Epoch 4[90/312] Time:8.597, Train Loss:0.29456856846809387\n",
      "Epoch 4[91/312] Time:8.673, Train Loss:0.36567702889442444\n",
      "Epoch 4[92/312] Time:0.705, Train Loss:0.31809189915657043\n",
      "Epoch 4[93/312] Time:8.64, Train Loss:0.309844434261322\n",
      "Epoch 4[94/312] Time:8.681, Train Loss:0.3658426105976105\n",
      "Epoch 4[95/312] Time:0.714, Train Loss:0.26665768027305603\n",
      "Epoch 4[96/312] Time:8.602, Train Loss:0.3339082896709442\n",
      "Epoch 4[97/312] Time:8.674, Train Loss:0.4158565104007721\n",
      "Epoch 4[98/312] Time:0.705, Train Loss:0.3425307273864746\n",
      "Epoch 4[99/312] Time:8.647, Train Loss:0.3764803111553192\n",
      "Epoch 4[100/312] Time:8.686, Train Loss:0.40085485577583313\n",
      "Epoch 4[101/312] Time:0.704, Train Loss:0.3494318127632141\n",
      "Epoch 4[102/312] Time:8.572, Train Loss:0.27494579553604126\n",
      "Epoch 4[103/312] Time:8.643, Train Loss:0.3902193605899811\n",
      "Epoch 4[104/312] Time:0.709, Train Loss:0.4652663469314575\n",
      "Epoch 4[105/312] Time:8.646, Train Loss:0.3959544599056244\n",
      "Epoch 4[106/312] Time:8.631, Train Loss:0.4013102650642395\n",
      "Epoch 4[107/312] Time:0.709, Train Loss:0.40432265400886536\n",
      "Epoch 4[108/312] Time:8.584, Train Loss:0.3398053050041199\n",
      "Epoch 4[109/312] Time:8.665, Train Loss:0.33659735321998596\n",
      "Epoch 4[110/312] Time:0.706, Train Loss:0.44775891304016113\n",
      "Epoch 4[111/312] Time:8.562, Train Loss:0.36497941613197327\n",
      "Epoch 4[112/312] Time:8.694, Train Loss:0.32637181878089905\n",
      "Epoch 4[113/312] Time:0.705, Train Loss:0.33500123023986816\n",
      "Epoch 4[114/312] Time:8.52, Train Loss:0.33385005593299866\n",
      "Epoch 4[115/312] Time:8.678, Train Loss:0.3053639233112335\n",
      "Epoch 4[116/312] Time:0.71, Train Loss:0.35847368836402893\n",
      "Epoch 4[117/312] Time:8.571, Train Loss:0.3700120747089386\n",
      "Epoch 4[118/312] Time:8.634, Train Loss:0.3853205740451813\n",
      "Epoch 4[119/312] Time:0.706, Train Loss:0.3982674777507782\n",
      "Epoch 4[120/312] Time:8.694, Train Loss:0.3578341007232666\n",
      "Epoch 4[121/312] Time:8.688, Train Loss:0.4040776193141937\n",
      "Epoch 4[122/312] Time:0.706, Train Loss:0.31929510831832886\n",
      "Epoch 4[123/312] Time:8.588, Train Loss:0.37250185012817383\n",
      "Epoch 4[124/312] Time:8.652, Train Loss:0.35125282406806946\n",
      "Epoch 4[125/312] Time:0.706, Train Loss:0.35252365469932556\n",
      "Epoch 4[126/312] Time:8.426, Train Loss:0.2942814230918884\n",
      "Epoch 4[127/312] Time:8.674, Train Loss:0.3291926681995392\n",
      "Epoch 4[128/312] Time:0.706, Train Loss:0.3609229028224945\n",
      "Epoch 4[129/312] Time:8.628, Train Loss:0.27111685276031494\n",
      "Epoch 4[130/312] Time:8.685, Train Loss:0.2817111909389496\n",
      "Epoch 4[131/312] Time:0.706, Train Loss:0.22350938618183136\n",
      "Epoch 4[132/312] Time:8.572, Train Loss:0.35886794328689575\n",
      "Epoch 4[133/312] Time:8.64, Train Loss:0.2978243827819824\n",
      "Epoch 4[134/312] Time:0.706, Train Loss:0.33419138193130493\n",
      "Epoch 4[135/312] Time:8.626, Train Loss:0.32997915148735046\n",
      "Epoch 4[136/312] Time:8.72, Train Loss:0.33571603894233704\n",
      "Epoch 4[137/312] Time:0.708, Train Loss:0.31745919585227966\n",
      "Epoch 4[138/312] Time:8.589, Train Loss:0.2425301969051361\n",
      "Epoch 4[139/312] Time:8.629, Train Loss:0.3154851496219635\n",
      "Epoch 4[140/312] Time:0.704, Train Loss:0.2946668267250061\n",
      "Epoch 4[141/312] Time:8.645, Train Loss:0.3211253881454468\n",
      "Epoch 4[142/312] Time:8.738, Train Loss:0.31514403223991394\n",
      "Epoch 4[143/312] Time:0.705, Train Loss:0.3648359179496765\n",
      "Epoch 4[144/312] Time:8.543, Train Loss:0.27581557631492615\n",
      "Epoch 4[145/312] Time:8.638, Train Loss:0.33737972378730774\n",
      "Epoch 4[146/312] Time:0.707, Train Loss:0.31188830733299255\n",
      "Epoch 4[147/312] Time:8.505, Train Loss:0.3576573431491852\n",
      "Epoch 4[148/312] Time:8.619, Train Loss:0.342380166053772\n",
      "Epoch 4[149/312] Time:0.709, Train Loss:0.27323219180107117\n",
      "Epoch 4[150/312] Time:8.619, Train Loss:0.2967972755432129\n",
      "Epoch 4[151/312] Time:8.681, Train Loss:0.2984302043914795\n",
      "Epoch 4[152/312] Time:0.706, Train Loss:0.3798045217990875\n",
      "Epoch 4[153/312] Time:8.596, Train Loss:0.39126303791999817\n",
      "Epoch 4[154/312] Time:8.675, Train Loss:0.38523054122924805\n",
      "Epoch 4[155/312] Time:0.706, Train Loss:0.29180580377578735\n",
      "Epoch 4[156/312] Time:8.553, Train Loss:0.35098251700401306\n",
      "Epoch 4[157/312] Time:8.672, Train Loss:0.3779599666595459\n",
      "Epoch 4[158/312] Time:0.704, Train Loss:0.37416255474090576\n",
      "Epoch 4[159/312] Time:8.485, Train Loss:0.32306715846061707\n",
      "Epoch 4[160/312] Time:8.656, Train Loss:0.3288213312625885\n",
      "Epoch 4[161/312] Time:0.705, Train Loss:0.2794138193130493\n",
      "Epoch 4[162/312] Time:8.635, Train Loss:0.3729393482208252\n",
      "Epoch 4[163/312] Time:8.679, Train Loss:0.3857080936431885\n",
      "Epoch 4[164/312] Time:0.706, Train Loss:0.3095468282699585\n",
      "Epoch 4[165/312] Time:8.578, Train Loss:0.2673332393169403\n",
      "Epoch 4[166/312] Time:8.614, Train Loss:0.3364948034286499\n",
      "Epoch 4[167/312] Time:0.705, Train Loss:0.35174569487571716\n",
      "Epoch 4[168/312] Time:8.633, Train Loss:0.34115999937057495\n",
      "Epoch 4[169/312] Time:8.688, Train Loss:0.30922821164131165\n",
      "Epoch 4[170/312] Time:0.705, Train Loss:0.26636752486228943\n",
      "Epoch 4[171/312] Time:8.592, Train Loss:0.3909076750278473\n",
      "Epoch 4[172/312] Time:8.639, Train Loss:0.3487209379673004\n",
      "Epoch 4[173/312] Time:0.707, Train Loss:0.3473888039588928\n",
      "Epoch 4[174/312] Time:8.625, Train Loss:0.3285449743270874\n",
      "Epoch 4[175/312] Time:8.647, Train Loss:0.342891126871109\n",
      "Epoch 4[176/312] Time:0.706, Train Loss:0.35492777824401855\n",
      "Epoch 4[177/312] Time:8.525, Train Loss:0.32312047481536865\n",
      "Epoch 4[178/312] Time:8.632, Train Loss:0.31989985704421997\n",
      "Epoch 4[179/312] Time:0.707, Train Loss:0.3002391457557678\n",
      "Epoch 4[180/312] Time:8.588, Train Loss:0.27448806166648865\n",
      "Epoch 4[181/312] Time:8.675, Train Loss:0.2861339747905731\n",
      "Epoch 4[182/312] Time:0.712, Train Loss:0.3130534589290619\n",
      "Epoch 4[183/312] Time:8.577, Train Loss:0.30817845463752747\n",
      "Epoch 4[184/312] Time:8.682, Train Loss:0.4192947447299957\n",
      "Epoch 4[185/312] Time:0.709, Train Loss:0.41842976212501526\n",
      "Epoch 4[186/312] Time:8.574, Train Loss:0.39083248376846313\n",
      "Epoch 4[187/312] Time:8.584, Train Loss:0.30422794818878174\n",
      "Epoch 4[188/312] Time:0.704, Train Loss:0.3048592209815979\n",
      "Epoch 4[189/312] Time:8.501, Train Loss:0.321898877620697\n",
      "Epoch 4[190/312] Time:8.667, Train Loss:0.3396255075931549\n",
      "Epoch 4[191/312] Time:0.704, Train Loss:0.25748124718666077\n",
      "Epoch 4[192/312] Time:8.592, Train Loss:0.335178941488266\n",
      "Epoch 4[193/312] Time:8.682, Train Loss:0.2911522686481476\n",
      "Epoch 4[194/312] Time:0.704, Train Loss:0.29371562600135803\n",
      "Epoch 4[195/312] Time:8.623, Train Loss:0.3591524362564087\n",
      "Epoch 4[196/312] Time:8.676, Train Loss:0.32121407985687256\n",
      "Epoch 4[197/312] Time:0.706, Train Loss:0.3498494327068329\n",
      "Epoch 4[198/312] Time:8.607, Train Loss:0.27777180075645447\n",
      "Epoch 4[199/312] Time:8.624, Train Loss:0.2941586971282959\n",
      "Epoch 4[200/312] Time:0.707, Train Loss:0.27859359979629517\n",
      "Epoch 4[201/312] Time:8.392, Train Loss:0.37978029251098633\n",
      "Epoch 4[202/312] Time:8.687, Train Loss:0.3426496088504791\n",
      "Epoch 4[203/312] Time:0.705, Train Loss:0.36609530448913574\n",
      "Epoch 4[204/312] Time:8.59, Train Loss:0.29872071743011475\n",
      "Epoch 4[205/312] Time:8.676, Train Loss:0.29022496938705444\n",
      "Epoch 4[206/312] Time:0.707, Train Loss:0.35674330592155457\n",
      "Epoch 4[207/312] Time:8.622, Train Loss:0.22350336611270905\n",
      "Epoch 4[208/312] Time:8.647, Train Loss:0.3692845404148102\n",
      "Epoch 4[209/312] Time:0.705, Train Loss:0.3193754255771637\n",
      "Epoch 4[210/312] Time:8.495, Train Loss:0.3547869324684143\n",
      "Epoch 4[211/312] Time:8.635, Train Loss:0.47596168518066406\n",
      "Epoch 4[212/312] Time:0.727, Train Loss:0.27315419912338257\n",
      "Epoch 4[213/312] Time:8.545, Train Loss:0.33872777223587036\n",
      "Epoch 4[214/312] Time:8.631, Train Loss:0.34549909830093384\n",
      "Epoch 4[215/312] Time:0.703, Train Loss:0.3419863283634186\n",
      "Epoch 4[216/312] Time:8.456, Train Loss:0.2411912977695465\n",
      "Epoch 4[217/312] Time:8.631, Train Loss:0.3392021954059601\n",
      "Epoch 4[218/312] Time:0.709, Train Loss:0.350352019071579\n",
      "Epoch 4[219/312] Time:8.523, Train Loss:0.2832310199737549\n",
      "Epoch 4[220/312] Time:8.643, Train Loss:0.3385542333126068\n",
      "Epoch 4[221/312] Time:0.706, Train Loss:0.3315662741661072\n",
      "Epoch 4[222/312] Time:8.46, Train Loss:0.30895864963531494\n",
      "Epoch 4[223/312] Time:8.662, Train Loss:0.3308904767036438\n",
      "Epoch 4[224/312] Time:0.708, Train Loss:0.36170002818107605\n",
      "Epoch 4[225/312] Time:8.455, Train Loss:0.3631436228752136\n",
      "Epoch 4[226/312] Time:8.614, Train Loss:0.3339427411556244\n",
      "Epoch 4[227/312] Time:0.706, Train Loss:0.37061017751693726\n",
      "Epoch 4[228/312] Time:8.48, Train Loss:0.2995555102825165\n",
      "Epoch 4[229/312] Time:8.652, Train Loss:0.3530046045780182\n",
      "Epoch 4[230/312] Time:0.706, Train Loss:0.26740381121635437\n",
      "Epoch 4[231/312] Time:8.558, Train Loss:0.3986160457134247\n",
      "Epoch 4[232/312] Time:8.633, Train Loss:0.334044486284256\n",
      "Epoch 4[233/312] Time:0.71, Train Loss:0.30343833565711975\n",
      "Epoch 4[234/312] Time:8.556, Train Loss:0.29742008447647095\n",
      "Epoch 4[235/312] Time:8.704, Train Loss:0.34367164969444275\n",
      "Epoch 4[236/312] Time:0.707, Train Loss:0.3149389624595642\n",
      "Epoch 4[237/312] Time:8.506, Train Loss:0.3209807276725769\n",
      "Epoch 4[238/312] Time:8.646, Train Loss:0.30823156237602234\n",
      "Epoch 4[239/312] Time:0.707, Train Loss:0.3957318067550659\n",
      "Epoch 4[240/312] Time:8.507, Train Loss:0.34528928995132446\n",
      "Epoch 4[241/312] Time:8.604, Train Loss:0.3641878664493561\n",
      "Epoch 4[242/312] Time:0.706, Train Loss:0.3684183359146118\n",
      "Epoch 4[243/312] Time:8.54, Train Loss:0.3230692744255066\n",
      "Epoch 4[244/312] Time:8.727, Train Loss:0.3723730444908142\n",
      "Epoch 4[245/312] Time:0.706, Train Loss:0.37666773796081543\n",
      "Epoch 4[246/312] Time:8.639, Train Loss:0.48574674129486084\n",
      "Epoch 4[247/312] Time:8.685, Train Loss:0.3025590181350708\n",
      "Epoch 4[248/312] Time:0.706, Train Loss:0.323386549949646\n",
      "Epoch 4[249/312] Time:8.596, Train Loss:0.30807170271873474\n",
      "Epoch 4[250/312] Time:8.641, Train Loss:0.3196832835674286\n",
      "Epoch 4[251/312] Time:0.705, Train Loss:0.31850969791412354\n",
      "Epoch 4[252/312] Time:8.596, Train Loss:0.34056514501571655\n",
      "Epoch 4[253/312] Time:8.71, Train Loss:0.34762266278266907\n",
      "Epoch 4[254/312] Time:0.706, Train Loss:0.2986370623111725\n",
      "Epoch 4[255/312] Time:8.469, Train Loss:0.34404030442237854\n",
      "Epoch 4[256/312] Time:8.615, Train Loss:0.3150100111961365\n",
      "Epoch 4[257/312] Time:0.706, Train Loss:0.26932165026664734\n",
      "Epoch 4[258/312] Time:8.522, Train Loss:0.28621166944503784\n",
      "Epoch 4[259/312] Time:8.669, Train Loss:0.31562158465385437\n",
      "Epoch 4[260/312] Time:0.71, Train Loss:0.3202517628669739\n",
      "Epoch 4[261/312] Time:8.582, Train Loss:0.4092441201210022\n",
      "Epoch 4[262/312] Time:8.708, Train Loss:0.2811286747455597\n",
      "Epoch 4[263/312] Time:0.706, Train Loss:0.2504323720932007\n",
      "Epoch 4[264/312] Time:8.547, Train Loss:0.29216712713241577\n",
      "Epoch 4[265/312] Time:8.66, Train Loss:0.32732439041137695\n",
      "Epoch 4[266/312] Time:0.705, Train Loss:0.3909773528575897\n",
      "Epoch 4[267/312] Time:8.603, Train Loss:0.3753221333026886\n",
      "Epoch 4[268/312] Time:8.681, Train Loss:0.3072797954082489\n",
      "Epoch 4[269/312] Time:0.705, Train Loss:0.39897024631500244\n",
      "Epoch 4[270/312] Time:8.541, Train Loss:0.3955652415752411\n",
      "Epoch 4[271/312] Time:8.714, Train Loss:0.28351256251335144\n",
      "Epoch 4[272/312] Time:0.705, Train Loss:0.35580649971961975\n",
      "Epoch 4[273/312] Time:8.576, Train Loss:0.35936596989631653\n",
      "Epoch 4[274/312] Time:8.593, Train Loss:0.2915007770061493\n",
      "Epoch 4[275/312] Time:0.706, Train Loss:0.33497992157936096\n",
      "Epoch 4[276/312] Time:8.462, Train Loss:0.3756975829601288\n",
      "Epoch 4[277/312] Time:8.652, Train Loss:0.3509061336517334\n",
      "Epoch 4[278/312] Time:0.707, Train Loss:0.3013867139816284\n",
      "Epoch 4[279/312] Time:8.557, Train Loss:0.31318697333335876\n",
      "Epoch 4[280/312] Time:8.661, Train Loss:0.3272651135921478\n",
      "Epoch 4[281/312] Time:0.706, Train Loss:0.28687962889671326\n",
      "Epoch 4[282/312] Time:8.596, Train Loss:0.263933002948761\n",
      "Epoch 4[283/312] Time:8.684, Train Loss:0.27653658390045166\n",
      "Epoch 4[284/312] Time:0.706, Train Loss:0.31964555382728577\n",
      "Epoch 4[285/312] Time:8.53, Train Loss:0.33701956272125244\n",
      "Epoch 4[286/312] Time:8.637, Train Loss:0.3449486196041107\n",
      "Epoch 4[287/312] Time:0.707, Train Loss:0.325883686542511\n",
      "Epoch 4[288/312] Time:8.543, Train Loss:0.41813600063323975\n",
      "Epoch 4[289/312] Time:8.655, Train Loss:0.3086898624897003\n",
      "Epoch 4[290/312] Time:0.706, Train Loss:0.31710201501846313\n",
      "Epoch 4[291/312] Time:8.51, Train Loss:0.30172207951545715\n",
      "Epoch 4[292/312] Time:8.656, Train Loss:0.3176504373550415\n",
      "Epoch 4[293/312] Time:0.705, Train Loss:0.30879780650138855\n",
      "Epoch 4[294/312] Time:8.504, Train Loss:0.2951313555240631\n",
      "Epoch 4[295/312] Time:8.599, Train Loss:0.22720476984977722\n",
      "Epoch 4[296/312] Time:0.71, Train Loss:0.38652512431144714\n",
      "Epoch 4[297/312] Time:8.491, Train Loss:0.2912823557853699\n",
      "Epoch 4[298/312] Time:8.657, Train Loss:0.2748459279537201\n",
      "Epoch 4[299/312] Time:0.705, Train Loss:0.31665635108947754\n",
      "Epoch 4[300/312] Time:8.578, Train Loss:0.2507916986942291\n",
      "Epoch 4[301/312] Time:8.682, Train Loss:0.32145193219184875\n",
      "Epoch 4[302/312] Time:0.706, Train Loss:0.270361989736557\n",
      "Epoch 4[303/312] Time:8.619, Train Loss:0.24817650020122528\n",
      "Epoch 4[304/312] Time:8.653, Train Loss:0.39619308710098267\n",
      "Epoch 4[305/312] Time:0.706, Train Loss:0.34574997425079346\n",
      "Epoch 4[306/312] Time:8.609, Train Loss:0.31312739849090576\n",
      "Epoch 4[307/312] Time:8.59, Train Loss:0.2772369384765625\n",
      "Epoch 4[308/312] Time:0.716, Train Loss:0.3182573914527893\n",
      "Epoch 4[309/312] Time:8.545, Train Loss:0.3571929335594177\n",
      "Epoch 4[310/312] Time:8.683, Train Loss:0.4651639461517334\n",
      "Epoch 4[311/312] Time:0.706, Train Loss:0.34755831956863403\n",
      "Epoch 4[0/39] Val Loss:0.10740650445222855\n",
      "Epoch 4[1/39] Val Loss:0.09436039626598358\n",
      "Epoch 4[2/39] Val Loss:0.10321477055549622\n",
      "Epoch 4[3/39] Val Loss:0.11769858747720718\n",
      "Epoch 4[4/39] Val Loss:0.09751327335834503\n",
      "Epoch 4[5/39] Val Loss:0.08963775634765625\n",
      "Epoch 4[6/39] Val Loss:0.08838772028684616\n",
      "Epoch 4[7/39] Val Loss:0.0834573432803154\n",
      "Epoch 4[8/39] Val Loss:0.16901986300945282\n",
      "Epoch 4[9/39] Val Loss:0.16268454492092133\n",
      "Epoch 4[10/39] Val Loss:0.0606699138879776\n",
      "Epoch 4[11/39] Val Loss:0.04532255232334137\n",
      "Epoch 4[12/39] Val Loss:0.42420998215675354\n",
      "Epoch 4[13/39] Val Loss:0.2513609826564789\n",
      "Epoch 4[14/39] Val Loss:0.05806645378470421\n",
      "Epoch 4[15/39] Val Loss:0.08482971042394638\n",
      "Epoch 4[16/39] Val Loss:0.09561967104673386\n",
      "Epoch 4[17/39] Val Loss:0.11220608651638031\n",
      "Epoch 4[18/39] Val Loss:0.10538706183433533\n",
      "Epoch 4[19/39] Val Loss:0.13945941627025604\n",
      "Epoch 4[20/39] Val Loss:1.156828761100769\n",
      "Epoch 4[21/39] Val Loss:1.7088593244552612\n",
      "Epoch 4[22/39] Val Loss:0.44323956966400146\n",
      "Epoch 4[23/39] Val Loss:0.4923880696296692\n",
      "Epoch 4[24/39] Val Loss:0.4857633411884308\n",
      "Epoch 4[25/39] Val Loss:0.5328541398048401\n",
      "Epoch 4[26/39] Val Loss:0.5322412848472595\n",
      "Epoch 4[27/39] Val Loss:0.36946985125541687\n",
      "Epoch 4[28/39] Val Loss:0.035691581666469574\n",
      "Epoch 4[29/39] Val Loss:0.4525388777256012\n",
      "Epoch 4[30/39] Val Loss:0.9804683327674866\n",
      "Epoch 4[31/39] Val Loss:1.1463314294815063\n",
      "Epoch 4[32/39] Val Loss:0.36995938420295715\n",
      "Epoch 4[33/39] Val Loss:0.07078950107097626\n",
      "Epoch 4[34/39] Val Loss:0.09288361668586731\n",
      "Epoch 4[35/39] Val Loss:0.04589534178376198\n",
      "Epoch 4[36/39] Val Loss:0.0946054607629776\n",
      "Epoch 4[37/39] Val Loss:0.18799497187137604\n",
      "Epoch 4[38/39] Val Loss:0.10156190395355225\n",
      "Epoch 4[39/39] Val Loss:0.03994235023856163\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.94      8608\n",
      "           1       0.68      0.33      0.45      1392\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.79      0.65      0.69     10000\n",
      "weighted avg       0.87      0.89      0.87     10000\n",
      "\n",
      "Epoch 4: Train Loss 0.33404971558887225, Val Loss 0.3033543508977462, Train Time 2079.565374135971, Val Time 27.763945817947388\n",
      "New best model at epoch 4\n",
      "Epoch 5[0/312] Time:8.342, Train Loss:0.3743833005428314\n",
      "Epoch 5[1/312] Time:0.706, Train Loss:0.2824416160583496\n",
      "Epoch 5[2/312] Time:8.563, Train Loss:0.27510958909988403\n",
      "Epoch 5[3/312] Time:8.638, Train Loss:0.32672563195228577\n",
      "Epoch 5[4/312] Time:0.706, Train Loss:0.36495059728622437\n",
      "Epoch 5[5/312] Time:8.548, Train Loss:0.3046644926071167\n",
      "Epoch 5[6/312] Time:8.672, Train Loss:0.31720349192619324\n",
      "Epoch 5[7/312] Time:0.706, Train Loss:0.3454170823097229\n",
      "Epoch 5[8/312] Time:8.61, Train Loss:0.292279988527298\n",
      "Epoch 5[9/312] Time:8.645, Train Loss:0.3022350072860718\n",
      "Epoch 5[10/312] Time:0.706, Train Loss:0.28628575801849365\n",
      "Epoch 5[11/312] Time:8.591, Train Loss:0.31387782096862793\n",
      "Epoch 5[12/312] Time:8.645, Train Loss:0.4007662236690521\n",
      "Epoch 5[13/312] Time:0.705, Train Loss:0.328794926404953\n",
      "Epoch 5[14/312] Time:8.584, Train Loss:0.2980884611606598\n",
      "Epoch 5[15/312] Time:8.642, Train Loss:0.29579421877861023\n",
      "Epoch 5[16/312] Time:0.707, Train Loss:0.2801458537578583\n",
      "Epoch 5[17/312] Time:8.459, Train Loss:0.28951823711395264\n",
      "Epoch 5[18/312] Time:8.602, Train Loss:0.3389457166194916\n",
      "Epoch 5[19/312] Time:0.706, Train Loss:0.31020256876945496\n",
      "Epoch 5[20/312] Time:8.603, Train Loss:0.4206208884716034\n",
      "Epoch 5[21/312] Time:8.662, Train Loss:0.340810626745224\n",
      "Epoch 5[22/312] Time:0.706, Train Loss:0.30709022283554077\n",
      "Epoch 5[23/312] Time:8.546, Train Loss:0.3500843346118927\n",
      "Epoch 5[24/312] Time:8.732, Train Loss:0.23892071843147278\n",
      "Epoch 5[25/312] Time:0.705, Train Loss:0.34848228096961975\n",
      "Epoch 5[26/312] Time:8.641, Train Loss:0.2820933163166046\n",
      "Epoch 5[27/312] Time:8.691, Train Loss:0.39230722188949585\n",
      "Epoch 5[28/312] Time:0.705, Train Loss:0.34474417567253113\n",
      "Epoch 5[29/312] Time:8.535, Train Loss:0.22767433524131775\n",
      "Epoch 5[30/312] Time:8.709, Train Loss:0.26784247159957886\n",
      "Epoch 5[31/312] Time:0.709, Train Loss:0.35138705372810364\n",
      "Epoch 5[32/312] Time:8.531, Train Loss:0.4628349542617798\n",
      "Epoch 5[33/312] Time:8.548, Train Loss:0.37758028507232666\n",
      "Epoch 5[34/312] Time:0.706, Train Loss:0.2735978364944458\n",
      "Epoch 5[35/312] Time:8.679, Train Loss:0.32115638256073\n",
      "Epoch 5[36/312] Time:8.672, Train Loss:0.3486115634441376\n",
      "Epoch 5[37/312] Time:0.708, Train Loss:0.43878889083862305\n",
      "Epoch 5[38/312] Time:8.534, Train Loss:0.33815550804138184\n",
      "Epoch 5[39/312] Time:8.655, Train Loss:0.29543519020080566\n",
      "Epoch 5[40/312] Time:0.702, Train Loss:0.3818972706794739\n",
      "Epoch 5[41/312] Time:8.507, Train Loss:0.33570167422294617\n",
      "Epoch 5[42/312] Time:8.705, Train Loss:0.28827470541000366\n",
      "Epoch 5[43/312] Time:0.707, Train Loss:0.3773348927497864\n",
      "Epoch 5[44/312] Time:8.544, Train Loss:0.3189477026462555\n",
      "Epoch 5[45/312] Time:8.606, Train Loss:0.2712487578392029\n",
      "Epoch 5[46/312] Time:0.707, Train Loss:0.29372239112854004\n",
      "Epoch 5[47/312] Time:8.675, Train Loss:0.36963170766830444\n",
      "Epoch 5[48/312] Time:8.626, Train Loss:0.3699895441532135\n",
      "Epoch 5[49/312] Time:0.707, Train Loss:0.2927809953689575\n",
      "Epoch 5[50/312] Time:8.612, Train Loss:0.35714903473854065\n",
      "Epoch 5[51/312] Time:8.629, Train Loss:0.32612374424934387\n",
      "Epoch 5[52/312] Time:0.706, Train Loss:0.3485833406448364\n",
      "Epoch 5[53/312] Time:8.609, Train Loss:0.29280149936676025\n",
      "Epoch 5[54/312] Time:8.659, Train Loss:0.30850884318351746\n",
      "Epoch 5[55/312] Time:0.709, Train Loss:0.33218881487846375\n",
      "Epoch 5[56/312] Time:8.59, Train Loss:0.358512282371521\n",
      "Epoch 5[57/312] Time:8.634, Train Loss:0.29365479946136475\n",
      "Epoch 5[58/312] Time:0.707, Train Loss:0.37024423480033875\n",
      "Epoch 5[59/312] Time:8.592, Train Loss:0.2830599546432495\n",
      "Epoch 5[60/312] Time:8.659, Train Loss:0.3815740644931793\n",
      "Epoch 5[61/312] Time:0.705, Train Loss:0.3278714120388031\n",
      "Epoch 5[62/312] Time:8.559, Train Loss:0.2753479778766632\n",
      "Epoch 5[63/312] Time:8.643, Train Loss:0.33200666308403015\n",
      "Epoch 5[64/312] Time:0.705, Train Loss:0.3661029636859894\n",
      "Epoch 5[65/312] Time:8.541, Train Loss:0.2901078164577484\n",
      "Epoch 5[66/312] Time:8.632, Train Loss:0.4169641435146332\n",
      "Epoch 5[67/312] Time:0.71, Train Loss:0.2821851670742035\n",
      "Epoch 5[68/312] Time:8.508, Train Loss:0.33830955624580383\n",
      "Epoch 5[69/312] Time:8.674, Train Loss:0.28763046860694885\n",
      "Epoch 5[70/312] Time:0.708, Train Loss:0.30546730756759644\n",
      "Epoch 5[71/312] Time:8.553, Train Loss:0.31613147258758545\n",
      "Epoch 5[72/312] Time:8.714, Train Loss:0.30828675627708435\n",
      "Epoch 5[73/312] Time:0.707, Train Loss:0.2617461681365967\n",
      "Epoch 5[74/312] Time:8.53, Train Loss:0.31562259793281555\n",
      "Epoch 5[75/312] Time:8.677, Train Loss:0.3284415006637573\n",
      "Epoch 5[76/312] Time:0.708, Train Loss:0.3744548261165619\n",
      "Epoch 5[77/312] Time:8.578, Train Loss:0.2708960771560669\n",
      "Epoch 5[78/312] Time:8.626, Train Loss:0.32422634959220886\n",
      "Epoch 5[79/312] Time:0.719, Train Loss:0.3406831920146942\n",
      "Epoch 5[80/312] Time:8.459, Train Loss:0.31302517652511597\n",
      "Epoch 5[81/312] Time:8.641, Train Loss:0.2765994668006897\n",
      "Epoch 5[82/312] Time:0.708, Train Loss:0.3153086304664612\n",
      "Epoch 5[83/312] Time:8.61, Train Loss:0.20072481036186218\n",
      "Epoch 5[84/312] Time:8.62, Train Loss:0.3834335207939148\n",
      "Epoch 5[85/312] Time:0.707, Train Loss:0.3416813015937805\n",
      "Epoch 5[86/312] Time:8.56, Train Loss:0.2673725485801697\n",
      "Epoch 5[87/312] Time:8.615, Train Loss:0.3115857243537903\n",
      "Epoch 5[88/312] Time:0.709, Train Loss:0.26131105422973633\n",
      "Epoch 5[89/312] Time:8.528, Train Loss:0.33323934674263\n",
      "Epoch 5[90/312] Time:8.637, Train Loss:0.3358626067638397\n",
      "Epoch 5[91/312] Time:0.708, Train Loss:0.3258426785469055\n",
      "Epoch 5[92/312] Time:8.504, Train Loss:0.3085741400718689\n",
      "Epoch 5[93/312] Time:8.563, Train Loss:0.2929345667362213\n",
      "Epoch 5[94/312] Time:0.707, Train Loss:0.2757757604122162\n",
      "Epoch 5[95/312] Time:8.432, Train Loss:0.3029443621635437\n",
      "Epoch 5[96/312] Time:8.67, Train Loss:0.2670515775680542\n",
      "Epoch 5[97/312] Time:0.707, Train Loss:0.4012877345085144\n",
      "Epoch 5[98/312] Time:8.565, Train Loss:0.3170567452907562\n",
      "Epoch 5[99/312] Time:8.65, Train Loss:0.2496950775384903\n",
      "Epoch 5[100/312] Time:0.71, Train Loss:0.25118008255958557\n",
      "Epoch 5[101/312] Time:8.473, Train Loss:0.21478907763957977\n",
      "Epoch 5[102/312] Time:8.663, Train Loss:0.21396468579769135\n",
      "Epoch 5[103/312] Time:0.707, Train Loss:0.3686518371105194\n",
      "Epoch 5[104/312] Time:8.518, Train Loss:0.4200858175754547\n",
      "Epoch 5[105/312] Time:8.638, Train Loss:0.2268463522195816\n",
      "Epoch 5[106/312] Time:0.707, Train Loss:0.44832661747932434\n",
      "Epoch 5[107/312] Time:8.604, Train Loss:0.3388111889362335\n",
      "Epoch 5[108/312] Time:8.63, Train Loss:0.3769877254962921\n",
      "Epoch 5[109/312] Time:0.722, Train Loss:0.35423365235328674\n",
      "Epoch 5[110/312] Time:8.435, Train Loss:0.33353570103645325\n",
      "Epoch 5[111/312] Time:8.63, Train Loss:0.2989763617515564\n",
      "Epoch 5[112/312] Time:0.705, Train Loss:0.28165730834007263\n",
      "Epoch 5[113/312] Time:8.532, Train Loss:0.3288058936595917\n",
      "Epoch 5[114/312] Time:8.619, Train Loss:0.2828892171382904\n",
      "Epoch 5[115/312] Time:0.706, Train Loss:0.32433611154556274\n",
      "Epoch 5[116/312] Time:8.544, Train Loss:0.2737921178340912\n",
      "Epoch 5[117/312] Time:8.632, Train Loss:0.3069075644016266\n",
      "Epoch 5[118/312] Time:0.711, Train Loss:0.3099948465824127\n",
      "Epoch 5[119/312] Time:8.538, Train Loss:0.34362873435020447\n",
      "Epoch 5[120/312] Time:8.709, Train Loss:0.3452552556991577\n",
      "Epoch 5[121/312] Time:0.708, Train Loss:0.18710963428020477\n",
      "Epoch 5[122/312] Time:8.446, Train Loss:0.3591367304325104\n",
      "Epoch 5[123/312] Time:8.552, Train Loss:0.28707602620124817\n",
      "Epoch 5[124/312] Time:0.708, Train Loss:0.29483550786972046\n",
      "Epoch 5[125/312] Time:8.493, Train Loss:0.2622978985309601\n",
      "Epoch 5[126/312] Time:8.755, Train Loss:0.3199535310268402\n",
      "Epoch 5[127/312] Time:0.705, Train Loss:0.38876232504844666\n",
      "Epoch 5[128/312] Time:8.603, Train Loss:0.29895085096359253\n",
      "Epoch 5[129/312] Time:8.697, Train Loss:0.29733800888061523\n",
      "Epoch 5[130/312] Time:0.705, Train Loss:0.30612820386886597\n",
      "Epoch 5[131/312] Time:8.637, Train Loss:0.31861481070518494\n",
      "Epoch 5[132/312] Time:8.67, Train Loss:0.24901871383190155\n",
      "Epoch 5[133/312] Time:0.706, Train Loss:0.3213457763195038\n",
      "Epoch 5[134/312] Time:8.578, Train Loss:0.30738401412963867\n",
      "Epoch 5[135/312] Time:8.655, Train Loss:0.27494436502456665\n",
      "Epoch 5[136/312] Time:0.704, Train Loss:0.2984825074672699\n",
      "Epoch 5[137/312] Time:8.58, Train Loss:0.32042640447616577\n",
      "Epoch 5[138/312] Time:8.556, Train Loss:0.30562523007392883\n",
      "Epoch 5[139/312] Time:0.706, Train Loss:0.3304283916950226\n",
      "Epoch 5[140/312] Time:8.49, Train Loss:0.3230533003807068\n",
      "Epoch 5[141/312] Time:8.662, Train Loss:0.33512091636657715\n",
      "Epoch 5[142/312] Time:0.708, Train Loss:0.25942081212997437\n",
      "Epoch 5[143/312] Time:8.538, Train Loss:0.25845015048980713\n",
      "Epoch 5[144/312] Time:8.667, Train Loss:0.31529295444488525\n",
      "Epoch 5[145/312] Time:0.707, Train Loss:0.3207058906555176\n",
      "Epoch 5[146/312] Time:8.563, Train Loss:0.3233806788921356\n",
      "Epoch 5[147/312] Time:8.652, Train Loss:0.43464043736457825\n",
      "Epoch 5[148/312] Time:0.706, Train Loss:0.38539400696754456\n",
      "Epoch 5[149/312] Time:8.573, Train Loss:0.34580278396606445\n",
      "Epoch 5[150/312] Time:8.705, Train Loss:0.28363269567489624\n",
      "Epoch 5[151/312] Time:0.705, Train Loss:0.4009920060634613\n",
      "Epoch 5[152/312] Time:8.61, Train Loss:0.3917286992073059\n",
      "Epoch 5[153/312] Time:8.669, Train Loss:0.2917938828468323\n",
      "Epoch 5[154/312] Time:0.705, Train Loss:0.260264128446579\n",
      "Epoch 5[155/312] Time:8.658, Train Loss:0.34625765681266785\n",
      "Epoch 5[156/312] Time:8.609, Train Loss:0.32835105061531067\n",
      "Epoch 5[157/312] Time:0.707, Train Loss:0.3536736071109772\n",
      "Epoch 5[158/312] Time:8.444, Train Loss:0.285147100687027\n",
      "Epoch 5[159/312] Time:8.666, Train Loss:0.2815573513507843\n",
      "Epoch 5[160/312] Time:0.704, Train Loss:0.2843392789363861\n",
      "Epoch 5[161/312] Time:8.537, Train Loss:0.3072400987148285\n",
      "Epoch 5[162/312] Time:8.732, Train Loss:0.251556932926178\n",
      "Epoch 5[163/312] Time:0.705, Train Loss:0.26174986362457275\n",
      "Epoch 5[164/312] Time:8.565, Train Loss:0.3236438035964966\n",
      "Epoch 5[165/312] Time:8.643, Train Loss:0.3641623258590698\n",
      "Epoch 5[166/312] Time:0.704, Train Loss:0.3487495183944702\n",
      "Epoch 5[167/312] Time:8.593, Train Loss:0.34427502751350403\n",
      "Epoch 5[168/312] Time:8.684, Train Loss:0.2491386979818344\n",
      "Epoch 5[169/312] Time:0.705, Train Loss:0.3467390239238739\n",
      "Epoch 5[170/312] Time:8.556, Train Loss:0.3875023424625397\n",
      "Epoch 5[171/312] Time:8.697, Train Loss:0.32159313559532166\n",
      "Epoch 5[172/312] Time:0.706, Train Loss:0.37059393525123596\n",
      "Epoch 5[173/312] Time:8.508, Train Loss:0.3390870690345764\n",
      "Epoch 5[174/312] Time:8.613, Train Loss:0.3012455999851227\n",
      "Epoch 5[175/312] Time:0.707, Train Loss:0.3751954436302185\n",
      "Epoch 5[176/312] Time:8.582, Train Loss:0.3281089961528778\n",
      "Epoch 5[177/312] Time:8.699, Train Loss:0.37092095613479614\n",
      "Epoch 5[178/312] Time:0.705, Train Loss:0.37165606021881104\n",
      "Epoch 5[179/312] Time:8.534, Train Loss:0.313692569732666\n",
      "Epoch 5[180/312] Time:8.632, Train Loss:0.3705635964870453\n",
      "Epoch 5[181/312] Time:0.707, Train Loss:0.34288108348846436\n",
      "Epoch 5[182/312] Time:8.57, Train Loss:0.2570705711841583\n",
      "Epoch 5[183/312] Time:8.687, Train Loss:0.28539732098579407\n",
      "Epoch 5[184/312] Time:0.708, Train Loss:0.2994321882724762\n",
      "Epoch 5[185/312] Time:8.546, Train Loss:0.36935150623321533\n",
      "Epoch 5[186/312] Time:8.703, Train Loss:0.3160494267940521\n",
      "Epoch 5[187/312] Time:0.72, Train Loss:0.2876819968223572\n",
      "Epoch 5[188/312] Time:8.443, Train Loss:0.34408214688301086\n",
      "Epoch 5[189/312] Time:8.617, Train Loss:0.26252496242523193\n",
      "Epoch 5[190/312] Time:0.706, Train Loss:0.4283105731010437\n",
      "Epoch 5[191/312] Time:8.632, Train Loss:0.3339771330356598\n",
      "Epoch 5[192/312] Time:8.641, Train Loss:0.2855818569660187\n",
      "Epoch 5[193/312] Time:0.706, Train Loss:0.28052735328674316\n",
      "Epoch 5[194/312] Time:8.617, Train Loss:0.33031004667282104\n",
      "Epoch 5[195/312] Time:8.679, Train Loss:0.2559501826763153\n",
      "Epoch 5[196/312] Time:0.706, Train Loss:0.27014780044555664\n",
      "Epoch 5[197/312] Time:8.61, Train Loss:0.35415035486221313\n",
      "Epoch 5[198/312] Time:8.613, Train Loss:0.2774587869644165\n",
      "Epoch 5[199/312] Time:0.707, Train Loss:0.35088539123535156\n",
      "Epoch 5[200/312] Time:8.644, Train Loss:0.2509218752384186\n",
      "Epoch 5[201/312] Time:8.64, Train Loss:0.2717476189136505\n",
      "Epoch 5[202/312] Time:0.715, Train Loss:0.3526656925678253\n",
      "Epoch 5[203/312] Time:8.536, Train Loss:0.3205637037754059\n",
      "Epoch 5[204/312] Time:8.67, Train Loss:0.30443066358566284\n",
      "Epoch 5[205/312] Time:0.711, Train Loss:0.3114825189113617\n",
      "Epoch 5[206/312] Time:8.574, Train Loss:0.3167053461074829\n",
      "Epoch 5[207/312] Time:8.676, Train Loss:0.2463122010231018\n",
      "Epoch 5[208/312] Time:0.708, Train Loss:0.31188634037971497\n",
      "Epoch 5[209/312] Time:8.55, Train Loss:0.3747563362121582\n",
      "Epoch 5[210/312] Time:8.689, Train Loss:0.2799517512321472\n",
      "Epoch 5[211/312] Time:0.705, Train Loss:0.3118751049041748\n",
      "Epoch 5[212/312] Time:8.495, Train Loss:0.26316410303115845\n",
      "Epoch 5[213/312] Time:8.685, Train Loss:0.248485267162323\n",
      "Epoch 5[214/312] Time:0.706, Train Loss:0.26613396406173706\n",
      "Epoch 5[215/312] Time:8.522, Train Loss:0.341297447681427\n",
      "Epoch 5[216/312] Time:8.664, Train Loss:0.3843059837818146\n",
      "Epoch 5[217/312] Time:0.705, Train Loss:0.4216560125350952\n",
      "Epoch 5[218/312] Time:8.556, Train Loss:0.4495812952518463\n",
      "Epoch 5[219/312] Time:8.633, Train Loss:0.26344162225723267\n",
      "Epoch 5[220/312] Time:0.705, Train Loss:0.31613314151763916\n",
      "Epoch 5[221/312] Time:8.468, Train Loss:0.3343740403652191\n",
      "Epoch 5[222/312] Time:8.641, Train Loss:0.3074231743812561\n",
      "Epoch 5[223/312] Time:0.707, Train Loss:0.278916597366333\n",
      "Epoch 5[224/312] Time:8.578, Train Loss:0.25924694538116455\n",
      "Epoch 5[225/312] Time:8.683, Train Loss:0.33977946639060974\n",
      "Epoch 5[226/312] Time:0.706, Train Loss:0.3282194137573242\n",
      "Epoch 5[227/312] Time:8.518, Train Loss:0.3580334484577179\n",
      "Epoch 5[228/312] Time:8.679, Train Loss:0.2797496020793915\n",
      "Epoch 5[229/312] Time:0.707, Train Loss:0.29883283376693726\n",
      "Epoch 5[230/312] Time:8.495, Train Loss:0.2876072824001312\n",
      "Epoch 5[231/312] Time:8.574, Train Loss:0.32912540435791016\n",
      "Epoch 5[232/312] Time:0.705, Train Loss:0.23425500094890594\n",
      "Epoch 5[233/312] Time:8.568, Train Loss:0.31860101222991943\n",
      "Epoch 5[234/312] Time:8.623, Train Loss:0.2991649806499481\n",
      "Epoch 5[235/312] Time:0.706, Train Loss:0.27415621280670166\n",
      "Epoch 5[236/312] Time:8.619, Train Loss:0.3400915563106537\n",
      "Epoch 5[237/312] Time:8.563, Train Loss:0.31316232681274414\n",
      "Epoch 5[238/312] Time:0.712, Train Loss:0.3552345037460327\n",
      "Epoch 5[239/312] Time:8.572, Train Loss:0.26153308153152466\n",
      "Epoch 5[240/312] Time:8.546, Train Loss:0.3237552046775818\n",
      "Epoch 5[241/312] Time:0.707, Train Loss:0.2664187550544739\n",
      "Epoch 5[242/312] Time:8.632, Train Loss:0.3568136692047119\n",
      "Epoch 5[243/312] Time:8.593, Train Loss:0.2909078299999237\n",
      "Epoch 5[244/312] Time:0.707, Train Loss:0.3443545997142792\n",
      "Epoch 5[245/312] Time:8.631, Train Loss:0.3248414695262909\n",
      "Epoch 5[246/312] Time:8.636, Train Loss:0.3748043179512024\n",
      "Epoch 5[247/312] Time:0.709, Train Loss:0.2859311103820801\n",
      "Epoch 5[248/312] Time:8.541, Train Loss:0.38083115220069885\n",
      "Epoch 5[249/312] Time:8.636, Train Loss:0.3456963896751404\n",
      "Epoch 5[250/312] Time:0.707, Train Loss:0.3077992796897888\n",
      "Epoch 5[251/312] Time:8.536, Train Loss:0.31807777285575867\n",
      "Epoch 5[252/312] Time:8.615, Train Loss:0.29953789710998535\n",
      "Epoch 5[253/312] Time:0.707, Train Loss:0.29059743881225586\n",
      "Epoch 5[254/312] Time:8.585, Train Loss:0.28186357021331787\n",
      "Epoch 5[255/312] Time:8.565, Train Loss:0.341844767332077\n",
      "Epoch 5[256/312] Time:0.704, Train Loss:0.27392396330833435\n",
      "Epoch 5[257/312] Time:8.505, Train Loss:0.32457059621810913\n",
      "Epoch 5[258/312] Time:8.706, Train Loss:0.23399591445922852\n",
      "Epoch 5[259/312] Time:0.708, Train Loss:0.38133370876312256\n",
      "Epoch 5[260/312] Time:8.666, Train Loss:0.2616937458515167\n",
      "Epoch 5[261/312] Time:8.664, Train Loss:0.3640972375869751\n",
      "Epoch 5[262/312] Time:0.709, Train Loss:0.33089131116867065\n",
      "Epoch 5[263/312] Time:8.606, Train Loss:0.3651005029678345\n",
      "Epoch 5[264/312] Time:8.606, Train Loss:0.346118301153183\n",
      "Epoch 5[265/312] Time:0.705, Train Loss:0.270648717880249\n",
      "Epoch 5[266/312] Time:8.527, Train Loss:0.27152374386787415\n",
      "Epoch 5[267/312] Time:8.666, Train Loss:0.30465662479400635\n",
      "Epoch 5[268/312] Time:0.706, Train Loss:0.36093398928642273\n",
      "Epoch 5[269/312] Time:8.584, Train Loss:0.4537826478481293\n",
      "Epoch 5[270/312] Time:8.681, Train Loss:0.3214207887649536\n",
      "Epoch 5[271/312] Time:0.723, Train Loss:0.30626267194747925\n",
      "Epoch 5[272/312] Time:8.531, Train Loss:0.2569100856781006\n",
      "Epoch 5[273/312] Time:8.647, Train Loss:0.30191195011138916\n",
      "Epoch 5[274/312] Time:0.706, Train Loss:0.32633137702941895\n",
      "Epoch 5[275/312] Time:8.665, Train Loss:0.3543236255645752\n",
      "Epoch 5[276/312] Time:8.631, Train Loss:0.30646806955337524\n",
      "Epoch 5[277/312] Time:0.706, Train Loss:0.30421558022499084\n",
      "Epoch 5[278/312] Time:8.653, Train Loss:0.31490907073020935\n",
      "Epoch 5[279/312] Time:8.575, Train Loss:0.30572259426116943\n",
      "Epoch 5[280/312] Time:0.707, Train Loss:0.28161242604255676\n",
      "Epoch 5[281/312] Time:8.538, Train Loss:0.31589409708976746\n",
      "Epoch 5[282/312] Time:8.641, Train Loss:0.2959071397781372\n",
      "Epoch 5[283/312] Time:0.709, Train Loss:0.30034977197647095\n",
      "Epoch 5[284/312] Time:8.59, Train Loss:0.4100407660007477\n",
      "Epoch 5[285/312] Time:8.676, Train Loss:0.31536364555358887\n",
      "Epoch 5[286/312] Time:0.72, Train Loss:0.3784632086753845\n",
      "Epoch 5[287/312] Time:8.576, Train Loss:0.353972464799881\n",
      "Epoch 5[288/312] Time:8.689, Train Loss:0.3537713885307312\n",
      "Epoch 5[289/312] Time:0.709, Train Loss:0.3563306927680969\n",
      "Epoch 5[290/312] Time:8.547, Train Loss:0.3661886155605316\n",
      "Epoch 5[291/312] Time:8.655, Train Loss:0.3662906289100647\n",
      "Epoch 5[292/312] Time:0.703, Train Loss:0.3034821152687073\n",
      "Epoch 5[293/312] Time:8.614, Train Loss:0.2610250413417816\n",
      "Epoch 5[294/312] Time:8.625, Train Loss:0.33513569831848145\n",
      "Epoch 5[295/312] Time:0.71, Train Loss:0.3153192102909088\n",
      "Epoch 5[296/312] Time:8.517, Train Loss:0.30700919032096863\n",
      "Epoch 5[297/312] Time:8.691, Train Loss:0.3276119530200958\n",
      "Epoch 5[298/312] Time:0.707, Train Loss:0.23997950553894043\n",
      "Epoch 5[299/312] Time:8.623, Train Loss:0.29769137501716614\n",
      "Epoch 5[300/312] Time:8.68, Train Loss:0.39973700046539307\n",
      "Epoch 5[301/312] Time:0.706, Train Loss:0.36617162823677063\n",
      "Epoch 5[302/312] Time:8.531, Train Loss:0.26487618684768677\n",
      "Epoch 5[303/312] Time:8.619, Train Loss:0.2919183075428009\n",
      "Epoch 5[304/312] Time:0.707, Train Loss:0.2686808705329895\n",
      "Epoch 5[305/312] Time:8.642, Train Loss:0.28000324964523315\n",
      "Epoch 5[306/312] Time:8.626, Train Loss:0.29186609387397766\n",
      "Epoch 5[307/312] Time:0.707, Train Loss:0.26956692337989807\n",
      "Epoch 5[308/312] Time:8.519, Train Loss:0.42289379239082336\n",
      "Epoch 5[309/312] Time:8.691, Train Loss:0.2917042374610901\n",
      "Epoch 5[310/312] Time:0.708, Train Loss:0.30159837007522583\n",
      "Epoch 5[311/312] Time:8.474, Train Loss:0.33729687333106995\n",
      "Epoch 5[0/39] Val Loss:0.8211793899536133\n",
      "Epoch 5[1/39] Val Loss:0.8532893657684326\n",
      "Epoch 5[2/39] Val Loss:0.7550055980682373\n",
      "Epoch 5[3/39] Val Loss:0.8289147019386292\n",
      "Epoch 5[4/39] Val Loss:0.8322890400886536\n",
      "Epoch 5[5/39] Val Loss:0.6222744584083557\n",
      "Epoch 5[6/39] Val Loss:0.5024764537811279\n",
      "Epoch 5[7/39] Val Loss:0.4616945683956146\n",
      "Epoch 5[8/39] Val Loss:0.5956892967224121\n",
      "Epoch 5[9/39] Val Loss:0.5562673211097717\n",
      "Epoch 5[10/39] Val Loss:0.2447749674320221\n",
      "Epoch 5[11/39] Val Loss:0.1882094293832779\n",
      "Epoch 5[12/39] Val Loss:0.6198607087135315\n",
      "Epoch 5[13/39] Val Loss:0.3906002938747406\n",
      "Epoch 5[14/39] Val Loss:0.22974592447280884\n",
      "Epoch 5[15/39] Val Loss:0.33796802163124084\n",
      "Epoch 5[16/39] Val Loss:0.6409439444541931\n",
      "Epoch 5[17/39] Val Loss:0.7593430876731873\n",
      "Epoch 5[18/39] Val Loss:0.6680186986923218\n",
      "Epoch 5[19/39] Val Loss:0.7847594618797302\n",
      "Epoch 5[20/39] Val Loss:0.7321583032608032\n",
      "Epoch 5[21/39] Val Loss:0.6169715523719788\n",
      "Epoch 5[22/39] Val Loss:0.3981563448905945\n",
      "Epoch 5[23/39] Val Loss:0.4330314099788666\n",
      "Epoch 5[24/39] Val Loss:0.43155258893966675\n",
      "Epoch 5[25/39] Val Loss:0.4537503719329834\n",
      "Epoch 5[26/39] Val Loss:0.46254661679267883\n",
      "Epoch 5[27/39] Val Loss:0.33531418442726135\n",
      "Epoch 5[28/39] Val Loss:0.07850939780473709\n",
      "Epoch 5[29/39] Val Loss:0.22828282415866852\n",
      "Epoch 5[30/39] Val Loss:0.4997364282608032\n",
      "Epoch 5[31/39] Val Loss:0.4703100919723511\n",
      "Epoch 5[32/39] Val Loss:0.436284601688385\n",
      "Epoch 5[33/39] Val Loss:0.2999383211135864\n",
      "Epoch 5[34/39] Val Loss:0.31615063548088074\n",
      "Epoch 5[35/39] Val Loss:0.1486498862504959\n",
      "Epoch 5[36/39] Val Loss:0.33495455980300903\n",
      "Epoch 5[37/39] Val Loss:0.5358195304870605\n",
      "Epoch 5[38/39] Val Loss:0.2524333894252777\n",
      "Epoch 5[39/39] Val Loss:0.15638746321201324\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.77      0.86      8608\n",
      "           1       0.37      0.85      0.52      1392\n",
      "\n",
      "    accuracy                           0.78     10000\n",
      "   macro avg       0.67      0.81      0.69     10000\n",
      "weighted avg       0.89      0.78      0.81     10000\n",
      "\n",
      "Epoch 5: Train Loss 0.3184616919129323, Val Loss 0.49523700601779497, Train Time 2080.060651063919, Val Time 21.18846368789673\n",
      "Epoch 6[0/312] Time:7.666, Train Loss:0.3808343708515167\n",
      "Epoch 6[1/312] Time:8.765, Train Loss:0.3808109164237976\n",
      "Epoch 6[2/312] Time:0.7, Train Loss:0.2555803954601288\n",
      "Epoch 6[3/312] Time:8.504, Train Loss:0.360799640417099\n",
      "Epoch 6[4/312] Time:8.613, Train Loss:0.22187532484531403\n",
      "Epoch 6[5/312] Time:0.712, Train Loss:0.26304030418395996\n",
      "Epoch 6[6/312] Time:8.555, Train Loss:0.2815893590450287\n",
      "Epoch 6[7/312] Time:8.676, Train Loss:0.3427227735519409\n",
      "Epoch 6[8/312] Time:0.708, Train Loss:0.360643208026886\n",
      "Epoch 6[9/312] Time:8.661, Train Loss:0.2815453112125397\n",
      "Epoch 6[10/312] Time:8.651, Train Loss:0.2958914637565613\n",
      "Epoch 6[11/312] Time:0.708, Train Loss:0.263278990983963\n",
      "Epoch 6[12/312] Time:8.618, Train Loss:0.34027156233787537\n",
      "Epoch 6[13/312] Time:8.6, Train Loss:0.2856242060661316\n",
      "Epoch 6[14/312] Time:0.709, Train Loss:0.31769227981567383\n",
      "Epoch 6[15/312] Time:8.463, Train Loss:0.33129552006721497\n",
      "Epoch 6[16/312] Time:8.57, Train Loss:0.331228643655777\n",
      "Epoch 6[17/312] Time:0.706, Train Loss:0.27688392996788025\n",
      "Epoch 6[18/312] Time:8.465, Train Loss:0.30321162939071655\n",
      "Epoch 6[19/312] Time:8.669, Train Loss:0.2540071904659271\n",
      "Epoch 6[20/312] Time:0.707, Train Loss:0.2782207429409027\n",
      "Epoch 6[21/312] Time:8.612, Train Loss:0.26457616686820984\n",
      "Epoch 6[22/312] Time:8.663, Train Loss:0.27945148944854736\n",
      "Epoch 6[23/312] Time:0.708, Train Loss:0.36126571893692017\n",
      "Epoch 6[24/312] Time:8.536, Train Loss:0.2511592507362366\n",
      "Epoch 6[25/312] Time:8.718, Train Loss:0.3322254717350006\n",
      "Epoch 6[26/312] Time:0.706, Train Loss:0.37347009778022766\n",
      "Epoch 6[27/312] Time:8.639, Train Loss:0.25836265087127686\n",
      "Epoch 6[28/312] Time:8.645, Train Loss:0.30220451951026917\n",
      "Epoch 6[29/312] Time:0.707, Train Loss:0.2844613194465637\n",
      "Epoch 6[30/312] Time:8.645, Train Loss:0.26555994153022766\n",
      "Epoch 6[31/312] Time:8.667, Train Loss:0.32255369424819946\n",
      "Epoch 6[32/312] Time:0.705, Train Loss:0.38210633397102356\n",
      "Epoch 6[33/312] Time:8.492, Train Loss:0.3143664300441742\n",
      "Epoch 6[34/312] Time:8.698, Train Loss:0.2735394835472107\n",
      "Epoch 6[35/312] Time:0.704, Train Loss:0.2890389561653137\n",
      "Epoch 6[36/312] Time:8.588, Train Loss:0.3397727310657501\n",
      "Epoch 6[37/312] Time:8.63, Train Loss:0.22379101812839508\n",
      "Epoch 6[38/312] Time:0.707, Train Loss:0.2750442922115326\n",
      "Epoch 6[39/312] Time:8.544, Train Loss:0.29304179549217224\n",
      "Epoch 6[40/312] Time:8.652, Train Loss:0.2782573997974396\n",
      "Epoch 6[41/312] Time:0.709, Train Loss:0.2788911461830139\n",
      "Epoch 6[42/312] Time:8.573, Train Loss:0.30034035444259644\n",
      "Epoch 6[43/312] Time:8.628, Train Loss:0.3621148467063904\n",
      "Epoch 6[44/312] Time:0.712, Train Loss:0.2805632948875427\n",
      "Epoch 6[45/312] Time:8.602, Train Loss:0.300025075674057\n",
      "Epoch 6[46/312] Time:8.683, Train Loss:0.2751370072364807\n",
      "Epoch 6[47/312] Time:0.705, Train Loss:0.24465018510818481\n",
      "Epoch 6[48/312] Time:8.544, Train Loss:0.2891669273376465\n",
      "Epoch 6[49/312] Time:8.631, Train Loss:0.32415181398391724\n",
      "Epoch 6[50/312] Time:0.706, Train Loss:0.26908624172210693\n",
      "Epoch 6[51/312] Time:8.565, Train Loss:0.33680933713912964\n",
      "Epoch 6[52/312] Time:8.701, Train Loss:0.2696685194969177\n",
      "Epoch 6[53/312] Time:0.705, Train Loss:0.2500503659248352\n",
      "Epoch 6[54/312] Time:8.627, Train Loss:0.26762858033180237\n",
      "Epoch 6[55/312] Time:8.69, Train Loss:0.295886367559433\n",
      "Epoch 6[56/312] Time:0.704, Train Loss:0.31509214639663696\n",
      "Epoch 6[57/312] Time:8.601, Train Loss:0.416658878326416\n",
      "Epoch 6[58/312] Time:8.68, Train Loss:0.32986220717430115\n",
      "Epoch 6[59/312] Time:0.705, Train Loss:0.272277295589447\n",
      "Epoch 6[60/312] Time:8.6, Train Loss:0.3235558271408081\n",
      "Epoch 6[61/312] Time:8.67, Train Loss:0.28200066089630127\n",
      "Epoch 6[62/312] Time:0.705, Train Loss:0.2771904170513153\n",
      "Epoch 6[63/312] Time:8.558, Train Loss:0.37271416187286377\n",
      "Epoch 6[64/312] Time:8.669, Train Loss:0.2795047163963318\n",
      "Epoch 6[65/312] Time:0.707, Train Loss:0.3308398723602295\n",
      "Epoch 6[66/312] Time:8.631, Train Loss:0.2828657329082489\n",
      "Epoch 6[67/312] Time:8.656, Train Loss:0.2749035358428955\n",
      "Epoch 6[68/312] Time:0.709, Train Loss:0.3967074155807495\n",
      "Epoch 6[69/312] Time:8.597, Train Loss:0.23282109200954437\n",
      "Epoch 6[70/312] Time:8.629, Train Loss:0.38497406244277954\n",
      "Epoch 6[71/312] Time:0.71, Train Loss:0.33309292793273926\n",
      "Epoch 6[72/312] Time:8.583, Train Loss:0.31389230489730835\n",
      "Epoch 6[73/312] Time:8.622, Train Loss:0.21846136450767517\n",
      "Epoch 6[74/312] Time:0.706, Train Loss:0.3177972733974457\n",
      "Epoch 6[75/312] Time:8.531, Train Loss:0.42055076360702515\n",
      "Epoch 6[76/312] Time:8.635, Train Loss:0.2717757225036621\n",
      "Epoch 6[77/312] Time:0.706, Train Loss:0.3196141719818115\n",
      "Epoch 6[78/312] Time:8.654, Train Loss:0.3235703408718109\n",
      "Epoch 6[79/312] Time:8.667, Train Loss:0.3090769648551941\n",
      "Epoch 6[80/312] Time:0.705, Train Loss:0.31425371766090393\n",
      "Epoch 6[81/312] Time:8.575, Train Loss:0.28079378604888916\n",
      "Epoch 6[82/312] Time:8.706, Train Loss:0.23268745839595795\n",
      "Epoch 6[83/312] Time:0.706, Train Loss:0.3276835083961487\n",
      "Epoch 6[84/312] Time:8.601, Train Loss:0.32577770948410034\n",
      "Epoch 6[85/312] Time:8.62, Train Loss:0.2928321063518524\n",
      "Epoch 6[86/312] Time:0.706, Train Loss:0.3784792125225067\n",
      "Epoch 6[87/312] Time:8.596, Train Loss:0.22342431545257568\n",
      "Epoch 6[88/312] Time:8.725, Train Loss:0.2932797968387604\n",
      "Epoch 6[89/312] Time:0.707, Train Loss:0.274656742811203\n",
      "Epoch 6[90/312] Time:8.411, Train Loss:0.37928295135498047\n",
      "Epoch 6[91/312] Time:8.599, Train Loss:0.323679119348526\n",
      "Epoch 6[92/312] Time:0.71, Train Loss:0.2890305519104004\n",
      "Epoch 6[93/312] Time:8.558, Train Loss:0.3281446099281311\n",
      "Epoch 6[94/312] Time:8.725, Train Loss:0.27073171734809875\n",
      "Epoch 6[95/312] Time:0.707, Train Loss:0.29021087288856506\n",
      "Epoch 6[96/312] Time:8.53, Train Loss:0.2230646312236786\n",
      "Epoch 6[97/312] Time:8.68, Train Loss:0.3092385530471802\n",
      "Epoch 6[98/312] Time:0.706, Train Loss:0.3551499545574188\n",
      "Epoch 6[99/312] Time:8.608, Train Loss:0.3162209093570709\n",
      "Epoch 6[100/312] Time:8.603, Train Loss:0.2659412920475006\n",
      "Epoch 6[101/312] Time:0.707, Train Loss:0.36743706464767456\n",
      "Epoch 6[102/312] Time:8.572, Train Loss:0.2787320613861084\n",
      "Epoch 6[103/312] Time:8.631, Train Loss:0.30442488193511963\n",
      "Epoch 6[104/312] Time:0.707, Train Loss:0.33759549260139465\n",
      "Epoch 6[105/312] Time:8.554, Train Loss:0.2952645421028137\n",
      "Epoch 6[106/312] Time:8.688, Train Loss:0.32238778471946716\n",
      "Epoch 6[107/312] Time:0.724, Train Loss:0.38993096351623535\n",
      "Epoch 6[108/312] Time:8.558, Train Loss:0.3667681813240051\n",
      "Epoch 6[109/312] Time:8.636, Train Loss:0.310619592666626\n",
      "Epoch 6[110/312] Time:0.727, Train Loss:0.2431974709033966\n",
      "Epoch 6[111/312] Time:8.518, Train Loss:0.32284998893737793\n",
      "Epoch 6[112/312] Time:8.656, Train Loss:0.3784300982952118\n",
      "Epoch 6[113/312] Time:0.706, Train Loss:0.28393006324768066\n",
      "Epoch 6[114/312] Time:8.507, Train Loss:0.2548138499259949\n",
      "Epoch 6[115/312] Time:8.664, Train Loss:0.266862154006958\n",
      "Epoch 6[116/312] Time:0.707, Train Loss:0.37698930501937866\n",
      "Epoch 6[117/312] Time:8.469, Train Loss:0.36305472254753113\n",
      "Epoch 6[118/312] Time:8.649, Train Loss:0.23544614017009735\n",
      "Epoch 6[119/312] Time:0.707, Train Loss:0.2597211003303528\n",
      "Epoch 6[120/312] Time:8.513, Train Loss:0.24576885998249054\n",
      "Epoch 6[121/312] Time:8.605, Train Loss:0.2530663311481476\n",
      "Epoch 6[122/312] Time:0.706, Train Loss:0.23176909983158112\n",
      "Epoch 6[123/312] Time:8.527, Train Loss:0.3245098292827606\n",
      "Epoch 6[124/312] Time:8.649, Train Loss:0.356297105550766\n",
      "Epoch 6[125/312] Time:0.705, Train Loss:0.33030247688293457\n",
      "Epoch 6[126/312] Time:8.638, Train Loss:0.27971696853637695\n",
      "Epoch 6[127/312] Time:8.651, Train Loss:0.2928737699985504\n",
      "Epoch 6[128/312] Time:0.706, Train Loss:0.23207969963550568\n",
      "Epoch 6[129/312] Time:8.598, Train Loss:0.31281301379203796\n",
      "Epoch 6[130/312] Time:8.668, Train Loss:0.27883419394493103\n",
      "Epoch 6[131/312] Time:0.707, Train Loss:0.4069439768791199\n",
      "Epoch 6[132/312] Time:8.572, Train Loss:0.3100859820842743\n",
      "Epoch 6[133/312] Time:8.685, Train Loss:0.35268014669418335\n",
      "Epoch 6[134/312] Time:0.705, Train Loss:0.2730944752693176\n",
      "Epoch 6[135/312] Time:8.475, Train Loss:0.31398534774780273\n",
      "Epoch 6[136/312] Time:8.623, Train Loss:0.3304349184036255\n",
      "Epoch 6[137/312] Time:0.707, Train Loss:0.284070760011673\n",
      "Epoch 6[138/312] Time:8.58, Train Loss:0.2985335886478424\n",
      "Epoch 6[139/312] Time:8.653, Train Loss:0.28472548723220825\n",
      "Epoch 6[140/312] Time:0.705, Train Loss:0.28976568579673767\n",
      "Epoch 6[141/312] Time:8.511, Train Loss:0.3124208152294159\n",
      "Epoch 6[142/312] Time:8.651, Train Loss:0.3582778871059418\n",
      "Epoch 6[143/312] Time:0.706, Train Loss:0.26749205589294434\n",
      "Epoch 6[144/312] Time:8.582, Train Loss:0.26178231835365295\n",
      "Epoch 6[145/312] Time:8.705, Train Loss:0.32870906591415405\n",
      "Epoch 6[146/312] Time:0.705, Train Loss:0.3426416218280792\n",
      "Epoch 6[147/312] Time:8.501, Train Loss:0.27102890610694885\n",
      "Epoch 6[148/312] Time:8.648, Train Loss:0.3089711368083954\n",
      "Epoch 6[149/312] Time:0.706, Train Loss:0.35016030073165894\n",
      "Epoch 6[150/312] Time:8.489, Train Loss:0.306485652923584\n",
      "Epoch 6[151/312] Time:8.62, Train Loss:0.2901834547519684\n",
      "Epoch 6[152/312] Time:0.706, Train Loss:0.27363255620002747\n",
      "Epoch 6[153/312] Time:8.541, Train Loss:0.3398170471191406\n",
      "Epoch 6[154/312] Time:8.553, Train Loss:0.31982001662254333\n",
      "Epoch 6[155/312] Time:0.705, Train Loss:0.33971884846687317\n",
      "Epoch 6[156/312] Time:8.477, Train Loss:0.2723417580127716\n",
      "Epoch 6[157/312] Time:8.707, Train Loss:0.21765851974487305\n",
      "Epoch 6[158/312] Time:0.701, Train Loss:0.30459317564964294\n",
      "Epoch 6[159/312] Time:8.575, Train Loss:0.27798980474472046\n",
      "Epoch 6[160/312] Time:8.666, Train Loss:0.27330875396728516\n",
      "Epoch 6[161/312] Time:0.705, Train Loss:0.33993974328041077\n",
      "Epoch 6[162/312] Time:8.521, Train Loss:0.32058215141296387\n",
      "Epoch 6[163/312] Time:8.693, Train Loss:0.36177945137023926\n",
      "Epoch 6[164/312] Time:0.723, Train Loss:0.2716761529445648\n",
      "Epoch 6[165/312] Time:8.563, Train Loss:0.3111021816730499\n",
      "Epoch 6[166/312] Time:8.676, Train Loss:0.29083746671676636\n",
      "Epoch 6[167/312] Time:0.704, Train Loss:0.29320988059043884\n",
      "Epoch 6[168/312] Time:8.445, Train Loss:0.28742340207099915\n",
      "Epoch 6[169/312] Time:8.728, Train Loss:0.23341861367225647\n",
      "Epoch 6[170/312] Time:0.706, Train Loss:0.3049250841140747\n",
      "Epoch 6[171/312] Time:8.62, Train Loss:0.3539038598537445\n",
      "Epoch 6[172/312] Time:8.648, Train Loss:0.43954339623451233\n",
      "Epoch 6[173/312] Time:0.717, Train Loss:0.3050795793533325\n",
      "Epoch 6[174/312] Time:8.487, Train Loss:0.3078765273094177\n",
      "Epoch 6[175/312] Time:8.69, Train Loss:0.3016558587551117\n",
      "Epoch 6[176/312] Time:0.705, Train Loss:0.295468270778656\n",
      "Epoch 6[177/312] Time:8.551, Train Loss:0.307794451713562\n",
      "Epoch 6[178/312] Time:8.701, Train Loss:0.33245426416397095\n",
      "Epoch 6[179/312] Time:0.708, Train Loss:0.29059702157974243\n",
      "Epoch 6[180/312] Time:8.474, Train Loss:0.2880774140357971\n",
      "Epoch 6[181/312] Time:8.631, Train Loss:0.32056525349617004\n",
      "Epoch 6[182/312] Time:0.707, Train Loss:0.2773684561252594\n",
      "Epoch 6[183/312] Time:8.515, Train Loss:0.3098480701446533\n",
      "Epoch 6[184/312] Time:8.694, Train Loss:0.2897045910358429\n",
      "Epoch 6[185/312] Time:0.706, Train Loss:0.2677697539329529\n",
      "Epoch 6[186/312] Time:8.559, Train Loss:0.24033160507678986\n",
      "Epoch 6[187/312] Time:8.659, Train Loss:0.3298328220844269\n",
      "Epoch 6[188/312] Time:0.705, Train Loss:0.2614077031612396\n",
      "Epoch 6[189/312] Time:8.611, Train Loss:0.2885638177394867\n",
      "Epoch 6[190/312] Time:8.676, Train Loss:0.403270959854126\n",
      "Epoch 6[191/312] Time:0.704, Train Loss:0.33480384945869446\n",
      "Epoch 6[192/312] Time:8.63, Train Loss:0.526690661907196\n",
      "Epoch 6[193/312] Time:8.63, Train Loss:0.3904871344566345\n",
      "Epoch 6[194/312] Time:0.713, Train Loss:0.23965221643447876\n",
      "Epoch 6[195/312] Time:8.598, Train Loss:0.30192676186561584\n",
      "Epoch 6[196/312] Time:8.754, Train Loss:0.39538154006004333\n",
      "Epoch 6[197/312] Time:0.707, Train Loss:0.2513108253479004\n",
      "Epoch 6[198/312] Time:8.417, Train Loss:0.23635515570640564\n",
      "Epoch 6[199/312] Time:8.64, Train Loss:0.31549072265625\n",
      "Epoch 6[200/312] Time:0.705, Train Loss:0.307975709438324\n",
      "Epoch 6[201/312] Time:8.616, Train Loss:0.328605055809021\n",
      "Epoch 6[202/312] Time:8.72, Train Loss:0.33247336745262146\n",
      "Epoch 6[203/312] Time:0.705, Train Loss:0.28910109400749207\n",
      "Epoch 6[204/312] Time:8.518, Train Loss:0.23154941201210022\n",
      "Epoch 6[205/312] Time:8.699, Train Loss:0.2686651349067688\n",
      "Epoch 6[206/312] Time:0.705, Train Loss:0.3615393340587616\n",
      "Epoch 6[207/312] Time:8.582, Train Loss:0.31002187728881836\n",
      "Epoch 6[208/312] Time:8.679, Train Loss:0.33551302552223206\n",
      "Epoch 6[209/312] Time:0.706, Train Loss:0.30168309807777405\n",
      "Epoch 6[210/312] Time:8.57, Train Loss:0.2409822642803192\n",
      "Epoch 6[211/312] Time:8.717, Train Loss:0.2674955129623413\n",
      "Epoch 6[212/312] Time:0.704, Train Loss:0.27897360920906067\n",
      "Epoch 6[213/312] Time:8.578, Train Loss:0.256984144449234\n",
      "Epoch 6[214/312] Time:8.65, Train Loss:0.27883368730545044\n",
      "Epoch 6[215/312] Time:0.705, Train Loss:0.35167428851127625\n",
      "Epoch 6[216/312] Time:8.622, Train Loss:0.30396944284439087\n",
      "Epoch 6[217/312] Time:8.694, Train Loss:0.30640968680381775\n",
      "Epoch 6[218/312] Time:0.705, Train Loss:0.3502879738807678\n",
      "Epoch 6[219/312] Time:8.552, Train Loss:0.32304656505584717\n",
      "Epoch 6[220/312] Time:8.692, Train Loss:0.3159237205982208\n",
      "Epoch 6[221/312] Time:0.704, Train Loss:0.3816572427749634\n",
      "Epoch 6[222/312] Time:8.641, Train Loss:0.28696444630622864\n",
      "Epoch 6[223/312] Time:8.675, Train Loss:0.3386414647102356\n",
      "Epoch 6[224/312] Time:0.705, Train Loss:0.29530131816864014\n",
      "Epoch 6[225/312] Time:8.599, Train Loss:0.28639018535614014\n",
      "Epoch 6[226/312] Time:8.685, Train Loss:0.29002612829208374\n",
      "Epoch 6[227/312] Time:0.706, Train Loss:0.24391117691993713\n",
      "Epoch 6[228/312] Time:8.565, Train Loss:0.28889206051826477\n",
      "Epoch 6[229/312] Time:8.687, Train Loss:0.3070042133331299\n",
      "Epoch 6[230/312] Time:0.707, Train Loss:0.3578733205795288\n",
      "Epoch 6[231/312] Time:8.53, Train Loss:0.2909162640571594\n",
      "Epoch 6[232/312] Time:8.693, Train Loss:0.24404750764369965\n",
      "Epoch 6[233/312] Time:0.705, Train Loss:0.27023983001708984\n",
      "Epoch 6[234/312] Time:8.598, Train Loss:0.3306526243686676\n",
      "Epoch 6[235/312] Time:8.721, Train Loss:0.3749696910381317\n",
      "Epoch 6[236/312] Time:0.705, Train Loss:0.3167782127857208\n",
      "Epoch 6[237/312] Time:8.629, Train Loss:0.28575795888900757\n",
      "Epoch 6[238/312] Time:8.643, Train Loss:0.23009245097637177\n",
      "Epoch 6[239/312] Time:0.706, Train Loss:0.41054534912109375\n",
      "Epoch 6[240/312] Time:8.571, Train Loss:0.40370500087738037\n",
      "Epoch 6[241/312] Time:8.65, Train Loss:0.23041529953479767\n",
      "Epoch 6[242/312] Time:0.706, Train Loss:0.3235766887664795\n",
      "Epoch 6[243/312] Time:8.582, Train Loss:0.2911922037601471\n",
      "Epoch 6[244/312] Time:8.631, Train Loss:0.37533265352249146\n",
      "Epoch 6[245/312] Time:0.706, Train Loss:0.25511130690574646\n",
      "Epoch 6[246/312] Time:8.661, Train Loss:0.26945292949676514\n",
      "Epoch 6[247/312] Time:8.554, Train Loss:0.2642560303211212\n",
      "Epoch 6[248/312] Time:0.707, Train Loss:0.3181501626968384\n",
      "Epoch 6[249/312] Time:8.657, Train Loss:0.3317115306854248\n",
      "Epoch 6[250/312] Time:8.692, Train Loss:0.24424447119235992\n",
      "Epoch 6[251/312] Time:0.706, Train Loss:0.2905639708042145\n",
      "Epoch 6[252/312] Time:8.588, Train Loss:0.4374379515647888\n",
      "Epoch 6[253/312] Time:8.619, Train Loss:0.31018903851509094\n",
      "Epoch 6[254/312] Time:0.707, Train Loss:0.23460128903388977\n",
      "Epoch 6[255/312] Time:8.659, Train Loss:0.30228331685066223\n",
      "Epoch 6[256/312] Time:8.672, Train Loss:0.35600513219833374\n",
      "Epoch 6[257/312] Time:0.704, Train Loss:0.24402043223381042\n",
      "Epoch 6[258/312] Time:8.569, Train Loss:0.316911906003952\n",
      "Epoch 6[259/312] Time:8.655, Train Loss:0.3093610107898712\n",
      "Epoch 6[260/312] Time:0.705, Train Loss:0.37922123074531555\n",
      "Epoch 6[261/312] Time:8.501, Train Loss:0.33038878440856934\n",
      "Epoch 6[262/312] Time:8.702, Train Loss:0.2783333957195282\n",
      "Epoch 6[263/312] Time:0.705, Train Loss:0.27624258399009705\n",
      "Epoch 6[264/312] Time:8.505, Train Loss:0.30964431166648865\n",
      "Epoch 6[265/312] Time:8.667, Train Loss:0.2613908648490906\n",
      "Epoch 6[266/312] Time:0.705, Train Loss:0.25433453917503357\n",
      "Epoch 6[267/312] Time:8.607, Train Loss:0.25903746485710144\n",
      "Epoch 6[268/312] Time:8.681, Train Loss:0.28642135858535767\n",
      "Epoch 6[269/312] Time:0.707, Train Loss:0.3582868278026581\n",
      "Epoch 6[270/312] Time:8.549, Train Loss:0.26573169231414795\n",
      "Epoch 6[271/312] Time:8.638, Train Loss:0.24700942635536194\n",
      "Epoch 6[272/312] Time:0.722, Train Loss:0.3622886836528778\n",
      "Epoch 6[273/312] Time:8.522, Train Loss:0.33777716755867004\n",
      "Epoch 6[274/312] Time:8.718, Train Loss:0.3298813998699188\n",
      "Epoch 6[275/312] Time:0.708, Train Loss:0.2792283594608307\n",
      "Epoch 6[276/312] Time:8.588, Train Loss:0.29390111565589905\n",
      "Epoch 6[277/312] Time:8.619, Train Loss:0.27838948369026184\n",
      "Epoch 6[278/312] Time:0.706, Train Loss:0.3425787091255188\n",
      "Epoch 6[279/312] Time:8.5, Train Loss:0.37231558561325073\n",
      "Epoch 6[280/312] Time:8.671, Train Loss:0.3056584596633911\n",
      "Epoch 6[281/312] Time:0.705, Train Loss:0.3021412789821625\n",
      "Epoch 6[282/312] Time:8.595, Train Loss:0.2858266234397888\n",
      "Epoch 6[283/312] Time:8.696, Train Loss:0.33648860454559326\n",
      "Epoch 6[284/312] Time:0.708, Train Loss:0.3669562339782715\n",
      "Epoch 6[285/312] Time:8.591, Train Loss:0.2645447552204132\n",
      "Epoch 6[286/312] Time:8.742, Train Loss:0.2446584701538086\n",
      "Epoch 6[287/312] Time:0.706, Train Loss:0.28305956721305847\n",
      "Epoch 6[288/312] Time:8.568, Train Loss:0.4106251895427704\n",
      "Epoch 6[289/312] Time:8.678, Train Loss:0.27411413192749023\n",
      "Epoch 6[290/312] Time:0.706, Train Loss:0.3003583550453186\n",
      "Epoch 6[291/312] Time:8.648, Train Loss:0.30120280385017395\n",
      "Epoch 6[292/312] Time:8.726, Train Loss:0.2928447425365448\n",
      "Epoch 6[293/312] Time:0.698, Train Loss:0.30281510949134827\n",
      "Epoch 6[294/312] Time:8.676, Train Loss:0.33485954999923706\n",
      "Epoch 6[295/312] Time:8.639, Train Loss:0.39149010181427\n",
      "Epoch 6[296/312] Time:0.705, Train Loss:0.28269168734550476\n",
      "Epoch 6[297/312] Time:8.56, Train Loss:0.272389680147171\n",
      "Epoch 6[298/312] Time:8.603, Train Loss:0.2597913444042206\n",
      "Epoch 6[299/312] Time:0.707, Train Loss:0.29347530007362366\n",
      "Epoch 6[300/312] Time:8.672, Train Loss:0.3840197026729584\n",
      "Epoch 6[301/312] Time:8.665, Train Loss:0.21603916585445404\n",
      "Epoch 6[302/312] Time:0.707, Train Loss:0.411090612411499\n",
      "Epoch 6[303/312] Time:8.623, Train Loss:0.40160757303237915\n",
      "Epoch 6[304/312] Time:8.677, Train Loss:0.3589990437030792\n",
      "Epoch 6[305/312] Time:0.705, Train Loss:0.33012527227401733\n",
      "Epoch 6[306/312] Time:8.55, Train Loss:0.2914765775203705\n",
      "Epoch 6[307/312] Time:8.629, Train Loss:0.34052687883377075\n",
      "Epoch 6[308/312] Time:0.704, Train Loss:0.26210734248161316\n",
      "Epoch 6[309/312] Time:8.495, Train Loss:0.340983510017395\n",
      "Epoch 6[310/312] Time:8.701, Train Loss:0.42348042130470276\n",
      "Epoch 6[311/312] Time:0.706, Train Loss:0.2796163856983185\n",
      "Epoch 6[0/39] Val Loss:0.453319787979126\n",
      "Epoch 6[1/39] Val Loss:0.47362473607063293\n",
      "Epoch 6[2/39] Val Loss:0.44729846715927124\n",
      "Epoch 6[3/39] Val Loss:0.48171091079711914\n",
      "Epoch 6[4/39] Val Loss:0.3992931544780731\n",
      "Epoch 6[5/39] Val Loss:0.3289754390716553\n",
      "Epoch 6[6/39] Val Loss:0.2460499405860901\n",
      "Epoch 6[7/39] Val Loss:0.21703161299228668\n",
      "Epoch 6[8/39] Val Loss:0.31715643405914307\n",
      "Epoch 6[9/39] Val Loss:0.31782856583595276\n",
      "Epoch 6[10/39] Val Loss:0.13533151149749756\n",
      "Epoch 6[11/39] Val Loss:0.1117931678891182\n",
      "Epoch 6[12/39] Val Loss:0.4555417597293854\n",
      "Epoch 6[13/39] Val Loss:0.2748611867427826\n",
      "Epoch 6[14/39] Val Loss:0.1348472535610199\n",
      "Epoch 6[15/39] Val Loss:0.16896431148052216\n",
      "Epoch 6[16/39] Val Loss:0.25196680426597595\n",
      "Epoch 6[17/39] Val Loss:0.3310852348804474\n",
      "Epoch 6[18/39] Val Loss:0.3017343580722809\n",
      "Epoch 6[19/39] Val Loss:0.35569536685943604\n",
      "Epoch 6[20/39] Val Loss:0.8227313160896301\n",
      "Epoch 6[21/39] Val Loss:1.02463698387146\n",
      "Epoch 6[22/39] Val Loss:0.36503395438194275\n",
      "Epoch 6[23/39] Val Loss:0.3905554711818695\n",
      "Epoch 6[24/39] Val Loss:0.47639375925064087\n",
      "Epoch 6[25/39] Val Loss:0.5132133364677429\n",
      "Epoch 6[26/39] Val Loss:0.4091895818710327\n",
      "Epoch 6[27/39] Val Loss:0.30129748582839966\n",
      "Epoch 6[28/39] Val Loss:0.08659406006336212\n",
      "Epoch 6[29/39] Val Loss:0.2152167111635208\n",
      "Epoch 6[30/39] Val Loss:0.4835318624973297\n",
      "Epoch 6[31/39] Val Loss:0.4249473810195923\n",
      "Epoch 6[32/39] Val Loss:0.26630571484565735\n",
      "Epoch 6[33/39] Val Loss:0.15087534487247467\n",
      "Epoch 6[34/39] Val Loss:0.17943251132965088\n",
      "Epoch 6[35/39] Val Loss:0.15428408980369568\n",
      "Epoch 6[36/39] Val Loss:0.23927323520183563\n",
      "Epoch 6[37/39] Val Loss:0.32834523916244507\n",
      "Epoch 6[38/39] Val Loss:0.22424185276031494\n",
      "Epoch 6[39/39] Val Loss:0.09845809638500214\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91      8608\n",
      "           1       0.47      0.68      0.56      1392\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.71      0.78      0.73     10000\n",
      "weighted avg       0.88      0.85      0.86     10000\n",
      "\n",
      "Epoch 6: Train Loss 0.3064587716108713, Val Loss 0.34252994851424146, Train Time 2079.5274012088776, Val Time 27.790170192718506\n",
      "Epoch 7[0/312] Time:0.705, Train Loss:0.30878040194511414\n",
      "Epoch 7[1/312] Time:8.67, Train Loss:0.2875930964946747\n",
      "Epoch 7[2/312] Time:0.708, Train Loss:0.2941003441810608\n",
      "Epoch 7[3/312] Time:8.154, Train Loss:0.2969178557395935\n",
      "Epoch 7[4/312] Time:8.658, Train Loss:0.3844802975654602\n",
      "Epoch 7[5/312] Time:0.707, Train Loss:0.2185758352279663\n",
      "Epoch 7[6/312] Time:8.582, Train Loss:0.301232248544693\n",
      "Epoch 7[7/312] Time:8.642, Train Loss:0.292740136384964\n",
      "Epoch 7[8/312] Time:0.707, Train Loss:0.26943257451057434\n",
      "Epoch 7[9/312] Time:8.593, Train Loss:0.31358763575553894\n",
      "Epoch 7[10/312] Time:8.652, Train Loss:0.4315199851989746\n",
      "Epoch 7[11/312] Time:0.707, Train Loss:0.30276572704315186\n",
      "Epoch 7[12/312] Time:8.474, Train Loss:0.32036635279655457\n",
      "Epoch 7[13/312] Time:8.653, Train Loss:0.2857300937175751\n",
      "Epoch 7[14/312] Time:0.706, Train Loss:0.2711177468299866\n",
      "Epoch 7[15/312] Time:8.506, Train Loss:0.28912198543548584\n",
      "Epoch 7[16/312] Time:8.636, Train Loss:0.2691185474395752\n",
      "Epoch 7[17/312] Time:0.706, Train Loss:0.37165242433547974\n",
      "Epoch 7[18/312] Time:8.556, Train Loss:0.3064403831958771\n",
      "Epoch 7[19/312] Time:8.705, Train Loss:0.299480140209198\n",
      "Epoch 7[20/312] Time:0.703, Train Loss:0.2789270877838135\n",
      "Epoch 7[21/312] Time:8.586, Train Loss:0.32994431257247925\n",
      "Epoch 7[22/312] Time:8.677, Train Loss:0.3599179685115814\n",
      "Epoch 7[23/312] Time:0.706, Train Loss:0.3271617889404297\n",
      "Epoch 7[24/312] Time:8.517, Train Loss:0.30091163516044617\n",
      "Epoch 7[25/312] Time:8.667, Train Loss:0.3438093066215515\n",
      "Epoch 7[26/312] Time:0.705, Train Loss:0.2782876491546631\n",
      "Epoch 7[27/312] Time:8.572, Train Loss:0.28683993220329285\n",
      "Epoch 7[28/312] Time:8.658, Train Loss:0.24522554874420166\n",
      "Epoch 7[29/312] Time:0.708, Train Loss:0.2516038715839386\n",
      "Epoch 7[30/312] Time:8.533, Train Loss:0.3207099735736847\n",
      "Epoch 7[31/312] Time:8.644, Train Loss:0.39301612973213196\n",
      "Epoch 7[32/312] Time:0.705, Train Loss:0.2304610013961792\n",
      "Epoch 7[33/312] Time:8.546, Train Loss:0.3606175482273102\n",
      "Epoch 7[34/312] Time:8.658, Train Loss:0.3222915232181549\n",
      "Epoch 7[35/312] Time:0.705, Train Loss:0.29728102684020996\n",
      "Epoch 7[36/312] Time:8.557, Train Loss:0.2457875907421112\n",
      "Epoch 7[37/312] Time:8.703, Train Loss:0.278896689414978\n",
      "Epoch 7[38/312] Time:0.707, Train Loss:0.2623181641101837\n",
      "Epoch 7[39/312] Time:8.644, Train Loss:0.32139426469802856\n",
      "Epoch 7[40/312] Time:8.696, Train Loss:0.28516313433647156\n",
      "Epoch 7[41/312] Time:0.706, Train Loss:0.2737618684768677\n",
      "Epoch 7[42/312] Time:8.579, Train Loss:0.3043363094329834\n",
      "Epoch 7[43/312] Time:8.651, Train Loss:0.26316797733306885\n",
      "Epoch 7[44/312] Time:0.699, Train Loss:0.20708897709846497\n",
      "Epoch 7[45/312] Time:8.365, Train Loss:0.2518220841884613\n",
      "Epoch 7[46/312] Time:8.604, Train Loss:0.277385413646698\n",
      "Epoch 7[47/312] Time:0.705, Train Loss:0.3131335377693176\n",
      "Epoch 7[48/312] Time:8.624, Train Loss:0.3661944270133972\n",
      "Epoch 7[49/312] Time:8.672, Train Loss:0.29435840249061584\n",
      "Epoch 7[50/312] Time:0.707, Train Loss:0.3435060977935791\n",
      "Epoch 7[51/312] Time:8.554, Train Loss:0.29357245564460754\n",
      "Epoch 7[52/312] Time:8.714, Train Loss:0.3135985732078552\n",
      "Epoch 7[53/312] Time:0.704, Train Loss:0.26517054438591003\n",
      "Epoch 7[54/312] Time:8.61, Train Loss:0.21251191198825836\n",
      "Epoch 7[55/312] Time:8.677, Train Loss:0.30782872438430786\n",
      "Epoch 7[56/312] Time:0.711, Train Loss:0.3271543085575104\n",
      "Epoch 7[57/312] Time:8.581, Train Loss:0.22855468094348907\n",
      "Epoch 7[58/312] Time:8.652, Train Loss:0.224897101521492\n",
      "Epoch 7[59/312] Time:0.707, Train Loss:0.2964536249637604\n",
      "Epoch 7[60/312] Time:8.51, Train Loss:0.2637803852558136\n",
      "Epoch 7[61/312] Time:8.598, Train Loss:0.2966054677963257\n",
      "Epoch 7[62/312] Time:0.705, Train Loss:0.28403833508491516\n",
      "Epoch 7[63/312] Time:8.528, Train Loss:0.3424849808216095\n",
      "Epoch 7[64/312] Time:8.658, Train Loss:0.2866670489311218\n",
      "Epoch 7[65/312] Time:0.705, Train Loss:0.3202931880950928\n",
      "Epoch 7[66/312] Time:8.52, Train Loss:0.250271737575531\n",
      "Epoch 7[67/312] Time:8.688, Train Loss:0.31114813685417175\n",
      "Epoch 7[68/312] Time:0.705, Train Loss:0.3857152760028839\n",
      "Epoch 7[69/312] Time:8.554, Train Loss:0.3247922956943512\n",
      "Epoch 7[70/312] Time:8.589, Train Loss:0.292921781539917\n",
      "Epoch 7[71/312] Time:0.705, Train Loss:0.2538941204547882\n",
      "Epoch 7[72/312] Time:8.546, Train Loss:0.3041671812534332\n",
      "Epoch 7[73/312] Time:8.727, Train Loss:0.30760785937309265\n",
      "Epoch 7[74/312] Time:0.706, Train Loss:0.327781081199646\n",
      "Epoch 7[75/312] Time:8.581, Train Loss:0.30202412605285645\n",
      "Epoch 7[76/312] Time:8.66, Train Loss:0.23871707916259766\n",
      "Epoch 7[77/312] Time:0.706, Train Loss:0.2941843867301941\n",
      "Epoch 7[78/312] Time:8.522, Train Loss:0.3351539075374603\n",
      "Epoch 7[79/312] Time:8.696, Train Loss:0.2686893045902252\n",
      "Epoch 7[80/312] Time:0.707, Train Loss:0.2573765814304352\n",
      "Epoch 7[81/312] Time:8.493, Train Loss:0.28783267736434937\n",
      "Epoch 7[82/312] Time:8.762, Train Loss:0.2841533124446869\n",
      "Epoch 7[83/312] Time:0.704, Train Loss:0.2892916202545166\n",
      "Epoch 7[84/312] Time:8.486, Train Loss:0.40032243728637695\n",
      "Epoch 7[85/312] Time:8.654, Train Loss:0.31248560547828674\n",
      "Epoch 7[86/312] Time:0.705, Train Loss:0.31292426586151123\n",
      "Epoch 7[87/312] Time:8.63, Train Loss:0.2922908365726471\n",
      "Epoch 7[88/312] Time:8.678, Train Loss:0.27510833740234375\n",
      "Epoch 7[89/312] Time:0.706, Train Loss:0.29897692799568176\n",
      "Epoch 7[90/312] Time:8.532, Train Loss:0.33953621983528137\n",
      "Epoch 7[91/312] Time:8.667, Train Loss:0.25929147005081177\n",
      "Epoch 7[92/312] Time:0.71, Train Loss:0.30952179431915283\n",
      "Epoch 7[93/312] Time:8.553, Train Loss:0.3403828740119934\n",
      "Epoch 7[94/312] Time:8.579, Train Loss:0.2735741436481476\n",
      "Epoch 7[95/312] Time:0.705, Train Loss:0.37442728877067566\n",
      "Epoch 7[96/312] Time:8.68, Train Loss:0.3207351267337799\n",
      "Epoch 7[97/312] Time:8.672, Train Loss:0.23121869564056396\n",
      "Epoch 7[98/312] Time:0.709, Train Loss:0.30720821022987366\n",
      "Epoch 7[99/312] Time:8.546, Train Loss:0.36242789030075073\n",
      "Epoch 7[100/312] Time:8.714, Train Loss:0.30007681250572205\n",
      "Epoch 7[101/312] Time:0.705, Train Loss:0.2872268557548523\n",
      "Epoch 7[102/312] Time:8.634, Train Loss:0.35035309195518494\n",
      "Epoch 7[103/312] Time:8.68, Train Loss:0.3173891603946686\n",
      "Epoch 7[104/312] Time:0.705, Train Loss:0.34412887692451477\n",
      "Epoch 7[105/312] Time:8.639, Train Loss:0.3809102773666382\n",
      "Epoch 7[106/312] Time:8.51, Train Loss:0.33591195940971375\n",
      "Epoch 7[107/312] Time:0.706, Train Loss:0.26751509308815\n",
      "Epoch 7[108/312] Time:8.532, Train Loss:0.377582311630249\n",
      "Epoch 7[109/312] Time:8.683, Train Loss:0.29533514380455017\n",
      "Epoch 7[110/312] Time:0.706, Train Loss:0.2641562521457672\n",
      "Epoch 7[111/312] Time:8.585, Train Loss:0.23713167011737823\n",
      "Epoch 7[112/312] Time:8.673, Train Loss:0.23942382633686066\n",
      "Epoch 7[113/312] Time:0.706, Train Loss:0.37430039048194885\n",
      "Epoch 7[114/312] Time:8.555, Train Loss:0.3028620779514313\n",
      "Epoch 7[115/312] Time:8.664, Train Loss:0.25502896308898926\n",
      "Epoch 7[116/312] Time:0.706, Train Loss:0.2439723014831543\n",
      "Epoch 7[117/312] Time:8.544, Train Loss:0.2783757448196411\n",
      "Epoch 7[118/312] Time:8.703, Train Loss:0.27382493019104004\n",
      "Epoch 7[119/312] Time:0.706, Train Loss:0.29246899485588074\n",
      "Epoch 7[120/312] Time:8.583, Train Loss:0.3168271780014038\n",
      "Epoch 7[121/312] Time:8.717, Train Loss:0.2407388687133789\n",
      "Epoch 7[122/312] Time:0.706, Train Loss:0.26848360896110535\n",
      "Epoch 7[123/312] Time:8.505, Train Loss:0.3227995038032532\n",
      "Epoch 7[124/312] Time:8.714, Train Loss:0.25241199135780334\n",
      "Epoch 7[125/312] Time:0.706, Train Loss:0.3102884888648987\n",
      "Epoch 7[126/312] Time:8.587, Train Loss:0.32625386118888855\n",
      "Epoch 7[127/312] Time:8.731, Train Loss:0.29786384105682373\n",
      "Epoch 7[128/312] Time:0.705, Train Loss:0.2986094653606415\n",
      "Epoch 7[129/312] Time:8.626, Train Loss:0.2728218138217926\n",
      "Epoch 7[130/312] Time:8.717, Train Loss:0.24497996270656586\n",
      "Epoch 7[131/312] Time:0.705, Train Loss:0.3238627016544342\n",
      "Epoch 7[132/312] Time:8.606, Train Loss:0.30298101902008057\n",
      "Epoch 7[133/312] Time:8.628, Train Loss:0.3748948574066162\n",
      "Epoch 7[134/312] Time:0.707, Train Loss:0.23228244483470917\n",
      "Epoch 7[135/312] Time:8.577, Train Loss:0.3556510806083679\n",
      "Epoch 7[136/312] Time:8.665, Train Loss:0.27367809414863586\n",
      "Epoch 7[137/312] Time:0.705, Train Loss:0.2591865360736847\n",
      "Epoch 7[138/312] Time:8.528, Train Loss:0.3603915274143219\n",
      "Epoch 7[139/312] Time:8.665, Train Loss:0.2777775526046753\n",
      "Epoch 7[140/312] Time:0.706, Train Loss:0.2703731656074524\n",
      "Epoch 7[141/312] Time:8.621, Train Loss:0.22805771231651306\n",
      "Epoch 7[142/312] Time:8.686, Train Loss:0.30649855732917786\n",
      "Epoch 7[143/312] Time:0.706, Train Loss:0.3168225586414337\n",
      "Epoch 7[144/312] Time:8.516, Train Loss:0.27259743213653564\n",
      "Epoch 7[145/312] Time:8.666, Train Loss:0.24263395369052887\n",
      "Epoch 7[146/312] Time:0.706, Train Loss:0.2953703701496124\n",
      "Epoch 7[147/312] Time:8.489, Train Loss:0.2616654336452484\n",
      "Epoch 7[148/312] Time:8.693, Train Loss:0.21606528759002686\n",
      "Epoch 7[149/312] Time:0.706, Train Loss:0.4412584602832794\n",
      "Epoch 7[150/312] Time:8.577, Train Loss:0.2926986813545227\n",
      "Epoch 7[151/312] Time:8.602, Train Loss:0.3197871744632721\n",
      "Epoch 7[152/312] Time:0.705, Train Loss:0.29595351219177246\n",
      "Epoch 7[153/312] Time:8.636, Train Loss:0.31272590160369873\n",
      "Epoch 7[154/312] Time:8.509, Train Loss:0.2940621078014374\n",
      "Epoch 7[155/312] Time:0.703, Train Loss:0.25544974207878113\n",
      "Epoch 7[156/312] Time:8.304, Train Loss:0.30983319878578186\n",
      "Epoch 7[157/312] Time:8.705, Train Loss:0.35161900520324707\n",
      "Epoch 7[158/312] Time:0.706, Train Loss:0.2987789809703827\n",
      "Epoch 7[159/312] Time:8.606, Train Loss:0.20455411076545715\n",
      "Epoch 7[160/312] Time:8.642, Train Loss:0.32687389850616455\n",
      "Epoch 7[161/312] Time:0.709, Train Loss:0.3391806185245514\n",
      "Epoch 7[162/312] Time:8.577, Train Loss:0.28157833218574524\n",
      "Epoch 7[163/312] Time:8.685, Train Loss:0.28104367852211\n",
      "Epoch 7[164/312] Time:0.707, Train Loss:0.3583909869194031\n",
      "Epoch 7[165/312] Time:8.55, Train Loss:0.28434181213378906\n",
      "Epoch 7[166/312] Time:8.664, Train Loss:0.3415367007255554\n",
      "Epoch 7[167/312] Time:0.706, Train Loss:0.2550249397754669\n",
      "Epoch 7[168/312] Time:8.462, Train Loss:0.31563055515289307\n",
      "Epoch 7[169/312] Time:8.649, Train Loss:0.27653828263282776\n",
      "Epoch 7[170/312] Time:0.706, Train Loss:0.24734653532505035\n",
      "Epoch 7[171/312] Time:8.555, Train Loss:0.2569286525249481\n",
      "Epoch 7[172/312] Time:8.643, Train Loss:0.32134512066841125\n",
      "Epoch 7[173/312] Time:0.704, Train Loss:0.2995861768722534\n",
      "Epoch 7[174/312] Time:8.48, Train Loss:0.3361694812774658\n",
      "Epoch 7[175/312] Time:8.672, Train Loss:0.3187854588031769\n",
      "Epoch 7[176/312] Time:0.705, Train Loss:0.2884688377380371\n",
      "Epoch 7[177/312] Time:8.532, Train Loss:0.29548293352127075\n",
      "Epoch 7[178/312] Time:8.657, Train Loss:0.3451506495475769\n",
      "Epoch 7[179/312] Time:0.706, Train Loss:0.29207876324653625\n",
      "Epoch 7[180/312] Time:8.653, Train Loss:0.3605912923812866\n",
      "Epoch 7[181/312] Time:8.662, Train Loss:0.34206926822662354\n",
      "Epoch 7[182/312] Time:0.704, Train Loss:0.3246251046657562\n",
      "Epoch 7[183/312] Time:8.543, Train Loss:0.2720085680484772\n",
      "Epoch 7[184/312] Time:8.686, Train Loss:0.30560898780822754\n",
      "Epoch 7[185/312] Time:0.704, Train Loss:0.2291254997253418\n",
      "Epoch 7[186/312] Time:8.607, Train Loss:0.26632794737815857\n",
      "Epoch 7[187/312] Time:8.661, Train Loss:0.258403480052948\n",
      "Epoch 7[188/312] Time:0.719, Train Loss:0.2603597939014435\n",
      "Epoch 7[189/312] Time:8.597, Train Loss:0.22105176746845245\n",
      "Epoch 7[190/312] Time:8.735, Train Loss:0.289407342672348\n",
      "Epoch 7[191/312] Time:0.706, Train Loss:0.27671346068382263\n",
      "Epoch 7[192/312] Time:8.539, Train Loss:0.3271058201789856\n",
      "Epoch 7[193/312] Time:8.681, Train Loss:0.25886768102645874\n",
      "Epoch 7[194/312] Time:0.705, Train Loss:0.3357868492603302\n",
      "Epoch 7[195/312] Time:8.575, Train Loss:0.3022908866405487\n",
      "Epoch 7[196/312] Time:8.668, Train Loss:0.21454592049121857\n",
      "Epoch 7[197/312] Time:0.705, Train Loss:0.20176975429058075\n",
      "Epoch 7[198/312] Time:8.643, Train Loss:0.28188231587409973\n",
      "Epoch 7[199/312] Time:8.646, Train Loss:0.22406350076198578\n",
      "Epoch 7[200/312] Time:0.704, Train Loss:0.27879124879837036\n",
      "Epoch 7[201/312] Time:8.546, Train Loss:0.28632622957229614\n",
      "Epoch 7[202/312] Time:8.643, Train Loss:0.35434019565582275\n",
      "Epoch 7[203/312] Time:0.71, Train Loss:0.325545072555542\n",
      "Epoch 7[204/312] Time:8.535, Train Loss:0.25395241379737854\n",
      "Epoch 7[205/312] Time:8.646, Train Loss:0.29258039593696594\n",
      "Epoch 7[206/312] Time:0.705, Train Loss:0.3310321867465973\n",
      "Epoch 7[207/312] Time:8.532, Train Loss:0.27768370509147644\n",
      "Epoch 7[208/312] Time:8.683, Train Loss:0.2393811196088791\n",
      "Epoch 7[209/312] Time:0.704, Train Loss:0.28542032837867737\n",
      "Epoch 7[210/312] Time:8.638, Train Loss:0.3503629267215729\n",
      "Epoch 7[211/312] Time:8.68, Train Loss:0.21119189262390137\n",
      "Epoch 7[212/312] Time:0.706, Train Loss:0.4050004184246063\n",
      "Epoch 7[213/312] Time:8.493, Train Loss:0.26705074310302734\n",
      "Epoch 7[214/312] Time:8.643, Train Loss:0.21777814626693726\n",
      "Epoch 7[215/312] Time:0.704, Train Loss:0.31617236137390137\n",
      "Epoch 7[216/312] Time:8.532, Train Loss:0.32196488976478577\n",
      "Epoch 7[217/312] Time:8.684, Train Loss:0.2596436142921448\n",
      "Epoch 7[218/312] Time:0.705, Train Loss:0.28594449162483215\n",
      "Epoch 7[219/312] Time:8.522, Train Loss:0.2929762601852417\n",
      "Epoch 7[220/312] Time:8.666, Train Loss:0.28031566739082336\n",
      "Epoch 7[221/312] Time:0.719, Train Loss:0.3187824487686157\n",
      "Epoch 7[222/312] Time:8.603, Train Loss:0.27386805415153503\n",
      "Epoch 7[223/312] Time:8.688, Train Loss:0.2811940908432007\n",
      "Epoch 7[224/312] Time:0.705, Train Loss:0.32313892245292664\n",
      "Epoch 7[225/312] Time:8.52, Train Loss:0.3113074004650116\n",
      "Epoch 7[226/312] Time:8.703, Train Loss:0.25702306628227234\n",
      "Epoch 7[227/312] Time:0.707, Train Loss:0.3062116205692291\n",
      "Epoch 7[228/312] Time:8.531, Train Loss:0.3424457013607025\n",
      "Epoch 7[229/312] Time:8.664, Train Loss:0.2939685881137848\n",
      "Epoch 7[230/312] Time:0.706, Train Loss:0.33227455615997314\n",
      "Epoch 7[231/312] Time:8.595, Train Loss:0.28272387385368347\n",
      "Epoch 7[232/312] Time:8.659, Train Loss:0.27969223260879517\n",
      "Epoch 7[233/312] Time:0.706, Train Loss:0.33898138999938965\n",
      "Epoch 7[234/312] Time:8.559, Train Loss:0.30521678924560547\n",
      "Epoch 7[235/312] Time:8.628, Train Loss:0.3054434359073639\n",
      "Epoch 7[236/312] Time:0.707, Train Loss:0.22908605635166168\n",
      "Epoch 7[237/312] Time:8.622, Train Loss:0.3378913402557373\n",
      "Epoch 7[238/312] Time:8.621, Train Loss:0.2670151889324188\n",
      "Epoch 7[239/312] Time:0.707, Train Loss:0.31213217973709106\n",
      "Epoch 7[240/312] Time:8.437, Train Loss:0.34659141302108765\n",
      "Epoch 7[241/312] Time:8.567, Train Loss:0.34566089510917664\n",
      "Epoch 7[242/312] Time:0.707, Train Loss:0.2536867558956146\n",
      "Epoch 7[243/312] Time:8.527, Train Loss:0.22380919754505157\n",
      "Epoch 7[244/312] Time:8.622, Train Loss:0.3007413148880005\n",
      "Epoch 7[245/312] Time:0.706, Train Loss:0.27817320823669434\n",
      "Epoch 7[246/312] Time:8.67, Train Loss:0.2856884300708771\n",
      "Epoch 7[247/312] Time:8.758, Train Loss:0.26826798915863037\n",
      "Epoch 7[248/312] Time:0.706, Train Loss:0.3072780668735504\n",
      "Epoch 7[249/312] Time:8.629, Train Loss:0.29434749484062195\n",
      "Epoch 7[250/312] Time:8.688, Train Loss:0.285685271024704\n",
      "Epoch 7[251/312] Time:0.705, Train Loss:0.32775336503982544\n",
      "Epoch 7[252/312] Time:8.553, Train Loss:0.21671314537525177\n",
      "Epoch 7[253/312] Time:8.69, Train Loss:0.2802642285823822\n",
      "Epoch 7[254/312] Time:0.708, Train Loss:0.29876628518104553\n",
      "Epoch 7[255/312] Time:8.504, Train Loss:0.27579551935195923\n",
      "Epoch 7[256/312] Time:8.595, Train Loss:0.2759203612804413\n",
      "Epoch 7[257/312] Time:0.707, Train Loss:0.2931683361530304\n",
      "Epoch 7[258/312] Time:8.589, Train Loss:0.2416357547044754\n",
      "Epoch 7[259/312] Time:8.686, Train Loss:0.2824398875236511\n",
      "Epoch 7[260/312] Time:0.705, Train Loss:0.3493870496749878\n",
      "Epoch 7[261/312] Time:8.542, Train Loss:0.251395583152771\n",
      "Epoch 7[262/312] Time:8.699, Train Loss:0.2671508491039276\n",
      "Epoch 7[263/312] Time:0.704, Train Loss:0.1930253505706787\n",
      "Epoch 7[264/312] Time:8.522, Train Loss:0.25908297300338745\n",
      "Epoch 7[265/312] Time:8.687, Train Loss:0.37482690811157227\n",
      "Epoch 7[266/312] Time:0.706, Train Loss:0.2792918086051941\n",
      "Epoch 7[267/312] Time:8.548, Train Loss:0.314216285943985\n",
      "Epoch 7[268/312] Time:8.647, Train Loss:0.29110705852508545\n",
      "Epoch 7[269/312] Time:0.704, Train Loss:0.26967141032218933\n",
      "Epoch 7[270/312] Time:8.512, Train Loss:0.3646571636199951\n",
      "Epoch 7[271/312] Time:8.612, Train Loss:0.3226906359195709\n",
      "Epoch 7[272/312] Time:0.721, Train Loss:0.30355292558670044\n",
      "Epoch 7[273/312] Time:8.595, Train Loss:0.30733293294906616\n",
      "Epoch 7[274/312] Time:8.649, Train Loss:0.28805628418922424\n",
      "Epoch 7[275/312] Time:0.719, Train Loss:0.30029088258743286\n",
      "Epoch 7[276/312] Time:8.557, Train Loss:0.2762638032436371\n",
      "Epoch 7[277/312] Time:8.671, Train Loss:0.22151294350624084\n",
      "Epoch 7[278/312] Time:0.705, Train Loss:0.24658887088298798\n",
      "Epoch 7[279/312] Time:8.569, Train Loss:0.2936725318431854\n",
      "Epoch 7[280/312] Time:8.681, Train Loss:0.24997709691524506\n",
      "Epoch 7[281/312] Time:0.713, Train Loss:0.3425247371196747\n",
      "Epoch 7[282/312] Time:8.556, Train Loss:0.26233115792274475\n",
      "Epoch 7[283/312] Time:8.705, Train Loss:0.3355576992034912\n",
      "Epoch 7[284/312] Time:0.706, Train Loss:0.3125564455986023\n",
      "Epoch 7[285/312] Time:8.571, Train Loss:0.30003681778907776\n",
      "Epoch 7[286/312] Time:8.672, Train Loss:0.22905077040195465\n",
      "Epoch 7[287/312] Time:0.706, Train Loss:0.2864723205566406\n",
      "Epoch 7[288/312] Time:8.497, Train Loss:0.23831088840961456\n",
      "Epoch 7[289/312] Time:8.625, Train Loss:0.3417906165122986\n",
      "Epoch 7[290/312] Time:0.697, Train Loss:0.3125542402267456\n",
      "Epoch 7[291/312] Time:8.698, Train Loss:0.3045734465122223\n",
      "Epoch 7[292/312] Time:8.679, Train Loss:0.25983548164367676\n",
      "Epoch 7[293/312] Time:0.707, Train Loss:0.2462616115808487\n",
      "Epoch 7[294/312] Time:8.578, Train Loss:0.30892324447631836\n",
      "Epoch 7[295/312] Time:8.683, Train Loss:0.2134598195552826\n",
      "Epoch 7[296/312] Time:0.706, Train Loss:0.2707321047782898\n",
      "Epoch 7[297/312] Time:8.495, Train Loss:0.23147748410701752\n",
      "Epoch 7[298/312] Time:8.634, Train Loss:0.33113789558410645\n",
      "Epoch 7[299/312] Time:0.707, Train Loss:0.3795062005519867\n",
      "Epoch 7[300/312] Time:8.567, Train Loss:0.22947396337985992\n",
      "Epoch 7[301/312] Time:8.605, Train Loss:0.27671730518341064\n",
      "Epoch 7[302/312] Time:0.703, Train Loss:0.24560441076755524\n",
      "Epoch 7[303/312] Time:8.57, Train Loss:0.27155178785324097\n",
      "Epoch 7[304/312] Time:8.679, Train Loss:0.36220672726631165\n",
      "Epoch 7[305/312] Time:0.708, Train Loss:0.3106314539909363\n",
      "Epoch 7[306/312] Time:8.619, Train Loss:0.3022371232509613\n",
      "Epoch 7[307/312] Time:8.619, Train Loss:0.3032459020614624\n",
      "Epoch 7[308/312] Time:0.706, Train Loss:0.31106290221214294\n",
      "Epoch 7[309/312] Time:8.536, Train Loss:0.301008015871048\n",
      "Epoch 7[310/312] Time:8.702, Train Loss:0.2802727520465851\n",
      "Epoch 7[311/312] Time:0.704, Train Loss:0.3718334138393402\n",
      "Epoch 7[0/39] Val Loss:0.26208052039146423\n",
      "Epoch 7[1/39] Val Loss:0.2689046561717987\n",
      "Epoch 7[2/39] Val Loss:0.25952470302581787\n",
      "Epoch 7[3/39] Val Loss:0.2878735065460205\n",
      "Epoch 7[4/39] Val Loss:0.2422347515821457\n",
      "Epoch 7[5/39] Val Loss:0.18637719750404358\n",
      "Epoch 7[6/39] Val Loss:0.18180711567401886\n",
      "Epoch 7[7/39] Val Loss:0.13692517578601837\n",
      "Epoch 7[8/39] Val Loss:0.22448043525218964\n",
      "Epoch 7[9/39] Val Loss:0.221389040350914\n",
      "Epoch 7[10/39] Val Loss:0.0849270299077034\n",
      "Epoch 7[11/39] Val Loss:0.06851925700902939\n",
      "Epoch 7[12/39] Val Loss:0.4199012517929077\n",
      "Epoch 7[13/39] Val Loss:0.279018759727478\n",
      "Epoch 7[14/39] Val Loss:0.10318321734666824\n",
      "Epoch 7[15/39] Val Loss:0.13189947605133057\n",
      "Epoch 7[16/39] Val Loss:0.19355377554893494\n",
      "Epoch 7[17/39] Val Loss:0.27164098620414734\n",
      "Epoch 7[18/39] Val Loss:0.3232460021972656\n",
      "Epoch 7[19/39] Val Loss:0.381427139043808\n",
      "Epoch 7[20/39] Val Loss:0.938353955745697\n",
      "Epoch 7[21/39] Val Loss:0.965228259563446\n",
      "Epoch 7[22/39] Val Loss:0.4029521346092224\n",
      "Epoch 7[23/39] Val Loss:0.438261479139328\n",
      "Epoch 7[24/39] Val Loss:0.41553521156311035\n",
      "Epoch 7[25/39] Val Loss:0.4494657814502716\n",
      "Epoch 7[26/39] Val Loss:0.4649069309234619\n",
      "Epoch 7[27/39] Val Loss:0.3220890760421753\n",
      "Epoch 7[28/39] Val Loss:0.03673030063509941\n",
      "Epoch 7[29/39] Val Loss:0.19138634204864502\n",
      "Epoch 7[30/39] Val Loss:0.5346295237541199\n",
      "Epoch 7[31/39] Val Loss:0.4656691551208496\n",
      "Epoch 7[32/39] Val Loss:0.2569637596607208\n",
      "Epoch 7[33/39] Val Loss:0.08869072794914246\n",
      "Epoch 7[34/39] Val Loss:0.10581406950950623\n",
      "Epoch 7[35/39] Val Loss:0.07863356918096542\n",
      "Epoch 7[36/39] Val Loss:0.13804098963737488\n",
      "Epoch 7[37/39] Val Loss:0.2141992747783661\n",
      "Epoch 7[38/39] Val Loss:0.16994526982307434\n",
      "Epoch 7[39/39] Val Loss:0.08197430521249771\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93      8608\n",
      "           1       0.56      0.67      0.61      1392\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.75      0.79      0.77     10000\n",
      "weighted avg       0.89      0.88      0.89     10000\n",
      "\n",
      "Epoch 7: Train Loss 0.294430837703821, Val Loss 0.28944574649899435, Train Time 2072.206745147705, Val Time 27.874032735824585\n",
      "New best model at epoch 7\n",
      "Epoch 8[0/312] Time:8.716, Train Loss:0.2997552156448364\n",
      "Epoch 8[1/312] Time:0.706, Train Loss:0.3353428542613983\n",
      "Epoch 8[2/312] Time:8.192, Train Loss:0.3005293905735016\n",
      "Epoch 8[3/312] Time:8.681, Train Loss:0.3434991240501404\n",
      "Epoch 8[4/312] Time:0.706, Train Loss:0.23553438484668732\n",
      "Epoch 8[5/312] Time:8.521, Train Loss:0.2802312672138214\n",
      "Epoch 8[6/312] Time:8.69, Train Loss:0.282067209482193\n",
      "Epoch 8[7/312] Time:0.706, Train Loss:0.3132779598236084\n",
      "Epoch 8[8/312] Time:8.491, Train Loss:0.3142683207988739\n",
      "Epoch 8[9/312] Time:8.668, Train Loss:0.2705462872982025\n",
      "Epoch 8[10/312] Time:0.705, Train Loss:0.34141767024993896\n",
      "Epoch 8[11/312] Time:8.564, Train Loss:0.3448576331138611\n",
      "Epoch 8[12/312] Time:8.625, Train Loss:0.22739720344543457\n",
      "Epoch 8[13/312] Time:0.713, Train Loss:0.31931960582733154\n",
      "Epoch 8[14/312] Time:8.586, Train Loss:0.3455314636230469\n",
      "Epoch 8[15/312] Time:8.629, Train Loss:0.30379852652549744\n",
      "Epoch 8[16/312] Time:0.719, Train Loss:0.28155824542045593\n",
      "Epoch 8[17/312] Time:8.504, Train Loss:0.2697237730026245\n",
      "Epoch 8[18/312] Time:8.678, Train Loss:0.37919965386390686\n",
      "Epoch 8[19/312] Time:0.707, Train Loss:0.3277730643749237\n",
      "Epoch 8[20/312] Time:8.595, Train Loss:0.3100948929786682\n",
      "Epoch 8[21/312] Time:8.68, Train Loss:0.2677331566810608\n",
      "Epoch 8[22/312] Time:0.711, Train Loss:0.29672449827194214\n",
      "Epoch 8[23/312] Time:8.553, Train Loss:0.22771358489990234\n",
      "Epoch 8[24/312] Time:8.73, Train Loss:0.30478179454803467\n",
      "Epoch 8[25/312] Time:0.705, Train Loss:0.25545600056648254\n",
      "Epoch 8[26/312] Time:8.565, Train Loss:0.23614847660064697\n",
      "Epoch 8[27/312] Time:8.701, Train Loss:0.3009797930717468\n",
      "Epoch 8[28/312] Time:0.717, Train Loss:0.3181115388870239\n",
      "Epoch 8[29/312] Time:8.62, Train Loss:0.25497114658355713\n",
      "Epoch 8[30/312] Time:8.61, Train Loss:0.3055014908313751\n",
      "Epoch 8[31/312] Time:0.693, Train Loss:0.2550511956214905\n",
      "Epoch 8[32/312] Time:8.736, Train Loss:0.24944916367530823\n",
      "Epoch 8[33/312] Time:8.77, Train Loss:0.3399355709552765\n",
      "Epoch 8[34/312] Time:0.698, Train Loss:0.2621988356113434\n",
      "Epoch 8[35/312] Time:8.657, Train Loss:0.3007165789604187\n",
      "Epoch 8[36/312] Time:8.76, Train Loss:0.25442785024642944\n",
      "Epoch 8[37/312] Time:0.704, Train Loss:0.3508002758026123\n",
      "Epoch 8[38/312] Time:8.643, Train Loss:0.2565922737121582\n",
      "Epoch 8[39/312] Time:8.763, Train Loss:0.2624681293964386\n",
      "Epoch 8[40/312] Time:0.703, Train Loss:0.29500091075897217\n",
      "Epoch 8[41/312] Time:8.665, Train Loss:0.23784877359867096\n",
      "Epoch 8[42/312] Time:0.706, Train Loss:0.33607572317123413\n",
      "Epoch 8[43/312] Time:8.208, Train Loss:0.3403388261795044\n",
      "Epoch 8[44/312] Time:0.706, Train Loss:0.3458239436149597\n",
      "Epoch 8[45/312] Time:8.181, Train Loss:0.3532988131046295\n",
      "Epoch 8[46/312] Time:0.713, Train Loss:0.2700062692165375\n",
      "Epoch 8[47/312] Time:8.19, Train Loss:0.3489077091217041\n",
      "Epoch 8[48/312] Time:0.705, Train Loss:0.30346304178237915\n",
      "Epoch 8[49/312] Time:8.192, Train Loss:0.35510945320129395\n",
      "Epoch 8[50/312] Time:0.704, Train Loss:0.3727591931819916\n",
      "Epoch 8[51/312] Time:8.143, Train Loss:0.2863326966762543\n",
      "Epoch 8[52/312] Time:8.763, Train Loss:0.360391229391098\n",
      "Epoch 8[53/312] Time:0.704, Train Loss:0.22656914591789246\n",
      "Epoch 8[54/312] Time:8.651, Train Loss:0.31400349736213684\n",
      "Epoch 8[55/312] Time:0.706, Train Loss:0.30683496594429016\n",
      "Epoch 8[56/312] Time:8.164, Train Loss:0.22781269252300262\n",
      "Epoch 8[57/312] Time:8.732, Train Loss:0.22799478471279144\n",
      "Epoch 8[58/312] Time:0.703, Train Loss:0.3055371344089508\n",
      "Epoch 8[59/312] Time:8.738, Train Loss:0.25875306129455566\n",
      "Epoch 8[60/312] Time:0.705, Train Loss:0.329628050327301\n",
      "Epoch 8[61/312] Time:8.199, Train Loss:0.28022217750549316\n",
      "Epoch 8[62/312] Time:0.706, Train Loss:0.3304988443851471\n",
      "Epoch 8[63/312] Time:8.178, Train Loss:0.23923763632774353\n",
      "Epoch 8[64/312] Time:0.706, Train Loss:0.2775214612483978\n",
      "Epoch 8[65/312] Time:8.191, Train Loss:0.3669322431087494\n",
      "Epoch 8[66/312] Time:0.705, Train Loss:0.3599646985530853\n",
      "Epoch 8[67/312] Time:8.182, Train Loss:0.3404901325702667\n",
      "Epoch 8[68/312] Time:8.772, Train Loss:0.369960755109787\n",
      "Epoch 8[69/312] Time:0.7, Train Loss:0.28201839327812195\n",
      "Epoch 8[70/312] Time:8.68, Train Loss:0.19989126920700073\n",
      "Epoch 8[71/312] Time:8.697, Train Loss:0.24513109028339386\n",
      "Epoch 8[72/312] Time:0.706, Train Loss:0.23621004819869995\n",
      "Epoch 8[73/312] Time:8.746, Train Loss:0.2725209593772888\n",
      "Epoch 8[74/312] Time:0.705, Train Loss:0.24835819005966187\n",
      "Epoch 8[75/312] Time:8.198, Train Loss:0.2666192650794983\n",
      "Epoch 8[76/312] Time:0.704, Train Loss:0.22291506826877594\n",
      "Epoch 8[77/312] Time:8.202, Train Loss:0.34926265478134155\n",
      "Epoch 8[78/312] Time:4.763, Train Loss:0.24200406670570374\n",
      "Epoch 8[79/312] Time:4.059, Train Loss:0.2906204164028168\n",
      "Epoch 8[80/312] Time:0.706, Train Loss:0.2623354196548462\n",
      "Epoch 8[81/312] Time:8.184, Train Loss:0.3076702356338501\n",
      "Epoch 8[82/312] Time:0.706, Train Loss:0.3021782338619232\n",
      "Epoch 8[83/312] Time:8.191, Train Loss:0.30747026205062866\n",
      "Epoch 8[84/312] Time:0.706, Train Loss:0.26515886187553406\n",
      "Epoch 8[85/312] Time:8.177, Train Loss:0.21163460612297058\n",
      "Epoch 8[86/312] Time:0.705, Train Loss:0.25872623920440674\n",
      "Epoch 8[87/312] Time:8.224, Train Loss:0.2983250916004181\n",
      "Epoch 8[88/312] Time:0.705, Train Loss:0.30177921056747437\n",
      "Epoch 8[89/312] Time:8.173, Train Loss:0.32387852668762207\n",
      "Epoch 8[90/312] Time:0.706, Train Loss:0.19234785437583923\n",
      "Epoch 8[91/312] Time:8.161, Train Loss:0.25341519713401794\n",
      "Epoch 8[92/312] Time:0.706, Train Loss:0.2518576383590698\n",
      "Epoch 8[93/312] Time:8.161, Train Loss:0.29884058237075806\n",
      "Epoch 8[94/312] Time:8.703, Train Loss:0.2999783754348755\n",
      "Epoch 8[95/312] Time:0.704, Train Loss:0.24828554689884186\n",
      "Epoch 8[96/312] Time:8.706, Train Loss:0.1897915005683899\n",
      "Epoch 8[97/312] Time:0.697, Train Loss:0.25964033603668213\n",
      "Epoch 8[98/312] Time:8.104, Train Loss:0.23721466958522797\n",
      "Epoch 8[99/312] Time:8.73, Train Loss:0.31268879771232605\n",
      "Epoch 8[100/312] Time:0.699, Train Loss:0.16721124947071075\n",
      "Epoch 8[101/312] Time:8.729, Train Loss:0.2916083037853241\n",
      "Epoch 8[102/312] Time:8.777, Train Loss:0.3363974988460541\n",
      "Epoch 8[103/312] Time:0.712, Train Loss:0.3075668513774872\n",
      "Epoch 8[104/312] Time:8.703, Train Loss:0.23355050384998322\n",
      "Epoch 8[105/312] Time:8.748, Train Loss:0.34976106882095337\n",
      "Epoch 8[106/312] Time:0.706, Train Loss:0.345914751291275\n",
      "Epoch 8[107/312] Time:8.706, Train Loss:0.36588361859321594\n",
      "Epoch 8[108/312] Time:0.707, Train Loss:0.3445901870727539\n",
      "Epoch 8[109/312] Time:8.134, Train Loss:0.2770703434944153\n",
      "Epoch 8[110/312] Time:8.774, Train Loss:0.31359514594078064\n",
      "Epoch 8[111/312] Time:0.7, Train Loss:0.22955070436000824\n",
      "Epoch 8[112/312] Time:8.711, Train Loss:0.21798008680343628\n",
      "Epoch 8[113/312] Time:8.767, Train Loss:0.27056077122688293\n",
      "Epoch 8[114/312] Time:0.704, Train Loss:0.20333231985569\n",
      "Epoch 8[115/312] Time:8.664, Train Loss:0.3131769001483917\n",
      "Epoch 8[116/312] Time:0.706, Train Loss:0.33863765001296997\n",
      "Epoch 8[117/312] Time:8.262, Train Loss:0.2543683648109436\n",
      "Epoch 8[118/312] Time:0.704, Train Loss:0.324062705039978\n",
      "Epoch 8[119/312] Time:8.175, Train Loss:0.2621693015098572\n",
      "Epoch 8[120/312] Time:0.705, Train Loss:0.24364463984966278\n",
      "Epoch 8[121/312] Time:8.212, Train Loss:0.28286731243133545\n",
      "Epoch 8[122/312] Time:0.705, Train Loss:0.2741536498069763\n",
      "Epoch 8[123/312] Time:8.157, Train Loss:0.29324111342430115\n",
      "Epoch 8[124/312] Time:0.704, Train Loss:0.23258548974990845\n",
      "Epoch 8[125/312] Time:8.209, Train Loss:0.26493561267852783\n",
      "Epoch 8[126/312] Time:0.713, Train Loss:0.25069740414619446\n",
      "Epoch 8[127/312] Time:8.181, Train Loss:0.2898191511631012\n",
      "Epoch 8[128/312] Time:0.706, Train Loss:0.22935356199741364\n",
      "Epoch 8[129/312] Time:8.201, Train Loss:0.2650613784790039\n",
      "Epoch 8[130/312] Time:0.706, Train Loss:0.3196302354335785\n",
      "Epoch 8[131/312] Time:8.164, Train Loss:0.31037378311157227\n",
      "Epoch 8[132/312] Time:8.739, Train Loss:0.319449245929718\n",
      "Epoch 8[133/312] Time:0.75, Train Loss:0.3065682351589203\n",
      "Epoch 8[134/312] Time:8.62, Train Loss:0.24584168195724487\n",
      "Epoch 8[135/312] Time:8.749, Train Loss:0.29246485233306885\n",
      "Epoch 8[136/312] Time:0.705, Train Loss:0.2109544277191162\n",
      "Epoch 8[137/312] Time:8.564, Train Loss:0.294611394405365\n",
      "Epoch 8[138/312] Time:0.705, Train Loss:0.29084232449531555\n",
      "Epoch 8[139/312] Time:8.46, Train Loss:0.29292231798171997\n",
      "Epoch 8[140/312] Time:0.706, Train Loss:0.30504682660102844\n",
      "Epoch 8[141/312] Time:8.197, Train Loss:0.30235815048217773\n",
      "Epoch 8[142/312] Time:0.705, Train Loss:0.2910114824771881\n",
      "Epoch 8[143/312] Time:8.2, Train Loss:0.3800315260887146\n",
      "Epoch 8[144/312] Time:0.706, Train Loss:0.27396321296691895\n",
      "Epoch 8[145/312] Time:8.169, Train Loss:0.2792622447013855\n",
      "Epoch 8[146/312] Time:0.705, Train Loss:0.2876548171043396\n",
      "Epoch 8[147/312] Time:8.222, Train Loss:0.3291679322719574\n",
      "Epoch 8[148/312] Time:0.703, Train Loss:0.2515760064125061\n",
      "Epoch 8[149/312] Time:8.211, Train Loss:0.2681272327899933\n",
      "Epoch 8[150/312] Time:0.706, Train Loss:0.2885110676288605\n",
      "Epoch 8[151/312] Time:8.184, Train Loss:0.32503849267959595\n",
      "Epoch 8[152/312] Time:8.717, Train Loss:0.3358120322227478\n",
      "Epoch 8[153/312] Time:0.7, Train Loss:0.318227082490921\n",
      "Epoch 8[154/312] Time:8.704, Train Loss:0.3302767276763916\n",
      "Epoch 8[155/312] Time:0.701, Train Loss:0.21816149353981018\n",
      "Epoch 8[156/312] Time:8.046, Train Loss:0.29344356060028076\n",
      "Epoch 8[157/312] Time:8.718, Train Loss:0.21802569925785065\n",
      "Epoch 8[158/312] Time:0.705, Train Loss:0.24061256647109985\n",
      "Epoch 8[159/312] Time:8.498, Train Loss:0.2826215624809265\n",
      "Epoch 8[160/312] Time:0.707, Train Loss:0.34360745549201965\n",
      "Epoch 8[161/312] Time:8.483, Train Loss:0.18733720481395721\n",
      "Epoch 8[162/312] Time:0.706, Train Loss:0.38048118352890015\n",
      "Epoch 8[163/312] Time:8.161, Train Loss:0.3165377080440521\n",
      "Epoch 8[164/312] Time:0.707, Train Loss:0.23440903425216675\n",
      "Epoch 8[165/312] Time:8.238, Train Loss:0.2637242376804352\n",
      "Epoch 8[166/312] Time:0.707, Train Loss:0.282529354095459\n",
      "Epoch 8[167/312] Time:8.147, Train Loss:0.31830620765686035\n",
      "Epoch 8[168/312] Time:0.705, Train Loss:0.26092830300331116\n",
      "Epoch 8[169/312] Time:8.174, Train Loss:0.26911163330078125\n",
      "Epoch 8[170/312] Time:0.704, Train Loss:0.2160106897354126\n",
      "Epoch 8[171/312] Time:8.201, Train Loss:0.28922760486602783\n",
      "Epoch 8[172/312] Time:0.706, Train Loss:0.31158190965652466\n",
      "Epoch 8[173/312] Time:8.21, Train Loss:0.25070998072624207\n",
      "Epoch 8[174/312] Time:0.707, Train Loss:0.30012834072113037\n",
      "Epoch 8[175/312] Time:8.199, Train Loss:0.26641660928726196\n",
      "Epoch 8[176/312] Time:0.705, Train Loss:0.23124711215496063\n",
      "Epoch 8[177/312] Time:8.221, Train Loss:0.27119746804237366\n",
      "Epoch 8[178/312] Time:8.756, Train Loss:0.23047959804534912\n",
      "Epoch 8[179/312] Time:0.695, Train Loss:0.22927351295948029\n",
      "Epoch 8[180/312] Time:8.568, Train Loss:0.26194193959236145\n",
      "Epoch 8[181/312] Time:8.758, Train Loss:0.2906436324119568\n",
      "Epoch 8[182/312] Time:0.694, Train Loss:0.30605390667915344\n",
      "Epoch 8[183/312] Time:8.661, Train Loss:0.2566521465778351\n",
      "Epoch 8[184/312] Time:8.707, Train Loss:0.2493046224117279\n",
      "Epoch 8[185/312] Time:0.695, Train Loss:0.23117156326770782\n",
      "Epoch 8[186/312] Time:8.603, Train Loss:0.2961188852787018\n",
      "Epoch 8[187/312] Time:8.678, Train Loss:0.32273709774017334\n",
      "Epoch 8[188/312] Time:0.694, Train Loss:0.22010555863380432\n",
      "Epoch 8[189/312] Time:8.649, Train Loss:0.3504682183265686\n",
      "Epoch 8[190/312] Time:0.705, Train Loss:0.24228522181510925\n",
      "Epoch 8[191/312] Time:8.143, Train Loss:0.25483381748199463\n",
      "Epoch 8[192/312] Time:0.706, Train Loss:0.34125715494155884\n",
      "Epoch 8[193/312] Time:8.227, Train Loss:0.19539637863636017\n",
      "Epoch 8[194/312] Time:8.763, Train Loss:0.30389824509620667\n",
      "Epoch 8[195/312] Time:0.698, Train Loss:0.3153700530529022\n",
      "Epoch 8[196/312] Time:8.494, Train Loss:0.33465906977653503\n",
      "Epoch 8[197/312] Time:0.705, Train Loss:0.33317312598228455\n",
      "Epoch 8[198/312] Time:8.466, Train Loss:0.3162296712398529\n",
      "Epoch 8[199/312] Time:0.705, Train Loss:0.3753991425037384\n",
      "Epoch 8[200/312] Time:8.215, Train Loss:0.27047067880630493\n",
      "Epoch 8[201/312] Time:0.707, Train Loss:0.24366289377212524\n",
      "Epoch 8[202/312] Time:8.211, Train Loss:0.27247074246406555\n",
      "Epoch 8[203/312] Time:0.705, Train Loss:0.2832484841346741\n",
      "Epoch 8[204/312] Time:8.204, Train Loss:0.36505693197250366\n",
      "Epoch 8[205/312] Time:0.705, Train Loss:0.23710031807422638\n",
      "Epoch 8[206/312] Time:8.192, Train Loss:0.2944919764995575\n",
      "Epoch 8[207/312] Time:8.758, Train Loss:0.26056328415870667\n",
      "Epoch 8[208/312] Time:0.704, Train Loss:0.21729229390621185\n",
      "Epoch 8[209/312] Time:8.651, Train Loss:0.2676219642162323\n",
      "Epoch 8[210/312] Time:0.7, Train Loss:0.2511827051639557\n",
      "Epoch 8[211/312] Time:8.113, Train Loss:0.24455393850803375\n",
      "Epoch 8[212/312] Time:8.77, Train Loss:0.2867053747177124\n",
      "Epoch 8[213/312] Time:0.696, Train Loss:0.3183307945728302\n",
      "Epoch 8[214/312] Time:8.683, Train Loss:0.2832414507865906\n",
      "Epoch 8[215/312] Time:0.707, Train Loss:0.24774807691574097\n",
      "Epoch 8[216/312] Time:8.168, Train Loss:0.22658731043338776\n",
      "Epoch 8[217/312] Time:0.704, Train Loss:0.27862977981567383\n",
      "Epoch 8[218/312] Time:8.192, Train Loss:0.29318100214004517\n",
      "Epoch 8[219/312] Time:8.754, Train Loss:0.3026641309261322\n",
      "Epoch 8[220/312] Time:0.698, Train Loss:0.27723708748817444\n",
      "Epoch 8[221/312] Time:8.65, Train Loss:0.31411972641944885\n",
      "Epoch 8[222/312] Time:8.745, Train Loss:0.21989956498146057\n",
      "Epoch 8[223/312] Time:0.694, Train Loss:0.29562509059906006\n",
      "Epoch 8[224/312] Time:8.719, Train Loss:0.18487124145030975\n",
      "Epoch 8[225/312] Time:0.695, Train Loss:0.24181592464447021\n",
      "Epoch 8[226/312] Time:8.135, Train Loss:0.3066326081752777\n",
      "Epoch 8[227/312] Time:0.696, Train Loss:0.22922755777835846\n",
      "Epoch 8[228/312] Time:8.166, Train Loss:0.28784647583961487\n",
      "Epoch 8[229/312] Time:8.755, Train Loss:0.30419501662254333\n",
      "Epoch 8[230/312] Time:0.7, Train Loss:0.27870500087738037\n",
      "Epoch 8[231/312] Time:8.71, Train Loss:0.27854159474372864\n",
      "Epoch 8[232/312] Time:8.721, Train Loss:0.3578648567199707\n",
      "Epoch 8[233/312] Time:0.694, Train Loss:0.306911826133728\n",
      "Epoch 8[234/312] Time:8.675, Train Loss:0.33519554138183594\n",
      "Epoch 8[235/312] Time:8.749, Train Loss:0.29378387331962585\n",
      "Epoch 8[236/312] Time:0.695, Train Loss:0.3701738715171814\n",
      "Epoch 8[237/312] Time:8.605, Train Loss:0.24837356805801392\n",
      "Epoch 8[238/312] Time:8.732, Train Loss:0.27324438095092773\n",
      "Epoch 8[239/312] Time:0.705, Train Loss:0.44215360283851624\n",
      "Epoch 8[240/312] Time:8.53, Train Loss:0.34509870409965515\n",
      "Epoch 8[241/312] Time:0.705, Train Loss:0.36168989539146423\n",
      "Epoch 8[242/312] Time:8.466, Train Loss:0.22372369468212128\n",
      "Epoch 8[243/312] Time:0.707, Train Loss:0.3032624423503876\n",
      "Epoch 8[244/312] Time:8.234, Train Loss:0.26604989171028137\n",
      "Epoch 8[245/312] Time:0.708, Train Loss:0.2901821732521057\n",
      "Epoch 8[246/312] Time:8.163, Train Loss:0.31866684556007385\n",
      "Epoch 8[247/312] Time:0.706, Train Loss:0.31367525458335876\n",
      "Epoch 8[248/312] Time:8.161, Train Loss:0.25984498858451843\n",
      "Epoch 8[249/312] Time:0.704, Train Loss:0.31317049264907837\n",
      "Epoch 8[250/312] Time:8.175, Train Loss:0.2368767410516739\n",
      "Epoch 8[251/312] Time:0.704, Train Loss:0.32122620940208435\n",
      "Epoch 8[252/312] Time:8.21, Train Loss:0.31339266896247864\n",
      "Epoch 8[253/312] Time:0.707, Train Loss:0.246261864900589\n",
      "Epoch 8[254/312] Time:8.206, Train Loss:0.24580083787441254\n",
      "Epoch 8[255/312] Time:0.707, Train Loss:0.26858675479888916\n",
      "Epoch 8[256/312] Time:8.208, Train Loss:0.2473686784505844\n",
      "Epoch 8[257/312] Time:0.704, Train Loss:0.2479434460401535\n",
      "Epoch 8[258/312] Time:8.188, Train Loss:0.3217840790748596\n",
      "Epoch 8[259/312] Time:0.706, Train Loss:0.2537860572338104\n",
      "Epoch 8[260/312] Time:8.241, Train Loss:0.24141325056552887\n",
      "Epoch 8[261/312] Time:0.705, Train Loss:0.22875303030014038\n",
      "Epoch 8[262/312] Time:8.128, Train Loss:0.29590195417404175\n",
      "Epoch 8[263/312] Time:8.715, Train Loss:0.3395364284515381\n",
      "Epoch 8[264/312] Time:0.699, Train Loss:0.2185935527086258\n",
      "Epoch 8[265/312] Time:8.663, Train Loss:0.3016616702079773\n",
      "Epoch 8[266/312] Time:0.698, Train Loss:0.31222036480903625\n",
      "Epoch 8[267/312] Time:8.124, Train Loss:0.30990031361579895\n",
      "Epoch 8[268/312] Time:8.695, Train Loss:0.22027587890625\n",
      "Epoch 8[269/312] Time:0.694, Train Loss:0.27005159854888916\n",
      "Epoch 8[270/312] Time:8.69, Train Loss:0.30516234040260315\n",
      "Epoch 8[271/312] Time:8.703, Train Loss:0.3039562702178955\n",
      "Epoch 8[272/312] Time:0.694, Train Loss:0.22691300511360168\n",
      "Epoch 8[273/312] Time:8.735, Train Loss:0.1948554664850235\n",
      "Epoch 8[274/312] Time:4.736, Train Loss:0.3785475790500641\n",
      "Epoch 8[275/312] Time:4.034, Train Loss:0.3977756202220917\n",
      "Epoch 8[276/312] Time:8.755, Train Loss:0.3332087993621826\n",
      "Epoch 8[277/312] Time:0.699, Train Loss:0.39721259474754333\n",
      "Epoch 8[278/312] Time:8.584, Train Loss:0.2853899598121643\n",
      "Epoch 8[279/312] Time:0.696, Train Loss:0.27755269408226013\n",
      "Epoch 8[280/312] Time:8.322, Train Loss:0.33082184195518494\n",
      "Epoch 8[281/312] Time:0.705, Train Loss:0.2566310465335846\n",
      "Epoch 8[282/312] Time:8.182, Train Loss:0.34286126494407654\n",
      "Epoch 8[283/312] Time:0.704, Train Loss:0.3152538239955902\n",
      "Epoch 8[284/312] Time:8.172, Train Loss:0.29121920466423035\n",
      "Epoch 8[285/312] Time:0.704, Train Loss:0.2610330283641815\n",
      "Epoch 8[286/312] Time:8.22, Train Loss:0.20152945816516876\n",
      "Epoch 8[287/312] Time:0.705, Train Loss:0.29005396366119385\n",
      "Epoch 8[288/312] Time:8.2, Train Loss:0.2692842483520508\n",
      "Epoch 8[289/312] Time:8.731, Train Loss:0.3505820333957672\n",
      "Epoch 8[290/312] Time:0.696, Train Loss:0.26739150285720825\n",
      "Epoch 8[291/312] Time:8.617, Train Loss:0.24890895187854767\n",
      "Epoch 8[292/312] Time:0.705, Train Loss:0.24698719382286072\n",
      "Epoch 8[293/312] Time:8.187, Train Loss:0.21930739283561707\n",
      "Epoch 8[294/312] Time:0.709, Train Loss:0.28329771757125854\n",
      "Epoch 8[295/312] Time:8.177, Train Loss:0.23949596285820007\n",
      "Epoch 8[296/312] Time:0.705, Train Loss:0.2943934500217438\n",
      "Epoch 8[297/312] Time:8.203, Train Loss:0.2594575881958008\n",
      "Epoch 8[298/312] Time:0.705, Train Loss:0.3189926743507385\n",
      "Epoch 8[299/312] Time:8.176, Train Loss:0.2672552168369293\n",
      "Epoch 8[300/312] Time:0.705, Train Loss:0.32894235849380493\n",
      "Epoch 8[301/312] Time:8.195, Train Loss:0.2795569598674774\n",
      "Epoch 8[302/312] Time:0.705, Train Loss:0.3185942471027374\n",
      "Epoch 8[303/312] Time:8.165, Train Loss:0.302172988653183\n",
      "Epoch 8[304/312] Time:0.714, Train Loss:0.31128057837486267\n",
      "Epoch 8[305/312] Time:8.157, Train Loss:0.258684903383255\n",
      "Epoch 8[306/312] Time:0.705, Train Loss:0.2582297623157501\n",
      "Epoch 8[307/312] Time:8.192, Train Loss:0.279201865196228\n",
      "Epoch 8[308/312] Time:0.705, Train Loss:0.29417189955711365\n",
      "Epoch 8[309/312] Time:8.173, Train Loss:0.3527233600616455\n",
      "Epoch 8[310/312] Time:0.704, Train Loss:0.3188968598842621\n",
      "Epoch 8[311/312] Time:8.19, Train Loss:0.24403560161590576\n",
      "Epoch 8[0/39] Val Loss:0.29855725169181824\n",
      "Epoch 8[1/39] Val Loss:0.32967081665992737\n",
      "Epoch 8[2/39] Val Loss:0.3013710081577301\n",
      "Epoch 8[3/39] Val Loss:0.34327319264411926\n",
      "Epoch 8[4/39] Val Loss:0.3130655884742737\n",
      "Epoch 8[5/39] Val Loss:0.2479642629623413\n",
      "Epoch 8[6/39] Val Loss:0.22231145203113556\n",
      "Epoch 8[7/39] Val Loss:0.18833985924720764\n",
      "Epoch 8[8/39] Val Loss:0.2644645571708679\n",
      "Epoch 8[9/39] Val Loss:0.27461937069892883\n",
      "Epoch 8[10/39] Val Loss:0.09680937975645065\n",
      "Epoch 8[11/39] Val Loss:0.08433432877063751\n",
      "Epoch 8[12/39] Val Loss:0.4361676871776581\n",
      "Epoch 8[13/39] Val Loss:0.2731253206729889\n",
      "Epoch 8[14/39] Val Loss:0.1454257220029831\n",
      "Epoch 8[15/39] Val Loss:0.17107494175434113\n",
      "Epoch 8[16/39] Val Loss:0.24232253432273865\n",
      "Epoch 8[17/39] Val Loss:0.33039045333862305\n",
      "Epoch 8[18/39] Val Loss:0.3369336724281311\n",
      "Epoch 8[19/39] Val Loss:0.38956722617149353\n",
      "Epoch 8[20/39] Val Loss:0.7165191769599915\n",
      "Epoch 8[21/39] Val Loss:0.7739418745040894\n",
      "Epoch 8[22/39] Val Loss:0.34216615557670593\n",
      "Epoch 8[23/39] Val Loss:0.35142403841018677\n",
      "Epoch 8[24/39] Val Loss:0.4196667969226837\n",
      "Epoch 8[25/39] Val Loss:0.4590893089771271\n",
      "Epoch 8[26/39] Val Loss:0.37617939710617065\n",
      "Epoch 8[27/39] Val Loss:0.2792764902114868\n",
      "Epoch 8[28/39] Val Loss:0.05329640582203865\n",
      "Epoch 8[29/39] Val Loss:0.1954895406961441\n",
      "Epoch 8[30/39] Val Loss:0.4736483097076416\n",
      "Epoch 8[31/39] Val Loss:0.4072209298610687\n",
      "Epoch 8[32/39] Val Loss:0.24473483860492706\n",
      "Epoch 8[33/39] Val Loss:0.11139529943466187\n",
      "Epoch 8[34/39] Val Loss:0.1391998678445816\n",
      "Epoch 8[35/39] Val Loss:0.0989479348063469\n",
      "Epoch 8[36/39] Val Loss:0.17389780282974243\n",
      "Epoch 8[37/39] Val Loss:0.2189129889011383\n",
      "Epoch 8[38/39] Val Loss:0.15317733585834503\n",
      "Epoch 8[39/39] Val Loss:0.07379981130361557\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93      8608\n",
      "           1       0.56      0.74      0.64      1392\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.76      0.82      0.78     10000\n",
      "weighted avg       0.90      0.88      0.89     10000\n",
      "\n",
      "Epoch 8: Train Loss 0.285859607733213, Val Loss 0.2910711007813613, Train Time 1789.9187259674072, Val Time 18.502274990081787\n",
      "Epoch 9[0/312] Time:0.694, Train Loss:0.2630905210971832\n",
      "Epoch 9[1/312] Time:8.73, Train Loss:0.21018505096435547\n",
      "Epoch 9[2/312] Time:0.704, Train Loss:0.28682103753089905\n",
      "Epoch 9[3/312] Time:8.694, Train Loss:0.31691229343414307\n",
      "Epoch 9[4/312] Time:8.765, Train Loss:0.332595556974411\n",
      "Epoch 9[5/312] Time:0.695, Train Loss:0.31700369715690613\n",
      "Epoch 9[6/312] Time:8.582, Train Loss:0.2936737537384033\n",
      "Epoch 9[7/312] Time:0.704, Train Loss:0.237763449549675\n",
      "Epoch 9[8/312] Time:8.407, Train Loss:0.2790546119213104\n",
      "Epoch 9[9/312] Time:0.706, Train Loss:0.27992185950279236\n",
      "Epoch 9[10/312] Time:8.186, Train Loss:0.22494998574256897\n",
      "Epoch 9[11/312] Time:0.704, Train Loss:0.19344551861286163\n",
      "Epoch 9[12/312] Time:8.115, Train Loss:0.3520437479019165\n",
      "Epoch 9[13/312] Time:8.709, Train Loss:0.3209240138530731\n",
      "Epoch 9[14/312] Time:0.703, Train Loss:0.3407531678676605\n",
      "Epoch 9[15/312] Time:8.705, Train Loss:0.309899240732193\n",
      "Epoch 9[16/312] Time:0.706, Train Loss:0.2715597450733185\n",
      "Epoch 9[17/312] Time:8.234, Train Loss:0.21299180388450623\n",
      "Epoch 9[18/312] Time:0.705, Train Loss:0.29192280769348145\n",
      "Epoch 9[19/312] Time:8.201, Train Loss:0.34079208970069885\n",
      "Epoch 9[20/312] Time:0.706, Train Loss:0.274190217256546\n",
      "Epoch 9[21/312] Time:8.177, Train Loss:0.3046385943889618\n",
      "Epoch 9[22/312] Time:0.709, Train Loss:0.3813880383968353\n",
      "Epoch 9[23/312] Time:8.17, Train Loss:0.27507102489471436\n",
      "Epoch 9[24/312] Time:0.707, Train Loss:0.3056720197200775\n",
      "Epoch 9[25/312] Time:8.193, Train Loss:0.2946408689022064\n",
      "Epoch 9[26/312] Time:0.705, Train Loss:0.23222345113754272\n",
      "Epoch 9[27/312] Time:8.208, Train Loss:0.27577582001686096\n",
      "Epoch 9[28/312] Time:0.706, Train Loss:0.2914808988571167\n",
      "Epoch 9[29/312] Time:8.206, Train Loss:0.19963333010673523\n",
      "Epoch 9[30/312] Time:0.704, Train Loss:0.30782201886177063\n",
      "Epoch 9[31/312] Time:8.188, Train Loss:0.3214370906352997\n",
      "Epoch 9[32/312] Time:8.73, Train Loss:0.2614096403121948\n",
      "Epoch 9[33/312] Time:0.704, Train Loss:0.2625884413719177\n",
      "Epoch 9[34/312] Time:8.651, Train Loss:0.2983855903148651\n",
      "Epoch 9[35/312] Time:0.707, Train Loss:0.23914819955825806\n",
      "Epoch 9[36/312] Time:8.318, Train Loss:0.2443714588880539\n",
      "Epoch 9[37/312] Time:0.706, Train Loss:0.2792389988899231\n",
      "Epoch 9[38/312] Time:8.206, Train Loss:0.19823119044303894\n",
      "Epoch 9[39/312] Time:8.742, Train Loss:0.30120283365249634\n",
      "Epoch 9[40/312] Time:0.705, Train Loss:0.2749931514263153\n",
      "Epoch 9[41/312] Time:8.725, Train Loss:0.23163016140460968\n",
      "Epoch 9[42/312] Time:0.706, Train Loss:0.2750526964664459\n",
      "Epoch 9[43/312] Time:8.206, Train Loss:0.29526883363723755\n",
      "Epoch 9[44/312] Time:0.707, Train Loss:0.3625842034816742\n",
      "Epoch 9[45/312] Time:8.183, Train Loss:0.2516040802001953\n",
      "Epoch 9[46/312] Time:0.707, Train Loss:0.2743414640426636\n",
      "Epoch 9[47/312] Time:8.177, Train Loss:0.2630218267440796\n",
      "Epoch 9[48/312] Time:0.706, Train Loss:0.2266858071088791\n",
      "Epoch 9[49/312] Time:8.17, Train Loss:0.25842705368995667\n",
      "Epoch 9[50/312] Time:0.705, Train Loss:0.27930235862731934\n",
      "Epoch 9[51/312] Time:8.204, Train Loss:0.3007439374923706\n",
      "Epoch 9[52/312] Time:0.703, Train Loss:0.21261045336723328\n",
      "Epoch 9[53/312] Time:8.181, Train Loss:0.2065548598766327\n",
      "Epoch 9[54/312] Time:8.763, Train Loss:0.2859613299369812\n",
      "Epoch 9[55/312] Time:0.701, Train Loss:0.2701075077056885\n",
      "Epoch 9[56/312] Time:8.517, Train Loss:0.32906612753868103\n",
      "Epoch 9[57/312] Time:0.706, Train Loss:0.2759043872356415\n",
      "Epoch 9[58/312] Time:8.473, Train Loss:0.2426682859659195\n",
      "Epoch 9[59/312] Time:8.729, Train Loss:0.22667035460472107\n",
      "Epoch 9[60/312] Time:0.705, Train Loss:0.2300398051738739\n",
      "Epoch 9[61/312] Time:8.729, Train Loss:0.38433730602264404\n",
      "Epoch 9[62/312] Time:8.704, Train Loss:0.37539809942245483\n",
      "Epoch 9[63/312] Time:0.712, Train Loss:0.29256516695022583\n",
      "Epoch 9[64/312] Time:8.525, Train Loss:0.2784106135368347\n",
      "Epoch 9[65/312] Time:0.705, Train Loss:0.2684294879436493\n",
      "Epoch 9[66/312] Time:8.457, Train Loss:0.19912134110927582\n",
      "Epoch 9[67/312] Time:0.705, Train Loss:0.2865504026412964\n",
      "Epoch 9[68/312] Time:8.12, Train Loss:0.2638985514640808\n",
      "Epoch 9[69/312] Time:8.743, Train Loss:0.25628772377967834\n",
      "Epoch 9[70/312] Time:0.705, Train Loss:0.284449964761734\n",
      "Epoch 9[71/312] Time:8.516, Train Loss:0.41636306047439575\n",
      "Epoch 9[72/312] Time:0.704, Train Loss:0.21992266178131104\n",
      "Epoch 9[73/312] Time:8.468, Train Loss:0.20735031366348267\n",
      "Epoch 9[74/312] Time:0.705, Train Loss:0.2635330259799957\n",
      "Epoch 9[75/312] Time:8.184, Train Loss:0.23186106979846954\n",
      "Epoch 9[76/312] Time:0.707, Train Loss:0.2736009657382965\n",
      "Epoch 9[77/312] Time:8.136, Train Loss:0.2886945307254791\n",
      "Epoch 9[78/312] Time:0.708, Train Loss:0.2672386169433594\n",
      "Epoch 9[79/312] Time:8.145, Train Loss:0.2484128624200821\n",
      "Epoch 9[80/312] Time:4.738, Train Loss:0.25290337204933167\n",
      "Epoch 9[81/312] Time:4.056, Train Loss:0.2591018080711365\n",
      "Epoch 9[82/312] Time:0.705, Train Loss:0.2862134277820587\n",
      "Epoch 9[83/312] Time:8.179, Train Loss:0.2151474952697754\n",
      "Epoch 9[84/312] Time:0.705, Train Loss:0.23917637765407562\n",
      "Epoch 9[85/312] Time:8.152, Train Loss:0.27654969692230225\n",
      "Epoch 9[86/312] Time:0.705, Train Loss:0.34944018721580505\n",
      "Epoch 9[87/312] Time:8.168, Train Loss:0.22446246445178986\n",
      "Epoch 9[88/312] Time:0.705, Train Loss:0.28406432271003723\n",
      "Epoch 9[89/312] Time:8.15, Train Loss:0.2948421537876129\n",
      "Epoch 9[90/312] Time:8.703, Train Loss:0.337092787027359\n",
      "Epoch 9[91/312] Time:0.705, Train Loss:0.2501384913921356\n",
      "Epoch 9[92/312] Time:8.498, Train Loss:0.2955847978591919\n",
      "Epoch 9[93/312] Time:0.704, Train Loss:0.2711937129497528\n",
      "Epoch 9[94/312] Time:8.486, Train Loss:0.32229283452033997\n",
      "Epoch 9[95/312] Time:0.704, Train Loss:0.2880616784095764\n",
      "Epoch 9[96/312] Time:8.181, Train Loss:0.2791847884654999\n",
      "Epoch 9[97/312] Time:0.705, Train Loss:0.2896524667739868\n",
      "Epoch 9[98/312] Time:8.202, Train Loss:0.28534388542175293\n",
      "Epoch 9[99/312] Time:0.705, Train Loss:0.3278833031654358\n",
      "Epoch 9[100/312] Time:8.118, Train Loss:0.24847209453582764\n",
      "Epoch 9[101/312] Time:8.695, Train Loss:0.29410427808761597\n",
      "Epoch 9[102/312] Time:0.703, Train Loss:0.35751834511756897\n",
      "Epoch 9[103/312] Time:8.618, Train Loss:0.3000313937664032\n",
      "Epoch 9[104/312] Time:0.704, Train Loss:0.2576715350151062\n",
      "Epoch 9[105/312] Time:8.304, Train Loss:0.3311471939086914\n",
      "Epoch 9[106/312] Time:0.705, Train Loss:0.2079460620880127\n",
      "Epoch 9[107/312] Time:8.155, Train Loss:0.23158761858940125\n",
      "Epoch 9[108/312] Time:0.705, Train Loss:0.36181768774986267\n",
      "Epoch 9[109/312] Time:8.234, Train Loss:0.3104702830314636\n",
      "Epoch 9[110/312] Time:8.716, Train Loss:0.253554105758667\n",
      "Epoch 9[111/312] Time:0.703, Train Loss:0.2718343734741211\n",
      "Epoch 9[112/312] Time:8.71, Train Loss:0.41091200709342957\n",
      "Epoch 9[113/312] Time:0.705, Train Loss:0.305260568857193\n",
      "Epoch 9[114/312] Time:8.25, Train Loss:0.24607262015342712\n",
      "Epoch 9[115/312] Time:8.718, Train Loss:0.24046868085861206\n",
      "Epoch 9[116/312] Time:0.703, Train Loss:0.24713826179504395\n",
      "Epoch 9[117/312] Time:8.726, Train Loss:0.33756938576698303\n",
      "Epoch 9[118/312] Time:0.705, Train Loss:0.23388409614562988\n",
      "Epoch 9[119/312] Time:8.153, Train Loss:0.3027193248271942\n",
      "Epoch 9[120/312] Time:0.704, Train Loss:0.23837223649024963\n",
      "Epoch 9[121/312] Time:8.201, Train Loss:0.22079457342624664\n",
      "Epoch 9[122/312] Time:0.708, Train Loss:0.3215917944908142\n",
      "Epoch 9[123/312] Time:8.192, Train Loss:0.29850783944129944\n",
      "Epoch 9[124/312] Time:0.707, Train Loss:0.32850420475006104\n",
      "Epoch 9[125/312] Time:8.159, Train Loss:0.24000848829746246\n",
      "Epoch 9[126/312] Time:0.706, Train Loss:0.28957369923591614\n",
      "Epoch 9[127/312] Time:8.154, Train Loss:0.26024025678634644\n",
      "Epoch 9[128/312] Time:8.765, Train Loss:0.20806507766246796\n",
      "Epoch 9[129/312] Time:0.712, Train Loss:0.23891179263591766\n",
      "Epoch 9[130/312] Time:8.743, Train Loss:0.30646488070487976\n",
      "Epoch 9[131/312] Time:0.703, Train Loss:0.2813977003097534\n",
      "Epoch 9[132/312] Time:8.185, Train Loss:0.246187224984169\n",
      "Epoch 9[133/312] Time:0.706, Train Loss:0.23874685168266296\n",
      "Epoch 9[134/312] Time:8.18, Train Loss:0.20531664788722992\n",
      "Epoch 9[135/312] Time:0.704, Train Loss:0.17226947844028473\n",
      "Epoch 9[136/312] Time:8.197, Train Loss:0.3034925162792206\n",
      "Epoch 9[137/312] Time:0.705, Train Loss:0.24449656903743744\n",
      "Epoch 9[138/312] Time:8.183, Train Loss:0.22798626124858856\n",
      "Epoch 9[139/312] Time:0.705, Train Loss:0.26767638325691223\n",
      "Epoch 9[140/312] Time:8.198, Train Loss:0.32566481828689575\n",
      "Epoch 9[141/312] Time:8.558, Train Loss:0.252235472202301\n",
      "Epoch 9[142/312] Time:0.695, Train Loss:0.26485148072242737\n",
      "Epoch 9[143/312] Time:8.728, Train Loss:0.2953605353832245\n",
      "Epoch 9[144/312] Time:8.731, Train Loss:0.26933854818344116\n",
      "Epoch 9[145/312] Time:0.695, Train Loss:0.3105292320251465\n",
      "Epoch 9[146/312] Time:8.661, Train Loss:0.18827560544013977\n",
      "Epoch 9[147/312] Time:0.704, Train Loss:0.3176567256450653\n",
      "Epoch 9[148/312] Time:8.182, Train Loss:0.28330934047698975\n",
      "Epoch 9[149/312] Time:0.705, Train Loss:0.3095010221004486\n",
      "Epoch 9[150/312] Time:8.22, Train Loss:0.285162091255188\n",
      "Epoch 9[151/312] Time:8.688, Train Loss:0.2799649238586426\n",
      "Epoch 9[152/312] Time:0.703, Train Loss:0.2923657298088074\n",
      "Epoch 9[153/312] Time:8.601, Train Loss:0.2832939624786377\n",
      "Epoch 9[154/312] Time:0.706, Train Loss:0.256181538105011\n",
      "Epoch 9[155/312] Time:8.373, Train Loss:0.294950395822525\n",
      "Epoch 9[156/312] Time:0.704, Train Loss:0.2378961145877838\n",
      "Epoch 9[157/312] Time:8.19, Train Loss:0.2750459909439087\n",
      "Epoch 9[158/312] Time:0.707, Train Loss:0.2620616853237152\n",
      "Epoch 9[159/312] Time:8.165, Train Loss:0.28961703181266785\n",
      "Epoch 9[160/312] Time:8.694, Train Loss:0.35290807485580444\n",
      "Epoch 9[161/312] Time:0.7, Train Loss:0.3138223886489868\n",
      "Epoch 9[162/312] Time:8.626, Train Loss:0.2429373413324356\n",
      "Epoch 9[163/312] Time:8.63, Train Loss:0.26451876759529114\n",
      "Epoch 9[164/312] Time:0.706, Train Loss:0.27668848633766174\n",
      "Epoch 9[165/312] Time:8.626, Train Loss:0.3303254544734955\n",
      "Epoch 9[166/312] Time:0.704, Train Loss:0.28338074684143066\n",
      "Epoch 9[167/312] Time:8.157, Train Loss:0.2718092203140259\n",
      "Epoch 9[168/312] Time:0.705, Train Loss:0.24921490252017975\n",
      "Epoch 9[169/312] Time:8.208, Train Loss:0.2527981996536255\n",
      "Epoch 9[170/312] Time:0.705, Train Loss:0.2938191592693329\n",
      "Epoch 9[171/312] Time:8.19, Train Loss:0.2531634271144867\n",
      "Epoch 9[172/312] Time:0.705, Train Loss:0.18297144770622253\n",
      "Epoch 9[173/312] Time:8.2, Train Loss:0.1727071851491928\n",
      "Epoch 9[174/312] Time:0.707, Train Loss:0.24671205878257751\n",
      "Epoch 9[175/312] Time:8.08, Train Loss:0.21015670895576477\n",
      "Epoch 9[176/312] Time:0.709, Train Loss:0.24822866916656494\n",
      "Epoch 9[177/312] Time:8.174, Train Loss:0.2637055814266205\n",
      "Epoch 9[178/312] Time:0.705, Train Loss:0.26242998242378235\n",
      "Epoch 9[179/312] Time:8.185, Train Loss:0.29508787393569946\n",
      "Epoch 9[180/312] Time:0.708, Train Loss:0.20010283589363098\n",
      "Epoch 9[181/312] Time:8.204, Train Loss:0.28259822726249695\n",
      "Epoch 9[182/312] Time:0.723, Train Loss:0.29594889283180237\n",
      "Epoch 9[183/312] Time:8.161, Train Loss:0.23082800209522247\n",
      "Epoch 9[184/312] Time:8.769, Train Loss:0.2806863486766815\n",
      "Epoch 9[185/312] Time:0.695, Train Loss:0.3450201153755188\n",
      "Epoch 9[186/312] Time:8.539, Train Loss:0.2654103636741638\n",
      "Epoch 9[187/312] Time:0.709, Train Loss:0.2958027124404907\n",
      "Epoch 9[188/312] Time:8.419, Train Loss:0.2406674325466156\n",
      "Epoch 9[189/312] Time:0.705, Train Loss:0.26002341508865356\n",
      "Epoch 9[190/312] Time:8.23, Train Loss:0.26171696186065674\n",
      "Epoch 9[191/312] Time:0.706, Train Loss:0.2624232769012451\n",
      "Epoch 9[192/312] Time:8.158, Train Loss:0.2446144074201584\n",
      "Epoch 9[193/312] Time:0.706, Train Loss:0.304490864276886\n",
      "Epoch 9[194/312] Time:8.152, Train Loss:0.20849855244159698\n",
      "Epoch 9[195/312] Time:0.706, Train Loss:0.2857420742511749\n",
      "Epoch 9[196/312] Time:8.206, Train Loss:0.2926129996776581\n",
      "Epoch 9[197/312] Time:0.705, Train Loss:0.2903500497341156\n",
      "Epoch 9[198/312] Time:8.193, Train Loss:0.31902945041656494\n",
      "Epoch 9[199/312] Time:0.705, Train Loss:0.30506259202957153\n",
      "Epoch 9[200/312] Time:8.214, Train Loss:0.23789618909358978\n",
      "Epoch 9[201/312] Time:0.707, Train Loss:0.2320542335510254\n",
      "Epoch 9[202/312] Time:8.175, Train Loss:0.26016706228256226\n",
      "Epoch 9[203/312] Time:0.706, Train Loss:0.327994167804718\n",
      "Epoch 9[204/312] Time:8.192, Train Loss:0.24897268414497375\n",
      "Epoch 9[205/312] Time:0.705, Train Loss:0.23903729021549225\n",
      "Epoch 9[206/312] Time:8.195, Train Loss:0.31157243251800537\n",
      "Epoch 9[207/312] Time:0.705, Train Loss:0.22624699771404266\n",
      "Epoch 9[208/312] Time:8.191, Train Loss:0.3453928828239441\n",
      "Epoch 9[209/312] Time:0.71, Train Loss:0.2232971489429474\n",
      "Epoch 9[210/312] Time:8.197, Train Loss:0.31204625964164734\n",
      "Epoch 9[211/312] Time:0.705, Train Loss:0.24944984912872314\n",
      "Epoch 9[212/312] Time:8.181, Train Loss:0.28009259700775146\n",
      "Epoch 9[213/312] Time:0.707, Train Loss:0.2659650444984436\n",
      "Epoch 9[214/312] Time:8.159, Train Loss:0.20350107550621033\n",
      "Epoch 9[215/312] Time:8.749, Train Loss:0.25208353996276855\n",
      "Epoch 9[216/312] Time:0.697, Train Loss:0.2773442268371582\n",
      "Epoch 9[217/312] Time:8.502, Train Loss:0.26394835114479065\n",
      "Epoch 9[218/312] Time:0.705, Train Loss:0.21878957748413086\n",
      "Epoch 9[219/312] Time:8.496, Train Loss:0.2259880155324936\n",
      "Epoch 9[220/312] Time:0.705, Train Loss:0.29287752509117126\n",
      "Epoch 9[221/312] Time:8.174, Train Loss:0.244950532913208\n",
      "Epoch 9[222/312] Time:0.705, Train Loss:0.2426878809928894\n",
      "Epoch 9[223/312] Time:8.2, Train Loss:0.2846672832965851\n",
      "Epoch 9[224/312] Time:8.734, Train Loss:0.2723178267478943\n",
      "Epoch 9[225/312] Time:0.704, Train Loss:0.21718956530094147\n",
      "Epoch 9[226/312] Time:8.702, Train Loss:0.3109458386898041\n",
      "Epoch 9[227/312] Time:0.706, Train Loss:0.2822633981704712\n",
      "Epoch 9[228/312] Time:8.28, Train Loss:0.2395203560590744\n",
      "Epoch 9[229/312] Time:0.705, Train Loss:0.23636651039123535\n",
      "Epoch 9[230/312] Time:8.175, Train Loss:0.3473503589630127\n",
      "Epoch 9[231/312] Time:0.707, Train Loss:0.29692965745925903\n",
      "Epoch 9[232/312] Time:8.13, Train Loss:0.1888996809720993\n",
      "Epoch 9[233/312] Time:0.707, Train Loss:0.2130703330039978\n",
      "Epoch 9[234/312] Time:8.204, Train Loss:0.29414665699005127\n",
      "Epoch 9[235/312] Time:0.704, Train Loss:0.21782812476158142\n",
      "Epoch 9[236/312] Time:8.136, Train Loss:0.30038148164749146\n",
      "Epoch 9[237/312] Time:0.708, Train Loss:0.26098594069480896\n",
      "Epoch 9[238/312] Time:8.247, Train Loss:0.28124871850013733\n",
      "Epoch 9[239/312] Time:8.763, Train Loss:0.23921342194080353\n",
      "Epoch 9[240/312] Time:0.7, Train Loss:0.2321874499320984\n",
      "Epoch 9[241/312] Time:8.709, Train Loss:0.28577470779418945\n",
      "Epoch 9[242/312] Time:0.705, Train Loss:0.35096681118011475\n",
      "Epoch 9[243/312] Time:8.194, Train Loss:0.2753073275089264\n",
      "Epoch 9[244/312] Time:0.705, Train Loss:0.2567206919193268\n",
      "Epoch 9[245/312] Time:8.197, Train Loss:0.3368513882160187\n",
      "Epoch 9[246/312] Time:0.705, Train Loss:0.31432104110717773\n",
      "Epoch 9[247/312] Time:8.222, Train Loss:0.33379408717155457\n",
      "Epoch 9[248/312] Time:0.705, Train Loss:0.17119629681110382\n",
      "Epoch 9[249/312] Time:8.203, Train Loss:0.2561747133731842\n",
      "Epoch 9[250/312] Time:0.705, Train Loss:0.23598115146160126\n",
      "Epoch 9[251/312] Time:8.169, Train Loss:0.21375755965709686\n",
      "Epoch 9[252/312] Time:0.707, Train Loss:0.2584345042705536\n",
      "Epoch 9[253/312] Time:8.192, Train Loss:0.2707512378692627\n",
      "Epoch 9[254/312] Time:0.705, Train Loss:0.2820049822330475\n",
      "Epoch 9[255/312] Time:8.225, Train Loss:0.2856721878051758\n",
      "Epoch 9[256/312] Time:0.704, Train Loss:0.27127596735954285\n",
      "Epoch 9[257/312] Time:8.215, Train Loss:0.1777922362089157\n",
      "Epoch 9[258/312] Time:0.709, Train Loss:0.29934069514274597\n",
      "Epoch 9[259/312] Time:8.172, Train Loss:0.42624548077583313\n",
      "Epoch 9[260/312] Time:0.705, Train Loss:0.32315540313720703\n",
      "Epoch 9[261/312] Time:8.192, Train Loss:0.24340973794460297\n",
      "Epoch 9[262/312] Time:0.704, Train Loss:0.3176511228084564\n",
      "Epoch 9[263/312] Time:8.192, Train Loss:0.28757160902023315\n",
      "Epoch 9[264/312] Time:0.706, Train Loss:0.2616663873195648\n",
      "Epoch 9[265/312] Time:8.181, Train Loss:0.31743454933166504\n",
      "Epoch 9[266/312] Time:0.707, Train Loss:0.2949478030204773\n",
      "Epoch 9[267/312] Time:8.186, Train Loss:0.2633971571922302\n",
      "Epoch 9[268/312] Time:0.706, Train Loss:0.38083842396736145\n",
      "Epoch 9[269/312] Time:8.148, Train Loss:0.19985562562942505\n",
      "Epoch 9[270/312] Time:0.708, Train Loss:0.26502376794815063\n",
      "Epoch 9[271/312] Time:8.164, Train Loss:0.2767108380794525\n",
      "Epoch 9[272/312] Time:0.706, Train Loss:0.2561339735984802\n",
      "Epoch 9[273/312] Time:8.187, Train Loss:0.2478797882795334\n",
      "Epoch 9[274/312] Time:0.704, Train Loss:0.2271723449230194\n",
      "Epoch 9[275/312] Time:8.191, Train Loss:0.21773749589920044\n",
      "Epoch 9[276/312] Time:0.704, Train Loss:0.3510475754737854\n",
      "Epoch 9[277/312] Time:8.207, Train Loss:0.3748210072517395\n",
      "Epoch 9[278/312] Time:8.765, Train Loss:0.24098578095436096\n",
      "Epoch 9[279/312] Time:0.685, Train Loss:0.2848586440086365\n",
      "Epoch 9[280/312] Time:8.683, Train Loss:0.25513896346092224\n",
      "Epoch 9[281/312] Time:8.747, Train Loss:0.21529290080070496\n",
      "Epoch 9[282/312] Time:0.691, Train Loss:0.2547775208950043\n",
      "Epoch 9[283/312] Time:8.683, Train Loss:0.29440227150917053\n",
      "Epoch 9[284/312] Time:8.769, Train Loss:0.23042340576648712\n",
      "Epoch 9[285/312] Time:0.692, Train Loss:0.30243396759033203\n",
      "Epoch 9[286/312] Time:8.655, Train Loss:0.23376163840293884\n",
      "Epoch 9[287/312] Time:8.689, Train Loss:0.27086079120635986\n",
      "Epoch 9[288/312] Time:0.708, Train Loss:0.3118942379951477\n",
      "Epoch 9[289/312] Time:8.592, Train Loss:0.3370886445045471\n",
      "Epoch 9[290/312] Time:0.705, Train Loss:0.2562958896160126\n",
      "Epoch 9[291/312] Time:8.323, Train Loss:0.25328296422958374\n",
      "Epoch 9[292/312] Time:0.706, Train Loss:0.2894158959388733\n",
      "Epoch 9[293/312] Time:8.208, Train Loss:0.20824483036994934\n",
      "Epoch 9[294/312] Time:0.707, Train Loss:0.3392808437347412\n",
      "Epoch 9[295/312] Time:8.177, Train Loss:0.2672208845615387\n",
      "Epoch 9[296/312] Time:0.707, Train Loss:0.26294243335723877\n",
      "Epoch 9[297/312] Time:8.195, Train Loss:0.219808891415596\n",
      "Epoch 9[298/312] Time:0.705, Train Loss:0.31160807609558105\n",
      "Epoch 9[299/312] Time:8.185, Train Loss:0.256715327501297\n",
      "Epoch 9[300/312] Time:0.707, Train Loss:0.3005731701850891\n",
      "Epoch 9[301/312] Time:8.194, Train Loss:0.25322943925857544\n",
      "Epoch 9[302/312] Time:0.706, Train Loss:0.3649148941040039\n",
      "Epoch 9[303/312] Time:8.204, Train Loss:0.30340641736984253\n",
      "Epoch 9[304/312] Time:0.708, Train Loss:0.24164997041225433\n",
      "Epoch 9[305/312] Time:8.143, Train Loss:0.3435332477092743\n",
      "Epoch 9[306/312] Time:0.705, Train Loss:0.3185988664627075\n",
      "Epoch 9[307/312] Time:8.189, Train Loss:0.2994697093963623\n",
      "Epoch 9[308/312] Time:0.707, Train Loss:0.3143042027950287\n",
      "Epoch 9[309/312] Time:8.185, Train Loss:0.30719611048698425\n",
      "Epoch 9[310/312] Time:0.705, Train Loss:0.23113135993480682\n",
      "Epoch 9[311/312] Time:8.176, Train Loss:0.25941553711891174\n",
      "Epoch 9[0/39] Val Loss:0.38796520233154297\n",
      "Epoch 9[1/39] Val Loss:0.45239701867103577\n",
      "Epoch 9[2/39] Val Loss:0.3793286085128784\n",
      "Epoch 9[3/39] Val Loss:0.4688979983329773\n",
      "Epoch 9[4/39] Val Loss:0.3952300548553467\n",
      "Epoch 9[5/39] Val Loss:0.34149324893951416\n",
      "Epoch 9[6/39] Val Loss:0.2824549674987793\n",
      "Epoch 9[7/39] Val Loss:0.2623891532421112\n",
      "Epoch 9[8/39] Val Loss:0.3621455132961273\n",
      "Epoch 9[9/39] Val Loss:0.3311523497104645\n",
      "Epoch 9[10/39] Val Loss:0.11460327357053757\n",
      "Epoch 9[11/39] Val Loss:0.08926103264093399\n",
      "Epoch 9[12/39] Val Loss:0.48752039670944214\n",
      "Epoch 9[13/39] Val Loss:0.29779917001724243\n",
      "Epoch 9[14/39] Val Loss:0.16648627817630768\n",
      "Epoch 9[15/39] Val Loss:0.19991613924503326\n",
      "Epoch 9[16/39] Val Loss:0.31348416209220886\n",
      "Epoch 9[17/39] Val Loss:0.4260561764240265\n",
      "Epoch 9[18/39] Val Loss:0.4227389097213745\n",
      "Epoch 9[19/39] Val Loss:0.5011271834373474\n",
      "Epoch 9[20/39] Val Loss:0.8025577068328857\n",
      "Epoch 9[21/39] Val Loss:0.8095217347145081\n",
      "Epoch 9[22/39] Val Loss:0.34705105423927307\n",
      "Epoch 9[23/39] Val Loss:0.3684787154197693\n",
      "Epoch 9[24/39] Val Loss:0.41890525817871094\n",
      "Epoch 9[25/39] Val Loss:0.46116870641708374\n",
      "Epoch 9[26/39] Val Loss:0.381949245929718\n",
      "Epoch 9[27/39] Val Loss:0.2822149395942688\n",
      "Epoch 9[28/39] Val Loss:0.04834179952740669\n",
      "Epoch 9[29/39] Val Loss:0.19013211131095886\n",
      "Epoch 9[30/39] Val Loss:0.4992942810058594\n",
      "Epoch 9[31/39] Val Loss:0.42886754870414734\n",
      "Epoch 9[32/39] Val Loss:0.290070116519928\n",
      "Epoch 9[33/39] Val Loss:0.14305470883846283\n",
      "Epoch 9[34/39] Val Loss:0.1709602326154709\n",
      "Epoch 9[35/39] Val Loss:0.1122548058629036\n",
      "Epoch 9[36/39] Val Loss:0.20991049706935883\n",
      "Epoch 9[37/39] Val Loss:0.280223548412323\n",
      "Epoch 9[38/39] Val Loss:0.20112045109272003\n",
      "Epoch 9[39/39] Val Loss:0.11494225263595581\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.87      0.91      8608\n",
      "           1       0.49      0.77      0.60      1392\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.72      0.82      0.75     10000\n",
      "weighted avg       0.89      0.86      0.87     10000\n",
      "\n",
      "Epoch 9: Train Loss 0.27395754374372655, Val Loss 0.33957606544479346, Train Time 1691.4920263290405, Val Time 18.940637588500977\n",
      "Saving best model at epoch 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (features): Sequential(\n",
       "    (init_block): ResInitBlock(\n",
       "      (conv): ConvBlock(\n",
       "        (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "      (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (stage1): Sequential(\n",
       "      (unit1): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (identity_conv): ConvBlock(\n",
       "          (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit2): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit3): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (stage2): Sequential(\n",
       "      (unit1): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (identity_conv): ConvBlock(\n",
       "          (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit2): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit3): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit4): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (stage3): Sequential(\n",
       "      (unit1): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (identity_conv): ConvBlock(\n",
       "          (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit2): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit3): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit4): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit5): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit6): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (stage4): Sequential(\n",
       "      (unit1): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (identity_conv): ConvBlock(\n",
       "          (conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit2): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "      (unit3): ResUnit(\n",
       "        (body): ResBottleneck(\n",
       "          (conv1): ConvBlock(\n",
       "            (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv2): ConvBlock(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activ): ReLU(inplace=True)\n",
       "          )\n",
       "          (conv3): ConvBlock(\n",
       "            (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (activ): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (final_pool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  )\n",
       "  (output): Linear(in_features=2048, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathpretrain.train_model import train_model\n",
    "train_model(inputs_dir=\"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/cnn_model_input/\",\n",
    "                learning_rate=1e-3,\n",
    "                n_epochs=10,\n",
    "                batch_size=256,\n",
    "                model_save_loc='/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/cnn_model.pkl',\n",
    "                verbose=2,\n",
    "                class_balance=True,\n",
    "                predict=False,\n",
    "                pickle_dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e0dcb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "!sleep 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fe9b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathpretrain.predict import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "747fac65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': <pathpretrain.datasets.PickleDataset object at 0x2b0e973d8b50>, 'val': <pathpretrain.datasets.PickleDataset object at 0x2b0e93fb2d60>, 'test': <pathpretrain.datasets.PickleDataset object at 0x2b0e93fb2670>}\n",
      "ResNet(\n",
      "  (features): Sequential(\n",
      "    (init_block): ResInitBlock(\n",
      "      (conv): ConvBlock(\n",
      "        (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (stage1): Sequential(\n",
      "      (unit1): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (identity_conv): ConvBlock(\n",
      "          (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit2): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit3): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (stage2): Sequential(\n",
      "      (unit1): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (identity_conv): ConvBlock(\n",
      "          (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit2): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit3): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit4): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (stage3): Sequential(\n",
      "      (unit1): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (identity_conv): ConvBlock(\n",
      "          (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit2): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit3): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit4): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit5): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit6): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (stage4): Sequential(\n",
      "      (unit1): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (identity_conv): ConvBlock(\n",
      "          (conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit2): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "      (unit3): ResUnit(\n",
      "        (body): ResBottleneck(\n",
      "          (conv1): ConvBlock(\n",
      "            (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv2): ConvBlock(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activ): ReLU(inplace=True)\n",
      "          )\n",
      "          (conv3): ConvBlock(\n",
      "            (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activ): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (final_pool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  )\n",
      "  (output): Linear(in_features=2048, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:23,  6.66it/s]                                                        \n"
     ]
    }
   ],
   "source": [
    "predict(inputs_dir='/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/cnn_model_input/',\n",
    "                batch_size=64,\n",
    "                model_save_loc=\"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/cnn_model.pkl\",\n",
    "                predict_set='test',\n",
    "                pickle_dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78fa265c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9328501673141041"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import roc_auc_score\n",
    "predictions=torch.load(\"/dartfs-hpc/rc/home/3/f006n33/predictions.pkl\")\n",
    "predictions['pred']=softmax(predictions['pred'])\n",
    "roc_auc_score(predictions['true'],predictions['pred'][:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17602649",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pred': array([[9.0002138e-11, 1.0975756e-14],\n",
      "       [1.4268853e-10, 1.6489827e-14],\n",
      "       [2.8379356e-12, 3.5260439e-13],\n",
      "       ...,\n",
      "       [9.7527607e-13, 1.0388833e-12],\n",
      "       [6.7847537e-12, 1.4861227e-13],\n",
      "       [1.2934858e-12, 7.6911800e-13]], dtype=float32), 'true': array([0, 0, 0, ..., 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "print (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d6dfe58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "# fpr and tpr of all thresohlds\n",
    "true = predictions['true']\n",
    "preds = predictions['pred'][:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(true, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59825228",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ipykernel_259557/2083274585.py:10: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEcCAYAAAAGD4lRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABJIElEQVR4nO3dd3gU5drH8e+WbHonlSqICAISCU2I0gkYigKiFCmK50UFxIoiRcGCR48Kih5RugUQDRIQEKwoTQGD0qQH0ntPtsz7R2AxhxCWsC2b+3NdXFd2Mzvzy5DsvTPPzP2oFEVREEIIIWpA7egAQgghai8pIkIIIWpMiogQQogakyIihBCixqSICCGEqDEpIkIIIWpMiogQQogakyIiBDBmzBg6dOhAeXl5pefWrl1babndu3dzxx13mB8risKKFSuIi4ujXbt23HHHHUyZMoWjR49WuZ3du3dz8803ExUVRVRUFP369WPdunWVllEUhY8++oi+ffvStm1bunfvzptvvlkpG0BiYiITJ04kOjqajh07MmzYsMvWJYStSRERdd65c+f47bffUKlUbN++/Zpe+/LLL7NixQpmzJjBnj172LJlC7179+bHH3+84mtCQ0PZv38/+/bt4/nnn2fmzJmcPHnS/P158+axZs0a5s+fz759+1i8eDE7d+5k6tSp5mX279/P2LFj6dChA1u3bmX37t3MmTOHn3766dp3gBDXQevoAEI4Wnx8PLfeeiu33nor8fHx9O/f36LXnT59mk8++YTVq1fTtm1b8/ODBg2y6PUqlYo777wTf39/jh49StOmTTl9+jSffvpppXU2b96chQsX0qdPH3bu3EmXLl14/fXXGTJkCA8//LB5fa1bt+add965hp9ciOsnRyKizlu/fj0DBw5k4MCB7Nixg8zMTItet3PnTsLDwysVkGthMpnYvn07OTk5NG7cuNp1RkRE0K5dO3799VdKSko4cOAA/fr1q9F2hbAmORIRddpvv/1GcnIy/fv3JygoiIYNG5KQkMC4ceOu+trc3FxCQkKueZvp6elER0dTWlqK0Whk+vTptGrVCoCcnJwrrjMkJIScnBzy8/MxmUw12rYQ1iZHIqJOi4+Pp2vXrgQFBQEQFxfHV199BYBGo0Gv11da3mAwoNVWfPYKCAggIyPjiutOTk42D6BHRUWZnw8NDeW3335j3759jBkzhl27dpm/FxgYeMV1ZmRkEBgYiJ+fH2q1utptC2EvUkREnVVaWso333zD3r176dq1K127dmX58uUcOXKEI0eOEBERwfnz5yu95ty5c9SvXx+ALl26kJqaysGDB6tcf2RkJPv37zf/+186nY6nnnqKY8eOsW3bNgA6d+5MSkoKiYmJlZZNSUnhwIEDdOnSBU9PT9q1a8fWrVutsRuEuC5SRESdtW3bNjQaDRs3biQ+Pp74+Hg2bdpEdHQ08fHxDBgwgC+//JLExEQUReHUqVMsW7aMAQMGANCkSRNGjhzJk08+ye7duykvL6esrIyNGzfy4YcfWpRBp9MxYcIE3nvvPQBuuOEG7rvvPp566ikOHDiA0Wjk77//ZvLkydx+++3cfvvtADz99NN89dVXfPTRR+Tk5ABw5MgRpk2bZoM9JcSVqWQ+EVFXPfjggzRv3pzp06dXen7Tpk28/PLL/Pjjj8THx7N06VJSUlIIDg5m+PDhPPTQQ6jVFZ+/Lt4nsmbNGs6dO4efnx/t27fn0UcfpXnz5pdtc/fu3Tz99NOVLsUtKSmhe/fuvPrqq/Ts2ROTycRHH33E2rVrSUtLIzAwkLvuuoupU6fi7u5ufl1iYiILFixg//79aDQaGjduzKhRoxgyZIhtdpgQVZAiIoQQosbkdJYQQogas0sRmT9/Pj179qRFixYcO3asymWMRiMvvvgivXv3pk+fPpe1mxBCCOF87FJEevXqxSeffGK+qqUqGzZs4OzZs2zdupXVq1ezcOFCzp07Z494QgghasguRSQ6OpqIiIhql9m0aRPDhw9HrVYTFBRE79692bx5sz3iCSGEqCGnGRNJSUkhMjLS/DgiIoLU1FQHJhJCCHE1TlNEhBBC1D5O0zsrIiKC5ORkc+O5/z0ysVROThEmk1y1HBzsQ1ZWoaNjOIXavC9MikJGTgkG4+W/0yeT80jKKEKnVXMuoxA/b91V1+fmpkWvN3A2tQAfLzdUKlWl76dlF1stu7Vc/LlMChQVl9Mo3I/8ojIahPhQZjDSIMTHovWUlRuJCPYiyM+jYr1+HuTnl6LTqvH1dDMv5++jM98H5IrKks6i9fdH4+ePITcXjYc79erXvA+b0xSR2NhY1q5dS9++fcnNzWXbtm188skn17wek0mRInKB7IdLasO+KC41sGXPWX47mk5KVjHubhrK9EaLXqtRq/D20BJ44Q3ySty0avQGEyiQmlVMk3DfSt9vEOKDyaTg5+1G4zDfK6yleqV6I2GBXrhpK78RG40KIQEeqNWqK7yyMm8PN4sKY02FhPiSkVFQ5fdqw+/LtTKVlZG1/ityvt2CX7cYwsdOQO3nj8rC/48rsUsRmTdvHlu3biUzM5Px48cTEBDAxo0bmThxIlOmTKFNmzYMHjyYP/74g759+wLw6KOP0rBhQ3vEE8JqFEVBbzCRW3RpFkKTSeHImRzzm+fBk1kYDCYOn81BUcBDp6GwRM8/b/vVadW0ahJIkJ8HBcXltG8RivZ//tiNJoUGoT6EB3lZnK+6N07huoqPHCZt+VL0Gen439mdekPvtdq6Xe6O9aysQpf8FHGt5M3iEmvvizK9kcJiPYkns/Bw02AwmTifUYTeaOL7feevvoILAnx05BaW07lVGO46DYXFeoL9PYjt1IgAH/err6AG5PfikrqyL/J++Zm0pR/jFhJK2NjxeN3cstL31WoVwcGWnRKsitOczhLCnopK9Rz4OxOdm4a/k3IpKNHj6X7pz+H4uVwCfNw5m1ZAfnFFO3jNhSMB41U+pLS+IYgmEX6EBXqan9MbTDRvGICnToNarcLfW3fZeIQQ1mQqLUXt4YFP23bo4wYS1D8Otbv1P5xIERG1ksFoQm8wkV1QhvKPN/XcwjL0RhPJmUXotBoycks4mVqAv1fFwGlSeiFuWjUpWZcPIOu0ajx0GgD0RoX03BLq1/PBTasmPMiLJhF+QMW5fTetGk93Le2a10OlAp1Wg7+3DlSgluIgHMhQkE/GZ59Snp5Go+deQOPrS70hQ222PSkiwunlFpZxLr2QQ6dz2LznLFqNqsqrlaqj1aiICPbGQ6clt7CMdjfWI9jfgzvbRaICQqsYCBaiNlEUhYI9u0j/7BNMJSUExw0CO4xWSBERTmdHYgq//plCTmF5lZecGowKXVuH4+etQ6NR0Si08lVEHjoNPl5uBPl5oFGraBAZQE52kb3iC2F3xsJCUpcspijxDzyaNiVs7IO4V9NmypqkiAiHys4v5WRyPj/9kUx2QRnJmZXf7BuE+KCg0L1dferX8yYsyItA32s7r6vVyBGGcG0qdx2GvDxCRtxPQK8+qOx4n4sUEWEzR87kUFRqMD8+eDILUPjpj5RqXxfo686ssdH42+gKJSFcQXlaGllfxxM25gHUHp40mjHLrsXjIiki4ropisKxpFz2Hknn+/3nCQv0IrWaO58v3vAW26kRhcV6Gob6cEOEH83q+8kVS0JchWI0krNtK1nxX6LSaik71xPPG5s7pICAFBFRQxm5JfycmELCr6cv+15qdjE3NwqgpMzI4G43EORXcUShUqkID5IBbCFqqiwpidTlSyg7fQrvdlGEjX4AbUCgQzNJERFXVVBczunUAhKPZ5GUUcixpNxK31ep4LabQuhwcygdW4Y5JqQQdUDG2s8xZGUS8a9H8Inu4BRH7lJERCUlZQaMJoW8onI+2nCI7IJSCi7cbPdP4UFe9OvYkA43h+HlIb9GQthKyYnjaIOCcQsMJGzcBNQ6dzQ+Nb/D3Nrkr19wOjWf9T+f4o8TWVV+v0XDACLqedPh5lBaNAywuIGeEKLmTGVlZH61jtzt35obJroFBTs61mWkiNQxpeUGsvLLWLnl6GWnpaCitcfQO5uh1ajw9dLRoWWo3IEthJ0VHfqL9BXL0Gdm4N+jJ/XuGe7oSFckRcTFKYrCuh9PsmnXGdx1GsrKK7cWb9M0GK1Gxa031uP21uFyT4UQDpa34yfSli3BLSyMBs88h9dNLRwdqVpSRFzY1j1n+fy74+bHZeVG2rcIIcjXg5sa+nPbTSFOMTAnhABTaQlqD0982t2GfmAWQf3vQq2z3Xwq1iJFxAXpDSbufX4jJWUVN/r5ebnxxqNd5ShDCCdkyMsj/bNP0GdmVDRM9PGh3uC7HR3LYlJEXIRJUfjzZBabd5/lyNlc8/OT72lD1E01n/pSCGEbiqJQsGsn6Z9/glJWRpCdGiZamxSRWs5kUtjw62nW7zhV6fmGYb7MGRctp6uEcELGggJSPl5M8Z+JeDS7kbCxE3CPjHR0rBqRIlKLnUkt4MVleys9N+OB9jQJ9yU8zL9OzNomRG2k8vDAWFhAyH2jCOjZy2EtS6xBikgttHVvEp9v/9v8ONjPnefHRF9zd1shhP2Up6ZUNEx8YFxFw8TnZ9bq4nGRFJFa5McD51m++Wil5yYNaU2Hm0MdlEgIcTWK0UjOlm/I+joelU5H2fnzeDa70SUKCEgRqRX+PJnFf9b8Uem5lyd2IiLY20GJhBCWKD17hrRlSyg7ewaf29oTOmoMWv8AR8eyKikiTsxkUnjo9e/Nj73ctTw3+jbqhzhP3xwhxJVlrluLITeHiEmP4tu+g6Pj2IQUESeUW1jGd/vOV2qz/vT9UbRs7NiWz0KIqys5/jfa4HoVDRPHTkCt0zlVw0RrkyLiRJLSC3ntk33mmwQv+u9T3WUODiGcnKm0lMwvvyD3++3/aJgY5OhYNidFxEn8eSqL/6y+NO7RJ7ohA7s2wcfTzYGphBCWKPrrT9JWLMWQnU1Aj57Uu2eYoyPZjRQRBzOZFCb950f0BhMAPaLqM6afczdcE0JcYm6YGB5Ow2eew7P5TY6OZFdSRBzsnwPno/rcRK/2DRyYRghhKWNJCRrPioaJhsE5BMb2R+3m/A0TrU2KiIOkZBUxY/Fu8+MPn+4uDRKFqAUMebmkf7oKfUYGjWbMQuPjQ/DAwY6O5TBSROysuFTPY2//XOm5BVNjpIAI4eQURSH/1x1krP4cpbyM4EFDHB3JKUgRsROTopDwy2ni/9Eo8d4eNxLbqZEDUwkhLGEoyCf1ow8p/utPPJvfRNjY8ejCIxwdyylIEbGx7/adY9XWY5WeCw3wZO5DneSyXSFqCbWHJ6aSYkJHjcH/zh4u07LEGqSI2IjRZGLKOzsq3fPRKNSHR+9pQ0iApwOTCSEsUZ6STOb6eMLHjUft4UnD6S9I8aiCFBEbee/LP80FZPLQNkQ1l4mhhKgNFIOB7C3fkL1hPSqdu8s1TLQ2KSI2kJ5bwoHjmYBcdSVEbVJ65nRFw8Sks/hEdyD0/tFo/f0dHcupSRGxMpOiMP2DnQC0bxEiBUSIWiTzyy8w5OcR8chkfG9r7+g4tYLdisipU6eYPn06ubm5BAQEMH/+fJo0aVJpmaysLJ577jlSUlIwGAx06tSJF154Aa229tS6h+Zfunnw0bvbODCJEMISxceO4lYvBLegoIqGie7uaLxlmgVL2e1j8uzZsxk5ciRbtmxh5MiRzJo167JlPvjgA5o1a8aGDRv4+uuv+euvv9i6dau9Il63OUv2mL9+9/E7HJhECHE1huIS0j5ZwbnXXyU7YT0AbkFBUkCukV2KSFZWFocOHSIuLg6AuLg4Dh06RHZ2dqXlVCoVRUVFmEwmysvL0ev1hIWF2SPidVu66TBn0wsBeOOR2/HyqD1HT0LUNUUHE9k/+XHyfviegN59CRkx0tGRai27vNOlpKQQFhaGRqMBQKPREBoaSkpKCkH/aJX8yCOPMHnyZLp160ZJSQmjRo2ifftrOy8ZHGzfvv05BaXMX/Ebf53MAuDFiV1o0cw5rsQKCfF1dASnIfvikrq+L1K3buP8e+/j2aABbV57Gb+bpeHp9XCqj8ubN2+mRYsWLF++nKKiIiZOnMjmzZuJjY21eB1ZWYWYTIoNU1Y24bXvzF8/encbGgZ7kpFRYLftX0lIiK9T5HAGsi8uqav7QlEUTCUlaLy8oPktBA+5h5tG30tWbmmd3B//pFarruvDt11OZ0VERJCWlobRaATAaDSSnp5ORETltgGrVq1i0KBBqNVqfH196dmzJ7t3765qlQ6nKEqlArJkek/at3COIxAhxCWG3FySFy3k3BvzUYzGioaJcYNQu8lcPdZglyISHBxMy5YtSUhIACAhIYGWLVtWOpUF0KBBA3766ScAysvL2blzJ82bN7dHxGs2dcEO89cvPBDtwCRCiKooikLejp84PfM5iv88iG/HTo6O5JLsdjprzpw5TJ8+nUWLFuHn58f8+fMBmDhxIlOmTKFNmzY8//zzzJ49m4EDB2I0GunUqRP33nuvvSJarKhUT2GJHpCpa4VwRoaCfFI//C/Fh//C86YWhD0wHl14uKNjuSSVoij2G0CwA1uPiZSVG5n0nx8BGNS1CUNimtpsW9ejrp77rorsi0vqyr4w6fWc+/dr+N3eFf87ulfZsqSu7Iurud4xEacaWHd2iqKYCwjgtAVEiLqoLPk8Weu/Imzcg2g8PWn43AuoVCpHx3J5UkSuwRPv/QJASIAHL0/s7OA0Qgi40DDxm41kb9yAysOD8pRkPJs2kwJiJ1JELLR4w1/kFZYDMGd8R+mJJYQTKD19itRlSyg/l4Rvx06E3D8Kra+fo2PVKVJELFBcamDnX2kAzHigPZ7ustuEcAaZX63DWFhA5GNT8WkX5eg4dZK8G16FyaTw2NsVlx3HdmxEs0hpCy2EIxUfPYJbSAhuQcGEj38QlU6Hxkv6XTmKnJO5ilc/+d389fAezRyYRIi6zVhSQtrK5Zz792tkJ3wNgDYgUAqIg8mRSDWy80s5cT4fqJhcSgbqhHCMwsQ/SF+5HENuDoF9+hE85B5HRxIXSBGpxlOLfgWgx231ZSBdCAfJ++lH0lYsRRdZn4aTHsWzqZwRcCYWF5FffvmFjRs3kp2dzQcffMDBgwcpLCykS5cutsznMCeT881fj+krXT6FsKeKhonFaLy88WkfjbGwgMC+sahq0QR1dYVFH69XrlzJnDlzaNKkCXv37gXAw8ODd955x6bhHKWwRM+8Fb8BMHVYWwenEaJu0efkkPzeAs79ez6KwYDG25ugAXFSQJyURf8ry5cvZ9myZTRo0IDFixcD0LRpU06dOmXTcI7y/b5zADQJ9+XWG+s5OI0QdYOiKOT9/COZa1ejGI3UG3IPVNGuRDgXi4pIUVGRuW37xcFlg8GAm4u2Uv7rdA4Az42+zcFJhKgbDPn5pHz4PiVHDuN5c8uKhomhoY6OJSxgUZnv0KEDH374YaXnVqxYQadOrtdaOaegjGNJuQC4aTWODSNEHaH29ETR6wl9YBwNnnxGCkgtYtGRyAsvvMD//d//sXbtWoqKiujXrx/e3t7897//tXU+u3t5ZcVYyO2tpW20ELZUdv4cWfFfETbhoYqGidNnyGX0tZBFRSQ0NJR169Zx8OBBzp8/T0REBG3btkXtYucrM3JLyM4vA+ChuFYOTiOEa1IMBrI3JZC1cQMaTy/KU1LwbNpUCkgtZVEVmDRpEiqVirZt29K/f3/atWuHWq3mscces3U+uyks0fPsBzsBGNXnJgenEcI1lZw8yZm5c8j6Oh7f6A40mfsKnk1lSoXazKIjkSvNc75nzx6rhnGkZ97/1fx1r/YNHJhECNeVtf5LTMVFRE5+HJ9b2zk6jrCCaovIxftA9Hr9ZfeEJCUlERkZabtkdpSZW0JpuRGAJdN7OjiNEK6l+Mhh3EJDLzVMdPdA4+np6FjCSqotIqmpqUDF9dsXv74oIiKCyZMn2y6ZHT1z4TTW8O7STkEIazEWF5H5xRryfvoR/zu6E/bAOLQBgY6OJays2iLy6quvAhAVFcW9995rl0D2diI5z/x1bKdGDkwihOsoPLCftFXLMeblERg7gOBBQxwdSdiIRWMiFwtIYWEhOTk5lb7XsGFD66eyo5dXVLR6nzSktVwdIoQV5P74A+krl6Gr34D6j03Fo8kNjo4kbMiiInLixAmefPJJjhw5gkqlQlEU8xvu4cOHbRrQls6kFpi/jm4R4sAkQtRuiqJgKi5G4+2Nb3QHTCXFBPbuK/2u6gCLLvGdM2cOnTp1Ys+ePfj4+LB3715GjBjBa6+9Zut8NvXZ9r8BeOb+KDkKEaKG9NlZJC98m3Nv/KNhYuwAKSB1hEX/y0eOHGHJkiW4ubmhKAq+vr4888wzxMXFMXjwYFtntAlFUcztTW5uLIN9QlwrxWS61DDRZKLe3UOlYWIdZFERcXd3NzdcDAwMJDk5GT8/P3Jzc20cz3Yy80oBuKWJFBAhrpUhL4+U/y6i5NhRvFq2IvSBcehCpN9VXWRREWnfvj3ffPMN99xzD/369WPixInodDo6d+5s63w2s//vTADubFffwUmEqH003t6gKISNm4Bf1xg5HVyHWVRE/nmj4RNPPMGNN95IcXExd999t82C2dqfJ7MAaN00yMFJhKgdypLOkrn+K8InTETj5UWDZ56T4iGufY51tVrNkCFDKC8vZ+3atYwaNcoWuWzKZFL481Q2AB46GfwTojomvZ7sjV+T/c0mNF7elKemSsNEYXbVd9CdO3dy+PBhGjVqRO/evTEYDHz66acsXryYgICAWllEdhxMAaC9XNYrRLVKThwnbdkSylOS8evSlZAR96Px8XF0LOFEqi0iH374Ie+//z433ngjx48f5/7772fPnj3odDrmzp1L9+7d7RTTun4/mgHA/b2aOziJEM4ta8N6TGWl1J/6BN5t2jo6jnBC1RaR1atXs3LlSlq3bs2BAwe4//77efbZZxk3bpyd4tlGbmEZGrWKID8PR0cRwukUHz6EW2gYbsHBhI97ELWHO2oPaZgoqlbtRd05OTm0bt0agHbt2qHT6Rg7dqxdgtlSUnohYUFejo4hhFMxFheRuuxjzr35OtkbNwCgDQiQAiKqddUxEUVRzP/c3d0BMJlM5u/XxtkNdW5qguUoRAizwv2/k7ZqJcaCfAL730XwwNp5E7Gwv2qLSHFxMa1aXZomVlEU8+OL/bNqW+8sRVEo15toGCqDg0IA5P74Pekrl+PesCH1Jz+OR5Mmjo4kapFqi8j27duttqFTp04xffp0cnNzCQgIYP78+TSp4pd106ZNvP/+++YitXTpUurVq2e1HOt3nAKg7MIkVELURYqiYCoqQuPjg2+HjphKSwns1Uf6XYlrVu1vTP361rube/bs2YwcOZLBgwezfv16Zs2axYoVKyotc/DgQd59912WL19OSEgIBQUF6HQ6q2UA+PqX0wAM7NbEqusVorbQZ2WRtnIZxrxcGs2YjcbLm6B+/R0dS9RSdhnQyMrK4tChQ8TFxQEQFxfHoUOHyM7OrrTcsmXLmDBhAiEhFfdv+Pr6msdhrOGfRx9+XtYtTkI4O8VkImXjN5yeNYOSv4/hF3OnNEwU180ux64pKSmEhYWh0WgA0Gg0hIaGkpKSQlDQpbYjJ06coEGDBowaNYri4mL69OnDpEmTrHZn7PELsxje1/NGq6xPiNrCkJdHygfvUfL3MbxuaU3YmLG41ZObbcX1c6oToEajkaNHj7J06VLKy8t56KGHiIyMZMiQIRavIzj4ygPmKfuTAejQJpKQEN/rjev06sLPaKm6vi9MAR5kurvRYOpjhPToLi1LLqjrvxfWcE1FJCUlhbS0NNq1a3dNG4mIiCAtLQ2j0YhGo8FoNJKenk5ERESl5SIjI4mNjUWn06HT6ejVqxeJiYnXVESysgoxmZQqv3f2wpGIj5uajIyCKpdxFSEhvi7/M1qqru6L0rNnyIr/kvCH/oXGy4uwqU8RGupXJ/dFVerq78X/UqtV1X74vurrLVkoOTmZ++67j/79+zN+/HgANm/ezIwZMyzaSHBwMC1btiQhIQGAhIQEWrZsWelUFlSMlezYsQNFUdDr9ezatYubb775Wn6eamXmlQDgppXzwMJ1mfTlZH75BWfnvUjp6VPo09MA5OhD2IRF76azZs2ie/fu7Nu3D+2FSwC7du3Kr7/+avGG5syZw6pVq+jXrx+rVq3ixRdfBGDixIkcPHgQgLvuuovg4GAGDBjAkCFDuPHGGxk2bNi1/kxXpFar8HR3qjN4QlhVyd9/c+bFWWRvSsCv8+00eekVPJrc4OhYwoVZ9I568OBBPvzwQ9RqtfnTjK+vLwUFlh8KNmvWjLVr1172/OLFi81fq9VqnnvuOZ577jmL13stDp3O4aYG/jZZtxDOIGvj1yjleuo//iTerds4Oo6oAywqIsHBwZw5c4Ybbrj0ieb48eOXjWk4s4LicgA85EhEuJiiPw+ii4jALbge4eMfRO0uDROF/Vh0OmvChAn83//9H+vWrcNgMJCQkMC0adOYOHGirfNZzfbfzwFw643Wu/tdCEcyFhaSumQx599+k+xNFeONWn9pmCjsy6KP5cOGDSMgIIDVq1cTERFBfHw8U6dOpXfv3rbOZzU//lFxee/tt4Q7OIkQ16/g972kf7ISY2EhQQPiCBo4yNGRRB1lURExGo307t27VhWNfzIYTeQVVpzOctdpHJxGiOuT+8N3pK9agXujxtR//Ek8GjV2dCRRh1lURLp27UpsbCwDBw6kffv2ts5kdUeTcgHo26GhY4MIUUOKomAqLETj64tvx84oej0BPXuj0siHIuFYFo2JLFmyBC8vL5588kl69uzJm2++ydGjR22dzWqSM4oAiL451MFJhLh2+swMzr/1Bklvvo5iMKDx8iKwTz8pIMIpWHQk0qpVK1q1asUzzzzDnj17SEhIYOzYsYSEhLBhwwZbZ7xuF49E6tfzdmwQIa6BYjKR+/12Mr/8AlARMmy4NEwUTuear3dt2rQpzZo1IzIyktOnT9sgkvVpNRX3tsiNhqK2MOTlkrzoXUpPHMerdZuKhonBcmWhcD4Wvavm5+ezZcsWEhIS+OOPP+jatSsPPfQQvXr1snU+qzhxPp+IYJlTXdQeGm8fVG5uhD84Ed/Ot0vLEuG0LCoiMTExREVFERcXx8KFC/Hz87N1LqvKyi+V6XCF0ys9c7qiYeLE/0Pj5UWDJ5+R4iGcnkVF5NtvvyU0tHYOSpfpKyaiqufv4eAkQlTNVF5O1tfx5GzdjMbXF316GpomN0gBEbXCFYvI3r176dChA1AxWdSJEyeqXK5Lly62SWYlZ9Mq+ns1jaxdR0+ibig+dpS05UvRp6Xi1+0OQoaPQOMtF4CI2uOKReTFF180t26/Ust3lUrF9u3bbZPMSnYdqmiDfVPDAMcGEaIK2Zs2gtFI/SeexrvVLY6OI8Q1u2IRuVhAAL777ju7hLGFjNyKOURurC/de4VzKDqYiC4y8lLDRA8P1O7ujo4lRI1YdNH5pEmTqnz+scces2oYW1AU0GrUcn5ZOJyxsJCUjz/k/Dv/qTgCAbT+/lJARK1m0cD67t27q3x+z549Vg1jC8mZRTQMlXPMwnEURaHw972kf7IKY3ERQXGDCLproKNjCWEV1RaRd955BwC9Xm/++qKkpCQiIyNtl8xKcgrKCAmQ1tjCcfJ++J70T1bg3rgJDZ54GveG0sNNuI5qi0hqaipQ8Unq4tcXRUREMHnyZNsls4LScgMAGrWcyhL2pSgKxsICtL5++HbqjGI0EtCjp/S7Ei6n2iLy6quvAhAVFcW9995rl0DWVFJWcY+ITEQl7Kk8I530Fcsx5OfReOacioaJvfs4OpYQNnHFInLu3DkaNGgAVNwLkpSUVOVyDZ340PzijYbeHtIzS9ieYjKRu/1bMr9ah0qtpt6we6VhonB5V3x3HThwIPv37wegT58+qFQqFEWptIxKpeLw4cO2TXgdkjMrWsCr5cosYWOG3FySFy2g9ORJvNveSujoB3ALCnZ0LCFs7opF5GIBAThy5IhdwljbuYxCAOqHyNVZwrY0Pj6o3T0In/gvfDt2lkvKRZ1Ro2PtpKQkzp07Z+0sVndxQD08SDr4CusrPXWSc2+9gbG4CJVWS/0nnsavUxcpIKJOsaiIPPHEE+zbtw+AdevWcddddxEXF8fatWttGu56nb8wo6GbVs5LC+sxlZWRsXY1Z1+ZS9n5c+gzMgCkeIg6yaIR5507d/Laa68BsGzZMpYuXYqfnx+PPvoow4cPt2nA6+FxYRIq+eMW1lJ85DBpK5ahT0/D/447qTdsBBovOdIVdZdFRUSv16PT6UhLSyM3N5f27dsDkJmZadNw1+tcRiFBftJSQlhP9uZvQDHR4Mln8GrZytFxhHA4i4pIy5Yt+e9//8v58+fp3r07AGlpafj4OPdETzn5pRgMJkfHELVcYeIB3Os3qGiYOOEh1O7u0u9KiAssGix4+eWXOXbsGGVlZUydOhWouHpr4EDn7v+j0ajx8dI5OoaopQwF+aQs/oDkBW+T/c0mALR+flJAhPgHi45EGjVqxJtvvlnpudjYWGJjY20SylrSc0rocku4o2OIWkZRFAr27ibj008wlhQTPGgIQQPiHB1LCKdk8a3c69atY/369aSlpREWFsbgwYMZOnSoLbNdF5Op4sbIi3etC2GpvB++I/2TlXjc0JQG4ybgXr+BoyMJ4bQsKiLvv/8+8fHxTJgwgcjISJKTk/noo49IT0+/4lwjjqY3VoyFyLS4whKKyYSxsBCtnx++nbqAouDfvScqaVsiRLUsKiJr165l5cqV1K9f3/xct27dGD16tNMWEcOFIqLVyJuAqF55WhppK5ZiLCw0N0wM6Nnb0bGEqBUsKiIlJSUEBQVVei4gIIDS0lKbhLKGi9PiXiwmQvwvxWQi59stZK3/CpVGQ73hI0BatQtxTSwqIjExMTz11FM8+eSTREZGcv78ed5++226detm63w1lpNfBkCEtDwRVTDk5nL+3XcoO30K71vbETp6LG6BgY6OJUStY9G5nlmzZuHt7c2gQYOIiopiyJAheHp6MnPmTFvnq7G8onIA6smshqIKGh8fNF5ehD/8f0Q+NlUKiBA1dNUiUlBQwJkzZ5g1axaJiYns2LGDP/74g9dffx0/P8sHrU+dOsWIESPo168fI0aM4PTp01dc9uTJk9x6663Mnz/f4vX/r6z8ilNt/j5yn4ioUHLyJOf+82+MRRUNExs88TR+0nFXiOtSbRH54YcfiImJYejQodx5553s2bOH4OBg1DW4YmX27NmMHDmSLVu2MHLkSGbNmlXlckajkdmzZ9O79/UNbF6cQ8THw+261iNqP2NpKRmrPyPp1bmUpySjz8xwdCQhXEa11eCdd97hqaeeYv/+/UyZMoW33367RhvJysri0KFDxMVV3LAVFxfHoUOHyM7OvmzZDz/8kO7du9OkSZMabeuiU6n5aDUq1DK/ep1WfPgQB6Y+Qc63W/C/ozuNX3oFj8ZNHB1LCJdR7cB6UlISo0ePBmDUqFF88MEHNdpISkoKYWFhaC5c+aLRaAgNDSUlJaXSVV9Hjhxhx44drFixgkWLFtVoW8HBFf28PNzdMBgVQkJ8a7QeV1CXf/aLDr2/DVQqWr/8Ev6tb3F0HKcgvxeXyL64ftUWEZPp0uWxWq0Wo9F2d3/r9XpmzpzJq6++ai42NZGVVYjJpJCeVUQ9fw8yMgqsmLL2CAnxrbM/e+GB/RUNE0NCCBw9jhb165GdX15n98c/1eXfi/8l+6KCWq0yf/iuiWqLSGlpKaNGjTI/LioqqvQY4JNPPrnqRiIiIkhLS8NoNKLRaDAajaSnpxMREWFeJiMjg7Nnz/Lwww8DkJ+fj6IoFBYWMnfu3Gv6oQAy8koJC5Qrs+oSQ34+GZ+tomDvHvx79CRs1ANoff3QuLsD5Y6OJ4RLqraIvPzyy5UeDxs2rEYbCQ4OpmXLliQkJDB48GASEhJo2bJlpVNZkZGR7N692/x44cKFFBcX8+yzz17z9owmEyVlBvP0uMK1KYpCwe6dpH/+KUppKcFD7iEodoCjYwlRJ1RbRO6++26rbWjOnDlMnz6dRYsW4efnZ758d+LEiUyZMoU2bdpYbVtFJQYAbmoYYLV1CueV9/120j9dhUfTZoSNm4B7ZP2rv0gIYRUWd/G9Xs2aNatyTvbFixdXufzkyZNrvK2LrU7C5G51l6WYTBgLCtD6++Pb+XZQqfG/s7s0TBTCzuxWROzJeKENvJzOck3laamkLV+KsajoUsPEHj0dHUuIOsmli4jcI+JaFKPxUsNErZaQEfdLw0QhHMwli8jFDr5yJOI6DLk5nF/4DmVnTuPdLoqw0Q+gDZB+V0I4mkVFpLy8nPfee4+EhARyc3P5/fff2bFjB6dPnzbfjOhMUjKLAPCV+dVdhsbHF42PDxH/egSf6A7S70oIJ2HRKOQrr7zCsWPHeOONN8x/vM2bN+ezzz6zabiaujir4Q0RcjdqbVZy4jhJb8y/1DBx2lP4dugoBUQIJ2LRkci2bdvYunUrXl5e5uaLYWFhpKWl2TRcTaXlVJzO0mnlfHltZCorI/OrL8jdvg1tYBD6rEw03t6OjiWEqIJFRcTNze2ylifZ2dkEBATYItN189BVFA8ZWK99ig79RdqKpRgyM/Hv0YuQocNQe0jnASGclUWns2JjY3n22WdJSkoCID09nZdeeom77rrLpuFqKj2nBB9PaQFfG+Vu24pKo6XBM88RNmqMFBAhnJxFRWTatGk0aNCAQYMGkZ+fT79+/QgNDeXRRx+1db4aycovpdxgu2aRwroK9/+OPqNijo/w8Q/RePZLeN3UwsGphBCWsOh0lk6n4/nnn+f5558nOzubwMBApx7c1GrUBPi4OzqGuApDXh7pn62i8Le9+PfoRdioMWh85WIIIWoTi4rIxdNYFxUVFZm/btiwoXUTWcGZ1ALatwhxdAxxBYqiULDz14qGieVlBN89lKB+/R0dSwhRAxYVkT59+qBSqVAUxfzcxSORw4cP2yZZDRkvzIFSUKx3cBJxJbnfbyfj01V4NLuR8HET0EVEOjqSEKKGLCoiR44cqfQ4IyODd999l+joaJuEuh4Xi4d08HUuismEMT8fbUAAfl26otJo8I+5UxomClHL1egvOCQkhBkzZvCf//zH2nmuW8aFe0QCfORudWdRnppC0uuvcu4//0YxGNB4ehJwZw8pIEK4gBr3zjp58iQlJSXWzGIVhgvNFxuHywCtoykGAzlbN5P1dTwqnbs0TBTCBVlUREaOHFnpaqySkhKOHz/ulJf45hSUAXK3uqMZcnM4v+Btys6ewad9NKEjR6P1D3B0LCGElVlURIYPH17psaenJzfffDNNmjSxRabrcrFzr6e7FBFH0vj4ovX3J2jSo/i27+DoOEIIG7lqETEajezatYu5c+ei0zn/OMPFuUTkSMT+Sv7+m8z4dUQ+MhmNtzf1pz7h6EhCCBu7ahHRaDT88ssvTn1z4T+ZLs5qqKkdeV2BqbSUzC+/IPf77WiDpGGiEHWJRZfHjB07loULF6LXO/+9F5l5FYP9Wrnyxy6K/vqT07NnkPv9dgJ69qbJiy/j0aixo2MJIeyk2iORhIQE4uLiWLVqFZmZmSxdupSgoKBKRyU//PCDrTNeE7cLp7HcdXI6yx5yt3+L2k1Hw2eex7N5c0fHEULYWbVFZNasWcTFxfHvf//bXnmuW0p2ETqtHIXYUsHve3Fv1BhdSCjh4x9C5eGO2s35x8uEENZXbRG52OakY8eOdgljDZ46rcwjYiOG3FzSP11J4b7fpWGiEAK4ShExmUzs2rWrUs+s/9WlSxerh7oeaTnF1PP3cHQMl6IoCvm/7CBjzWco5eXUGzqcwL6xjo4lhHAC1RaR8vJyZsyYccUiolKp2L59u02C1ZSnu1aaL1pZ7nfbyPjsEzyb30TY2AnowsMdHUkI4SSqLSKenp5OVySuxqRAZD25vPR6VTRMzEMbEIjf7d1Q63T4dY2RfldCiEpq3DvLWSkmxXzXuqiZsuRk0pYvwVRSTONZL6Hx9MQ/5k5HxxJCOCGLBtZrE6NJkYH1GlIMBrI3byI74WtU7u6E3jdSGiYKIapVbRHZv3+/vXJYTV5hGX5ecrnptdLn5JC84D+UJSXhE92R0PtHofX3d3QsIYSTc7nTWYUletzd5NPztdL6+aENDCJ40BB8oto7Oo4QopZwyVHSAB93R0eoFYqPHSVp/isYCwtRaTTUnzJNCogQ4pq43JEIQP0QuTqrOsaSEjK/XEve99/hVi8EQ04OGh8fR8cSQtRCLllEyvVGR0dwWkUHE0lbuQxDTg4BffpRb8g9qN3lyE0IUTMuWUQahMin6ivJ/eE71B4eNJw+A89mNzo6jhCilrNbETl16hTTp08nNzeXgIAA5s+ff9nMiO+99x6bNm1CrVbj5ubGtGnTiImJueZt6dxccqinRhRFofC3vbg3aXKpYaK7O2o3N0dHE0K4ALu9286ePZuRI0eyZcsWRo4cyaxZsy5bpm3btnzxxRds2LCBV155hWnTplFaWnrN21LXkgm0bM2Qm0PyooWk/HcRud9uAUDj4yMFRAhhNXYpIllZWRw6dIi4uDgA4uLiOHToENnZ2ZWWi4mJwdPTE4AWLVqgKAq5ubnXvD0/77p9n4iiKKR9u43TM5+n+M+D1Bt2LyEjRjo6lhDCBdnldFZKSgphYWFoLtz9rNFoCA0NJSUlhaCgoCpfEx8fT6NGjQivQbM/L3eXHOqxWO72bWR8/gmeN7UgbOx4dGHSMFEIYRtO+W67Z88e3nnnHZYsWVKj1zdqEIi3Z906ZaMYjZTn5uIeHEzg4P74hwQQ2rO7NEy8ICRE5j25SPbFJbIvrp9dikhERARpaWkYjUY0Gg1Go5H09HQiIiIuW3b//v08/fTTLFq0iKZNm9ZoeznZRRTXoelxy86fJ235x5hKSmk8+yVUWi1hvXuSkVHg6GhOISTEV/bFBbIvLpF9UUGtVhEcXPMrWu3yMTU4OJiWLVuSkJAAVMzd3rJly8tOZSUmJjJt2jQWLFjALbfcUuPtudWR6XEVg4GsDes589Is9OkZBMUNkoaJQgi7stvprDlz5jB9+nQWLVqEn58f8+fPB2DixIlMmTKFNm3a8OKLL1JaWlrpyq3XX3+dFi1aWLwdD52mTnTx1efkcP7tNyk/fw7fjp0JuX8kWl8/R8cSQtQxdisizZo1Y+3atZc9v3jxYvPX69atu+7t1JXLe7V+friFhFDv7qH4tItydBwhRB1VN877uIjiI4c5+9rLlxomPjZVCogQwqGc8uqs61FcZnB0BKszFheTuW4NeT/+gFuINEwUQjgPlysiN0S61rhAYeIB0lcux5CbS2DfWIIH3y0NE4UQTsPliohO61pXJ+X9+ANqL28aTpqMZw0veRZCCFtxuSKiqeVXZimKQsGe3Xg0uQFdWBjh4x9C7eGBSuty/1VCCBcgA+tORJ+dTfLCt0ld/AG5320DKhomSgERQjgrl3t3qo0TUikmE3k//0jm2tUoJhMh995PQO8+jo4lhBBX5XJFJMiv9g065363jYzPP8Xz5paEPTAeXWiooyMJIYRFXK6I1JabDRWTCUNuDm5Bwfh3i0Hj5Y1vl9tR1ZL8QggBrlhEasHAetn5c6Qu/RiltJTGc+ai9vDE7/aujo4lhBDXzOWKiDN/klcMBrI2biB7UwIaLy9C7x8tDROFELWayxURk6I4OkKV9NnZFQ0Tk8/j27kLoSNGovGVuQyEELWbyxURNc51JKIoCiqVCq2/P7qwcOoNG45P23aOjiWEEFbhcveJ+Hk7z4yGxYcPkfTqPHPDxMhHJ0sBEUK4FNc7EnGC6WCNxUVkrF1N/s8/4RYWhiEvVxomijrDaDSQk5OBwVDu6CjVSk9XYzKZHB3DbrRaHYGBIWg01n3bd7kiojc49mbDwv37SFu1AmN+HoGxAwgeNAS1TufQTELYU05OBh4eXnh7hzv1hS5arRqDoW4UEUVRKCrKJycng3r1Lp+W/Hq4XBHx8XTsG3beLz+j8fWl/uSpeDS5waFZhHAEg6Hc6QtIXaNSqfD29qOwMNfq63a5IuLmZt9fXEVRKNi9E48bml1qmOjuLv2uRJ0mBcT52Or/xPEDCFZmzzvW9dlZJC94i9SPPrzUMNHbWwqIEKLOcLl3O40dBtYVk4m8H38gc92aioaJ940koGdvm29XCFEz+fn5DBnSn0GD7ubxx58yP//xx/+lpKSExx573PzcunWrOXLkMDNmzAHg7NkzvP/+Qo4f/xs/Pz90Ojfuv/8B7riju1WyLVv2EZs2bQBgwICBjBv3UJXLbdz4NWvWfIrRaCIysj4vvDAHPz9/TCYTkyY9SGlpKQDBwfV4+unniIiItEq+q3HBIxHbbyN3+7ekf7ICjxua0eTFlwns3ReVE1wVJoSo2rffbuaWW1qzbdsW9Hq9xa/LzMzkscce5s47e7B27Xo+/ngl8+a9TlFRoVVyHTiwj++/38bKlatZuXI133+/jQMH9l223OnTp1i8+H3efvt9Vq1aQ6tWt/Df/74HVFyR+uabC1m+/DOWL/+Mzp1vZ+HCt6ySzxIu985nNNnmjnXFaESflQWAf8ydhD/4MPWfeAq3kBCbbE8IYT0bN37N2LEP0qxZc37++UeLX/fll2uIimpPbOxd5ueCg+vRv3+cVXJt3/4t/frdhbu7B+7uHvTrdxfbt3972XInT56gefObCAwMBKBLl25s3brZ/H2ff9xCUFxcZNcegi53OivAx/qt4MuSzpK69GNM5WU0mTMPtYcHfl1ut/p2hHA1vxxMYUdiik3W3a1tBF3bXP1y1ePH/yY/P4/27TuQnZ3Fxo1f09PC08/Hjh2hY8fOFi1bUFDA5Mn/qvJ7N9zQlNmz5132fFpaKlFR7c2Pw8LC+eOPy49EbryxOYcPHyI5+TwREZF8++1mSkqKyc/Pw8/PH4CnnprCsWNH8ff356233rMoszW4XBGxZgU26fVkb/ya7G82ofHyJnTUGGmYKEQtk5CwntjYu1CpVNx5Zw/eeuvfZGSkExFx5cuQa3Ilk6+vL8uWfXq9cavUqFFjHn/8KWbPfg5QERNzJwCaf7wfvfHGAkwmEytXLmXZso956qnpNsnyv1yuiPh4WOdH0mdncf4/b1CemoLf7V0Jufd+uetciGvUtY1lRwu2otfr2bZtM25uOjZv3giAwWBg06YNPPjgRAICAklNrXyklJubS0BAxWmjm266mUOH/rJoWzU5EgkLC6+0/bS0VEJDw6tcR+/e/ejdux8Ahw79yVdffYG3d+X3JLVaTVzcYO677x4pIjWl1lzfMM+lhokB6OrXJ+S++/Fu3dZK6YQQ9vTzzz/SsGFj3n//Y/Nzf/6ZyLx5s3nwwYncdls0y5d/THp6GqGhYeTn5/Hdd98ybdozANxzz3DGjx/F1q2b6ds3FoCcnGx27fr1snGRmhyJ9OjRi3feeYOhQ4cDsGXLRh5//Okql83KyiQ4uB5lZWV8/PGH3H//6At5clCpVAQEBADw/ffbaNbsxmvKcT1crohoruN0VtFff5K1/kvqT56GxteXyEmPWTGZEMLeNm78mr59+1d6rnXrtphMJvbt+522baOYMuUJpk9/EpPJhKIoDB06gujojgDUqxfCu+9+yPvvL2Dx4vfx9PTA09OL0aPHWiXfbbdFc8cdPRg9egQAsbEDzGMkO3b8yI4dPzF9+kwAXnnlJdLSUtDr9fTq1Zdhw+4DIDs7i5dfnoPRaEBRFCIiIpk58yWr5LOESlGcdAKOGkpOzcPtGo9GjEVFZKz9nPwdP+MWFk7kI4/hXr+BjRLaR0iILxkZBY6O4RRkX1xij32RmnqG8PDGNt2GNdSl3lkXVfV/o1arCA6u+al6lzsScdNc25FIwb7fSf9kBcaCAoIGxBE0cBBqN2mYKIQQlnC5IoJKBddwbJW/8xe0fv7UnzINj8ZNbBZLCCFckcsVEbVKVe0UuYqikP/rL3jeeCO6sHDCxz+IWicNE4UQoiZc7o716uizMjn/9pukLf2I3O+/A0DjJQ0ThbA2FxtqdQm2+j+pE++eislE7g/fkbluLQAhI0cT0L2ng1MJ4Zq0Wh1FRfl4e/tJS3gncXFSKq3W+uO9daKI5G77low1n+F1S2vCHhiHW3A9R0cSwmUFBoaQk5NhkwmQrEmtrpvT41p9vVZfo5NQDAYMebm4BdfD/4470fj749uxk3wyEsLGNBqt1adgtQW59Ns67DYmcurUKUaMGEG/fv0YMWIEp0+fvmwZo9HIiy++SO/evenTpw9r166t0bZKz57h7CtzOffWGygGQ0XDxE6dpYAIIYSV2a2IzJ49m5EjR7JlyxZGjhzJrFmzLltmw4YNnD17lq1bt7J69WoWLlzIuXPnrmk7OZu/4ey8FzHk5lDv7mEyaC6EEDZkl3fYrKwsDh06xNKlSwGIi4tj7ty5ZGdnExQUZF5u06ZNDB8+HLVaTVBQEL1792bz5s089FDVM31VpfjgAYL79Sc4biBqT0+r/yy1iT3nFHB2si8ukX1xieyL698HdikiKSkphIWFmdsWazQaQkNDSUlJqVREUlJSiIy8NKVjREQEqamp17Sttq+9bJ3QLuB6Whm4GtkXl8i+uET2xfWrU/eJCCGEsC67FJGIiAjS0tIwGo1AxQB6eno6ERERly2XnJxsfpySkkJ4eNW99YUQQjieXYpIcHAwLVu2JCEhAYCEhARatmxZ6VQWQGxsLGvXrsVkMpGdnc22bdvo16+fPSIKIYSoAbu1gj9x4gTTp08nPz8fPz8/5s+fT9OmTZk4cSJTpkyhTZs2GI1GXnrpJX755RcAJk6cyIgRI+wRTwghRA243HwiQggh7EcG1oUQQtSYFBEhhBA1JkVECCFEjUkREUIIUWO1rojYs5Gjs7NkX7z33nvcddddDBw4kHvuuYeff/7Z/kHtwJJ9cdHJkye59dZbmT9/vv0C2pGl+2LTpk0MHDiQuLg4Bg4cSGZmpn2D2oEl+yIrK4uHH36YgQMH0r9/f+bMmYPBYLB/WBuaP38+PXv2pEWLFhw7dqzKZWr8vqnUMmPGjFHi4+MVRVGU+Ph4ZcyYMZct89VXXykTJkxQjEajkpWVpcTExChJSUn2jmpzluyLn376SSkuLlYURVEOHz6stG/fXikpKbFrTnuwZF8oiqIYDAZl9OjRyhNPPKG89tpr9oxoN5bsi8TERKV///5Kenq6oiiKkp+fr5SWlto1pz1Ysi/mzZtn/l0oLy9Xhg0bpmzcuNGuOW1t7969SnJystKjRw/l6NGjVS5T0/fNWnUkcrGRY1xcHFDRyPHQoUNkZ2dXWu5KjRxdiaX7IiYmBs8LjShbtGiBoijk5ubaO65NWbovAD788EO6d+9OkyZN7JzSPizdF8uWLWPChAmEhFRMUuTr64u7u7vd89qSpftCpVJRVFSEyWSivLwcvV5PWFiYIyLbTHR09GUdQv5XTd83a1URqa6R4/8ud72NHJ2dpfvin+Lj42nUqJHLtZKxdF8cOXKEHTt2MG7cOAektA9L98WJEydISkpi1KhR3H333SxatMjl5kW3dF888sgjnDp1im7dupn/tW/f3hGRHaqm75u1qoiImtuzZw/vvPMOb775pqOjOIRer2fmzJm8+OKL5jeVusxoNHL06FGWLl3KypUr+emnn1i/fr2jYznE5s2badGiBTt27OCnn37it99+c7kzF7ZUq4qINHK8xNJ9AbB//36efvpp3nvvPZo2bWrvqDZnyb7IyMjg7NmzPPzww/Ts2ZPly5ezZs0aZs6c6ajYNmHp70VkZCSxsbHodDp8fHzo1asXiYmJjohsM5bui1WrVjFo0CDUajW+vr707NmT3bt3OyKyQ9X0fbNWFRFp5HiJpfsiMTGRadOmsWDBAm655RZHRLU5S/ZFZGQku3fv5rvvvuO7775j7Nix3HvvvcydO9dRsW3C0t+LuLg4duzYgaIo6PV6du3axc033+yIyDZj6b5o0KABP/30EwDl5eXs3LmT5s2b2z2vo9X4fdOqlwDYwfHjx5Vhw4Ypffv2VYYNG6acOHFCURRFeeihh5TExERFUSquwJk1a5bSq1cvpVevXsrnn3/uyMg2Y8m+uOeee5ROnTopgwYNMv87cuSII2PbhCX74p8WLFjgsldnWbIvjEaj8sorryixsbHKgAEDlFdeeUUxGo2OjG0TluyLM2fOKOPGjVPi4uKU/v37K3PmzFH0er0jY1vd3LlzlZiYGKVly5bK7bffrgwYMEBRFOu8b0oDRiGEEDVWq05nCSGEcC5SRIQQQtSYFBEhhBA1JkVECCFEjUkREUIIUWNSREStNmbMGKfv0vz1118zYcKEK37/t99+c7n7mETdIUVEOI2ePXvStm1boqKizP/S0tLsnmPMmDG0adOGqKgoOnXqxGOPPUZ6enqN1zdo0CCWLFliftyiRQvOnDljfhwdHc2WLVuuK3NVFi5cyC233EJUVBTR0dHcd9997N+/3+LX/29OIaoiRUQ4lQ8++ID9+/eb/zmqm+qsWbPYv38/W7ZsIT8/n1dffdUhOa5X//792b9/P7t27aJTp05MnTrV0ZGEi5EiIpxaXl4e//rXv+jcuTMdOnTgX//61xU7i545c4bRo0fTvn17OnXqxOOPP27+3okTJxg/fjwdO3akX79+bNq0yaLtBwQE0K9fP/7++28A9u3bx9ChQ2nfvj1Dhw5l37595mW//PJLevXqRVRUFD179uTrr782P3///fcDMGrUKAAGDx5MVFQUmzZtYvfu3dxxxx1ARav6KVOmVMowb9485s2bB0BBQQHPP/883bp1IyYmhrfeesvcG6o6Wq2WgQMHkpaWZm6FnpiYyIgRI4iOjqZbt2689NJLlJeXXzEnwPfff8/gwYPNRzZHjhyxaD8KF2aju+yFuGY9evRQfvnll0rPZWdnK5s3b1aKi4uVgoICZfLkycqkSZPM3x89erSyZs0aRVEUZdq0acqiRYsUo9GolJaWKnv37lUURVGKioqUO+64Q/niiy8UvV6v/PXXX0rHjh2Vv//+u8oc/1xnVlaWMmbMGOWpp55ScnJylOjoaOWrr75S9Hq9smHDBiU6OlrJzs5WioqKlKioKHNbjbS0NOXYsWOKoijKunXrlPvuu8+8/ptuukk5ffq0+fGuXbuUmJgYRVEU5dy5c0rbtm2VgoICRVEqWlF07dpV2b9/v6IoivLII48oM2fOVIqKipTMzExl6NChymeffVblz7FgwQLlySefVBRFUcrKypR///vfSseOHc0tPQ4ePKjs379f0ev1SlJSkhIbG6ssXbr0ijn/+usvpXPnzsqBAwcUg8GgfPnll0qPHj2UsrKyKrcv6gY5EhFO5dFHHyU6Opro6GgeeeQRAgMD6devH56envj4+DBp0iT27t1b5Wu1Wi3Jycmkp6fj7u5OdHQ0AD/88AP169dn6NChaLVaWrVqRb9+/apt9z1v3jyio6MZPHgwISEhPPfcc/zwww80btyYIUOGoNVqiYuLo2nTpnz//fcAqNVq/v77b0pLSwkNDa1RE7/69evTqlUrtm3bBsCuXbvw8PCgXbt2ZGZm8uOPP/L888/j5eVFcHAw48aNY+PGjVdc3+bNm4mOjubWW29l7dq1LFiwAK1WC0Dr1q1p164dWq2WBg0aMGLEiCvuW4DVq1czYsQIbr31VjQaDXfffTdubm4cOHDgmn9O4Tq0jg4gxD+999573H777ebHJSUlvPrqq/z888/k5eUBUFRUhNFovGxekKeffpp33nmHYcOG4e/vz/jx4xk2bBjnz58nMTHRXFSgoi34oEGDrpjjhRdeYPjw4ZWeS09PrzRpD1R0B05LS8PLy4u33nqLJUuWMGPGDG677TaeffZZmjVrds37IC4ujoSEBIYMGUJCQoJ5Zr7k5GQMBgPdunUzL2symaqdsS42NpY33niD7OxspkyZwl9//UWnTp2AivnHX3vtNf78809KSkowGo3VdnpOTk4mPj6eVatWmZ/T6/XXddGBqP2kiAintmTJEk6dOsWaNWsICQnh8OHDDBkypMpZ+EJCQsxjB7/99hvjx4+nQ4cORERE0KFDB5YuXXpdWUJDQyvNtwAVcy7ExMQAFVMRx8TEUFpayttvv83MmTP59NNPr3k7/fv3Z/78+aSmpvLtt9+yevVqAMLDw9HpdOzatct8NGGpoKAgXnrpJYYOHUpcXByhoaHMmTOHVq1a8eabb+Lj48OyZcuqvUosIiKC//u//2PSpEnX/DMJ1yWns4RTKyoqwt3dHT8/P3Jzc3n33XevuOw333xjHnT39/dHpVKhVqvp3r07p0+fJj4+Hr1ej16vJzExkRMnTlxTljvvvJPTp0+zYcMGDAYDmzZt4vjx43Tv3p3MzEy2bdtGcXExOp0OLy8v1Oqq/7zq1atHUlLSFbcTFBREx44dee6552jQoIH5aCY0NJSuXbvy2muvUVhYiMlk4uzZs+zZs8ei/E2bNiUmJoaPPvoIqNi33t7eeHt7c+LECT777LNqcw4fPpzPP/+cP/74A0VRKC4u5ocffqCwsNCi7QvXJEVEOLWxY8dSVlZG586dGTFihPlTf1UOHjzI8OHDiYqKYtKkScyYMYOGDRvi4+PDxx9/zKZNm4iJiaFbt2688cYb5iuRLBUYGMgHH3zA0qVL6dSpEx999BEffPABQUFBmEwmli1bRkxMDB07dmTv3r3MmTOnyvU89thjTJ8+nejo6CteJRYXF8evv/5qPpV10euvv45er2fAgAF06NCBKVOmkJGRYfHP8OCDD7JmzRqysrJ49tlnSUhI4LbbbmPmzJkMGDCg2pxt2rRh7ty5vPTSS3To0IG+ffvy5ZdfWrxt4ZpkPhEhhBA1JkciQgghakyKiBBCiBqTIiKEEKLGpIgIIYSoMSkiQgghakyKiBBCiBqTIiKEEKLGpIgIIYSoMSkiQgghauz/AVDpimJdZzKGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('AUC-ROC')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "777f9f46",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pathpretrain 0.1\n",
      "Uninstalling pathpretrain-0.1:\n",
      "  Successfully uninstalled pathpretrain-0.1\n",
      "Collecting git+https://github.com/jlevy44/PathPretrain\n",
      "  Cloning https://github.com/jlevy44/PathPretrain to /scratch/pip-req-build-b9lj_yhd\n",
      "  Running command git clone --quiet https://github.com/jlevy44/PathPretrain /scratch/pip-req-build-b9lj_yhd\n",
      "  Resolved https://github.com/jlevy44/PathPretrain to commit 6b1172c055c45fb01d80ad02ed6d334247c24e1b\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tifffile==2021.11.2 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from pathpretrain==0.1) (2021.11.2)\n",
      "Requirement already satisfied: scikit-image==0.18.3 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from pathpretrain==0.1) (0.18.3)\n",
      "Requirement already satisfied: networkx>=2.0 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from scikit-image==0.18.3->pathpretrain==0.1) (2.8.5)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from scikit-image==0.18.3->pathpretrain==0.1) (3.5.2)\n",
      "Requirement already satisfied: scipy>=1.0.1 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from scikit-image==0.18.3->pathpretrain==0.1) (1.9.0)\n",
      "Requirement already satisfied: numpy>=1.16.5 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from scikit-image==0.18.3->pathpretrain==0.1) (1.23.1)\n",
      "Requirement already satisfied: imageio>=2.3.0 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from scikit-image==0.18.3->pathpretrain==0.1) (2.20.0)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from scikit-image==0.18.3->pathpretrain==0.1) (9.2.0)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from scikit-image==0.18.3->pathpretrain==0.1) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.18.3->pathpretrain==0.1) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.18.3->pathpretrain==0.1) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.18.3->pathpretrain==0.1) (4.34.4)\n",
      "Requirement already satisfied: cycler>=0.10 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.18.3->pathpretrain==0.1) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.18.3->pathpretrain==0.1) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.18.3->pathpretrain==0.1) (1.4.4)\n",
      "Requirement already satisfied: six>=1.5 in ./anaconda3/envs/hiss/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib!=3.0.0,>=2.0.0->scikit-image==0.18.3->pathpretrain==0.1) (1.16.0)\n",
      "Building wheels for collected packages: pathpretrain\n",
      "  Building wheel for pathpretrain (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathpretrain: filename=pathpretrain-0.1-py3-none-any.whl size=23572 sha256=d2ff3bbeb61701a35c06694e40ae34db795074ec3ccb5f5ae9c515d64885be18\n",
      "  Stored in directory: /scratch/pip-ephem-wheel-cache-xtr84xbr/wheels/95/0c/6f/7498ced330de24503d0b19e7997fa3290b64e695b46d04ed61\n",
      "Successfully built pathpretrain\n",
      "Installing collected packages: pathpretrain\n",
      "Successfully installed pathpretrain-0.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y pathpretrain\n",
    "!pip install git+https://github.com/jlevy44/PathPretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a59c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathpretrain.embed import generate_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97c314fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1291it [01:28, 14.63it/s]                                                          \n"
     ]
    }
   ],
   "source": [
    "generate_embeddings(patch_info_file=\"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/scc_tumor_data/prelim_patch_info_v2/109_A1c_ASAP_tumor_map.pkl\",\n",
    "                   image_file=\"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/scc_tumor_data/prelim_patch_info/109_A1c_ASAP_tumor_map.npy\",\n",
    "                   model_save_loc=\"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/cnn_model.pkl\",\n",
    "                   architecture='resnet50',\n",
    "                   num_classes=2,\n",
    "                   image_stack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e3b9a68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "embeddings=torch.load(\"/dartfs-hpc/rc/home/3/f006n33/cnn_embeddings/112_b_ASAP_tumor_map.pkl\")\n",
    "import umap\n",
    "z=umap.UMAP(random_state=42, n_components=2).fit_transform(embeddings['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3ac150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5debc85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2ad980a3e6a0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib, matplotlib.pyplot as plt\n",
    "plt.scatter(z[:,0],z[:,1],c=embeddings['patch_info']['scc'],s=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8df5ce02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2ad9817cc0a0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(embeddings['patch_info']['x'],embeddings['patch_info']['y'],c=z[:,1], s=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "103f122c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1291it [01:30, 14.30it/s]                                                       \n",
      "515it [00:31, 16.53it/s]                                                        \n",
      "532it [00:34, 15.37it/s]                                                        \n",
      "576it [00:37, 15.55it/s]                                                        \n",
      "100%|█████████████████████████████████████████| 558/558 [00:32<00:00, 17.02it/s]\n",
      "195it [00:11, 17.34it/s]                                                        \n",
      "214it [00:12, 17.29it/s]                                                        \n",
      "500it [00:31, 15.77it/s]                                                        \n",
      "654it [00:42, 15.36it/s]                                                        \n",
      "457it [00:26, 17.12it/s]                                                        \n",
      "462it [00:26, 17.17it/s]                                                        \n",
      "486it [00:27, 17.64it/s]                                                        \n",
      "668it [00:42, 15.62it/s]                                                        \n",
      "648it [00:39, 16.42it/s]                                                        \n",
      "598it [00:38, 15.60it/s]                                                        \n",
      "232it [00:17, 13.12it/s]                                                        \n",
      "416it [00:27, 15.17it/s]                                                        \n",
      "450it [00:29, 15.35it/s]                                                        \n",
      "421it [00:28, 14.78it/s]                                                        \n",
      "386it [00:22, 17.42it/s]                                                        \n",
      " 95%|██████████████████████████████████████▉  | 772/813 [00:44<00:02, 17.55it/s]\n",
      "100%|█████████████████████████████████████████| 510/510 [00:33<00:00, 15.24it/s]\n",
      "1204it [01:19, 15.09it/s]                                                       \n",
      "294it [00:18, 15.77it/s]                                                        \n",
      "212it [00:12, 16.55it/s]                                                        \n",
      "195it [00:13, 14.33it/s]                                                        \n",
      "214it [00:13, 16.00it/s]                                                        \n",
      "533it [00:33, 16.11it/s]                                                        \n",
      "699it [00:46, 15.05it/s]                                                        \n",
      "726it [00:42, 17.20it/s]                                                        \n",
      "100%|█████████████████████████████████████████| 745/745 [00:43<00:00, 17.29it/s]\n",
      "100%|█████████████████████████████████████████| 312/312 [00:19<00:00, 16.01it/s]\n",
      "324it [00:21, 14.97it/s]                                                        \n",
      "455it [00:29, 15.69it/s]                                                        \n",
      "463it [00:32, 14.42it/s]                                                        \n",
      "343it [00:21, 16.10it/s]                                                        \n",
      "353it [00:22, 15.64it/s]                                                        \n",
      "501it [00:31, 16.12it/s]                                                        \n",
      "607it [00:35, 17.15it/s]                                                        \n",
      "100%|█████████████████████████████████████████| 578/578 [00:36<00:00, 15.88it/s]\n",
      "527it [00:34, 15.08it/s]                                                        \n",
      "100%|█████████████████████████████████████████| 451/451 [00:29<00:00, 15.26it/s]\n",
      "322it [00:19, 16.49it/s]                                                        \n",
      "100%|█████████████████████████████████████████| 664/664 [00:41<00:00, 16.17it/s]\n",
      "647it [00:39, 16.25it/s]                                                        \n",
      "532it [00:32, 16.21it/s]                                                        \n",
      "463it [00:29, 15.84it/s]                                                        \n",
      "456it [00:26, 16.89it/s]                                                        \n",
      "100%|█████████████████████████████████████████| 558/558 [00:33<00:00, 16.84it/s]\n",
      "423it [00:24, 17.10it/s]                                                        \n",
      "491it [00:30, 16.04it/s]                                                        \n",
      "569it [00:38, 14.92it/s]                                                        \n",
      "414it [00:26, 15.55it/s]                                                        \n",
      "685it [00:43, 15.86it/s]                                                        \n",
      "803it [00:52, 15.25it/s]                                                        \n",
      "470it [00:26, 17.69it/s]                                                        \n",
      "264it [00:16, 15.79it/s]                                                        \n",
      "337it [00:19, 17.00it/s]                                                        \n",
      "500it [00:29, 16.87it/s]                                                        \n",
      "579it [00:35, 16.54it/s]                                                        \n",
      "524it [00:31, 16.87it/s]                                                        \n",
      "676it [00:47, 14.34it/s]                                                        \n",
      "613it [00:37, 16.30it/s]                                                        \n",
      "664it [00:39, 16.79it/s]                                                        \n",
      "588it [00:38, 15.31it/s]                                                        \n",
      "650it [00:42, 15.21it/s]                                                        \n",
      "644it [00:38, 16.88it/s]                                                        \n",
      "790it [00:51, 15.20it/s]                                                        \n",
      "734it [00:43, 16.71it/s]                                                        \n",
      "696it [00:43, 16.13it/s]                                                        \n",
      "707it [00:43, 16.23it/s]                                                        \n",
      "688it [00:45, 15.09it/s]                                                        \n",
      "611it [00:41, 14.84it/s]                                                        \n",
      "572it [00:35, 15.98it/s]                                                        \n",
      "433it [00:26, 16.41it/s]                                                        \n",
      "521it [00:32, 16.01it/s]                                                        \n",
      "532it [00:34, 15.63it/s]                                                        \n",
      "350it [00:21, 16.56it/s]                                                        \n",
      "907it [00:53, 16.85it/s]                                                        \n",
      "996it [01:07, 14.76it/s]                                                        \n",
      "1067it [01:09, 15.38it/s]                                                       \n",
      "455it [00:30, 15.09it/s]                                                        \n",
      "407it [00:24, 16.96it/s]                                                        \n",
      "447it [00:26, 16.56it/s]                                                        \n",
      "324it [00:18, 17.35it/s]                                                        \n",
      "278it [00:20, 13.61it/s]                                                        \n",
      "301it [00:19, 15.41it/s]                                                        \n",
      "100%|█████████████████████████████████████████| 578/578 [00:34<00:00, 16.65it/s]\n",
      "527it [00:30, 17.11it/s]                                                        \n",
      "100%|█████████████████████████████████████████| 451/451 [00:30<00:00, 14.84it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathpretrain.embed import generate_embeddings\n",
    "for ID in df:\n",
    "    generate_embeddings(patch_info_file=f\"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/scc_tumor_data/prelim_patch_info_v2/{ID}.pkl\",\n",
    "                        image_file=f\"/dartfs/rc/nosnapshots/V/VaickusL-nb/EDIT_Students/users/Sophie_Chen/scc_tumor_data/prelim_patch_info/{ID}.npy\",\n",
    "                        model_save_loc='cnn_model.pkl',\n",
    "                        architecture=\"resnet50\",\n",
    "                        num_classes=3,\n",
    "                        image_stack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a96e129",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
